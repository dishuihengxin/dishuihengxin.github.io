[{"title":"About Me","url":"https://www.dishuihengxin.com/en/about/","summary":"About Me Hello! I\u0026rsquo;m Dishui Hengxin, a technical expert focused on cloud-native architecture and DevOps practices.\nMy Technical Journey Starting as a database engineer, I gradually deepened my expertise in cloud-native architecture and DevOps. Years of hands-on experience have given me rich expertise in database optimization, container orchestration, and CI/CD pipeline design.\nCore Competencies Cloud Native Architecture: Expert in Kubernetes cluster management, skilled in microservices architecture design and service mesh implementation Database Technologies: Deep understanding of PostgreSQL/MySQL internals, capable of large-scale database optimization and high availability architecture design DevOps Practices: Proficient in CI/CD, Infrastructure as Code (IaC), and automated operations System Architecture: Experienced in large-scale distributed system design, focusing on high availability, performance, and scalability Technical Philosophy Continuous Learning: Technology evolves rapidly; staying current is key to competitiveness Practice First: Combining theory with practice, focusing on solving real-world problems Knowledge Sharing: Sharing experience through blogs and technical talks to help others grow Open Source Contribution: Actively participating in open source communities to give back to the tech ecosystem Professional Certifications I hold multiple industry-recognized professional certifications, including:\n","content":"About Me Hello! I\u0026amp;rsquo;m Dishui Hengxin, a technical expert focused on cloud-native architecture and DevOps practices.\nMy Technical Journey Starting as a database engineer, I gradually deepened my expertise in cloud-native architecture and DevOps. Years of hands-on experience have given me rich expertise in database optimization, container orchestration, and CI/CD pipeline design.\nCore Competencies Cloud Native Architecture: Expert in Kubernetes cluster management, skilled in microservices …","date":"2025-12-31","lastmod":"2025-12-31","tags":null,"categories":null,"author":"Author","readingTime":2,"wordCount":292,"section":"","type":"about","draft":false,"featured":false,"series":null},{"title":"Building Enterprise Observability Platform: Comprehensive Monitoring, Logging, and Tracing Solutions","url":"https://www.dishuihengxin.com/en/posts/devops-monitoring-observability/","summary":"Building Enterprise Observability Platform: Comprehensive Monitoring, Logging, and Tracing Solutions Modern distributed systems require comprehensive observability to ensure reliability, performance, and operational excellence. This guide explores building an enterprise-grade observability platform that provides deep insights into system behavior through metrics, logs, and traces. We\u0026rsquo;ll cover the implementation of a complete observability stack using industry-standard tools and best practices.\nObservability Architecture Overview Three Pillars of Observability graph TB subgraph \u0026#34;Applications \u0026amp; Infrastructure\u0026#34; A[Microservices] B[Kubernetes Cluster] C[Databases] D[Load Balancers] E[Message Queues] end subgraph \u0026#34;Data Collection\u0026#34; F[Prometheus Exporters] G[Fluentd/Fluent Bit] H[OpenTelemetry Collectors] I[Custom Agents] end subgraph \u0026#34;Storage \u0026amp; Processing\u0026#34; J[Prometheus TSDB] K[Elasticsearch] L[Jaeger Storage] M[ClickHouse] end subgraph \u0026#34;Visualization \u0026amp; Alerting\u0026#34; N[Grafana Dashboards] O[Kibana] P[Jaeger UI] Q[AlertManager] end A --\u0026gt; F B --\u0026gt; F C --\u0026gt; F D --\u0026gt; F E --\u0026gt; F A --\u0026gt; G B --\u0026gt; G C --\u0026gt; G A --\u0026gt; H B --\u0026gt; H F --\u0026gt; J G --\u0026gt; K H --\u0026gt; L J --\u0026gt; N K --\u0026gt; O L --\u0026gt; P J --\u0026gt; Q Technology Stack Selection # observability-stack.yml observability_stack: metrics: collection: - prometheus - node_exporter - kube-state-metrics - custom_exporters storage: - prometheus_tsdb - thanos (long-term storage) - victoria_metrics (alternative) visualization: - grafana - prometheus_ui alerting: - alertmanager - grafana_alerts logging: collection: - fluentd - fluent_bit - filebeat - vector processing: - logstash - fluentd_filters storage: - elasticsearch - opensearch - loki visualization: - kibana - grafana_logs tracing: collection: - opentelemetry_collector - jaeger_agent - zipkin storage: - jaeger_backend - tempo - zipkin_storage visualization: - jaeger_ui - grafana_tempo infrastructure: orchestration: kubernetes service_mesh: istio ingress: nginx_ingress storage: persistent_volumes networking: calico Metrics Collection and Monitoring Prometheus Configuration # prometheus/prometheus.yml global: scrape_interval: 15s evaluation_interval: 15s external_labels: cluster: \u0026#39;production\u0026#39; region: \u0026#39;us-west-2\u0026#39; rule_files: - \u0026#34;/etc/prometheus/rules/*.yml\u0026#34; alerting: alertmanagers: - static_configs: - targets: - alertmanager:9093 path_prefix: /alertmanager scheme: http scrape_configs: # Prometheus itself - job_name: \u0026#39;prometheus\u0026#39; static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] scrape_interval: 30s metrics_path: /metrics # Kubernetes API Server - job_name: \u0026#39;kubernetes-apiservers\u0026#39; kubernetes_sd_configs: - role: endpoints namespaces: names: - default scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https # Kubernetes Nodes - job_name: \u0026#39;kubernetes-nodes\u0026#39; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics # Kubernetes Pods - job_name: \u0026#39;kubernetes-pods\u0026#39; kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name # Node Exporter - job_name: \u0026#39;node-exporter\u0026#39; kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_endpoints_name] regex: \u0026#39;node-exporter\u0026#39; action: keep - source_labels: [__meta_kubernetes_endpoint_port_name] regex: \u0026#39;metrics\u0026#39; action: keep - source_labels: [__meta_kubernetes_endpoint_address_target_name] target_label: node - action: labelmap regex: __meta_kubernetes_node_label_(.+) # kube-state-metrics - job_name: \u0026#39;kube-state-metrics\u0026#39; static_configs: - targets: [\u0026#39;kube-state-metrics:8080\u0026#39;] scrape_interval: 30s # Application metrics - job_name: \u0026#39;application-metrics\u0026#39; kubernetes_sd_configs: - role: endpoints relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name # Custom exporters - job_name: \u0026#39;custom-exporters\u0026#39; file_sd_configs: - files: - \u0026#39;/etc/prometheus/targets/*.json\u0026#39; refresh_interval: 30s # Remote write configuration for long-term storage remote_write: - url: \u0026#34;http://thanos-receive:19291/api/v1/receive\u0026#34; queue_config: max_samples_per_send: 1000 max_shards: 200 capacity: 2500 metadata_config: send: true send_interval: 30s write_relabel_configs: - source_labels: [__name__] regex: \u0026#39;go_.*\u0026#39; action: drop # Remote read configuration remote_read: - url: \u0026#34;http://thanos-query:9090/api/v1/query\u0026#34; read_recent: true Custom Application Metrics // metrics/application_metrics.go package metrics import ( \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promauto\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; ) // Application metrics var ( // HTTP request metrics httpRequestsTotal = promauto.NewCounterVec( prometheus.CounterOpts{ Name: \u0026#34;http_requests_total\u0026#34;, Help: \u0026#34;Total number of HTTP requests\u0026#34;, }, []string{\u0026#34;method\u0026#34;, \u0026#34;endpoint\u0026#34;, \u0026#34;status_code\u0026#34;}, ) httpRequestDuration = promauto.NewHistogramVec( prometheus.HistogramOpts{ Name: \u0026#34;http_request_duration_seconds\u0026#34;, Help: \u0026#34;HTTP request duration in seconds\u0026#34;, Buckets: prometheus.DefBuckets, }, []string{\u0026#34;method\u0026#34;, \u0026#34;endpoint\u0026#34;}, ) // Business metrics activeUsers = promauto.NewGauge( prometheus.GaugeOpts{ Name: \u0026#34;active_users_total\u0026#34;, Help: \u0026#34;Number of currently active users\u0026#34;, }, ) orderProcessingTime = promauto.NewHistogramVec( prometheus.HistogramOpts{ Name: \u0026#34;order_processing_duration_seconds\u0026#34;, Help: \u0026#34;Time taken to process orders\u0026#34;, Buckets: []float64{0.1, 0.5, 1.0, 2.5, 5.0, 10.0}, }, []string{\u0026#34;order_type\u0026#34;, \u0026#34;payment_method\u0026#34;}, ) databaseConnections = promauto.NewGaugeVec( prometheus.GaugeOpts{ Name: \u0026#34;database_connections_active\u0026#34;, Help: \u0026#34;Number of active database connections\u0026#34;, }, []string{\u0026#34;database\u0026#34;, \u0026#34;pool\u0026#34;}, ) cacheHitRatio = promauto.NewGaugeVec( prometheus.GaugeOpts{ Name: \u0026#34;cache_hit_ratio\u0026#34;, Help: \u0026#34;Cache hit ratio\u0026#34;, }, []string{\u0026#34;cache_type\u0026#34;}, ) // Infrastructure metrics cpuUsage = promauto.NewGaugeVec( prometheus.GaugeOpts{ Name: \u0026#34;cpu_usage_percent\u0026#34;, Help: \u0026#34;CPU usage percentage\u0026#34;, }, []string{\u0026#34;core\u0026#34;}, ) memoryUsage = promauto.NewGauge( prometheus.GaugeOpts{ Name: \u0026#34;memory_usage_bytes\u0026#34;, Help: \u0026#34;Memory usage in bytes\u0026#34;, }, ) diskUsage = promauto.NewGaugeVec( prometheus.GaugeOpts{ Name: \u0026#34;disk_usage_percent\u0026#34;, Help: \u0026#34;Disk usage percentage\u0026#34;, }, []string{\u0026#34;mount_point\u0026#34;}, ) ) // MetricsMiddleware wraps HTTP handlers to collect metrics func MetricsMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { start := time.Now() // Wrap the ResponseWriter to capture status code wrapped := \u0026amp;responseWriter{ResponseWriter: w, statusCode: http.StatusOK} // Call the next handler next.ServeHTTP(wrapped, r) // Record metrics duration := time.Since(start).Seconds() httpRequestDuration.WithLabelValues(r.Method, r.URL.Path).Observe(duration) httpRequestsTotal.WithLabelValues( r.Method, r.URL.Path, http.StatusText(wrapped.statusCode), ).Inc() }) } type responseWriter struct { http.ResponseWriter statusCode int } func (rw *responseWriter) WriteHeader(code int) { rw.statusCode = code rw.ResponseWriter.WriteHeader(code) } // Business metric functions func IncrementActiveUsers() { activeUsers.Inc() } func DecrementActiveUsers() { activeUsers.Dec() } func RecordOrderProcessingTime(orderType, paymentMethod string, duration time.Duration) { orderProcessingTime.WithLabelValues(orderType, paymentMethod).Observe(duration.Seconds()) } func UpdateDatabaseConnections(database, pool string, count float64) { databaseConnections.WithLabelValues(database, pool).Set(count) } func UpdateCacheHitRatio(cacheType string, ratio float64) { cacheHitRatio.WithLabelValues(cacheType).Set(ratio) } // Infrastructure metric functions func UpdateCPUUsage(core string, usage float64) { cpuUsage.WithLabelValues(core).Set(usage) } func UpdateMemoryUsage(usage float64) { memoryUsage.Set(usage) } func UpdateDiskUsage(mountPoint string, usage float64) { diskUsage.WithLabelValues(mountPoint).Set(usage) } // Custom collector for complex metrics type CustomCollector struct { // Add fields for external data sources } func NewCustomCollector() *CustomCollector { return \u0026amp;CustomCollector{} } func (c *CustomCollector) Describe(ch chan\u0026lt;- *prometheus.Desc) { // Describe custom metrics } func (c *CustomCollector) Collect(ch chan\u0026lt;- prometheus.Metric) { // Collect custom metrics from external sources // Example: database queries, API calls, etc. } // Initialize metrics server func StartMetricsServer(port string) { // Register custom collector prometheus.MustRegister(NewCustomCollector()) // Create metrics endpoint http.Handle(\u0026#34;/metrics\u0026#34;, promhttp.Handler()) // Health check endpoint http.HandleFunc(\u0026#34;/health\u0026#34;, func(w http.ResponseWriter, r *http.Request) { w.WriteHeader(http.StatusOK) w.Write([]byte(\u0026#34;OK\u0026#34;)) }) // Start server http.ListenAndServe(\u0026#34;:\u0026#34;+port, nil) } Alerting Rules # prometheus/rules/application-alerts.yml groups: - name: application.rules rules: # High error rate - alert: HighErrorRate expr: | ( rate(http_requests_total{status_code=~\u0026#34;5..\u0026#34;}[5m]) / rate(http_requests_total[5m]) ) \u0026gt; 0.05 for: 5m labels: severity: critical team: platform annotations: summary: \u0026#34;High error rate detected\u0026#34; description: \u0026#34;Error rate is {{ $value | humanizePercentage }} for {{ $labels.endpoint }}\u0026#34; runbook_url: \u0026#34;https://runbooks.company.com/high-error-rate\u0026#34; # High response time - alert: HighResponseTime expr: | histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) \u0026gt; 2 for: 10m labels: severity: warning team: platform annotations: summary: \u0026#34;High response time detected\u0026#34; description: \u0026#34;95th percentile response time is {{ $value }}s for {{ $labels.endpoint }}\u0026#34; # Low cache hit ratio - alert: LowCacheHitRatio expr: cache_hit_ratio \u0026lt; 0.8 for: 15m labels: severity: warning team: platform annotations: summary: \u0026#34;Low cache hit ratio\u0026#34; description: \u0026#34;Cache hit ratio is {{ $value | humanizePercentage }} for {{ $labels.cache_type }}\u0026#34; # Database connection pool exhaustion - alert: DatabaseConnectionPoolExhaustion expr: database_connections_active / database_connections_max \u0026gt; 0.9 for: 5m labels: severity: critical team: database annotations: summary: \u0026#34;Database connection pool near exhaustion\u0026#34; description: \u0026#34;Database {{ $labels.database }} connection pool is {{ $value | humanizePercentage }} full\u0026#34; - name: infrastructure.rules rules: # High CPU usage - alert: HighCPUUsage expr: cpu_usage_percent \u0026gt; 80 for: 10m labels: severity: warning team: infrastructure annotations: summary: \u0026#34;High CPU usage\u0026#34; description: \u0026#34;CPU usage is {{ $value }}% on {{ $labels.instance }}\u0026#34; # High memory usage - alert: HighMemoryUsage expr: (memory_usage_bytes / memory_total_bytes) \u0026gt; 0.9 for: 5m labels: severity: critical team: infrastructure annotations: summary: \u0026#34;High memory usage\u0026#34; description: \u0026#34;Memory usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}\u0026#34; # High disk usage - alert: HighDiskUsage expr: disk_usage_percent \u0026gt; 85 for: 5m labels: severity: warning team: infrastructure annotations: summary: \u0026#34;High disk usage\u0026#34; description: \u0026#34;Disk usage is {{ $value }}% on {{ $labels.mount_point }}\u0026#34; # Pod restart frequency - alert: PodRestartingFrequently expr: rate(kube_pod_container_status_restarts_total[1h]) \u0026gt; 0.1 for: 15m labels: severity: warning team: platform annotations: summary: \u0026#34;Pod restarting frequently\u0026#34; description: \u0026#34;Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is restarting frequently\u0026#34; - name: kubernetes.rules rules: # Node not ready - alert: NodeNotReady expr: kube_node_status_condition{condition=\u0026#34;Ready\u0026#34;,status=\u0026#34;true\u0026#34;} == 0 for: 5m labels: severity: critical team: infrastructure annotations: summary: \u0026#34;Node not ready\u0026#34; description: \u0026#34;Node {{ $labels.node }} is not ready\u0026#34; # Pod not ready - alert: PodNotReady expr: kube_pod_status_ready{condition=\u0026#34;true\u0026#34;} == 0 for: 10m labels: severity: warning team: platform annotations: summary: \u0026#34;Pod not ready\u0026#34; description: \u0026#34;Pod {{ $labels.pod }} in namespace {{ $labels.namespace }} is not ready\u0026#34; # Deployment replica mismatch - alert: DeploymentReplicaMismatch expr: | kube_deployment_spec_replicas != kube_deployment_status_available_replicas for: 15m labels: severity: warning team: platform annotations: summary: \u0026#34;Deployment replica mismatch\u0026#34; description: \u0026#34;Deployment {{ $labels.deployment }} has {{ $value }} available replicas, expected {{ $labels.spec_replicas }}\u0026#34; Logging Infrastructure Fluentd Configuration # fluentd/fluent.conf \u0026lt;system\u0026gt; log_level info workers 4 \u0026lt;/system\u0026gt; # Input sources \u0026lt;source\u0026gt; @type tail @id kubernetes_logs path /var/log/containers/*.log pos_file /var/log/fluentd-containers.log.pos tag kubernetes.* read_from_head true \u0026lt;parse\u0026gt; @type json time_format %Y-%m-%dT%H:%M:%S.%NZ time_key time keep_time_key true \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;source\u0026gt; @type tail @id system_logs path /var/log/syslog pos_file /var/log/fluentd-syslog.log.pos tag system.syslog \u0026lt;parse\u0026gt; @type syslog \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;source\u0026gt; @type tail @id nginx_access_logs path /var/log/nginx/access.log pos_file /var/log/fluentd-nginx-access.log.pos tag nginx.access \u0026lt;parse\u0026gt; @type nginx \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;source\u0026gt; @type tail @id application_logs path /var/log/application/*.log pos_file /var/log/fluentd-application.log.pos tag application.* \u0026lt;parse\u0026gt; @type json time_format %Y-%m-%d %H:%M:%S time_key timestamp \u0026lt;/parse\u0026gt; \u0026lt;/source\u0026gt; # Filters for log processing \u0026lt;filter kubernetes.**\u0026gt; @type kubernetes_metadata @id kubernetes_metadata kubernetes_url \u0026#34;#{ENV[\u0026#39;KUBERNETES_SERVICE_HOST\u0026#39;]}:#{ENV[\u0026#39;KUBERNETES_SERVICE_PORT_HTTPS\u0026#39;]}\u0026#34; verify_ssl \u0026#34;#{ENV[\u0026#39;KUBERNETES_VERIFY_SSL\u0026#39;] || true}\u0026#34; ca_file \u0026#34;#{ENV[\u0026#39;KUBERNETES_CA_FILE\u0026#39;]}\u0026#34; skip_labels false skip_container_metadata false skip_master_url false skip_namespace_metadata false \u0026lt;/filter\u0026gt; \u0026lt;filter kubernetes.**\u0026gt; @type parser @id kubernetes_parser key_name log reserve_data true remove_key_name_field true \u0026lt;parse\u0026gt; @type multi_format \u0026lt;pattern\u0026gt; format json time_key timestamp time_format %Y-%m-%dT%H:%M:%S.%NZ \u0026lt;/pattern\u0026gt; \u0026lt;pattern\u0026gt; format regexp expression /^(?\u0026lt;timestamp\u0026gt;\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) \\[(?\u0026lt;level\u0026gt;\\w+)\\] (?\u0026lt;message\u0026gt;.*)/ time_key timestamp time_format %Y-%m-%d %H:%M:%S \u0026lt;/pattern\u0026gt; \u0026lt;pattern\u0026gt; format none \u0026lt;/pattern\u0026gt; \u0026lt;/parse\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;filter **\u0026gt; @type record_transformer @id add_metadata \u0026lt;record\u0026gt; hostname \u0026#34;#{Socket.gethostname}\u0026#34; environment \u0026#34;#{ENV[\u0026#39;ENVIRONMENT\u0026#39;] || \u0026#39;unknown\u0026#39;}\u0026#34; cluster \u0026#34;#{ENV[\u0026#39;CLUSTER_NAME\u0026#39;] || \u0026#39;unknown\u0026#39;}\u0026#34; region \u0026#34;#{ENV[\u0026#39;AWS_REGION\u0026#39;] || \u0026#39;unknown\u0026#39;}\u0026#34; \u0026lt;/record\u0026gt; \u0026lt;/filter\u0026gt; # Log enrichment and parsing \u0026lt;filter application.**\u0026gt; @type parser @id application_parser key_name message reserve_data true \u0026lt;parse\u0026gt; @type json \u0026lt;/parse\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;filter nginx.**\u0026gt; @type parser @id nginx_parser key_name message reserve_data true \u0026lt;parse\u0026gt; @type regexp expression /^(?\u0026lt;remote_addr\u0026gt;\\S+) - (?\u0026lt;remote_user\u0026gt;\\S+) \\[(?\u0026lt;time_local\u0026gt;[^\\]]+)\\] \u0026#34;(?\u0026lt;method\u0026gt;\\S+) (?\u0026lt;path\u0026gt;\\S+) (?\u0026lt;protocol\u0026gt;\\S+)\u0026#34; (?\u0026lt;status\u0026gt;\\d+) (?\u0026lt;body_bytes_sent\u0026gt;\\d+) \u0026#34;(?\u0026lt;http_referer\u0026gt;[^\u0026#34;]*)\u0026#34; \u0026#34;(?\u0026lt;http_user_agent\u0026gt;[^\u0026#34;]*)\u0026#34; \u0026#34;(?\u0026lt;http_x_forwarded_for\u0026gt;[^\u0026#34;]*)\u0026#34; (?\u0026lt;request_time\u0026gt;\\S+) (?\u0026lt;upstream_response_time\u0026gt;\\S+)/ time_key time_local time_format %d/%b/%Y:%H:%M:%S %z \u0026lt;/parse\u0026gt; \u0026lt;/filter\u0026gt; # Error detection and alerting \u0026lt;filter **\u0026gt; @type grep @id error_detection \u0026lt;regexp\u0026gt; key level pattern ^(ERROR|FATAL|CRITICAL)$ \u0026lt;/regexp\u0026gt; \u0026lt;or\u0026gt; \u0026lt;regexp\u0026gt; key status pattern ^[45]\\d{2}$ \u0026lt;/regexp\u0026gt; \u0026lt;/or\u0026gt; \u0026lt;/filter\u0026gt; # Sampling for high-volume logs \u0026lt;filter kubernetes.var.log.containers.high-volume-app-**\u0026gt; @type sampling @id high_volume_sampling sampling_rate 10 sample_unit tag \u0026lt;/filter\u0026gt; # Output configurations \u0026lt;match kubernetes.**\u0026gt; @type elasticsearch @id elasticsearch_kubernetes host \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_HOST\u0026#39;] || \u0026#39;elasticsearch\u0026#39;}\u0026#34; port \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PORT\u0026#39;] || \u0026#39;9200\u0026#39;}\u0026#34; scheme \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_SCHEME\u0026#39;] || \u0026#39;http\u0026#39;}\u0026#34; user \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_USER\u0026#39;]}\u0026#34; password \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PASSWORD\u0026#39;]}\u0026#34; index_name kubernetes-logs type_name _doc # Index lifecycle management template_name kubernetes-logs template_file /fluentd/etc/kubernetes-template.json # Buffer configuration \u0026lt;buffer\u0026gt; @type file path /var/log/fluentd-buffers/kubernetes.buffer flush_mode interval retry_type exponential_backoff flush_thread_count 2 flush_interval 5s retry_forever retry_max_interval 30 chunk_limit_size 2M queue_limit_length 8 overflow_action block \u0026lt;/buffer\u0026gt; # Request configuration request_timeout 60s reload_connections false reconnect_on_error true reload_on_failure true # Slow log threshold slow_flush_log_threshold 40.0 \u0026lt;/match\u0026gt; \u0026lt;match application.**\u0026gt; @type elasticsearch @id elasticsearch_application host \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_HOST\u0026#39;] || \u0026#39;elasticsearch\u0026#39;}\u0026#34; port \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PORT\u0026#39;] || \u0026#39;9200\u0026#39;}\u0026#34; scheme \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_SCHEME\u0026#39;] || \u0026#39;http\u0026#39;}\u0026#34; user \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_USER\u0026#39;]}\u0026#34; password \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PASSWORD\u0026#39;]}\u0026#34; index_name application-logs-%Y.%m.%d type_name _doc \u0026lt;buffer time\u0026gt; @type file path /var/log/fluentd-buffers/application.buffer timekey 1d timekey_wait 10m timekey_use_utc true flush_mode interval flush_interval 10s chunk_limit_size 5M \u0026lt;/buffer\u0026gt; \u0026lt;/match\u0026gt; \u0026lt;match nginx.**\u0026gt; @type elasticsearch @id elasticsearch_nginx host \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_HOST\u0026#39;] || \u0026#39;elasticsearch\u0026#39;}\u0026#34; port \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PORT\u0026#39;] || \u0026#39;9200\u0026#39;}\u0026#34; scheme \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_SCHEME\u0026#39;] || \u0026#39;http\u0026#39;}\u0026#34; user \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_USER\u0026#39;]}\u0026#34; password \u0026#34;#{ENV[\u0026#39;ELASTICSEARCH_PASSWORD\u0026#39;]}\u0026#34; index_name nginx-logs-%Y.%m.%d type_name _doc \u0026lt;buffer time\u0026gt; @type file path /var/log/fluentd-buffers/nginx.buffer timekey 1d timekey_wait 10m flush_mode interval flush_interval 5s \u0026lt;/buffer\u0026gt; \u0026lt;/match\u0026gt; # Backup to S3 for long-term storage \u0026lt;match **\u0026gt; @type copy \u0026lt;store\u0026gt; @type s3 @id s3_backup aws_key_id \u0026#34;#{ENV[\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;]}\u0026#34; aws_sec_key \u0026#34;#{ENV[\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;]}\u0026#34; s3_bucket \u0026#34;#{ENV[\u0026#39;S3_BACKUP_BUCKET\u0026#39;]}\u0026#34; s3_region \u0026#34;#{ENV[\u0026#39;AWS_REGION\u0026#39;]}\u0026#34; path logs/%Y/%m/%d/ s3_object_key_format %{path}%{time_slice}_%{index}.%{file_extension} \u0026lt;buffer time\u0026gt; @type file path /var/log/fluentd-buffers/s3.buffer timekey 3600 timekey_wait 10m chunk_limit_size 256m \u0026lt;/buffer\u0026gt; \u0026lt;format\u0026gt; @type json \u0026lt;/format\u0026gt; \u0026lt;/store\u0026gt; \u0026lt;/match\u0026gt; Elasticsearch Index Templates { \u0026#34;index_patterns\u0026#34;: [\u0026#34;kubernetes-logs-*\u0026#34;], \u0026#34;template\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 3, \u0026#34;number_of_replicas\u0026#34;: 1, \u0026#34;index.refresh_interval\u0026#34;: \u0026#34;30s\u0026#34;, \u0026#34;index.codec\u0026#34;: \u0026#34;best_compression\u0026#34;, \u0026#34;index.lifecycle.name\u0026#34;: \u0026#34;kubernetes-logs-policy\u0026#34;, \u0026#34;index.lifecycle.rollover_alias\u0026#34;: \u0026#34;kubernetes-logs\u0026#34; }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;@timestamp\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, \u0026#34;kubernetes\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;namespace_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;pod_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;container_name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;labels\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;dynamic\u0026#34;: true } } }, \u0026#34;level\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;message\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34; }, \u0026#34;hostname\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;environment\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;cluster\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;region\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } } } Distributed Tracing OpenTelemetry Configuration # otel-collector/config.yml receivers: otlp: protocols: grpc: endpoint: 0.0.0.0:4317 http: endpoint: 0.0.0.0:4318 jaeger: protocols: grpc: endpoint: 0.0.0.0:14250 thrift_http: endpoint: 0.0.0.0:14268 thrift_compact: endpoint: 0.0.0.0:6831 thrift_binary: endpoint: 0.0.0.0:6832 zipkin: endpoint: 0.0.0.0:9411 prometheus: config: scrape_configs: - job_name: \u0026#39;otel-collector\u0026#39; scrape_interval: 10s static_configs: - targets: [\u0026#39;0.0.0.0:8888\u0026#39;] processors: batch: timeout: 1s send_batch_size: 1024 send_batch_max_size: 2048 memory_limiter: limit_mib: 512 spike_limit_mib: 128 check_interval: 5s resource: attributes: - key: environment value: production action: upsert - key: cluster value: main-cluster action: upsert attributes: actions: - key: http.user_agent action: delete - key: http.request.header.authorization action: delete span: name: to_attributes: rules: - ^\\/api\\/v1\\/users\\/(?P\u0026lt;user_id\u0026gt;\\d+)$ from_attributes: [\u0026#34;http.method\u0026#34;, \u0026#34;http.route\u0026#34;] probabilistic_sampler: hash_seed: 22 sampling_percentage: 15.3 exporters: jaeger: endpoint: jaeger-collector:14250 tls: insecure: true zipkin: endpoint: \u0026#34;http://zipkin:9411/api/v2/spans\u0026#34; format: proto prometheus: endpoint: \u0026#34;0.0.0.0:8889\u0026#34; const_labels: cluster: main-cluster logging: loglevel: debug elasticsearch/traces: endpoints: [\u0026#34;http://elasticsearch:9200\u0026#34;] index: traces-%Y.%m.%d mapping: mode: ecs extensions: health_check: endpoint: 0.0.0.0:13133 pprof: endpoint: 0.0.0.0:1777 zpages: endpoint: 0.0.0.0:55679 service: extensions: [health_check, pprof, zpages] pipelines: traces: receivers: [otlp, jaeger, zipkin] processors: [memory_limiter, resource, attributes, span, probabilistic_sampler, batch] exporters: [jaeger, elasticsearch/traces, logging] metrics: receivers: [otlp, prometheus] processors: [memory_limiter, resource, batch] exporters: [prometheus, logging] logs: receivers: [otlp] processors: [memory_limiter, resource, batch] exporters: [logging] telemetry: logs: level: \u0026#34;debug\u0026#34; metrics: address: 0.0.0.0:8888 Application Tracing Implementation // tracing/tracer.go package tracing import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;time\u0026#34; \u0026#34;go.opentelemetry.io/otel\u0026#34; \u0026#34;go.opentelemetry.io/otel/attribute\u0026#34; \u0026#34;go.opentelemetry.io/otel/exporters/jaeger\u0026#34; \u0026#34;go.opentelemetry.io/otel/exporters/otlp/otlptrace\u0026#34; \u0026#34;go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp\u0026#34; \u0026#34;go.opentelemetry.io/otel/propagation\u0026#34; \u0026#34;go.opentelemetry.io/otel/sdk/resource\u0026#34; \u0026#34;go.opentelemetry.io/otel/sdk/trace\u0026#34; semconv \u0026#34;go.opentelemetry.io/otel/semconv/v1.12.0\u0026#34; oteltrace \u0026#34;go.opentelemetry.io/otel/trace\u0026#34; ) // TracingConfig holds tracing configuration type TracingConfig struct { ServiceName string ServiceVersion string Environment string JaegerEndpoint string OTLPEndpoint string SamplingRatio float64 } // InitTracing initializes OpenTelemetry tracing func InitTracing(config TracingConfig) (func(), error) { // Create resource res, err := resource.New(context.Background(), resource.WithAttributes( semconv.ServiceNameKey.String(config.ServiceName), semconv.ServiceVersionKey.String(config.ServiceVersion), semconv.DeploymentEnvironmentKey.String(config.Environment), ), ) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to create resource: %w\u0026#34;, err) } // Create exporters var exporters []trace.SpanExporter // Jaeger exporter if config.JaegerEndpoint != \u0026#34;\u0026#34; { jaegerExporter, err := jaeger.New( jaeger.WithCollectorEndpoint(jaeger.WithEndpoint(config.JaegerEndpoint)), ) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to create Jaeger exporter: %w\u0026#34;, err) } exporters = append(exporters, jaegerExporter) } // OTLP exporter if config.OTLPEndpoint != \u0026#34;\u0026#34; { otlpExporter, err := otlptracehttp.New(context.Background(), otlptracehttp.WithEndpoint(config.OTLPEndpoint), otlptracehttp.WithInsecure(), ) if err != nil { return nil, fmt.Errorf(\u0026#34;failed to create OTLP exporter: %w\u0026#34;, err) } exporters = append(exporters, otlpExporter) } // Create span processor var spanProcessors []trace.SpanProcessor for _, exporter := range exporters { spanProcessors = append(spanProcessors, trace.NewBatchSpanProcessor(exporter)) } // Create tracer provider tp := trace.NewTracerProvider( trace.WithResource(res), trace.WithSampler(trace.TraceIDRatioBased(config.SamplingRatio)), ) // Add span processors for _, processor := range spanProcessors { tp.RegisterSpanProcessor(processor) } // Set global tracer provider otel.SetTracerProvider(tp) // Set global propagator otel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator( propagation.TraceContext{}, propagation.Baggage{}, )) // Return cleanup function return func() { ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second) defer cancel() tp.Shutdown(ctx) }, nil } // TracingMiddleware creates HTTP middleware for tracing func TracingMiddleware(serviceName string) func(http.Handler) http.Handler { tracer := otel.Tracer(serviceName) return func(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { // Extract context from headers ctx := otel.GetTextMapPropagator().Extract(r.Context(), propagation.HeaderCarrier(r.Header)) // Start span ctx, span := tracer.Start(ctx, fmt.Sprintf(\u0026#34;%s %s\u0026#34;, r.Method, r.URL.Path), oteltrace.WithAttributes( semconv.HTTPMethodKey.String(r.Method), semconv.HTTPURLKey.String(r.URL.String()), semconv.HTTPSchemeKey.String(r.URL.Scheme), semconv.HTTPHostKey.String(r.Host), semconv.HTTPTargetKey.String(r.URL.Path), semconv.HTTPUserAgentKey.String(r.UserAgent()), semconv.HTTPClientIPKey.String(getClientIP(r)), ), oteltrace.WithSpanKind(oteltrace.SpanKindServer), ) defer span.End() // Wrap response writer to capture status code wrapped := \u0026amp;tracingResponseWriter{ResponseWriter: w, statusCode: http.StatusOK} // Add span context to request r = r.WithContext(ctx) // Call next handler next.ServeHTTP(wrapped, r) // Set span attributes span.SetAttributes( semconv.HTTPStatusCodeKey.Int(wrapped.statusCode), semconv.HTTPResponseSizeKey.Int64(wrapped.bytesWritten), ) // Set span status if wrapped.statusCode \u0026gt;= 400 { span.SetAttributes(attribute.Bool(\u0026#34;error\u0026#34;, true)) if wrapped.statusCode \u0026gt;= 500 { span.SetStatus(oteltrace.StatusError, http.StatusText(wrapped.statusCode)) } } }) } } type tracingResponseWriter struct { http.ResponseWriter statusCode int bytesWritten int64 } func (w *tracingResponseWriter) WriteHeader(statusCode int) { w.statusCode = statusCode w.ResponseWriter.WriteHeader(statusCode) } func (w *tracingResponseWriter) Write(data []byte) (int, error) { n, err := w.ResponseWriter.Write(data) w.bytesWritten += int64(n) return n, err } func getClientIP(r *http.Request) string { // Check X-Forwarded-For header if xff := r.Header.Get(\u0026#34;X-Forwarded-For\u0026#34;); xff != \u0026#34;\u0026#34; { return xff } // Check X-Real-IP header if xri := r.Header.Get(\u0026#34;X-Real-IP\u0026#34;); xri != \u0026#34;\u0026#34; { return xri } // Fall back to remote address return r.RemoteAddr } // Database tracing helper func TraceDBOperation(ctx context.Context, operation, query string) (context.Context, oteltrace.Span) { tracer := otel.Tracer(\u0026#34;database\u0026#34;) return tracer.Start(ctx, operation, oteltrace.WithAttributes( semconv.DBStatementKey.String(query), semconv.DBSystemKey.String(\u0026#34;postgresql\u0026#34;), ), oteltrace.WithSpanKind(oteltrace.SpanKindClient), ) } // HTTP client tracing helper func TraceHTTPClient(ctx context.Context, method, url string) (context.Context, oteltrace.Span) { tracer := otel.Tracer(\u0026#34;http-client\u0026#34;) return tracer.Start(ctx, fmt.Sprintf(\u0026#34;%s %s\u0026#34;, method, url), oteltrace.WithAttributes( semconv.HTTPMethodKey.String(method), semconv.HTTPURLKey.String(url), ), oteltrace.WithSpanKind(oteltrace.SpanKindClient), ) } // Custom span helper func StartSpan(ctx context.Context, name string, attrs ...attribute.KeyValue) (context.Context, oteltrace.Span) { tracer := otel.Tracer(\u0026#34;application\u0026#34;) return tracer.Start(ctx, name, oteltrace.WithAttributes(attrs...)) } // Add custom attributes to current span func AddSpanAttributes(ctx context.Context, attrs ...attribute.KeyValue) { span := oteltrace.SpanFromContext(ctx) span.SetAttributes(attrs...) } // Add span event func AddSpanEvent(ctx context.Context, name string, attrs ...attribute.KeyValue) { span := oteltrace.SpanFromContext(ctx) span.AddEvent(name, oteltrace.WithAttributes(attrs...)) } // Record span error func RecordSpanError(ctx context.Context, err error) { span := oteltrace.SpanFromContext(ctx) span.RecordError(err) span.SetStatus(oteltrace.StatusError, err.Error()) } 现在我将更新todo列表，标记英文运维博文的进度：\n","content":"Building Enterprise Observability Platform: Comprehensive Monitoring, Logging, and Tracing Solutions Modern distributed systems require comprehensive observability to ensure reliability, performance, and operational excellence. This guide explores building an enterprise-grade observability platform that provides deep insights into system behavior through metrics, logs, and traces. We\u0026amp;rsquo;ll cover the implementation of a complete observability stack using industry-standard tools and best …","date":"2025-12-31","lastmod":"2025-12-31","tags":["Observability","Monitoring","Prometheus","Grafana","ELK","Jaeger","OpenTelemetry","Logging","Tracing","Metrics"],"categories":["DevOps"],"author":"Platform Engineering Team","readingTime":14,"wordCount":2804,"section":"posts","type":"posts","draft":false,"featured":false,"series":["Observability Platform"]},{"title":"Disaster Recovery and Business Continuity for Cloud-Native Applications: A Comprehensive Strategy","url":"https://www.dishuihengxin.com/en/posts/devops-disaster-recovery/","summary":"Disaster Recovery and Business Continuity for Cloud-Native Applications: A Comprehensive Strategy In today\u0026rsquo;s digital landscape, ensuring business continuity and implementing robust disaster recovery (DR) strategies is critical for maintaining operational resilience. This comprehensive guide explores how to design, implement, and maintain disaster recovery solutions for cloud-native applications, covering everything from backup strategies to automated failover mechanisms.\nUnderstanding Disaster Recovery Fundamentals Key Metrics and Objectives Before implementing any DR strategy, it\u0026rsquo;s essential to understand the key metrics that define your recovery requirements:\n","content":"Disaster Recovery and Business Continuity for Cloud-Native Applications: A Comprehensive Strategy In today\u0026amp;rsquo;s digital landscape, ensuring business continuity and implementing robust disaster recovery (DR) strategies is critical for maintaining operational resilience. This comprehensive guide explores how to design, implement, and maintain disaster recovery solutions for cloud-native applications, covering everything from backup strategies to automated failover mechanisms.\nUnderstanding …","date":"2025-12-31","lastmod":"2025-12-31","tags":["Disaster Recovery","Business Continuity","Backup","Multi-Region","High Availability","Kubernetes","Cloud Architecture"],"categories":["DevOps"],"author":"Infrastructure Reliability Team","readingTime":19,"wordCount":3855,"section":"posts","type":"posts","draft":false,"featured":false,"series":["Enterprise Reliability"]},{"title":"GitOps Workflow for Kubernetes: Implementing Continuous Delivery with Flux and ArgoCD","url":"https://www.dishuihengxin.com/en/posts/devops-gitops-workflow/","summary":"GitOps Workflow for Kubernetes: Implementing Continuous Delivery with Flux and ArgoCD GitOps has emerged as a powerful paradigm for managing Kubernetes infrastructure and applications. By using Git as the single source of truth for declarative infrastructure and applications, GitOps enables teams to increase deployment velocity, improve system reliability, and enhance security posture. This article provides a comprehensive guide to implementing GitOps workflows in enterprise environments using popular tools like Flux and ArgoCD.\n","content":"GitOps Workflow for Kubernetes: Implementing Continuous Delivery with Flux and ArgoCD GitOps has emerged as a powerful paradigm for managing Kubernetes infrastructure and applications. By using Git as the single source of truth for declarative infrastructure and applications, GitOps enables teams to increase deployment velocity, improve system reliability, and enhance security posture. This article provides a comprehensive guide to implementing GitOps workflows in enterprise environments using …","date":"2025-12-31","lastmod":"2025-12-31","tags":["GitOps","Kubernetes","Flux","ArgoCD","CI/CD","Infrastructure as Code","Continuous Delivery"],"categories":["DevOps"],"author":"DevOps Engineering Team","readingTime":10,"wordCount":2055,"section":"posts","type":"posts","draft":false,"featured":false,"series":["Kubernetes Best Practices"]},{"title":"Infrastructure Automation with Terraform: Enterprise-Scale Cloud Resource Management","url":"https://www.dishuihengxin.com/en/posts/devops-automation-terraform/","summary":"Infrastructure Automation with Terraform: Enterprise-Scale Cloud Resource Management Infrastructure as Code (IaC) has become essential for modern cloud operations, enabling teams to manage infrastructure with the same rigor and practices used for application code. Terraform, as a leading IaC tool, provides a declarative approach to infrastructure management across multiple cloud providers. This comprehensive guide explores enterprise-scale Terraform implementations, advanced patterns, and best practices for production environments.\nTerraform Enterprise Architecture Project Structure and Organization # Directory structure for enterprise Terraform projects terraform-infrastructure/ ├── environments/ │ ├── dev/ │ │ ├── main.tf │ │ ├── variables.tf │ │ ├── outputs.tf │ │ └── terraform.tfvars │ ├── staging/ │ └── production/ ├── modules/ │ ├── vpc/ │ ├── eks/ │ ├── rds/ │ ├── security-groups/ │ └── iam/ ├── shared/ │ ├── backend.tf │ ├── providers.tf │ └── versions.tf ├── policies/ │ ├── sentinel/ │ └── opa/ └── scripts/ ├── deploy.sh ├── plan.sh └── destroy.sh Backend Configuration with State Management # shared/backend.tf terraform { required_version = \u0026#34;\u0026gt;= 1.5.0\u0026#34; backend \u0026#34;s3\u0026#34; { bucket = \u0026#34;terraform-state-company-prod\u0026#34; key = \u0026#34;infrastructure/terraform.tfstate\u0026#34; region = \u0026#34;us-west-2\u0026#34; encrypt = true dynamodb_table = \u0026#34;terraform-state-lock\u0026#34; # Workspace-specific state files workspace_key_prefix = \u0026#34;workspaces\u0026#34; # Additional security kms_key_id = \u0026#34;arn:aws:kms:us-west-2:123456789012:key/12345678-1234-1234-1234-123456789012\u0026#34; # State file versioning versioning = true # Server-side encryption server_side_encryption_configuration { rule { apply_server_side_encryption_by_default { kms_master_key_id = \u0026#34;arn:aws:kms:us-west-2:123456789012:key/12345678-1234-1234-1234-123456789012\u0026#34; sse_algorithm = \u0026#34;aws:kms\u0026#34; } } } } } # State locking with DynamoDB resource \u0026#34;aws_dynamodb_table\u0026#34; \u0026#34;terraform_state_lock\u0026#34; { name = \u0026#34;terraform-state-lock\u0026#34; billing_mode = \u0026#34;PAY_PER_REQUEST\u0026#34; hash_key = \u0026#34;LockID\u0026#34; attribute { name = \u0026#34;LockID\u0026#34; type = \u0026#34;S\u0026#34; } server_side_encryption { enabled = true kms_key_arn = aws_kms_key.terraform_state.arn } point_in_time_recovery { enabled = true } tags = { Name = \u0026#34;terraform-state-lock\u0026#34; Environment = \u0026#34;shared\u0026#34; Purpose = \u0026#34;terraform-state-locking\u0026#34; } } # KMS key for state encryption resource \u0026#34;aws_kms_key\u0026#34; \u0026#34;terraform_state\u0026#34; { description = \u0026#34;KMS key for Terraform state encryption\u0026#34; deletion_window_in_days = 7 enable_key_rotation = true policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { Sid = \u0026#34;Enable IAM User Permissions\u0026#34; Effect = \u0026#34;Allow\u0026#34; Principal = { AWS = \u0026#34;arn:aws:iam::123456789012:root\u0026#34; } Action = \u0026#34;kms:*\u0026#34; Resource = \u0026#34;*\u0026#34; }, { Sid = \u0026#34;Allow Terraform Service Role\u0026#34; Effect = \u0026#34;Allow\u0026#34; Principal = { AWS = [ \u0026#34;arn:aws:iam::123456789012:role/TerraformExecutionRole\u0026#34;, \u0026#34;arn:aws:iam::123456789012:role/TerraformPlanRole\u0026#34; ] } Action = [ \u0026#34;kms:Encrypt\u0026#34;, \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:ReEncrypt*\u0026#34;, \u0026#34;kms:GenerateDataKey*\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34; ] Resource = \u0026#34;*\u0026#34; } ] }) tags = { Name = \u0026#34;terraform-state-key\u0026#34; Environment = \u0026#34;shared\u0026#34; Purpose = \u0026#34;terraform-state-encryption\u0026#34; } } resource \u0026#34;aws_kms_alias\u0026#34; \u0026#34;terraform_state\u0026#34; { name = \u0026#34;alias/terraform-state\u0026#34; target_key_id = aws_kms_key.terraform_state.key_id } Provider Configuration # shared/providers.tf terraform { required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } azurerm = { source = \u0026#34;hashicorp/azurerm\u0026#34; version = \u0026#34;~\u0026gt; 3.0\u0026#34; } google = { source = \u0026#34;hashicorp/google\u0026#34; version = \u0026#34;~\u0026gt; 4.0\u0026#34; } kubernetes = { source = \u0026#34;hashicorp/kubernetes\u0026#34; version = \u0026#34;~\u0026gt; 2.0\u0026#34; } helm = { source = \u0026#34;hashicorp/helm\u0026#34; version = \u0026#34;~\u0026gt; 2.0\u0026#34; } random = { source = \u0026#34;hashicorp/random\u0026#34; version = \u0026#34;~\u0026gt; 3.0\u0026#34; } tls = { source = \u0026#34;hashicorp/tls\u0026#34; version = \u0026#34;~\u0026gt; 4.0\u0026#34; } } } # AWS Provider Configuration provider \u0026#34;aws\u0026#34; { region = var.aws_region # Assume role for cross-account access assume_role { role_arn = var.aws_assume_role_arn session_name = \u0026#34;terraform-${var.environment}\u0026#34; external_id = var.aws_external_id } # Default tags for all resources default_tags { tags = { Environment = var.environment Project = var.project_name ManagedBy = \u0026#34;terraform\u0026#34; Owner = var.team_name CostCenter = var.cost_center CreatedDate = formatdate(\u0026#34;YYYY-MM-DD\u0026#34;, timestamp()) } } # Retry configuration retry_mode = \u0026#34;adaptive\u0026#34; max_retries = 3 # Request timeout http_timeout = \u0026#34;30s\u0026#34; } # Azure Provider Configuration provider \u0026#34;azurerm\u0026#34; { features { key_vault { purge_soft_delete_on_destroy = true recover_soft_deleted_key_vaults = true } resource_group { prevent_deletion_if_contains_resources = false } virtual_machine { delete_os_disk_on_deletion = true graceful_shutdown = false skip_shutdown_and_force_delete = false } } # Service Principal authentication client_id = var.azure_client_id client_secret = var.azure_client_secret tenant_id = var.azure_tenant_id subscription_id = var.azure_subscription_id # Skip provider registration skip_provider_registration = true } # Google Cloud Provider Configuration provider \u0026#34;google\u0026#34; { project = var.gcp_project_id region = var.gcp_region zone = var.gcp_zone # Service account key credentials = var.gcp_credentials_file # Request timeout request_timeout = \u0026#34;60s\u0026#34; # Batching configuration batching { send_after = \u0026#34;10s\u0026#34; enable_batching = true } } # Kubernetes Provider Configuration provider \u0026#34;kubernetes\u0026#34; { host = data.aws_eks_cluster.cluster.endpoint cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority[0].data) token = data.aws_eks_cluster_auth.cluster.token # Alternative: using kubeconfig # config_path = \u0026#34;~/.kube/config\u0026#34; # config_context = \u0026#34;production-cluster\u0026#34; } # Helm Provider Configuration provider \u0026#34;helm\u0026#34; { kubernetes { host = data.aws_eks_cluster.cluster.endpoint cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority[0].data) token = data.aws_eks_cluster_auth.cluster.token } # Helm repository cache repository_cache = \u0026#34;/tmp/.helmcache\u0026#34; repository_config_path = \u0026#34;/tmp/.helmrc\u0026#34; # Debug mode debug = var.helm_debug } Advanced Terraform Modules VPC Module with Advanced Networking # modules/vpc/main.tf locals { # Calculate subnet CIDRs automatically public_subnet_cidrs = [ for i in range(var.public_subnet_count) : cidrsubnet(var.vpc_cidr, 8, i) ] private_subnet_cidrs = [ for i in range(var.private_subnet_count) : cidrsubnet(var.vpc_cidr, 8, i + var.public_subnet_count) ] database_subnet_cidrs = [ for i in range(var.database_subnet_count) : cidrsubnet(var.vpc_cidr, 8, i + var.public_subnet_count + var.private_subnet_count) ] # Availability zones azs = slice(data.aws_availability_zones.available.names, 0, max( var.public_subnet_count, var.private_subnet_count, var.database_subnet_count )) } # Data sources data \u0026#34;aws_availability_zones\u0026#34; \u0026#34;available\u0026#34; { state = \u0026#34;available\u0026#34; filter { name = \u0026#34;opt-in-status\u0026#34; values = [\u0026#34;opt-in-not-required\u0026#34;] } } data \u0026#34;aws_region\u0026#34; \u0026#34;current\u0026#34; {} # VPC resource \u0026#34;aws_vpc\u0026#34; \u0026#34;main\u0026#34; { cidr_block = var.vpc_cidr enable_dns_hostnames = var.enable_dns_hostnames enable_dns_support = var.enable_dns_support # IPv6 support assign_generated_ipv6_cidr_block = var.enable_ipv6 tags = merge(var.tags, { Name = \u0026#34;${var.name}-vpc\u0026#34; Type = \u0026#34;vpc\u0026#34; }) } # Internet Gateway resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;main\u0026#34; { count = var.create_igw ? 1 : 0 vpc_id = aws_vpc.main.id tags = merge(var.tags, { Name = \u0026#34;${var.name}-igw\u0026#34; Type = \u0026#34;internet-gateway\u0026#34; }) } # Egress-only Internet Gateway for IPv6 resource \u0026#34;aws_egress_only_internet_gateway\u0026#34; \u0026#34;main\u0026#34; { count = var.enable_ipv6 \u0026amp;\u0026amp; var.create_egress_only_igw ? 1 : 0 vpc_id = aws_vpc.main.id tags = merge(var.tags, { Name = \u0026#34;${var.name}-eigw\u0026#34; Type = \u0026#34;egress-only-internet-gateway\u0026#34; }) } # Public Subnets resource \u0026#34;aws_subnet\u0026#34; \u0026#34;public\u0026#34; { count = var.public_subnet_count vpc_id = aws_vpc.main.id cidr_block = local.public_subnet_cidrs[count.index] availability_zone = local.azs[count.index] map_public_ip_on_launch = var.map_public_ip_on_launch # IPv6 support ipv6_cidr_block = var.enable_ipv6 ? cidrsubnet(aws_vpc.main.ipv6_cidr_block, 8, count.index) : null assign_ipv6_address_on_creation = var.enable_ipv6 ? var.assign_ipv6_address_on_creation : false tags = merge(var.tags, { Name = \u0026#34;${var.name}-public-${local.azs[count.index]}\u0026#34; Type = \u0026#34;public\u0026#34; Tier = \u0026#34;public\u0026#34; \u0026#34;kubernetes.io/role/elb\u0026#34; = \u0026#34;1\u0026#34; }) } # Private Subnets resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private\u0026#34; { count = var.private_subnet_count vpc_id = aws_vpc.main.id cidr_block = local.private_subnet_cidrs[count.index] availability_zone = local.azs[count.index] # IPv6 support ipv6_cidr_block = var.enable_ipv6 ? cidrsubnet(aws_vpc.main.ipv6_cidr_block, 8, count.index + var.public_subnet_count) : null assign_ipv6_address_on_creation = var.enable_ipv6 ? var.assign_ipv6_address_on_creation : false tags = merge(var.tags, { Name = \u0026#34;${var.name}-private-${local.azs[count.index]}\u0026#34; Type = \u0026#34;private\u0026#34; Tier = \u0026#34;private\u0026#34; \u0026#34;kubernetes.io/role/internal-elb\u0026#34; = \u0026#34;1\u0026#34; }) } # Database Subnets resource \u0026#34;aws_subnet\u0026#34; \u0026#34;database\u0026#34; { count = var.database_subnet_count vpc_id = aws_vpc.main.id cidr_block = local.database_subnet_cidrs[count.index] availability_zone = local.azs[count.index] tags = merge(var.tags, { Name = \u0026#34;${var.name}-database-${local.azs[count.index]}\u0026#34; Type = \u0026#34;database\u0026#34; Tier = \u0026#34;database\u0026#34; }) } # Elastic IPs for NAT Gateways resource \u0026#34;aws_eip\u0026#34; \u0026#34;nat\u0026#34; { count = var.enable_nat_gateway ? (var.single_nat_gateway ? 1 : var.private_subnet_count) : 0 domain = \u0026#34;vpc\u0026#34; depends_on = [aws_internet_gateway.main] tags = merge(var.tags, { Name = \u0026#34;${var.name}-nat-eip-${count.index + 1}\u0026#34; Type = \u0026#34;nat-gateway-eip\u0026#34; }) } # NAT Gateways resource \u0026#34;aws_nat_gateway\u0026#34; \u0026#34;main\u0026#34; { count = var.enable_nat_gateway ? (var.single_nat_gateway ? 1 : var.private_subnet_count) : 0 allocation_id = aws_eip.nat[count.index].id subnet_id = aws_subnet.public[var.single_nat_gateway ? 0 : count.index].id depends_on = [aws_internet_gateway.main] tags = merge(var.tags, { Name = \u0026#34;${var.name}-nat-gateway-${count.index + 1}\u0026#34; Type = \u0026#34;nat-gateway\u0026#34; }) } # Route Tables resource \u0026#34;aws_route_table\u0026#34; \u0026#34;public\u0026#34; { count = var.public_subnet_count \u0026gt; 0 ? 1 : 0 vpc_id = aws_vpc.main.id tags = merge(var.tags, { Name = \u0026#34;${var.name}-public-rt\u0026#34; Type = \u0026#34;public-route-table\u0026#34; }) } resource \u0026#34;aws_route_table\u0026#34; \u0026#34;private\u0026#34; { count = var.private_subnet_count vpc_id = aws_vpc.main.id tags = merge(var.tags, { Name = \u0026#34;${var.name}-private-rt-${count.index + 1}\u0026#34; Type = \u0026#34;private-route-table\u0026#34; }) } resource \u0026#34;aws_route_table\u0026#34; \u0026#34;database\u0026#34; { count = var.database_subnet_count \u0026gt; 0 \u0026amp;\u0026amp; var.create_database_subnet_route_table ? 1 : 0 vpc_id = aws_vpc.main.id tags = merge(var.tags, { Name = \u0026#34;${var.name}-database-rt\u0026#34; Type = \u0026#34;database-route-table\u0026#34; }) } # Routes resource \u0026#34;aws_route\u0026#34; \u0026#34;public_internet_gateway\u0026#34; { count = var.create_igw \u0026amp;\u0026amp; var.public_subnet_count \u0026gt; 0 ? 1 : 0 route_table_id = aws_route_table.public[0].id destination_cidr_block = \u0026#34;0.0.0.0/0\u0026#34; gateway_id = aws_internet_gateway.main[0].id timeouts { create = \u0026#34;5m\u0026#34; } } resource \u0026#34;aws_route\u0026#34; \u0026#34;public_internet_gateway_ipv6\u0026#34; { count = var.create_igw \u0026amp;\u0026amp; var.enable_ipv6 \u0026amp;\u0026amp; var.public_subnet_count \u0026gt; 0 ? 1 : 0 route_table_id = aws_route_table.public[0].id destination_ipv6_cidr_block = \u0026#34;::/0\u0026#34; gateway_id = aws_internet_gateway.main[0].id timeouts { create = \u0026#34;5m\u0026#34; } } resource \u0026#34;aws_route\u0026#34; \u0026#34;private_nat_gateway\u0026#34; { count = var.enable_nat_gateway ? var.private_subnet_count : 0 route_table_id = aws_route_table.private[count.index].id destination_cidr_block = \u0026#34;0.0.0.0/0\u0026#34; nat_gateway_id = aws_nat_gateway.main[var.single_nat_gateway ? 0 : count.index].id timeouts { create = \u0026#34;5m\u0026#34; } } resource \u0026#34;aws_route\u0026#34; \u0026#34;private_ipv6_egress\u0026#34; { count = var.enable_ipv6 \u0026amp;\u0026amp; var.create_egress_only_igw ? var.private_subnet_count : 0 route_table_id = aws_route_table.private[count.index].id destination_ipv6_cidr_block = \u0026#34;::/0\u0026#34; egress_only_gateway_id = aws_egress_only_internet_gateway.main[0].id timeouts { create = \u0026#34;5m\u0026#34; } } # Route Table Associations resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;public\u0026#34; { count = var.public_subnet_count subnet_id = aws_subnet.public[count.index].id route_table_id = aws_route_table.public[0].id } resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;private\u0026#34; { count = var.private_subnet_count subnet_id = aws_subnet.private[count.index].id route_table_id = aws_route_table.private[count.index].id } resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;database\u0026#34; { count = var.database_subnet_count \u0026gt; 0 \u0026amp;\u0026amp; var.create_database_subnet_route_table ? var.database_subnet_count : 0 subnet_id = aws_subnet.database[count.index].id route_table_id = aws_route_table.database[0].id } # Database Subnet Group resource \u0026#34;aws_db_subnet_group\u0026#34; \u0026#34;database\u0026#34; { count = var.database_subnet_count \u0026gt; 0 \u0026amp;\u0026amp; var.create_database_subnet_group ? 1 : 0 name = \u0026#34;${var.name}-database-subnet-group\u0026#34; subnet_ids = aws_subnet.database[*].id tags = merge(var.tags, { Name = \u0026#34;${var.name}-database-subnet-group\u0026#34; Type = \u0026#34;database-subnet-group\u0026#34; }) } # VPC Flow Logs resource \u0026#34;aws_flow_log\u0026#34; \u0026#34;vpc\u0026#34; { count = var.enable_flow_log ? 1 : 0 iam_role_arn = aws_iam_role.flow_log[0].arn log_destination = aws_cloudwatch_log_group.vpc_flow_log[0].arn traffic_type = var.flow_log_traffic_type vpc_id = aws_vpc.main.id tags = merge(var.tags, { Name = \u0026#34;${var.name}-vpc-flow-log\u0026#34; Type = \u0026#34;vpc-flow-log\u0026#34; }) } resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;vpc_flow_log\u0026#34; { count = var.enable_flow_log ? 1 : 0 name = \u0026#34;/aws/vpc/flow-logs/${var.name}\u0026#34; retention_in_days = var.flow_log_retention_in_days kms_key_id = var.flow_log_kms_key_id tags = merge(var.tags, { Name = \u0026#34;${var.name}-vpc-flow-log-group\u0026#34; Type = \u0026#34;cloudwatch-log-group\u0026#34; }) } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;flow_log\u0026#34; { count = var.enable_flow_log ? 1 : 0 name = \u0026#34;${var.name}-vpc-flow-log-role\u0026#34; assume_role_policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { Action = \u0026#34;sts:AssumeRole\u0026#34; Effect = \u0026#34;Allow\u0026#34; Principal = { Service = \u0026#34;vpc-flow-logs.amazonaws.com\u0026#34; } } ] }) tags = merge(var.tags, { Name = \u0026#34;${var.name}-vpc-flow-log-role\u0026#34; Type = \u0026#34;iam-role\u0026#34; }) } resource \u0026#34;aws_iam_role_policy\u0026#34; \u0026#34;flow_log\u0026#34; { count = var.enable_flow_log ? 1 : 0 name = \u0026#34;${var.name}-vpc-flow-log-policy\u0026#34; role = aws_iam_role.flow_log[0].id policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { Action = [ \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:DescribeLogStreams\u0026#34; ] Effect = \u0026#34;Allow\u0026#34; Resource = \u0026#34;*\u0026#34; } ] }) } # VPC Endpoints resource \u0026#34;aws_vpc_endpoint\u0026#34; \u0026#34;s3\u0026#34; { count = var.enable_s3_endpoint ? 1 : 0 vpc_id = aws_vpc.main.id service_name = \u0026#34;com.amazonaws.${data.aws_region.current.name}.s3\u0026#34; tags = merge(var.tags, { Name = \u0026#34;${var.name}-s3-endpoint\u0026#34; Type = \u0026#34;vpc-endpoint\u0026#34; }) } resource \u0026#34;aws_vpc_endpoint_route_table_association\u0026#34; \u0026#34;s3_private\u0026#34; { count = var.enable_s3_endpoint ? var.private_subnet_count : 0 vpc_endpoint_id = aws_vpc_endpoint.s3[0].id route_table_id = aws_route_table.private[count.index].id } resource \u0026#34;aws_vpc_endpoint\u0026#34; \u0026#34;dynamodb\u0026#34; { count = var.enable_dynamodb_endpoint ? 1 : 0 vpc_id = aws_vpc.main.id service_name = \u0026#34;com.amazonaws.${data.aws_region.current.name}.dynamodb\u0026#34; tags = merge(var.tags, { Name = \u0026#34;${var.name}-dynamodb-endpoint\u0026#34; Type = \u0026#34;vpc-endpoint\u0026#34; }) } resource \u0026#34;aws_vpc_endpoint_route_table_association\u0026#34; \u0026#34;dynamodb_private\u0026#34; { count = var.enable_dynamodb_endpoint ? var.private_subnet_count : 0 vpc_endpoint_id = aws_vpc_endpoint.dynamodb[0].id route_table_id = aws_route_table.private[count.index].id } # Interface VPC Endpoints resource \u0026#34;aws_vpc_endpoint\u0026#34; \u0026#34;interface_endpoints\u0026#34; { for_each = var.interface_endpoints vpc_id = aws_vpc.main.id service_name = \u0026#34;com.amazonaws.${data.aws_region.current.name}.${each.key}\u0026#34; vpc_endpoint_type = \u0026#34;Interface\u0026#34; subnet_ids = aws_subnet.private[*].id security_group_ids = [aws_security_group.vpc_endpoint[0].id] private_dns_enabled = true policy = each.value.policy tags = merge(var.tags, { Name = \u0026#34;${var.name}-${each.key}-endpoint\u0026#34; Type = \u0026#34;vpc-endpoint\u0026#34; }) } resource \u0026#34;aws_security_group\u0026#34; \u0026#34;vpc_endpoint\u0026#34; { count = length(var.interface_endpoints) \u0026gt; 0 ? 1 : 0 name_prefix = \u0026#34;${var.name}-vpc-endpoint-\u0026#34; vpc_id = aws_vpc.main.id ingress { from_port = 443 to_port = 443 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [var.vpc_cidr] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = merge(var.tags, { Name = \u0026#34;${var.name}-vpc-endpoint-sg\u0026#34; Type = \u0026#34;security-group\u0026#34; }) } EKS Module with Advanced Configuration # modules/eks/main.tf locals { cluster_name = \u0026#34;${var.cluster_name}-${var.environment}\u0026#34; # Node group configurations node_groups = { for k, v in var.node_groups : k =\u0026gt; merge({ instance_types = [\u0026#34;t3.medium\u0026#34;] capacity_type = \u0026#34;ON_DEMAND\u0026#34; disk_size = 50 disk_type = \u0026#34;gp3\u0026#34; scaling_config = { desired_size = 2 max_size = 10 min_size = 1 } update_config = { max_unavailable_percentage = 25 } # Kubernetes labels labels = {} # Kubernetes taints taints = [] # Launch template launch_template = {} # User data user_data = \u0026#34;\u0026#34; # Security groups additional_security_group_ids = [] # Subnets subnet_ids = [] # Tags tags = {} }, v) } } # Data sources data \u0026#34;aws_caller_identity\u0026#34; \u0026#34;current\u0026#34; {} data \u0026#34;aws_region\u0026#34; \u0026#34;current\u0026#34; {} data \u0026#34;aws_partition\u0026#34; \u0026#34;current\u0026#34; {} data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;cluster_assume_role_policy\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRole\u0026#34;] principals { type = \u0026#34;Service\u0026#34; identifiers = [\u0026#34;eks.amazonaws.com\u0026#34;] } } } data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;node_group_assume_role_policy\u0026#34; { statement { actions = [\u0026#34;sts:AssumeRole\u0026#34;] principals { type = \u0026#34;Service\u0026#34; identifiers = [\u0026#34;ec2.amazonaws.com\u0026#34;] } } } # KMS key for EKS cluster encryption resource \u0026#34;aws_kms_key\u0026#34; \u0026#34;eks\u0026#34; { count = var.create_kms_key ? 1 : 0 description = \u0026#34;EKS Secret Encryption Key for ${local.cluster_name}\u0026#34; deletion_window_in_days = var.kms_key_deletion_window_in_days enable_key_rotation = var.enable_kms_key_rotation policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = [ { Sid = \u0026#34;Enable IAM User Permissions\u0026#34; Effect = \u0026#34;Allow\u0026#34; Principal = { AWS = \u0026#34;arn:${data.aws_partition.current.partition}:iam::${data.aws_caller_identity.current.account_id}:root\u0026#34; } Action = \u0026#34;kms:*\u0026#34; Resource = \u0026#34;*\u0026#34; }, { Sid = \u0026#34;Allow EKS Service\u0026#34; Effect = \u0026#34;Allow\u0026#34; Principal = { Service = \u0026#34;eks.amazonaws.com\u0026#34; } Action = [ \u0026#34;kms:Encrypt\u0026#34;, \u0026#34;kms:Decrypt\u0026#34;, \u0026#34;kms:ReEncrypt*\u0026#34;, \u0026#34;kms:GenerateDataKey*\u0026#34;, \u0026#34;kms:DescribeKey\u0026#34; ] Resource = \u0026#34;*\u0026#34; } ] }) tags = merge(var.tags, { Name = \u0026#34;${local.cluster_name}-eks-key\u0026#34; Type = \u0026#34;kms-key\u0026#34; }) } resource \u0026#34;aws_kms_alias\u0026#34; \u0026#34;eks\u0026#34; { count = var.create_kms_key ? 1 : 0 name = \u0026#34;alias/${local.cluster_name}-eks\u0026#34; target_key_id = aws_kms_key.eks[0].key_id } # EKS Cluster IAM Role resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;cluster\u0026#34; { name = \u0026#34;${local.cluster_name}-cluster-role\u0026#34; assume_role_policy = data.aws_iam_policy_document.cluster_assume_role_policy.json tags = merge(var.tags, { Name = \u0026#34;${local.cluster_name}-cluster-role\u0026#34; Type = \u0026#34;iam-role\u0026#34; }) } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;cluster_AmazonEKSClusterPolicy\u0026#34; { policy_arn = \u0026#34;arn:${data.aws_partition.current.partition}:iam::aws:policy/AmazonEKSClusterPolicy\u0026#34; role = aws_iam_role.cluster.name } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;cluster_AmazonEKSVPCResourceController\u0026#34; { policy_arn = \u0026#34;arn:${data.aws_partition.current.partition}:iam::aws:policy/AmazonEKSVPCResourceController\u0026#34; role = aws_iam_role.cluster.name } # Additional cluster policies resource \u0026#34;aws_iam_role_policy\u0026#34; \u0026#34;cluster_additional\u0026#34; { count = length(var.cluster_additional_policies) \u0026gt; 0 ? 1 : 0 name = \u0026#34;${local.cluster_name}-cluster-additional-policy\u0026#34; role = aws_iam_role.cluster.id policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = var.cluster_additional_policies }) } # EKS Cluster Security Group resource \u0026#34;aws_security_group\u0026#34; \u0026#34;cluster\u0026#34; { name_prefix = \u0026#34;${local.cluster_name}-cluster-\u0026#34; vpc_id = var.vpc_id description = \u0026#34;EKS cluster security group\u0026#34; # Allow all outbound traffic egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = merge(var.tags, { Name = \u0026#34;${local.cluster_name}-cluster-sg\u0026#34; Type = \u0026#34;security-group\u0026#34; }) } # Cluster security group rules resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;cluster_ingress_workstation_https\u0026#34; { count = length(var.cluster_endpoint_private_access_cidrs) \u0026gt; 0 ? 1 : 0 description = \u0026#34;Allow workstation to communicate with the cluster API Server\u0026#34; type = \u0026#34;ingress\u0026#34; from_port = 443 to_port = 443 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = var.cluster_endpoint_private_access_cidrs security_group_id = aws_security_group.cluster.id } # Node group security group resource \u0026#34;aws_security_group\u0026#34; \u0026#34;node_group\u0026#34; { name_prefix = \u0026#34;${local.cluster_name}-node-group-\u0026#34; vpc_id = var.vpc_id description = \u0026#34;EKS node group security group\u0026#34; # Allow all outbound traffic egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = merge(var.tags, { Name = \u0026#34;${local.cluster_name}-node-group-sg\u0026#34; Type = \u0026#34;security-group\u0026#34; \u0026#34;kubernetes.io/cluster/${local.cluster_name}\u0026#34; = \u0026#34;owned\u0026#34; }) } # Node group security group rules resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;node_group_ingress_self\u0026#34; { description = \u0026#34;Allow node to communicate with each other\u0026#34; type = \u0026#34;ingress\u0026#34; from_port = 0 to_port = 65535 protocol = \u0026#34;-1\u0026#34; source_security_group_id = aws_security_group.node_group.id security_group_id = aws_security_group.node_group.id } resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;node_group_ingress_cluster_https\u0026#34; { description = \u0026#34;Allow pods to communicate with the cluster API Server\u0026#34; type = \u0026#34;ingress\u0026#34; from_port = 443 to_port = 443 protocol = \u0026#34;tcp\u0026#34; source_security_group_id = aws_security_group.cluster.id security_group_id = aws_security_group.node_group.id } resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;node_group_ingress_cluster_kubelet\u0026#34; { description = \u0026#34;Allow cluster control plane to communicate with worker node kubelet\u0026#34; type = \u0026#34;ingress\u0026#34; from_port = 10250 to_port = 10250 protocol = \u0026#34;tcp\u0026#34; source_security_group_id = aws_security_group.cluster.id security_group_id = aws_security_group.node_group.id } resource \u0026#34;aws_security_group_rule\u0026#34; \u0026#34;cluster_ingress_node_group_https\u0026#34; { description = \u0026#34;Allow pods to communicate with the cluster API Server\u0026#34; type = \u0026#34;ingress\u0026#34; from_port = 443 to_port = 443 protocol = \u0026#34;tcp\u0026#34; source_security_group_id = aws_security_group.node_group.id security_group_id = aws_security_group.cluster.id } # EKS Cluster resource \u0026#34;aws_eks_cluster\u0026#34; \u0026#34;main\u0026#34; { name = local.cluster_name role_arn = aws_iam_role.cluster.arn version = var.cluster_version vpc_config { subnet_ids = var.subnet_ids endpoint_private_access = var.cluster_endpoint_private_access endpoint_public_access = var.cluster_endpoint_public_access public_access_cidrs = var.cluster_endpoint_public_access_cidrs security_group_ids = [aws_security_group.cluster.id] } # Encryption configuration dynamic \u0026#34;encryption_config\u0026#34; { for_each = var.cluster_encryption_config content { provider { key_arn = var.create_kms_key ? aws_kms_key.eks[0].arn : encryption_config.value.provider_key_arn } resources = encryption_config.value.resources } } # Logging configuration enabled_cluster_log_types = var.cluster_enabled_log_types # Add-ons will be managed separately depends_on = [ aws_iam_role_policy_attachment.cluster_AmazonEKSClusterPolicy, aws_iam_role_policy_attachment.cluster_AmazonEKSVPCResourceController, aws_cloudwatch_log_group.cluster, ] tags = merge(var.tags, { Name = local.cluster_name Type = \u0026#34;eks-cluster\u0026#34; }) } # CloudWatch Log Group for EKS cluster logs resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;cluster\u0026#34; { name = \u0026#34;/aws/eks/${local.cluster_name}/cluster\u0026#34; retention_in_days = var.cloudwatch_log_group_retention_in_days kms_key_id = var.cloudwatch_log_group_kms_key_id tags = merge(var.tags, { Name = \u0026#34;${local.cluster_name}-cluster-logs\u0026#34; Type = \u0026#34;cloudwatch-log-group\u0026#34; }) } # EKS Node Group IAM Role resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;node_group\u0026#34; { name = \u0026#34;${local.cluster_name}-node-group-role\u0026#34; assume_role_policy = data.aws_iam_policy_document.node_group_assume_role_policy.json tags = merge(var.tags, { Name = \u0026#34;${local.cluster_name}-node-group-role\u0026#34; Type = \u0026#34;iam-role\u0026#34; }) } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;node_group_AmazonEKSWorkerNodePolicy\u0026#34; { policy_arn = \u0026#34;arn:${data.aws_partition.current.partition}:iam::aws:policy/AmazonEKSWorkerNodePolicy\u0026#34; role = aws_iam_role.node_group.name } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;node_group_AmazonEKS_CNI_Policy\u0026#34; { policy_arn = \u0026#34;arn:${data.aws_partition.current.partition}:iam::aws:policy/AmazonEKS_CNI_Policy\u0026#34; role = aws_iam_role.node_group.name } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;node_group_AmazonEC2ContainerRegistryReadOnly\u0026#34; { policy_arn = \u0026#34;arn:${data.aws_partition.current.partition}:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\u0026#34; role = aws_iam_role.node_group.name } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;node_group_AmazonSSMManagedInstanceCore\u0026#34; { count = var.enable_ssm ? 1 : 0 policy_arn = \u0026#34;arn:${data.aws_partition.current.partition}:iam::aws:policy/AmazonSSMManagedInstanceCore\u0026#34; role = aws_iam_role.node_group.name } # Additional node group policies resource \u0026#34;aws_iam_role_policy\u0026#34; \u0026#34;node_group_additional\u0026#34; { count = length(var.node_group_additional_policies) \u0026gt; 0 ? 1 : 0 name = \u0026#34;${local.cluster_name}-node-group-additional-policy\u0026#34; role = aws_iam_role.node_group.id policy = jsonencode({ Version = \u0026#34;2012-10-17\u0026#34; Statement = var.node_group_additional_policies }) } # Launch template for node groups resource \u0026#34;aws_launch_template\u0026#34; \u0026#34;node_group\u0026#34; { for_each = local.node_groups name_prefix = \u0026#34;${local.cluster_name}-${each.key}-\u0026#34; vpc_security_group_ids = concat( [aws_security_group.node_group.id], each.value.additional_security_group_ids ) user_data = base64encode(templatefile(\u0026#34;${path.module}/user_data.sh\u0026#34;, { cluster_name = local.cluster_name cluster_endpoint = aws_eks_cluster.main.endpoint cluster_ca = aws_eks_cluster.main.certificate_authority[0].data bootstrap_arguments = each.value.bootstrap_arguments user_data_script = each.value.user_data })) dynamic \u0026#34;block_device_mappings\u0026#34; { for_each = each.value.block_device_mappings content { device_name = block_device_mappings.value.device_name ebs { volume_size = block_device_mappings.value.ebs.volume_size volume_type = block_device_mappings.value.ebs.volume_type iops = block_device_mappings.value.ebs.iops throughput = block_device_mappings.value.ebs.throughput encrypted = block_device_mappings.value.ebs.encrypted kms_key_id = block_device_mappings.value.ebs.kms_key_id delete_on_termination = block_device_mappings.value.ebs.delete_on_termination } } } dynamic \u0026#34;metadata_options\u0026#34; { for_each = each.value.metadata_options != null ? [each.value.metadata_options] : [] content { http_endpoint = metadata_options.value.http_endpoint http_tokens = metadata_options.value.http_tokens http_put_response_hop_limit = metadata_options.value.http_put_response_hop_limit instance_metadata_tags = metadata_options.value.instance_metadata_tags } } dynamic \u0026#34;monitoring\u0026#34; { for_each = each.value.enable_monitoring ? [1] : [] content { enabled = true } } tag_specifications { resource_type = \u0026#34;instance\u0026#34; tags = merge(var.tags, each.value.tags, { Name = \u0026#34;${local.cluster_name}-${each.key}-node\u0026#34; Type = \u0026#34;eks-node\u0026#34; }) } tag_specifications { resource_type = \u0026#34;volume\u0026#34; tags = merge(var.tags, each.value.tags, { Name = \u0026#34;${local.cluster_name}-${each.key}-volume\u0026#34; Type = \u0026#34;eks-node-volume\u0026#34; }) } tags = merge(var.tags, { Name = \u0026#34;${local.cluster_name}-${each.key}-lt\u0026#34; Type = \u0026#34;launch-template\u0026#34; }) } # EKS Node Groups resource \u0026#34;aws_eks_node_group\u0026#34; \u0026#34;main\u0026#34; { for_each = local.node_groups cluster_name = aws_eks_cluster.main.name node_group_name = \u0026#34;${local.cluster_name}-${each.key}\u0026#34; node_role_arn = aws_iam_role.node_group.arn subnet_ids = length(each.value.subnet_ids) \u0026gt; 0 ? each.value.subnet_ids : var.subnet_ids instance_types = each.value.instance_types capacity_type = each.value.capacity_type disk_size = each.value.disk_size ami_type = each.value.ami_type release_version = each.value.release_version version = each.value.version scaling_config { desired_size = each.value.scaling_config.desired_size max_size = each.value.scaling_config.max_size min_size = each.value.scaling_config.min_size } update_config { max_unavailable_percentage = each.value.update_config.max_unavailable_percentage } # Launch template launch_template { id = aws_launch_template.node_group[each.key].id version = aws_launch_template.node_group[each.key].latest_version } # Labels labels = merge(each.value.labels, { \u0026#34;node-group\u0026#34; = each.key }) # Taints dynamic \u0026#34;taint\u0026#34; { for_each = each.value.taints content { key = taint.value.key value = taint.value.value effect = taint.value.effect } } depends_on = [ aws_iam_role_policy_attachment.node_group_AmazonEKSWorkerNodePolicy, aws_iam_role_policy_attachment.node_group_AmazonEKS_CNI_Policy, aws_iam_role_policy_attachment.node_group_AmazonEC2ContainerRegistryReadOnly, ] tags = merge(var.tags, each.value.tags, { Name = \u0026#34;${local.cluster_name}-${each.key}\u0026#34; Type = \u0026#34;eks-node-group\u0026#34; }) } # EKS Add-ons resource \u0026#34;aws_eks_addon\u0026#34; \u0026#34;main\u0026#34; { for_each = var.cluster_addons cluster_name = aws_eks_cluster.main.name addon_name = each.key addon_version = each.value.addon_version resolve_conflicts = each.value.resolve_conflicts service_account_role_arn = each.value.service_account_role_arn depends_on = [aws_eks_node_group.main] tags = merge(var.tags, { Name = \u0026#34;${local.cluster_name}-${each.key}-addon\u0026#34; Type = \u0026#34;eks-addon\u0026#34; }) } # OIDC Identity Provider data \u0026#34;tls_certificate\u0026#34; \u0026#34;cluster\u0026#34; { url = aws_eks_cluster.main.identity[0].oidc[0].issuer } resource \u0026#34;aws_iam_openid_connect_provider\u0026#34; \u0026#34;cluster\u0026#34; { count = var.enable_irsa ? 1 : 0 client_id_list = [\u0026#34;sts.amazonaws.com\u0026#34;] thumbprint_list = [data.tls_certificate.cluster.certificates[0].sha1_fingerprint] url = aws_eks_cluster.main.identity[0].oidc[0].issuer tags = merge(var.tags, { Name = \u0026#34;${local.cluster_name}-oidc-provider\u0026#34; Type = \u0026#34;oidc-provider\u0026#34; }) } Security and Compliance Terraform Security Scanning # .github/workflows/terraform-security.yml name: Terraform Security Scan on: push: branches: [ main, develop ] pull_request: branches: [ main ] jobs: security-scan: runs-on: ubuntu-latest steps: - name: Checkout code uses: actions/checkout@v4 - name: Setup Terraform uses: hashicorp/setup-terraform@v3 with: terraform_version: 1.5.0 - name: Terraform Format Check run: terraform fmt -check -recursive - name: Terraform Init run: terraform init -backend=false - name: Terraform Validate run: terraform validate - name: Run Checkov uses: bridgecrewio/checkov-action@master with: directory: . framework: terraform output_format: sarif output_file_path: checkov-results.sarif - name: Upload Checkov results to GitHub Security uses: github/codeql-action/upload-sarif@v2 if: always() with: sarif_file: checkov-results.sarif - name: Run TFSec uses: aquasecurity/tfsec-action@v1.0.3 with: soft_fail: true - name: Run Terrascan uses: tenable/terrascan-action@main with: iac_type: terraform iac_version: v14 policy_type: aws only_warn: true - name: Run Infracost uses: infracost/actions/setup@v2 with: api-key: ${{ secrets.INFRACOST_API_KEY }} - name: Generate Infracost diff run: | infracost breakdown --path . \\ --format json \\ --out-file infracost-base.json - name: Post Infracost comment uses: infracost/actions/comment@v1 with: path: infracost-base.json behavior: update Policy as Code with Sentinel # policies/sentinel/aws-security-policies.sentinel import \u0026#34;tfplan/v2\u0026#34; as tfplan import \u0026#34;strings\u0026#34; import \u0026#34;types\u0026#34; # Helper functions get_resources = func(resource_type) { resources = {} for tfplan.resource_changes as address, rc { if rc.type is resource_type and rc.mode is \u0026#34;managed\u0026#34; and (rc.change.actions contains \u0026#34;create\u0026#34; or rc.change.actions contains \u0026#34;update\u0026#34;) { resources[address] = rc } } return resources } # Policy: Ensure S3 buckets are encrypted s3_buckets_encrypted = rule { all get_resources(\u0026#34;aws_s3_bucket\u0026#34;) as address, rc { rc.change.after.server_side_encryption_configuration is not null and length(rc.change.after.server_side_encryption_configuration) \u0026gt; 0 } } # Policy: Ensure RDS instances are encrypted rds_instances_encrypted = rule { all get_resources(\u0026#34;aws_db_instance\u0026#34;) as address, rc { rc.change.after.storage_encrypted is true } } # Policy: Ensure EBS volumes are encrypted ebs_volumes_encrypted = rule { all get_resources(\u0026#34;aws_ebs_volume\u0026#34;) as address, rc { rc.change.after.encrypted is true } } # Policy: Ensure security groups don\u0026#39;t allow 0.0.0.0/0 on port 22 no_ssh_from_anywhere = rule { all get_resources(\u0026#34;aws_security_group\u0026#34;) as address, rc { all rc.change.after.ingress as ingress { not (ingress.from_port \u0026lt;= 22 and ingress.to_port \u0026gt;= 22 and ingress.protocol is \u0026#34;tcp\u0026#34; and \u0026#34;0.0.0.0/0\u0026#34; in ingress.cidr_blocks) } } } # Policy: Ensure security groups don\u0026#39;t allow 0.0.0.0/0 on port 3389 no_rdp_from_anywhere = rule { all get_resources(\u0026#34;aws_security_group\u0026#34;) as address, rc { all rc.change.after.ingress as ingress { not (ingress.from_port \u0026lt;= 3389 and ingress.to_port \u0026gt;= 3389 and ingress.protocol is \u0026#34;tcp\u0026#34; and \u0026#34;0.0.0.0/0\u0026#34; in ingress.cidr_blocks) } } } # Policy: Ensure IAM policies don\u0026#39;t grant admin access no_admin_policies = rule { all get_resources(\u0026#34;aws_iam_policy\u0026#34;) as address, rc { policy_doc = json.unmarshal(rc.change.after.policy) all policy_doc.Statement as statement { not (statement.Effect is \u0026#34;Allow\u0026#34; and statement.Action contains \u0026#34;*\u0026#34; and statement.Resource contains \u0026#34;*\u0026#34;) } } } # Policy: Ensure resources have required tags required_tags = [\u0026#34;Environment\u0026#34;, \u0026#34;Project\u0026#34;, \u0026#34;Owner\u0026#34;, \u0026#34;CostCenter\u0026#34;] resources_have_required_tags = rule { all get_resources(\u0026#34;aws_instance\u0026#34;) as address, rc { all required_tags as tag { rc.change.after.tags contains tag } } and all get_resources(\u0026#34;aws_s3_bucket\u0026#34;) as address, rc { all required_tags as tag { rc.change.after.tags contains tag } } and all get_resources(\u0026#34;aws_rds_instance\u0026#34;) as address, rc { all required_tags as tag { rc.change.after.tags contains tag } } } # Policy: Ensure EKS clusters have logging enabled eks_logging_enabled = rule { all get_resources(\u0026#34;aws_eks_cluster\u0026#34;) as address, rc { rc.change.after.enabled_cluster_log_types is not null and length(rc.change.after.enabled_cluster_log_types) \u0026gt; 0 } } # Policy: Ensure VPC flow logs are enabled vpc_flow_logs_enabled = rule { vpcs = get_resources(\u0026#34;aws_vpc\u0026#34;) flow_logs = get_resources(\u0026#34;aws_flow_log\u0026#34;) all vpcs as vpc_address, vpc_rc { any flow_logs as fl_address, fl_rc { fl_rc.change.after.vpc_id is vpc_rc.change.after.id } } } # Main policy main = rule { s3_buckets_encrypted and rds_instances_encrypted and ebs_volumes_encrypted and no_ssh_from_anywhere and no_rdp_from_anywhere and no_admin_policies and resources_have_required_tags and eks_logging_enabled and vpc_flow_logs_enabled } CI/CD Integration GitLab CI Pipeline for Terraform # .gitlab-ci.yml stages: - validate - plan - security-scan - apply - destroy variables: TF_ROOT: ${CI_PROJECT_DIR} TF_ADDRESS: ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/terraform/state/${CI_ENVIRONMENT_NAME} TF_IN_AUTOMATION: \u0026#34;true\u0026#34; TF_INPUT: \u0026#34;false\u0026#34; TF_CLI_ARGS: \u0026#34;-no-color\u0026#34; cache: key: \u0026#34;${CI_COMMIT_REF_SLUG}\u0026#34; paths: - ${TF_ROOT}/.terraform before_script: - cd ${TF_ROOT} - terraform --version - terraform init -backend-config=\u0026#34;address=${TF_ADDRESS}\u0026#34; -backend-config=\u0026#34;lock_address=${TF_ADDRESS}/lock\u0026#34; -backend-config=\u0026#34;unlock_address=${TF_ADDRESS}/lock\u0026#34; -backend-config=\u0026#34;username=${GITLAB_USER_LOGIN}\u0026#34; -backend-config=\u0026#34;password=${CI_JOB_TOKEN}\u0026#34; -backend-config=\u0026#34;lock_method=POST\u0026#34; -backend-config=\u0026#34;unlock_method=DELETE\u0026#34; -backend-config=\u0026#34;retry_wait_min=5\u0026#34; validate: stage: validate script: - terraform fmt -check -recursive - terraform validate rules: - if: $CI_PIPELINE_SOURCE == \u0026#34;merge_request_event\u0026#34; - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH plan: stage: plan script: - terraform plan -var-file=\u0026#34;environments/${CI_ENVIRONMENT_NAME}.tfvars\u0026#34; -out=\u0026#34;planfile\u0026#34; - terraform show -json planfile \u0026gt; plan.json artifacts: name: plan paths: - ${TF_ROOT}/planfile - ${TF_ROOT}/plan.json expire_in: 1 week rules: - if: $CI_PIPELINE_SOURCE == \u0026#34;merge_request_event\u0026#34; - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH security-scan: stage: security-scan image: bridgecrew/checkov:latest script: - checkov -f plan.json --framework terraform_plan --output cli --output junitxml --output-file-path console,checkov-report.xml artifacts: reports: junit: checkov-report.xml paths: - checkov-report.xml expire_in: 1 week dependencies: - plan rules: - if: $CI_PIPELINE_SOURCE == \u0026#34;merge_request_event\u0026#34; - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH cost-estimation: stage: security-scan image: infracost/infracost:ci-0.10 script: - infracost breakdown --path plan.json --format json --out-file infracost.json - infracost output --path infracost.json --format table - infracost output --path infracost.json --format html --out-file infracost-report.html artifacts: paths: - infracost.json - infracost-report.html expire_in: 1 week dependencies: - plan rules: - if: $CI_PIPELINE_SOURCE == \u0026#34;merge_request_event\u0026#34; - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH apply: stage: apply script: - terraform apply -auto-approve planfile dependencies: - plan rules: - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH when: manual environment: name: ${CI_ENVIRONMENT_NAME} action: start destroy: stage: destroy script: - terraform destroy -var-file=\u0026#34;environments/${CI_ENVIRONMENT_NAME}.tfvars\u0026#34; -auto-approve rules: - if: $CI_COMMIT_BRANCH == $CI_DEFAULT_BRANCH when: manual environment: name: ${CI_ENVIRONMENT_NAME} action: stop Automated Deployment Script #!/bin/bash # scripts/deploy.sh set -e # Configuration SCRIPT_DIR=\u0026#34;$(cd \u0026#34;$(dirname \u0026#34;${BASH_SOURCE[0]}\u0026#34;)\u0026#34; \u0026amp;\u0026amp; pwd)\u0026#34; PROJECT_ROOT=\u0026#34;$(dirname \u0026#34;$SCRIPT_DIR\u0026#34;)\u0026#34; ENVIRONMENTS_DIR=\u0026#34;$PROJECT_ROOT/environments\u0026#34; MODULES_DIR=\u0026#34;$PROJECT_ROOT/modules\u0026#34; # Default values ENVIRONMENT=\u0026#34;\u0026#34; ACTION=\u0026#34;plan\u0026#34; AUTO_APPROVE=false DESTROY=false WORKSPACE=\u0026#34;\u0026#34; VAR_FILE=\u0026#34;\u0026#34; BACKEND_CONFIG=\u0026#34;\u0026#34; PARALLELISM=10 REFRESH=true # Colors for output RED=\u0026#39;\\033[0;31m\u0026#39; GREEN=\u0026#39;\\033[0;32m\u0026#39; YELLOW=\u0026#39;\\033[1;33m\u0026#39; BLUE=\u0026#39;\\033[0;34m\u0026#39; NC=\u0026#39;\\033[0m\u0026#39; # No Color # Logging functions log_info() { echo -e \u0026#34;${BLUE}[INFO]${NC} $1\u0026#34; } log_success() { echo -e \u0026#34;${GREEN}[SUCCESS]${NC} $1\u0026#34; } log_warning() { echo -e \u0026#34;${YELLOW}[WARNING]${NC} $1\u0026#34; } log_error() { echo -e \u0026#34;${RED}[ERROR]${NC} $1\u0026#34; } # Help function show_help() { cat \u0026lt;\u0026lt; EOF Terraform Deployment Script Usage: $0 [OPTIONS] OPTIONS: -e, --environment ENVIRONMENT Target environment (dev, staging, production) -a, --action ACTION Action to perform (plan, apply, destroy) -w, --workspace WORKSPACE Terraform workspace to use -f, --var-file FILE Variables file to use -b, --backend-config FILE Backend configuration file -p, --parallelism NUMBER Number of parallel operations (default: 10) --auto-approve Auto approve apply/destroy operations --no-refresh Skip refresh during plan/apply -h, --help Show this help message EXAMPLES: $0 -e dev -a plan $0 -e production -a apply --auto-approve $0 -e staging -a destroy --auto-approve $0 -e dev -w feature-branch -a plan EOF } # Parse command line arguments while [[ $# -gt 0 ]]; do case $1 in -e|--environment) ENVIRONMENT=\u0026#34;$2\u0026#34; shift 2 ;; -a|--action) ACTION=\u0026#34;$2\u0026#34; shift 2 ;; -w|--workspace) WORKSPACE=\u0026#34;$2\u0026#34; shift 2 ;; -f|--var-file) VAR_FILE=\u0026#34;$2\u0026#34; shift 2 ;; -b|--backend-config) BACKEND_CONFIG=\u0026#34;$2\u0026#34; shift 2 ;; -p|--parallelism) PARALLELISM=\u0026#34;$2\u0026#34; shift 2 ;; --auto-approve) AUTO_APPROVE=true shift ;; --no-refresh) REFRESH=false shift ;; -h|--help) show_help exit 0 ;; *) log_error \u0026#34;Unknown option: $1\u0026#34; show_help exit 1 ;; esac done # Validate required parameters if [[ -z \u0026#34;$ENVIRONMENT\u0026#34; ]]; then log_error \u0026#34;Environment is required. Use -e or --environment option.\u0026#34; exit 1 fi # Set default var file if not specified if [[ -z \u0026#34;$VAR_FILE\u0026#34; ]]; then VAR_FILE=\u0026#34;$ENVIRONMENTS_DIR/$ENVIRONMENT.tfvars\u0026#34; fi # Validate environment directory exists ENV_DIR=\u0026#34;$ENVIRONMENTS_DIR/$ENVIRONMENT\u0026#34; if [[ ! -d \u0026#34;$ENV_DIR\u0026#34; ]]; then log_error \u0026#34;Environment directory not found: $ENV_DIR\u0026#34; exit 1 fi # Validate var file exists if [[ ! -f \u0026#34;$VAR_FILE\u0026#34; ]]; then log_error \u0026#34;Variables file not found: $VAR_FILE\u0026#34; exit 1 fi # Change to environment directory cd \u0026#34;$ENV_DIR\u0026#34; log_info \u0026#34;Starting Terraform deployment for environment: $ENVIRONMENT\u0026#34; log_info \u0026#34;Action: $ACTION\u0026#34; log_info \u0026#34;Variables file: $VAR_FILE\u0026#34; # Initialize Terraform log_info \u0026#34;Initializing Terraform...\u0026#34; INIT_ARGS=() if [[ -n \u0026#34;$BACKEND_CONFIG\u0026#34; ]]; then INIT_ARGS+=(\u0026#34;-backend-config=$BACKEND_CONFIG\u0026#34;) fi if ! terraform init \u0026#34;${INIT_ARGS[@]}\u0026#34;; then log_error \u0026#34;Terraform initialization failed\u0026#34; exit 1 fi # Select or create workspace if [[ -n \u0026#34;$WORKSPACE\u0026#34; ]]; then log_info \u0026#34;Selecting workspace: $WORKSPACE\u0026#34; terraform workspace select \u0026#34;$WORKSPACE\u0026#34; || terraform workspace new \u0026#34;$WORKSPACE\u0026#34; fi # Perform the requested action case $ACTION in plan) log_info \u0026#34;Running Terraform plan...\u0026#34; PLAN_ARGS=( \u0026#34;-var-file=$VAR_FILE\u0026#34; \u0026#34;-parallelism=$PARALLELISM\u0026#34; \u0026#34;-out=tfplan\u0026#34; ) if [[ \u0026#34;$REFRESH\u0026#34; == \u0026#34;false\u0026#34; ]]; then PLAN_ARGS+=(\u0026#34;-refresh=false\u0026#34;) fi if terraform plan \u0026#34;${PLAN_ARGS[@]}\u0026#34;; then log_success \u0026#34;Terraform plan completed successfully\u0026#34; # Show plan summary log_info \u0026#34;Plan summary:\u0026#34; terraform show -json tfplan | jq -r \u0026#39; .resource_changes[] | select(.change.actions[] | . != \u0026#34;no-op\u0026#34;) | \u0026#34;\\(.change.actions | join(\u0026#34;,\u0026#34;)) \\(.address)\u0026#34; \u0026#39; | sort else log_error \u0026#34;Terraform plan failed\u0026#34; exit 1 fi ;; apply) log_info \u0026#34;Running Terraform apply...\u0026#34; APPLY_ARGS=( \u0026#34;-var-file=$VAR_FILE\u0026#34; \u0026#34;-parallelism=$PARALLELISM\u0026#34; ) if [[ \u0026#34;$REFRESH\u0026#34; == \u0026#34;false\u0026#34; ]]; then APPLY_ARGS+=(\u0026#34;-refresh=false\u0026#34;) fi if [[ \u0026#34;$AUTO_APPROVE\u0026#34; == \u0026#34;true\u0026#34; ]]; then APPLY_ARGS+=(\u0026#34;-auto-approve\u0026#34;) fi # Check if plan file exists if [[ -f \u0026#34;tfplan\u0026#34; ]]; then log_info \u0026#34;Using existing plan file\u0026#34; APPLY_ARGS=(\u0026#34;tfplan\u0026#34;) fi if terraform apply \u0026#34;${APPLY_ARGS[@]}\u0026#34;; then log_success \u0026#34;Terraform apply completed successfully\u0026#34; # Show outputs log_info \u0026#34;Terraform outputs:\u0026#34; terraform output else log_error \u0026#34;Terraform apply failed\u0026#34; exit 1 fi ;; destroy) log_warning \u0026#34;This will destroy all resources in environment: $ENVIRONMENT\u0026#34; if [[ \u0026#34;$AUTO_APPROVE\u0026#34; != \u0026#34;true\u0026#34; ]]; then read -p \u0026#34;Are you sure you want to continue? (yes/no): \u0026#34; -r if [[ ! $REPLY =~ ^[Yy][Ee][Ss]$ ]]; then log_info \u0026#34;Destroy operation cancelled\u0026#34; exit 0 fi fi log_info \u0026#34;Running Terraform destroy...\u0026#34; DESTROY_ARGS=( \u0026#34;-var-file=$VAR_FILE\u0026#34; \u0026#34;-parallelism=$PARALLELISM\u0026#34; ) if [[ \u0026#34;$AUTO_APPROVE\u0026#34; == \u0026#34;true\u0026#34; ]]; then DESTROY_ARGS+=(\u0026#34;-auto-approve\u0026#34;) fi if terraform destroy \u0026#34;${DESTROY_ARGS[@]}\u0026#34;; then log_success \u0026#34;Terraform destroy completed successfully\u0026#34; else log_error \u0026#34;Terraform destroy failed\u0026#34; exit 1 fi ;; *) log_error \u0026#34;Unknown action: $ACTION\u0026#34; log_info \u0026#34;Supported actions: plan, apply, destroy\u0026#34; exit 1 ;; esac log_success \u0026#34;Deployment script completed successfully\u0026#34; Best Practices and Operational Excellence State Management Best Practices Remote State Storage: Always use remote state with encryption and versioning State Locking: Implement state locking to prevent concurrent modifications State Backup: Regular automated backups of state files Workspace Strategy: Use workspaces for environment isolation State File Security: Restrict access to state files containing sensitive data Module Development Guidelines Single Responsibility: Each module should have a clear, single purpose Versioning: Use semantic versioning for module releases Documentation: Comprehensive README with examples and variable descriptions Testing: Implement automated testing with tools like Terratest Validation: Input validation and output consistency Security Hardening Least Privilege: Apply principle of least privilege to IAM roles and policies Encryption: Enable encryption at rest and in transit for all resources Network Security: Implement proper network segmentation and security groups Secrets Management: Use dedicated secret management services Compliance: Regular compliance scanning and policy enforcement Performance Optimization Parallelism: Optimize Terraform parallelism settings Resource Dependencies: Minimize unnecessary dependencies Provider Caching: Use provider plugin caching State Refresh: Optimize state refresh operations Resource Targeting: Use targeted operations when appropriate Conclusion Terraform provides a powerful foundation for Infrastructure as Code, enabling teams to manage complex cloud environments with consistency, reliability, and security. This comprehensive guide covers enterprise-scale implementations, from basic project structure to advanced security patterns and CI/CD integration.\n","content":"Infrastructure Automation with Terraform: Enterprise-Scale Cloud Resource Management Infrastructure as Code (IaC) has become essential for modern cloud operations, enabling teams to manage infrastructure with the same rigor and practices used for application code. Terraform, as a leading IaC tool, provides a declarative approach to infrastructure management across multiple cloud providers. This comprehensive guide explores enterprise-scale Terraform implementations, advanced patterns, and best …","date":"2025-12-31","lastmod":"2025-12-31","tags":["Terraform","Infrastructure as Code","Cloud Automation","AWS","Azure","GCP","DevOps","CI/CD"],"categories":["DevOps"],"author":"Cloud Infrastructure Engineer","readingTime":24,"wordCount":4901,"section":"posts","type":"posts","draft":false,"featured":false,"series":["Infrastructure Automation"]},{"title":"Kubernetes Production Best Practices: Building Enterprise-Grade Container Orchestration","url":"https://www.dishuihengxin.com/en/posts/devops-kubernetes-production/","summary":"Kubernetes Production Best Practices: Building Enterprise-Grade Container Orchestration Running Kubernetes in production requires careful planning, robust security measures, and comprehensive operational practices. This guide covers the essential aspects of building and maintaining enterprise-grade Kubernetes clusters that can handle mission-critical workloads with high availability, security, and performance.\nProduction Cluster Architecture Multi-Master High Availability Setup # kubeadm-ha-config.yaml apiVersion: kubeadm.k8s.io/v1beta3 kind: InitConfiguration localAPIEndpoint: advertiseAddress: 10.0.1.10 bindPort: 6443 nodeRegistration: criSocket: unix:///var/run/containerd/containerd.sock kubeletExtraArgs: cloud-provider: external container-runtime: remote container-runtime-endpoint: unix:///var/run/containerd/containerd.sock --- apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration kubernetesVersion: v1.28.2 clusterName: production-cluster controlPlaneEndpoint: k8s-api-lb.company.com:6443 apiServer: advertiseAddress: 10.0.1.10 bindPort: 6443 certSANs: - k8s-api-lb.company.com - k8s-master-1.company.com - k8s-master-2.company.com - k8s-master-3.company.com - 10.0.1.10 - 10.0.1.11 - 10.0.1.12 - 127.0.0.1 extraArgs: audit-log-maxage: \u0026#34;30\u0026#34; audit-log-maxbackup: \u0026#34;10\u0026#34; audit-log-maxsize: \u0026#34;100\u0026#34; audit-log-path: /var/log/kubernetes/audit.log audit-policy-file: /etc/kubernetes/audit-policy.yaml enable-admission-plugins: \u0026gt; NodeRestriction, ResourceQuota, LimitRanger, ServiceAccount, DefaultStorageClass, DefaultTolerationSeconds, MutatingAdmissionWebhook, ValidatingAdmissionWebhook, PodSecurityPolicy encryption-provider-config: /etc/kubernetes/encryption-config.yaml feature-gates: \u0026#34;RotateKubeletServerCertificate=true\u0026#34; service-account-lookup: \u0026#34;true\u0026#34; service-account-key-file: /etc/kubernetes/pki/sa.pub service-account-signing-key-file: /etc/kubernetes/pki/sa.key extraVolumes: - name: audit-policy hostPath: /etc/kubernetes/audit-policy.yaml mountPath: /etc/kubernetes/audit-policy.yaml readOnly: true pathType: File - name: audit-logs hostPath: /var/log/kubernetes mountPath: /var/log/kubernetes pathType: DirectoryOrCreate - name: encryption-config hostPath: /etc/kubernetes/encryption-config.yaml mountPath: /etc/kubernetes/encryption-config.yaml readOnly: true pathType: File etcd: local: dataDir: /var/lib/etcd extraArgs: listen-metrics-urls: http://0.0.0.0:2381 auto-compaction-mode: periodic auto-compaction-retention: \u0026#34;1\u0026#34; max-request-bytes: \u0026#34;33554432\u0026#34; quota-backend-bytes: \u0026#34;6442450944\u0026#34; heartbeat-interval: \u0026#34;250\u0026#34; election-timeout: \u0026#34;1250\u0026#34; snapshot-count: \u0026#34;10000\u0026#34; serverCertSANs: - k8s-master-1.company.com - k8s-master-2.company.com - k8s-master-3.company.com - 10.0.1.10 - 10.0.1.11 - 10.0.1.12 peerCertSANs: - k8s-master-1.company.com - k8s-master-2.company.com - k8s-master-3.company.com - 10.0.1.10 - 10.0.1.11 - 10.0.1.12 networking: serviceSubnet: 10.96.0.0/12 podSubnet: 10.244.0.0/16 dnsDomain: cluster.local controllerManager: extraArgs: bind-address: 0.0.0.0 secure-port: \u0026#34;10257\u0026#34; cluster-signing-duration: \u0026#34;8760h\u0026#34; feature-gates: \u0026#34;RotateKubeletServerCertificate=true\u0026#34; terminated-pod-gc-threshold: \u0026#34;50\u0026#34; scheduler: extraArgs: bind-address: 0.0.0.0 secure-port: \u0026#34;10259\u0026#34; --- apiVersion: kubeadm.k8s.io/v1beta3 kind: KubeletConfiguration cgroupDriver: systemd containerRuntimeEndpoint: unix:///var/run/containerd/containerd.sock resolvConf: /run/systemd/resolve/resolv.conf runtimeRequestTimeout: \u0026#34;15m\u0026#34; tlsCipherSuites: - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 protectKernelDefaults: true makeIPTablesUtilChains: true eventRecordQPS: 0 shutdownGracePeriod: 60s shutdownGracePeriodCriticalPods: 20s featureGates: RotateKubeletServerCertificate: true serverTLSBootstrap: true rotateCertificates: true Load Balancer Configuration (HAProxy) # /etc/haproxy/haproxy.cfg global log stdout local0 chroot /var/lib/haproxy stats socket /run/haproxy/admin.sock mode 660 level admin stats timeout 30s user haproxy group haproxy daemon defaults mode http log global option httplog option dontlognull option log-health-checks option forwardfor except 127.0.0.0/8 option redispatch retries 3 timeout http-request 10s timeout queue 20s timeout connect 10s timeout client 1m timeout server 1m timeout http-keep-alive 10s timeout check 10s # Kubernetes API Server frontend k8s-api-frontend bind *:6443 mode tcp option tcplog default_backend k8s-api-backend backend k8s-api-backend mode tcp option tcp-check balance roundrobin default-server inter 10s downinter 5s rise 2 fall 2 slowstart 60s maxconn 250 maxqueue 256 weight 100 server k8s-master-1 10.0.1.10:6443 check server k8s-master-2 10.0.1.11:6443 check server k8s-master-3 10.0.1.12:6443 check # HAProxy Stats frontend stats bind *:8404 stats enable stats uri /stats stats refresh 30s stats admin if TRUE Security Hardening Pod Security Standards # pod-security-standards.yaml apiVersion: v1 kind: Namespace metadata: name: production labels: pod-security.kubernetes.io/enforce: restricted pod-security.kubernetes.io/audit: restricted pod-security.kubernetes.io/warn: restricted --- apiVersion: v1 kind: Namespace metadata: name: staging labels: pod-security.kubernetes.io/enforce: baseline pod-security.kubernetes.io/audit: restricted pod-security.kubernetes.io/warn: restricted --- # Custom Pod Security Policy for legacy support apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: restricted-psp spec: privileged: false allowPrivilegeEscalation: false requiredDropCapabilities: - ALL volumes: - \u0026#39;configMap\u0026#39; - \u0026#39;emptyDir\u0026#39; - \u0026#39;projected\u0026#39; - \u0026#39;secret\u0026#39; - \u0026#39;downwardAPI\u0026#39; - \u0026#39;persistentVolumeClaim\u0026#39; - \u0026#39;csi\u0026#39; hostNetwork: false hostIPC: false hostPID: false runAsUser: rule: \u0026#39;MustRunAsNonRoot\u0026#39; supplementalGroups: rule: \u0026#39;MustRunAs\u0026#39; ranges: - min: 1 max: 65535 fsGroup: rule: \u0026#39;MustRunAs\u0026#39; ranges: - min: 1 max: 65535 readOnlyRootFilesystem: true seLinux: rule: \u0026#39;RunAsAny\u0026#39; --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: restricted-psp-user rules: - apiGroups: [\u0026#39;policy\u0026#39;] resources: [\u0026#39;podsecuritypolicies\u0026#39;] verbs: [\u0026#39;use\u0026#39;] resourceNames: - restricted-psp --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: restricted-psp-all-serviceaccounts roleRef: kind: ClusterRole name: restricted-psp-user apiGroup: rbac.authorization.k8s.io subjects: - kind: Group name: system:serviceaccounts apiGroup: rbac.authorization.k8s.io Network Security Policies # network-security-policies.yaml apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-all namespace: production spec: podSelector: {} policyTypes: - Ingress - Egress --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: allow-dns namespace: production spec: podSelector: {} policyTypes: - Egress egress: - to: - namespaceSelector: matchLabels: name: kube-system - podSelector: matchLabels: k8s-app: kube-dns ports: - protocol: UDP port: 53 - protocol: TCP port: 53 --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: web-app-netpol namespace: production spec: podSelector: matchLabels: app: web-app policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: ingress-nginx - podSelector: matchLabels: app.kubernetes.io/name: ingress-nginx ports: - protocol: TCP port: 8080 egress: - to: - podSelector: matchLabels: app: database ports: - protocol: TCP port: 5432 - to: - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 - to: [] ports: - protocol: UDP port: 53 - protocol: TCP port: 53 --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: database-netpol namespace: production spec: podSelector: matchLabels: app: database policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: app: web-app - podSelector: matchLabels: app: api-server ports: - protocol: TCP port: 5432 RBAC Configuration # rbac-configuration.yaml apiVersion: v1 kind: ServiceAccount metadata: name: app-service-account namespace: production --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: production name: app-role rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;, \u0026#34;services\u0026#34;, \u0026#34;endpoints\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;configmaps\u0026#34;, \u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;apps\u0026#34;] resources: [\u0026#34;deployments\u0026#34;, \u0026#34;replicasets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: app-role-binding namespace: production subjects: - kind: ServiceAccount name: app-service-account namespace: production roleRef: kind: Role name: app-role apiGroup: rbac.authorization.k8s.io --- # Cluster-level roles for monitoring apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: monitoring-reader rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;nodes\u0026#34;, \u0026#34;nodes/metrics\u0026#34;, \u0026#34;services\u0026#34;, \u0026#34;endpoints\u0026#34;, \u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;apps\u0026#34;] resources: [\u0026#34;deployments\u0026#34;, \u0026#34;daemonsets\u0026#34;, \u0026#34;replicasets\u0026#34;, \u0026#34;statefulsets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;networking.k8s.io\u0026#34;] resources: [\u0026#34;ingresses\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - nonResourceURLs: [\u0026#34;/metrics\u0026#34;] verbs: [\u0026#34;get\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: monitoring-reader-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: monitoring-reader subjects: - kind: ServiceAccount name: prometheus namespace: monitoring Resource Management and Optimization Resource Quotas and Limits # resource-management.yaml apiVersion: v1 kind: ResourceQuota metadata: name: production-quota namespace: production spec: hard: requests.cpu: \u0026#34;100\u0026#34; requests.memory: 200Gi limits.cpu: \u0026#34;200\u0026#34; limits.memory: 400Gi persistentvolumeclaims: \u0026#34;50\u0026#34; requests.storage: \u0026#34;1Ti\u0026#34; count/deployments.apps: \u0026#34;50\u0026#34; count/services: \u0026#34;25\u0026#34; count/secrets: \u0026#34;100\u0026#34; count/configmaps: \u0026#34;100\u0026#34; --- apiVersion: v1 kind: LimitRange metadata: name: production-limits namespace: production spec: limits: - default: cpu: \u0026#34;500m\u0026#34; memory: \u0026#34;512Mi\u0026#34; defaultRequest: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;128Mi\u0026#34; max: cpu: \u0026#34;2\u0026#34; memory: \u0026#34;4Gi\u0026#34; min: cpu: \u0026#34;50m\u0026#34; memory: \u0026#34;64Mi\u0026#34; type: Container - default: storage: \u0026#34;10Gi\u0026#34; max: storage: \u0026#34;100Gi\u0026#34; min: storage: \u0026#34;1Gi\u0026#34; type: PersistentVolumeClaim --- # Priority Classes apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: high-priority value: 1000 globalDefault: false description: \u0026#34;High priority class for critical applications\u0026#34; --- apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: medium-priority value: 500 globalDefault: true description: \u0026#34;Medium priority class for standard applications\u0026#34; --- apiVersion: scheduling.k8s.io/v1 kind: PriorityClass metadata: name: low-priority value: 100 globalDefault: false description: \u0026#34;Low priority class for batch jobs\u0026#34; Horizontal Pod Autoscaler (HPA) Configuration # hpa-configuration.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: web-app-hpa namespace: production spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: web-app minReplicas: 3 maxReplicas: 50 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 - type: Pods pods: metric: name: http_requests_per_second target: type: AverageValue averageValue: \u0026#34;100\u0026#34; behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 10 periodSeconds: 60 - type: Pods value: 2 periodSeconds: 60 selectPolicy: Min scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 50 periodSeconds: 60 - type: Pods value: 5 periodSeconds: 60 selectPolicy: Max --- # Vertical Pod Autoscaler (VPA) apiVersion: autoscaling.k8s.io/v1 kind: VerticalPodAutoscaler metadata: name: web-app-vpa namespace: production spec: targetRef: apiVersion: apps/v1 kind: Deployment name: web-app updatePolicy: updateMode: \u0026#34;Auto\u0026#34; resourcePolicy: containerPolicies: - containerName: web-app maxAllowed: cpu: 2 memory: 4Gi minAllowed: cpu: 100m memory: 128Mi controlledResources: [\u0026#34;cpu\u0026#34;, \u0026#34;memory\u0026#34;] Storage Management Storage Classes and Persistent Volumes # storage-configuration.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast-ssd annotations: storageclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34; provisioner: ebs.csi.aws.com parameters: type: gp3 iops: \u0026#34;3000\u0026#34; throughput: \u0026#34;125\u0026#34; encrypted: \u0026#34;true\u0026#34; kmsKeyId: arn:aws:kms:us-west-2:123456789012:key/12345678-1234-1234-1234-123456789012 volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true reclaimPolicy: Delete --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: standard-hdd provisioner: ebs.csi.aws.com parameters: type: gp2 encrypted: \u0026#34;true\u0026#34; volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true reclaimPolicy: Delete --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: high-iops-ssd provisioner: ebs.csi.aws.com parameters: type: io2 iops: \u0026#34;10000\u0026#34; encrypted: \u0026#34;true\u0026#34; volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true reclaimPolicy: Retain --- # Volume Snapshot Class apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshotClass metadata: name: ebs-snapshot-class driver: ebs.csi.aws.com deletionPolicy: Delete parameters: tagSpecification_1: \u0026#34;Name=*\u0026#34; tagSpecification_2: \u0026#34;Environment=production\u0026#34; --- # Persistent Volume Claim Template apiVersion: v1 kind: PersistentVolumeClaim metadata: name: database-pvc namespace: production spec: accessModes: - ReadWriteOnce storageClassName: fast-ssd resources: requests: storage: 100Gi Backup and Disaster Recovery # backup-configuration.yaml apiVersion: v1 kind: ConfigMap metadata: name: backup-scripts namespace: production data: backup.sh: | #!/bin/bash set -e NAMESPACE=${NAMESPACE:-production} BACKUP_BUCKET=${BACKUP_BUCKET:-k8s-backups} DATE=$(date +%Y%m%d-%H%M%S) # Backup etcd echo \u0026#34;Backing up etcd...\u0026#34; kubectl exec -n kube-system etcd-master-1 -- \\ etcdctl snapshot save /tmp/etcd-backup-${DATE}.db \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key # Copy etcd backup to S3 kubectl cp kube-system/etcd-master-1:/tmp/etcd-backup-${DATE}.db ./etcd-backup-${DATE}.db aws s3 cp ./etcd-backup-${DATE}.db s3://${BACKUP_BUCKET}/etcd/ # Backup application data echo \u0026#34;Backing up application data...\u0026#34; kubectl get all,pvc,secrets,configmaps -n ${NAMESPACE} -o yaml \u0026gt; app-backup-${DATE}.yaml aws s3 cp app-backup-${DATE}.yaml s3://${BACKUP_BUCKET}/applications/ # Backup persistent volumes echo \u0026#34;Creating volume snapshots...\u0026#34; kubectl get pvc -n ${NAMESPACE} -o json | jq -r \u0026#39;.items[].metadata.name\u0026#39; | while read pvc; do cat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: snapshot.storage.k8s.io/v1 kind: VolumeSnapshot metadata: name: ${pvc}-snapshot-${DATE} namespace: ${NAMESPACE} spec: volumeSnapshotClassName: ebs-snapshot-class source: persistentVolumeClaimName: ${pvc} EOF done echo \u0026#34;Backup completed successfully\u0026#34; restore.sh: | #!/bin/bash set -e BACKUP_DATE=${1:-latest} NAMESPACE=${NAMESPACE:-production} BACKUP_BUCKET=${BACKUP_BUCKET:-k8s-backups} if [ \u0026#34;$BACKUP_DATE\u0026#34; = \u0026#34;latest\u0026#34; ]; then BACKUP_DATE=$(aws s3 ls s3://${BACKUP_BUCKET}/etcd/ | sort | tail -n 1 | awk \u0026#39;{print $4}\u0026#39; | sed \u0026#39;s/etcd-backup-\\(.*\\)\\.db/\\1/\u0026#39;) fi echo \u0026#34;Restoring from backup: $BACKUP_DATE\u0026#34; # Restore etcd (requires cluster shutdown) echo \u0026#34;Restoring etcd...\u0026#34; aws s3 cp s3://${BACKUP_BUCKET}/etcd/etcd-backup-${BACKUP_DATE}.db ./etcd-backup.db # Stop etcd on all masters sudo systemctl stop kubelet sudo docker stop $(sudo docker ps -q --filter name=k8s_etcd) # Restore etcd data sudo etcdctl snapshot restore ./etcd-backup.db \\ --data-dir=/var/lib/etcd-restore \\ --name=master-1 \\ --initial-cluster=master-1=https://10.0.1.10:2380,master-2=https://10.0.1.11:2380,master-3=https://10.0.1.12:2380 \\ --initial-advertise-peer-urls=https://10.0.1.10:2380 # Replace etcd data directory sudo rm -rf /var/lib/etcd sudo mv /var/lib/etcd-restore /var/lib/etcd sudo chown -R etcd:etcd /var/lib/etcd # Start kubelet sudo systemctl start kubelet # Restore application resources echo \u0026#34;Restoring application resources...\u0026#34; aws s3 cp s3://${BACKUP_BUCKET}/applications/app-backup-${BACKUP_DATE}.yaml ./app-backup.yaml kubectl apply -f app-backup.yaml echo \u0026#34;Restore completed successfully\u0026#34; --- apiVersion: batch/v1 kind: CronJob metadata: name: cluster-backup namespace: production spec: schedule: \u0026#34;0 2 * * *\u0026#34; # Daily at 2 AM jobTemplate: spec: template: spec: serviceAccountName: backup-service-account containers: - name: backup image: amazon/aws-cli:latest command: [\u0026#34;/bin/bash\u0026#34;] args: [\u0026#34;/scripts/backup.sh\u0026#34;] env: - name: NAMESPACE value: \u0026#34;production\u0026#34; - name: BACKUP_BUCKET value: \u0026#34;k8s-backups\u0026#34; volumeMounts: - name: backup-scripts mountPath: /scripts - name: kubectl-config mountPath: /root/.kube volumes: - name: backup-scripts configMap: name: backup-scripts defaultMode: 0755 - name: kubectl-config secret: secretName: kubectl-config restartPolicy: OnFailure Monitoring and Observability Prometheus Configuration # prometheus-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: prometheus-config namespace: monitoring data: prometheus.yml: | global: scrape_interval: 15s evaluation_interval: 15s external_labels: cluster: \u0026#39;production\u0026#39; region: \u0026#39;us-west-2\u0026#39; rule_files: - \u0026#34;/etc/prometheus/rules/*.yml\u0026#34; alerting: alertmanagers: - static_configs: - targets: - alertmanager:9093 scrape_configs: # Kubernetes API Server - job_name: \u0026#39;kubernetes-apiservers\u0026#39; kubernetes_sd_configs: - role: endpoints scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https # Kubernetes Nodes - job_name: \u0026#39;kubernetes-nodes\u0026#39; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics # Kubernetes Pods - job_name: \u0026#39;kubernetes-pods\u0026#39; kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name # cAdvisor - job_name: \u0026#39;kubernetes-cadvisor\u0026#39; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor alerts.yml: | groups: - name: kubernetes-alerts rules: - alert: KubernetesNodeReady expr: kube_node_status_condition{condition=\u0026#34;Ready\u0026#34;,status=\u0026#34;true\u0026#34;} == 0 for: 10m labels: severity: critical annotations: summary: Kubernetes Node not ready (instance {{ $labels.instance }}) description: \u0026#34;Node {{ $labels.node }} has been unready for a long time\u0026#34; - alert: KubernetesMemoryPressure expr: kube_node_status_condition{condition=\u0026#34;MemoryPressure\u0026#34;,status=\u0026#34;true\u0026#34;} == 1 for: 2m labels: severity: critical annotations: summary: Kubernetes memory pressure (instance {{ $labels.instance }}) description: \u0026#34;Node {{ $labels.node }} has MemoryPressure condition\u0026#34; - alert: KubernetesDiskPressure expr: kube_node_status_condition{condition=\u0026#34;DiskPressure\u0026#34;,status=\u0026#34;true\u0026#34;} == 1 for: 2m labels: severity: critical annotations: summary: Kubernetes disk pressure (instance {{ $labels.instance }}) description: \u0026#34;Node {{ $labels.node }} has DiskPressure condition\u0026#34; - alert: KubernetesOutOfDisk expr: kube_node_status_condition{condition=\u0026#34;OutOfDisk\u0026#34;,status=\u0026#34;true\u0026#34;} == 1 for: 2m labels: severity: critical annotations: summary: Kubernetes out of disk (instance {{ $labels.instance }}) description: \u0026#34;Node {{ $labels.node }} has OutOfDisk condition\u0026#34; - alert: KubernetesOutOfCapacity expr: sum by (node) ((kube_pod_status_phase{phase=\u0026#34;Running\u0026#34;} == 1) + on(uid) group_left(node) (0 * kube_pod_info{pod_template_hash=\u0026#34;\u0026#34;})) / sum by (node) (kube_node_status_allocatable{resource=\u0026#34;pods\u0026#34;}) * 100 \u0026gt; 90 for: 2m labels: severity: warning annotations: summary: Kubernetes out of capacity (instance {{ $labels.instance }}) description: \u0026#34;Node {{ $labels.node }} is out of capacity\u0026#34; - alert: KubernetesContainerOomKiller expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m \u0026gt;= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason=\u0026#34;OOMKilled\u0026#34;}[10m]) == 1 for: 0m labels: severity: warning annotations: summary: Kubernetes container oom killer (instance {{ $labels.instance }}) description: \u0026#34;Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled\u0026#34; - alert: KubernetesPodCrashLooping expr: increase(kube_pod_container_status_restarts_total[1m]) \u0026gt; 3 for: 2m labels: severity: warning annotations: summary: Kubernetes pod crash looping (instance {{ $labels.instance }}) description: \u0026#34;Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping\u0026#34; - alert: KubernetesReplicassetMismatch expr: kube_replicaset_spec_replicas != kube_replicaset_status_ready_replicas for: 10m labels: severity: warning annotations: summary: Kubernetes ReplicasSet mismatch (instance {{ $labels.instance }}) description: \u0026#34;Deployment Replicas mismatch\u0026#34; - alert: KubernetesDeploymentReplicasMismatch expr: kube_deployment_spec_replicas != kube_deployment_status_available_replicas for: 10m labels: severity: warning annotations: summary: Kubernetes Deployment replicas mismatch (instance {{ $labels.instance }}) description: \u0026#34;Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 10 minutes.\u0026#34; - alert: KubernetesStatefulsetReplicasMismatch expr: kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas for: 10m labels: severity: warning annotations: summary: Kubernetes StatefulSet replicas mismatch (instance {{ $labels.instance }}) description: \u0026#34;A StatefulSet does not match the expected number of replicas.\u0026#34; - alert: KubernetesHpaScalingAbility expr: kube_horizontalpodautoscaler_status_condition{status=\u0026#34;false\u0026#34;, condition=\u0026#34;AbleToScale\u0026#34;} == 1 for: 2m labels: severity: warning annotations: summary: Kubernetes HPA scaling ability (instance {{ $labels.instance }}) description: \u0026#34;Pod is unable to scale\u0026#34; - alert: KubernetesHpaMetricAvailability expr: kube_horizontalpodautoscaler_status_condition{status=\u0026#34;false\u0026#34;, condition=\u0026#34;ScalingActive\u0026#34;} == 1 for: 2m labels: severity: warning annotations: summary: Kubernetes HPA metric availability (instance {{ $labels.instance }}) description: \u0026#34;HPA is not able to collect metrics\u0026#34; - alert: KubernetesHpaScaleCapability expr: kube_horizontalpodautoscaler_status_desired_replicas \u0026gt;= kube_horizontalpodautoscaler_spec_max_replicas for: 2m labels: severity: info annotations: summary: Kubernetes HPA scale capability (instance {{ $labels.instance }}) description: \u0026#34;The maximum number of desired Pods has been hit\u0026#34; - alert: KubernetesPodNotHealthy expr: min_over_time(sum by (namespace, pod) (kube_pod_status_phase{phase=~\u0026#34;Pending|Unknown|Failed\u0026#34;})[15m:1m]) \u0026gt; 0 for: 0m labels: severity: critical annotations: summary: Kubernetes Pod not healthy (instance {{ $labels.instance }}) description: \u0026#34;Pod has been in a non-ready state for longer than 15 minutes.\u0026#34; - alert: KubernetesPodCrashLooping expr: increase(kube_pod_container_status_restarts_total[1m]) \u0026gt; 3 for: 2m labels: severity: warning annotations: summary: Kubernetes pod crash looping (instance {{ $labels.instance }}) description: \u0026#34;Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping\u0026#34; - alert: KubernetesVolumeOutOfDiskSpace expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 \u0026lt; 10 for: 2m labels: severity: warning annotations: summary: Kubernetes Volume out of disk space (instance {{ $labels.instance }}) description: \u0026#34;Volume is almost full (\u0026lt; 10% left)\u0026#34; - alert: KubernetesVolumeFullInFourDays expr: predict_linear(kubelet_volume_stats_available_bytes[6h], 4 * 24 * 3600) \u0026lt; 0 for: 0m labels: severity: critical annotations: summary: Kubernetes Volume full in four days (instance {{ $labels.instance }}) description: \u0026#34;{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is expected to fill up within four days. Currently {{ $value | humanize }}% is available.\u0026#34; - alert: KubernetesPersistentVolumeError expr: kube_persistentvolume_status_phase{phase=~\u0026#34;Failed|Pending\u0026#34;, job=\u0026#34;kube-state-metrics\u0026#34;} \u0026gt; 0 for: 0m labels: severity: critical annotations: summary: Kubernetes PersistentVolume error (instance {{ $labels.instance }}) description: \u0026#34;Persistent volume is in bad state\u0026#34; - alert: KubernetesStatefulsetDown expr: (kube_statefulset_status_replicas_ready / kube_statefulset_status_replicas_current) != 1 for: 1m labels: severity: critical annotations: summary: Kubernetes StatefulSet down (instance {{ $labels.instance }}) description: \u0026#34;A StatefulSet went down\u0026#34; - alert: KubernetesHpaReplicasMismatch expr: (kube_horizontalpodautoscaler_status_desired_replicas != kube_horizontalpodautoscaler_status_current_replicas) and (kube_horizontalpodautoscaler_status_current_replicas \u0026gt; kube_horizontalpodautoscaler_spec_min_replicas) for: 15m labels: severity: warning annotations: summary: Kubernetes HPA replicas mismatch (instance {{ $labels.instance }}) description: \u0026#34;HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} has not matched the desired number of replicas for longer than 15 minutes.\u0026#34; Operational Best Practices Cluster Maintenance Procedures #!/bin/bash # cluster-maintenance.sh set -e CLUSTER_NAME=\u0026#34;production-cluster\u0026#34; BACKUP_BUCKET=\u0026#34;k8s-backups\u0026#34; DATE=$(date +%Y%m%d-%H%M%S) # Pre-maintenance checks echo \u0026#34;=== Pre-maintenance checks ===\u0026#34; # Check cluster health kubectl get nodes kubectl get pods --all-namespaces | grep -v Running | grep -v Completed # Check resource usage kubectl top nodes kubectl top pods --all-namespaces --sort-by=cpu # Create backup echo \u0026#34;=== Creating backup ===\u0026#34; ./backup.sh # Drain nodes for maintenance (one by one) drain_node() { local node=$1 echo \u0026#34;Draining node: $node\u0026#34; # Cordon the node kubectl cordon $node # Drain the node kubectl drain $node --ignore-daemonsets --delete-emptydir-data --force --grace-period=300 echo \u0026#34;Node $node drained successfully\u0026#34; } # Uncordon node after maintenance uncordon_node() { local node=$1 echo \u0026#34;Uncordoning node: $node\u0026#34; kubectl uncordon $node # Wait for node to be ready kubectl wait --for=condition=Ready node/$node --timeout=300s echo \u0026#34;Node $node is ready\u0026#34; } # Rolling update procedure rolling_update() { local deployment=$1 local namespace=${2:-default} local image=$3 echo \u0026#34;Updating deployment $deployment in namespace $namespace\u0026#34; # Update the deployment kubectl set image deployment/$deployment container=$image -n $namespace # Wait for rollout to complete kubectl rollout status deployment/$deployment -n $namespace --timeout=600s # Verify the update kubectl get pods -n $namespace -l app=$deployment echo \u0026#34;Deployment $deployment updated successfully\u0026#34; } # Certificate rotation rotate_certificates() { echo \u0026#34;=== Rotating certificates ===\u0026#34; # Check certificate expiration kubeadm certs check-expiration # Renew certificates kubeadm certs renew all # Restart control plane components kubectl -n kube-system delete pod -l component=kube-apiserver kubectl -n kube-system delete pod -l component=kube-controller-manager kubectl -n kube-system delete pod -l component=kube-scheduler # Update kubeconfig sudo cp /etc/kubernetes/admin.conf ~/.kube/config sudo chown $(id -u):$(id -g) ~/.kube/config echo \u0026#34;Certificates rotated successfully\u0026#34; } # Cleanup old resources cleanup_resources() { echo \u0026#34;=== Cleaning up old resources ===\u0026#34; # Remove completed jobs older than 7 days kubectl get jobs --all-namespaces -o json | jq -r \u0026#39;.items[] | select(.status.conditions[]?.type == \u0026#34;Complete\u0026#34;) | select(.metadata.creationTimestamp | fromdateiso8601 \u0026lt; (now - 604800)) | \u0026#34;\\(.metadata.namespace) \\(.metadata.name)\u0026#34;\u0026#39; | while read namespace job; do kubectl delete job $job -n $namespace done # Remove old replica sets kubectl get rs --all-namespaces -o json | jq -r \u0026#39;.items[] | select(.spec.replicas == 0) | select(.metadata.creationTimestamp | fromdateiso8601 \u0026lt; (now - 604800)) | \u0026#34;\\(.metadata.namespace) \\(.metadata.name)\u0026#34;\u0026#39; | while read namespace rs; do kubectl delete rs $rs -n $namespace done # Remove old pods in Succeeded state kubectl get pods --all-namespaces --field-selector=status.phase=Succeeded -o json | jq -r \u0026#39;.items[] | select(.metadata.creationTimestamp | fromdateiso8601 \u0026lt; (now - 86400)) | \u0026#34;\\(.metadata.namespace) \\(.metadata.name)\u0026#34;\u0026#39; | while read namespace pod; do kubectl delete pod $pod -n $namespace done echo \u0026#34;Cleanup completed\u0026#34; } # Performance optimization optimize_performance() { echo \u0026#34;=== Performance optimization ===\u0026#34; # Compact etcd kubectl -n kube-system exec etcd-master-1 -- etcdctl \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ compact $(kubectl -n kube-system exec etcd-master-1 -- etcdctl \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ endpoint status --write-out=\u0026#34;json\u0026#34; | jq -r \u0026#39;.[0].Status.header.revision\u0026#39;) # Defragment etcd kubectl -n kube-system exec etcd-master-1 -- etcdctl \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/kubernetes/pki/etcd/ca.crt \\ --cert=/etc/kubernetes/pki/etcd/server.crt \\ --key=/etc/kubernetes/pki/etcd/server.key \\ defrag echo \u0026#34;Performance optimization completed\u0026#34; } # Main maintenance function main() { case \u0026#34;$1\u0026#34; in \u0026#34;drain\u0026#34;) drain_node $2 ;; \u0026#34;uncordon\u0026#34;) uncordon_node $2 ;; \u0026#34;update\u0026#34;) rolling_update $2 $3 $4 ;; \u0026#34;certs\u0026#34;) rotate_certificates ;; \u0026#34;cleanup\u0026#34;) cleanup_resources ;; \u0026#34;optimize\u0026#34;) optimize_performance ;; \u0026#34;full\u0026#34;) echo \u0026#34;Starting full maintenance procedure...\u0026#34; cleanup_resources optimize_performance rotate_certificates echo \u0026#34;Full maintenance completed\u0026#34; ;; *) echo \u0026#34;Usage: $0 {drain|uncordon|update|certs|cleanup|optimize|full} [args...]\u0026#34; echo \u0026#34; drain \u0026lt;node\u0026gt; - Drain a node for maintenance\u0026#34; echo \u0026#34; uncordon \u0026lt;node\u0026gt; - Uncordon a node after maintenance\u0026#34; echo \u0026#34; update \u0026lt;deployment\u0026gt; \u0026lt;ns\u0026gt; \u0026lt;img\u0026gt; - Rolling update deployment\u0026#34; echo \u0026#34; certs - Rotate certificates\u0026#34; echo \u0026#34; cleanup - Cleanup old resources\u0026#34; echo \u0026#34; optimize - Optimize cluster performance\u0026#34; echo \u0026#34; full - Run full maintenance procedure\u0026#34; exit 1 ;; esac } main \u0026#34;$@\u0026#34; Conclusion Running Kubernetes in production requires a comprehensive approach covering architecture design, security hardening, resource management, monitoring, and operational procedures. The configurations and practices outlined in this guide provide a solid foundation for enterprise-grade Kubernetes deployments.\n","content":"Kubernetes Production Best Practices: Building Enterprise-Grade Container Orchestration Running Kubernetes in production requires careful planning, robust security measures, and comprehensive operational practices. This guide covers the essential aspects of building and maintaining enterprise-grade Kubernetes clusters that can handle mission-critical workloads with high availability, security, and performance.\nProduction Cluster Architecture Multi-Master High Availability Setup # …","date":"2025-12-31","lastmod":"2025-12-31","tags":["Kubernetes","Production","Container Orchestration","Security","Monitoring","High Availability","Disaster Recovery"],"categories":["DevOps"],"author":"Kubernetes Platform Engineer","readingTime":16,"wordCount":3342,"section":"posts","type":"posts","draft":false,"featured":false,"series":["Container Orchestration"]},{"title":"Microservices Architecture Design and Implementation: From Monolith to Distributed Systems","url":"https://www.dishuihengxin.com/en/posts/devops-microservices-architecture/","summary":"Microservices Architecture Design and Implementation: From Monolith to Distributed Systems Microservices architecture has become the de facto standard for building scalable, maintainable, and resilient enterprise applications. This comprehensive guide explores the journey from monolithic applications to distributed microservices, covering design principles, implementation strategies, and operational best practices.\nArchitecture Overview Microservices vs Monolithic Architecture graph TB subgraph \u0026#34;Monolithic Architecture\u0026#34; M1[User Interface] M2[Business Logic] M3[Data Access Layer] M4[Database] M1 --\u0026gt; M2 M2 --\u0026gt; M3 M3 --\u0026gt; M4 end subgraph \u0026#34;Microservices Architecture\u0026#34; subgraph \u0026#34;API Gateway\u0026#34; AG[Load Balancer \u0026amp; Routing] end subgraph \u0026#34;User Service\u0026#34; US1[User API] US2[User Logic] US3[User DB] end subgraph \u0026#34;Order Service\u0026#34; OS1[Order API] OS2[Order Logic] OS3[Order DB] end subgraph \u0026#34;Payment Service\u0026#34; PS1[Payment API] PS2[Payment Logic] PS3[Payment DB] end subgraph \u0026#34;Notification Service\u0026#34; NS1[Notification API] NS2[Message Queue] NS3[Email/SMS] end AG --\u0026gt; US1 AG --\u0026gt; OS1 AG --\u0026gt; PS1 AG --\u0026gt; NS1 OS2 --\u0026gt; US1 OS2 --\u0026gt; PS1 PS2 --\u0026gt; NS1 end Core Design Principles Single Responsibility: Each service owns a specific business capability Decentralized: Services manage their own data and business logic Fault Isolation: Failure in one service doesn\u0026rsquo;t cascade to others Technology Diversity: Services can use different tech stacks Independent Deployment: Services can be deployed independently Service Decomposition Strategy Domain-Driven Design (DDD) Approach // Domain model example for User Service package user import ( \u0026#34;time\u0026#34; \u0026#34;errors\u0026#34; ) // User aggregate root type User struct { ID string `json:\u0026#34;id\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` Username string `json:\u0026#34;username\u0026#34;` Profile Profile `json:\u0026#34;profile\u0026#34;` CreatedAt time.Time `json:\u0026#34;created_at\u0026#34;` UpdatedAt time.Time `json:\u0026#34;updated_at\u0026#34;` Version int `json:\u0026#34;version\u0026#34;` } type Profile struct { FirstName string `json:\u0026#34;first_name\u0026#34;` LastName string `json:\u0026#34;last_name\u0026#34;` Avatar string `json:\u0026#34;avatar\u0026#34;` Bio string `json:\u0026#34;bio\u0026#34;` } // Domain events type UserCreatedEvent struct { UserID string `json:\u0026#34;user_id\u0026#34;` Email string `json:\u0026#34;email\u0026#34;` Timestamp time.Time `json:\u0026#34;timestamp\u0026#34;` } type UserUpdatedEvent struct { UserID string `json:\u0026#34;user_id\u0026#34;` Changes map[string]interface{} `json:\u0026#34;changes\u0026#34;` Timestamp time.Time `json:\u0026#34;timestamp\u0026#34;` } // Repository interface type UserRepository interface { Create(user *User) error GetByID(id string) (*User, error) GetByEmail(email string) (*User, error) Update(user *User) error Delete(id string) error } // Domain service type UserService struct { repo UserRepository eventBus EventBus validator UserValidator } func (s *UserService) CreateUser(email, username string, profile Profile) (*User, error) { // Validate input if err := s.validator.ValidateEmail(email); err != nil { return nil, err } if err := s.validator.ValidateUsername(username); err != nil { return nil, err } // Check if user already exists existing, _ := s.repo.GetByEmail(email) if existing != nil { return nil, errors.New(\u0026#34;user already exists\u0026#34;) } // Create user user := \u0026amp;User{ ID: generateID(), Email: email, Username: username, Profile: profile, CreatedAt: time.Now(), UpdatedAt: time.Now(), Version: 1, } if err := s.repo.Create(user); err != nil { return nil, err } // Publish domain event event := UserCreatedEvent{ UserID: user.ID, Email: user.Email, Timestamp: time.Now(), } s.eventBus.Publish(\u0026#34;user.created\u0026#34;, event) return user, nil } func (s *UserService) UpdateUser(id string, updates map[string]interface{}) (*User, error) { user, err := s.repo.GetByID(id) if err != nil { return nil, err } if user == nil { return nil, errors.New(\u0026#34;user not found\u0026#34;) } // Apply updates with validation if email, ok := updates[\u0026#34;email\u0026#34;].(string); ok { if err := s.validator.ValidateEmail(email); err != nil { return nil, err } user.Email = email } if username, ok := updates[\u0026#34;username\u0026#34;].(string); ok { if err := s.validator.ValidateUsername(username); err != nil { return nil, err } user.Username = username } user.UpdatedAt = time.Now() user.Version++ if err := s.repo.Update(user); err != nil { return nil, err } // Publish domain event event := UserUpdatedEvent{ UserID: user.ID, Changes: updates, Timestamp: time.Now(), } s.eventBus.Publish(\u0026#34;user.updated\u0026#34;, event) return user, nil } Service Boundaries Identification # Service decomposition matrix services: user-service: responsibilities: - User registration and authentication - Profile management - User preferences data: - users - user_profiles - user_preferences apis: - POST /users - GET /users/{id} - PUT /users/{id} - DELETE /users/{id} order-service: responsibilities: - Order creation and management - Order status tracking - Order history data: - orders - order_items - order_status_history apis: - POST /orders - GET /orders/{id} - PUT /orders/{id}/status - GET /users/{id}/orders dependencies: - user-service (user validation) - inventory-service (stock check) - payment-service (payment processing) payment-service: responsibilities: - Payment processing - Payment method management - Transaction history data: - payments - payment_methods - transactions apis: - POST /payments - GET /payments/{id} - POST /payment-methods - GET /users/{id}/payment-methods dependencies: - user-service (user validation) - external payment gateways inventory-service: responsibilities: - Product catalog management - Stock management - Price management data: - products - inventory - pricing apis: - GET /products - GET /products/{id} - PUT /products/{id}/stock - GET /products/{id}/availability Communication Patterns Synchronous Communication (REST APIs) // API Gateway implementation package gateway import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;net/http/httputil\u0026#34; \u0026#34;net/url\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/gorilla/mux\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; ) type Gateway struct { router *mux.Router services map[string]*ServiceConfig circuitBreaker CircuitBreaker rateLimiter RateLimiter metrics *Metrics } type ServiceConfig struct { Name string BaseURL string Timeout time.Duration Retries int HealthCheck string } type Metrics struct { RequestsTotal prometheus.CounterVec RequestDuration prometheus.HistogramVec ErrorsTotal prometheus.CounterVec } func NewGateway() *Gateway { return \u0026amp;Gateway{ router: mux.NewRouter(), services: make(map[string]*ServiceConfig), metrics: initMetrics(), } } func (g *Gateway) RegisterService(name string, config *ServiceConfig) { g.services[name] = config // Register routes for the service serviceRouter := g.router.PathPrefix(fmt.Sprintf(\u0026#34;/%s\u0026#34;, name)).Subrouter() serviceRouter.PathPrefix(\u0026#34;/\u0026#34;).HandlerFunc(g.proxyHandler(name)) } func (g *Gateway) proxyHandler(serviceName string) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { start := time.Now() // Rate limiting if !g.rateLimiter.Allow(r) { http.Error(w, \u0026#34;Rate limit exceeded\u0026#34;, http.StatusTooManyRequests) return } service, exists := g.services[serviceName] if !exists { http.Error(w, \u0026#34;Service not found\u0026#34;, http.StatusNotFound) return } // Circuit breaker check if !g.circuitBreaker.Allow(serviceName) { http.Error(w, \u0026#34;Service unavailable\u0026#34;, http.StatusServiceUnavailable) return } // Create proxy target, _ := url.Parse(service.BaseURL) proxy := httputil.NewSingleHostReverseProxy(target) // Add timeout ctx, cancel := context.WithTimeout(r.Context(), service.Timeout) defer cancel() r = r.WithContext(ctx) // Add headers r.Header.Set(\u0026#34;X-Forwarded-Host\u0026#34;, r.Header.Get(\u0026#34;Host\u0026#34;)) r.Header.Set(\u0026#34;X-Origin-Host\u0026#34;, target.Host) // Proxy the request proxy.ServeHTTP(w, r) // Record metrics duration := time.Since(start) g.metrics.RequestsTotal.WithLabelValues(serviceName, r.Method).Inc() g.metrics.RequestDuration.WithLabelValues(serviceName, r.Method).Observe(duration.Seconds()) } } // Circuit breaker implementation type CircuitBreaker interface { Allow(service string) bool RecordSuccess(service string) RecordFailure(service string) } type circuitBreaker struct { states map[string]*CircuitState } type CircuitState struct { State string // CLOSED, OPEN, HALF_OPEN FailureCount int LastFailure time.Time Threshold int Timeout time.Duration } func (cb *circuitBreaker) Allow(service string) bool { state, exists := cb.states[service] if !exists { cb.states[service] = \u0026amp;CircuitState{ State: \u0026#34;CLOSED\u0026#34;, Threshold: 5, Timeout: 30 * time.Second, } return true } switch state.State { case \u0026#34;CLOSED\u0026#34;: return true case \u0026#34;OPEN\u0026#34;: if time.Since(state.LastFailure) \u0026gt; state.Timeout { state.State = \u0026#34;HALF_OPEN\u0026#34; return true } return false case \u0026#34;HALF_OPEN\u0026#34;: return true default: return false } } Asynchronous Communication (Event-Driven) // Event bus implementation using NATS package eventbus import ( \u0026#34;encoding/json\u0026#34; \u0026#34;log\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/nats-io/nats.go\u0026#34; ) type EventBus struct { conn *nats.Conn } type Event struct { ID string `json:\u0026#34;id\u0026#34;` Type string `json:\u0026#34;type\u0026#34;` Source string `json:\u0026#34;source\u0026#34;` Data map[string]interface{} `json:\u0026#34;data\u0026#34;` Timestamp time.Time `json:\u0026#34;timestamp\u0026#34;` Version string `json:\u0026#34;version\u0026#34;` } func NewEventBus(natsURL string) (*EventBus, error) { conn, err := nats.Connect(natsURL) if err != nil { return nil, err } return \u0026amp;EventBus{conn: conn}, nil } func (eb *EventBus) Publish(eventType string, data interface{}) error { event := Event{ ID: generateEventID(), Type: eventType, Source: getServiceName(), Data: convertToMap(data), Timestamp: time.Now(), Version: \u0026#34;1.0\u0026#34;, } eventData, err := json.Marshal(event) if err != nil { return err } return eb.conn.Publish(eventType, eventData) } func (eb *EventBus) Subscribe(eventType string, handler func(Event)) error { _, err := eb.conn.Subscribe(eventType, func(msg *nats.Msg) { var event Event if err := json.Unmarshal(msg.Data, \u0026amp;event); err != nil { log.Printf(\u0026#34;Failed to unmarshal event: %v\u0026#34;, err) return } handler(event) }) return err } // Saga pattern implementation for distributed transactions type SagaOrchestrator struct { eventBus *EventBus steps []SagaStep } type SagaStep struct { Name string Action func(data map[string]interface{}) error Compensation func(data map[string]interface{}) error } func (so *SagaOrchestrator) Execute(sagaID string, data map[string]interface{}) error { executedSteps := []int{} for i, step := range so.steps { if err := step.Action(data); err != nil { // Compensate executed steps in reverse order for j := len(executedSteps) - 1; j \u0026gt;= 0; j-- { stepIndex := executedSteps[j] if compErr := so.steps[stepIndex].Compensation(data); compErr != nil { log.Printf(\u0026#34;Compensation failed for step %s: %v\u0026#34;, so.steps[stepIndex].Name, compErr) } } return err } executedSteps = append(executedSteps, i) } return nil } // Order processing saga example func NewOrderProcessingSaga(eventBus *EventBus) *SagaOrchestrator { return \u0026amp;SagaOrchestrator{ eventBus: eventBus, steps: []SagaStep{ { Name: \u0026#34;ValidateUser\u0026#34;, Action: func(data map[string]interface{}) error { userID := data[\u0026#34;user_id\u0026#34;].(string) return validateUser(userID) }, Compensation: func(data map[string]interface{}) error { // No compensation needed for validation return nil }, }, { Name: \u0026#34;ReserveInventory\u0026#34;, Action: func(data map[string]interface{}) error { orderItems := data[\u0026#34;items\u0026#34;].([]interface{}) return reserveInventory(orderItems) }, Compensation: func(data map[string]interface{}) error { orderItems := data[\u0026#34;items\u0026#34;].([]interface{}) return releaseInventory(orderItems) }, }, { Name: \u0026#34;ProcessPayment\u0026#34;, Action: func(data map[string]interface{}) error { amount := data[\u0026#34;amount\u0026#34;].(float64) paymentMethod := data[\u0026#34;payment_method\u0026#34;].(string) return processPayment(amount, paymentMethod) }, Compensation: func(data map[string]interface{}) error { transactionID := data[\u0026#34;transaction_id\u0026#34;].(string) return refundPayment(transactionID) }, }, { Name: \u0026#34;CreateOrder\u0026#34;, Action: func(data map[string]interface{}) error { return createOrder(data) }, Compensation: func(data map[string]interface{}) error { orderID := data[\u0026#34;order_id\u0026#34;].(string) return cancelOrder(orderID) }, }, }, } } Data Management Strategies Database per Service Pattern # Docker Compose for multi-database setup version: \u0026#39;3.8\u0026#39; services: user-db: image: postgres:15 environment: POSTGRES_DB: userdb POSTGRES_USER: userservice POSTGRES_PASSWORD: userpass volumes: - user-db-data:/var/lib/postgresql/data ports: - \u0026#34;5432:5432\u0026#34; networks: - user-network order-db: image: postgres:15 environment: POSTGRES_DB: orderdb POSTGRES_USER: orderservice POSTGRES_PASSWORD: orderpass volumes: - order-db-data:/var/lib/postgresql/data ports: - \u0026#34;5433:5432\u0026#34; networks: - order-network payment-db: image: postgres:15 environment: POSTGRES_DB: paymentdb POSTGRES_USER: paymentservice POSTGRES_PASSWORD: paymentpass volumes: - payment-db-data:/var/lib/postgresql/data ports: - \u0026#34;5434:5432\u0026#34; networks: - payment-network inventory-db: image: mongodb:6.0 environment: MONGO_INITDB_ROOT_USERNAME: inventoryservice MONGO_INITDB_ROOT_PASSWORD: inventorypass MONGO_INITDB_DATABASE: inventorydb volumes: - inventory-db-data:/data/db ports: - \u0026#34;27017:27017\u0026#34; networks: - inventory-network cache: image: redis:7-alpine ports: - \u0026#34;6379:6379\u0026#34; volumes: - cache-data:/data networks: - shared-network volumes: user-db-data: order-db-data: payment-db-data: inventory-db-data: cache-data: networks: user-network: order-network: payment-network: inventory-network: shared-network: Event Sourcing Implementation // Event sourcing for order aggregate package order import ( \u0026#34;encoding/json\u0026#34; \u0026#34;time\u0026#34; ) type OrderAggregate struct { ID string Version int Events []DomainEvent State OrderState } type OrderState struct { ID string UserID string Items []OrderItem Status string TotalAmount float64 CreatedAt time.Time UpdatedAt time.Time } type OrderItem struct { ProductID string Quantity int Price float64 } type DomainEvent interface { GetEventType() string GetAggregateID() string GetVersion() int GetTimestamp() time.Time } type OrderCreatedEvent struct { AggregateID string `json:\u0026#34;aggregate_id\u0026#34;` Version int `json:\u0026#34;version\u0026#34;` Timestamp time.Time `json:\u0026#34;timestamp\u0026#34;` UserID string `json:\u0026#34;user_id\u0026#34;` Items []OrderItem `json:\u0026#34;items\u0026#34;` TotalAmount float64 `json:\u0026#34;total_amount\u0026#34;` } func (e OrderCreatedEvent) GetEventType() string { return \u0026#34;OrderCreated\u0026#34; } func (e OrderCreatedEvent) GetAggregateID() string { return e.AggregateID } func (e OrderCreatedEvent) GetVersion() int { return e.Version } func (e OrderCreatedEvent) GetTimestamp() time.Time { return e.Timestamp } type OrderStatusChangedEvent struct { AggregateID string `json:\u0026#34;aggregate_id\u0026#34;` Version int `json:\u0026#34;version\u0026#34;` Timestamp time.Time `json:\u0026#34;timestamp\u0026#34;` OldStatus string `json:\u0026#34;old_status\u0026#34;` NewStatus string `json:\u0026#34;new_status\u0026#34;` Reason string `json:\u0026#34;reason\u0026#34;` } func (e OrderStatusChangedEvent) GetEventType() string { return \u0026#34;OrderStatusChanged\u0026#34; } func (e OrderStatusChangedEvent) GetAggregateID() string { return e.AggregateID } func (e OrderStatusChangedEvent) GetVersion() int { return e.Version } func (e OrderStatusChangedEvent) GetTimestamp() time.Time { return e.Timestamp } // Event store interface type EventStore interface { SaveEvents(aggregateID string, events []DomainEvent, expectedVersion int) error GetEvents(aggregateID string) ([]DomainEvent, error) GetEventsFromVersion(aggregateID string, version int) ([]DomainEvent, error) } // Order aggregate methods func (o *OrderAggregate) CreateOrder(userID string, items []OrderItem) error { if o.State.Status != \u0026#34;\u0026#34; { return errors.New(\u0026#34;order already exists\u0026#34;) } totalAmount := 0.0 for _, item := range items { totalAmount += item.Price * float64(item.Quantity) } event := OrderCreatedEvent{ AggregateID: o.ID, Version: o.Version + 1, Timestamp: time.Now(), UserID: userID, Items: items, TotalAmount: totalAmount, } o.applyEvent(event) o.Events = append(o.Events, event) return nil } func (o *OrderAggregate) ChangeStatus(newStatus, reason string) error { if o.State.Status == newStatus { return errors.New(\u0026#34;status is already \u0026#34; + newStatus) } event := OrderStatusChangedEvent{ AggregateID: o.ID, Version: o.Version + 1, Timestamp: time.Now(), OldStatus: o.State.Status, NewStatus: newStatus, Reason: reason, } o.applyEvent(event) o.Events = append(o.Events, event) return nil } func (o *OrderAggregate) applyEvent(event DomainEvent) { switch e := event.(type) { case OrderCreatedEvent: o.State.ID = e.AggregateID o.State.UserID = e.UserID o.State.Items = e.Items o.State.Status = \u0026#34;CREATED\u0026#34; o.State.TotalAmount = e.TotalAmount o.State.CreatedAt = e.Timestamp o.State.UpdatedAt = e.Timestamp case OrderStatusChangedEvent: o.State.Status = e.NewStatus o.State.UpdatedAt = e.Timestamp } o.Version = event.GetVersion() } func (o *OrderAggregate) LoadFromHistory(events []DomainEvent) { for _, event := range events { o.applyEvent(event) } o.Events = []DomainEvent{} // Clear uncommitted events } // Repository implementation type OrderRepository struct { eventStore EventStore } func (r *OrderRepository) Save(aggregate *OrderAggregate) error { if len(aggregate.Events) == 0 { return nil } expectedVersion := aggregate.Version - len(aggregate.Events) return r.eventStore.SaveEvents(aggregate.ID, aggregate.Events, expectedVersion) } func (r *OrderRepository) GetByID(id string) (*OrderAggregate, error) { events, err := r.eventStore.GetEvents(id) if err != nil { return nil, err } if len(events) == 0 { return nil, errors.New(\u0026#34;order not found\u0026#34;) } aggregate := \u0026amp;OrderAggregate{ID: id} aggregate.LoadFromHistory(events) return aggregate, nil } Service Mesh Implementation Istio Configuration # Istio service mesh configuration apiVersion: networking.istio.io/v1beta1 kind: Gateway metadata: name: microservices-gateway namespace: production spec: selector: istio: ingressgateway servers: - port: number: 80 name: http protocol: HTTP hosts: - api.company.com tls: httpsRedirect: true - port: number: 443 name: https protocol: HTTPS tls: mode: SIMPLE credentialName: api-tls-secret hosts: - api.company.com --- apiVersion: networking.istio.io/v1beta1 kind: VirtualService metadata: name: microservices-routes namespace: production spec: hosts: - api.company.com gateways: - microservices-gateway http: - match: - uri: prefix: /api/v1/users route: - destination: host: user-service port: number: 80 timeout: 30s retries: attempts: 3 perTryTimeout: 10s - match: - uri: prefix: /api/v1/orders route: - destination: host: order-service port: number: 80 timeout: 60s retries: attempts: 3 perTryTimeout: 20s - match: - uri: prefix: /api/v1/payments route: - destination: host: payment-service port: number: 80 timeout: 45s retries: attempts: 2 perTryTimeout: 15s --- apiVersion: networking.istio.io/v1beta1 kind: DestinationRule metadata: name: user-service-destination namespace: production spec: host: user-service trafficPolicy: connectionPool: tcp: maxConnections: 100 http: http1MaxPendingRequests: 50 maxRequestsPerConnection: 10 loadBalancer: simple: LEAST_CONN outlierDetection: consecutiveErrors: 3 interval: 30s baseEjectionTime: 30s maxEjectionPercent: 50 --- apiVersion: security.istio.io/v1beta1 kind: PeerAuthentication metadata: name: default namespace: production spec: mtls: mode: STRICT --- apiVersion: security.istio.io/v1beta1 kind: AuthorizationPolicy metadata: name: user-service-authz namespace: production spec: selector: matchLabels: app: user-service rules: - from: - source: principals: [\u0026#34;cluster.local/ns/production/sa/api-gateway\u0026#34;] - source: principals: [\u0026#34;cluster.local/ns/production/sa/order-service\u0026#34;] to: - operation: methods: [\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;] paths: [\u0026#34;/api/v1/users/*\u0026#34;] Monitoring and Observability Distributed Tracing with Jaeger // Tracing middleware package tracing import ( \u0026#34;context\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;github.com/opentracing/opentracing-go\u0026#34; \u0026#34;github.com/opentracing/opentracing-go/ext\u0026#34; \u0026#34;github.com/uber/jaeger-client-go\u0026#34; \u0026#34;github.com/uber/jaeger-client-go/config\u0026#34; ) func InitTracer(serviceName string) (opentracing.Tracer, error) { cfg := config.Configuration{ ServiceName: serviceName, Sampler: \u0026amp;config.SamplerConfig{ Type: jaeger.SamplerTypeConst, Param: 1, }, Reporter: \u0026amp;config.ReporterConfig{ LogSpans: true, LocalAgentHostPort: \u0026#34;jaeger-agent:6831\u0026#34;, }, } tracer, _, err := cfg.NewTracer() if err != nil { return nil, err } opentracing.SetGlobalTracer(tracer) return tracer, nil } func TracingMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { spanCtx, _ := opentracing.GlobalTracer().Extract( opentracing.HTTPHeaders, opentracing.HTTPHeadersCarrier(r.Header), ) span := opentracing.GlobalTracer().StartSpan( r.URL.Path, ext.RPCServerOption(spanCtx), ) defer span.Finish() ext.HTTPMethod.Set(span, r.Method) ext.HTTPUrl.Set(span, r.URL.String()) ctx := opentracing.ContextWithSpan(r.Context(), span) r = r.WithContext(ctx) next.ServeHTTP(w, r) }) } func TraceHTTPClient(client *http.Client) *http.Client { client.Transport = \u0026amp;tracingTransport{ RoundTripper: client.Transport, } return client } type tracingTransport struct { http.RoundTripper } func (t *tracingTransport) RoundTrip(req *http.Request) (*http.Response, error) { span, ctx := opentracing.StartSpanFromContext( req.Context(), \u0026#34;HTTP \u0026#34;+req.Method, ) defer span.Finish() ext.HTTPMethod.Set(span, req.Method) ext.HTTPUrl.Set(span, req.URL.String()) ext.SpanKindRPCClient.Set(span) opentracing.GlobalTracer().Inject( span.Context(), opentracing.HTTPHeaders, opentracing.HTTPHeadersCarrier(req.Header), ) req = req.WithContext(ctx) resp, err := t.RoundTripper.RoundTrip(req) if err != nil { ext.Error.Set(span, true) span.SetTag(\u0026#34;error.message\u0026#34;, err.Error()) } else { ext.HTTPStatusCode.Set(span, uint16(resp.StatusCode)) if resp.StatusCode \u0026gt;= 400 { ext.Error.Set(span, true) } } return resp, err } Prometheus Metrics // Metrics collection package metrics import ( \u0026#34;net/http\u0026#34; \u0026#34;strconv\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; ) var ( httpRequestsTotal = prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \u0026#34;http_requests_total\u0026#34;, Help: \u0026#34;Total number of HTTP requests\u0026#34;, }, []string{\u0026#34;method\u0026#34;, \u0026#34;endpoint\u0026#34;, \u0026#34;status_code\u0026#34;, \u0026#34;service\u0026#34;}, ) httpRequestDuration = prometheus.NewHistogramVec( prometheus.HistogramOpts{ Name: \u0026#34;http_request_duration_seconds\u0026#34;, Help: \u0026#34;HTTP request duration in seconds\u0026#34;, Buckets: prometheus.DefBuckets, }, []string{\u0026#34;method\u0026#34;, \u0026#34;endpoint\u0026#34;, \u0026#34;service\u0026#34;}, ) businessMetrics = prometheus.NewCounterVec( prometheus.CounterOpts{ Name: \u0026#34;business_events_total\u0026#34;, Help: \u0026#34;Total number of business events\u0026#34;, }, []string{\u0026#34;event_type\u0026#34;, \u0026#34;service\u0026#34;}, ) activeConnections = prometheus.NewGaugeVec( prometheus.GaugeOpts{ Name: \u0026#34;active_connections\u0026#34;, Help: \u0026#34;Number of active connections\u0026#34;, }, []string{\u0026#34;service\u0026#34;}, ) ) func init() { prometheus.MustRegister(httpRequestsTotal) prometheus.MustRegister(httpRequestDuration) prometheus.MustRegister(businessMetrics) prometheus.MustRegister(activeConnections) } func MetricsMiddleware(serviceName string) func(http.Handler) http.Handler { return func(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { start := time.Now() // Wrap response writer to capture status code wrapped := \u0026amp;responseWriter{ResponseWriter: w, statusCode: 200} next.ServeHTTP(wrapped, r) duration := time.Since(start) statusCode := strconv.Itoa(wrapped.statusCode) httpRequestsTotal.WithLabelValues( r.Method, r.URL.Path, statusCode, serviceName, ).Inc() httpRequestDuration.WithLabelValues( r.Method, r.URL.Path, serviceName, ).Observe(duration.Seconds()) }) } } type responseWriter struct { http.ResponseWriter statusCode int } func (rw *responseWriter) WriteHeader(code int) { rw.statusCode = code rw.ResponseWriter.WriteHeader(code) } func RecordBusinessEvent(eventType, serviceName string) { businessMetrics.WithLabelValues(eventType, serviceName).Inc() } func SetActiveConnections(count float64, serviceName string) { activeConnections.WithLabelValues(serviceName).Set(count) } func MetricsHandler() http.Handler { return promhttp.Handler() } Deployment and Operations Kubernetes Deployment # Complete microservice deployment apiVersion: apps/v1 kind: Deployment metadata: name: user-service namespace: production labels: app: user-service version: v1.0.0 spec: replicas: 3 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 selector: matchLabels: app: user-service template: metadata: labels: app: user-service version: v1.0.0 annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/port: \u0026#34;8080\u0026#34; prometheus.io/path: \u0026#34;/metrics\u0026#34; sidecar.istio.io/inject: \u0026#34;true\u0026#34; spec: serviceAccountName: user-service containers: - name: user-service image: myregistry.com/user-service:v1.0.0 ports: - containerPort: 8080 name: http - containerPort: 8081 name: grpc env: - name: DATABASE_URL valueFrom: secretKeyRef: name: user-service-secrets key: database-url - name: JAEGER_AGENT_HOST valueFrom: fieldRef: fieldPath: status.hostIP - name: SERVICE_NAME value: \u0026#34;user-service\u0026#34; resources: requests: memory: \u0026#34;256Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;512Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; livenessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 30 periodSeconds: 10 readinessProbe: httpGet: path: /ready port: 8080 initialDelaySeconds: 5 periodSeconds: 5 volumeMounts: - name: config mountPath: /app/config readOnly: true volumes: - name: config configMap: name: user-service-config --- apiVersion: v1 kind: Service metadata: name: user-service namespace: production labels: app: user-service spec: ports: - port: 80 targetPort: 8080 name: http - port: 8081 targetPort: 8081 name: grpc selector: app: user-service --- apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: user-service-hpa namespace: production spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: user-service minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 Best Practices and Lessons Learned 1. Start Small and Evolve Begin with a modular monolith Extract services gradually based on business needs Use domain boundaries to guide decomposition 2. Design for Failure Implement circuit breakers and timeouts Use bulkhead patterns to isolate failures Plan for eventual consistency 3. Observability First Implement distributed tracing from day one Use structured logging with correlation IDs Monitor business metrics, not just technical ones 4. Data Consistency Strategies Embrace eventual consistency where possible Use saga patterns for distributed transactions Implement proper event sourcing for audit trails 5. Security Considerations Implement zero-trust networking Use service-to-service authentication Encrypt data in transit and at rest Conclusion Microservices architecture offers significant benefits in terms of scalability, maintainability, and team autonomy. However, it also introduces complexity in areas such as distributed systems management, data consistency, and operational overhead.\n","content":"Microservices Architecture Design and Implementation: From Monolith to Distributed Systems Microservices architecture has become the de facto standard for building scalable, maintainable, and resilient enterprise applications. This comprehensive guide explores the journey from monolithic applications to distributed microservices, covering design principles, implementation strategies, and operational best practices.\nArchitecture Overview Microservices vs Monolithic Architecture graph TB subgraph …","date":"2025-12-31","lastmod":"2025-12-31","tags":["Microservices","Architecture","Distributed Systems","API Gateway","Service Mesh","Docker","Kubernetes"],"categories":["DevOps"],"author":"Microservices Architect","readingTime":15,"wordCount":3054,"section":"posts","type":"posts","draft":false,"featured":false,"series":["Enterprise Architecture"]},{"title":"Performance Optimization for Cloud-Native Applications: From Profiling to Production Tuning","url":"https://www.dishuihengxin.com/en/posts/devops-performance-optimization/","summary":"Performance Optimization for Cloud-Native Applications: From Profiling to Production Tuning Performance optimization is a critical aspect of running successful cloud-native applications at scale. This comprehensive guide covers systematic approaches to identifying performance bottlenecks, implementing optimization strategies, and maintaining optimal performance in production environments.\nPerformance Optimization Methodology Performance Engineering Lifecycle graph TB subgraph \u0026#34;Performance Engineering Lifecycle\u0026#34; A[Requirements Analysis] --\u0026gt; B[Baseline Measurement] B --\u0026gt; C[Profiling \u0026amp; Analysis] C --\u0026gt; D[Optimization Implementation] D --\u0026gt; E[Testing \u0026amp; Validation] E --\u0026gt; F[Production Deployment] F --\u0026gt; G[Continuous Monitoring] G --\u0026gt; C end subgraph \u0026#34;Key Metrics\u0026#34; H[Response Time] I[Throughput] J[Resource Utilization] K[Error Rate] L[Scalability] end subgraph \u0026#34;Optimization Areas\u0026#34; M[Application Code] N[Database Queries] O[Caching Layer] P[Network I/O] Q[Resource Allocation] end C --\u0026gt; H C --\u0026gt; I C --\u0026gt; J C --\u0026gt; K C --\u0026gt; L D --\u0026gt; M D --\u0026gt; N D --\u0026gt; O D --\u0026gt; P D --\u0026gt; Q Performance Metrics Framework Category Metric Target Measurement Method Latency P50 Response Time \u0026lt; 100ms Application metrics P95 Response Time \u0026lt; 500ms Application metrics P99 Response Time \u0026lt; 1000ms Application metrics Throughput Requests per Second \u0026gt; 1000 RPS Load testing Transactions per Second \u0026gt; 500 TPS Database metrics Resource CPU Utilization \u0026lt; 70% System metrics Memory Utilization \u0026lt; 80% System metrics Disk I/O \u0026lt; 80% System metrics Availability Uptime \u0026gt; 99.9% Health checks Error Rate \u0026lt; 0.1% Application logs Application Profiling and Analysis Go Application Profiling // profiling/profiler.go package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; _ \u0026#34;net/http/pprof\u0026#34; \u0026#34;os\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;runtime/pprof\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promauto\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promhttp\u0026#34; ) // Custom metrics for performance monitoring var ( requestDuration = promauto.NewHistogramVec( prometheus.HistogramOpts{ Name: \u0026#34;http_request_duration_seconds\u0026#34;, Help: \u0026#34;Duration of HTTP requests in seconds\u0026#34;, Buckets: prometheus.DefBuckets, }, []string{\u0026#34;method\u0026#34;, \u0026#34;endpoint\u0026#34;, \u0026#34;status_code\u0026#34;}, ) memoryUsage = promauto.NewGaugeVec( prometheus.GaugeOpts{ Name: \u0026#34;memory_usage_bytes\u0026#34;, Help: \u0026#34;Current memory usage in bytes\u0026#34;, }, []string{\u0026#34;type\u0026#34;}, ) goroutineCount = promauto.NewGauge( prometheus.GaugeOpts{ Name: \u0026#34;goroutines_total\u0026#34;, Help: \u0026#34;Current number of goroutines\u0026#34;, }, ) gcDuration = promauto.NewHistogram( prometheus.HistogramOpts{ Name: \u0026#34;gc_duration_seconds\u0026#34;, Help: \u0026#34;Duration of garbage collection cycles\u0026#34;, Buckets: []float64{0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0}, }, ) ) type PerformanceProfiler struct { cpuProfileFile string memProfileFile string blockProfileFile string mutexProfileFile string traceFile string profilingEnabled bool } func NewPerformanceProfiler() *PerformanceProfiler { return \u0026amp;PerformanceProfiler{ cpuProfileFile: \u0026#34;cpu.prof\u0026#34;, memProfileFile: \u0026#34;mem.prof\u0026#34;, blockProfileFile: \u0026#34;block.prof\u0026#34;, mutexProfileFile: \u0026#34;mutex.prof\u0026#34;, traceFile: \u0026#34;trace.out\u0026#34;, profilingEnabled: os.Getenv(\u0026#34;ENABLE_PROFILING\u0026#34;) == \u0026#34;true\u0026#34;, } } func (pp *PerformanceProfiler) StartCPUProfiling() error { if !pp.profilingEnabled { return nil } f, err := os.Create(pp.cpuProfileFile) if err != nil { return fmt.Errorf(\u0026#34;could not create CPU profile: %w\u0026#34;, err) } if err := pprof.StartCPUProfile(f); err != nil { f.Close() return fmt.Errorf(\u0026#34;could not start CPU profile: %w\u0026#34;, err) } log.Printf(\u0026#34;CPU profiling started, writing to %s\u0026#34;, pp.cpuProfileFile) return nil } func (pp *PerformanceProfiler) StopCPUProfiling() { if !pp.profilingEnabled { return } pprof.StopCPUProfile() log.Printf(\u0026#34;CPU profiling stopped\u0026#34;) } func (pp *PerformanceProfiler) WriteMemoryProfile() error { if !pp.profilingEnabled { return nil } f, err := os.Create(pp.memProfileFile) if err != nil { return fmt.Errorf(\u0026#34;could not create memory profile: %w\u0026#34;, err) } defer f.Close() runtime.GC() // Force GC before memory profile if err := pprof.WriteHeapProfile(f); err != nil { return fmt.Errorf(\u0026#34;could not write memory profile: %w\u0026#34;, err) } log.Printf(\u0026#34;Memory profile written to %s\u0026#34;, pp.memProfileFile) return nil } func (pp *PerformanceProfiler) WriteBlockProfile() error { if !pp.profilingEnabled { return nil } f, err := os.Create(pp.blockProfileFile) if err != nil { return fmt.Errorf(\u0026#34;could not create block profile: %w\u0026#34;, err) } defer f.Close() if err := pprof.Lookup(\u0026#34;block\u0026#34;).WriteTo(f, 0); err != nil { return fmt.Errorf(\u0026#34;could not write block profile: %w\u0026#34;, err) } log.Printf(\u0026#34;Block profile written to %s\u0026#34;, pp.blockProfileFile) return nil } func (pp *PerformanceProfiler) WriteMutexProfile() error { if !pp.profilingEnabled { return nil } f, err := os.Create(pp.mutexProfileFile) if err != nil { return fmt.Errorf(\u0026#34;could not create mutex profile: %w\u0026#34;, err) } defer f.Close() if err := pprof.Lookup(\u0026#34;mutex\u0026#34;).WriteTo(f, 0); err != nil { return fmt.Errorf(\u0026#34;could not write mutex profile: %w\u0026#34;, err) } log.Printf(\u0026#34;Mutex profile written to %s\u0026#34;, pp.mutexProfileFile) return nil } // Performance monitoring middleware func (pp *PerformanceProfiler) PerformanceMiddleware(next http.Handler) http.Handler { return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) { start := time.Now() // Wrap response writer to capture status code wrapped := \u0026amp;responseWriter{ResponseWriter: w, statusCode: http.StatusOK} // Process request next.ServeHTTP(wrapped, r) // Record metrics duration := time.Since(start).Seconds() requestDuration.WithLabelValues( r.Method, r.URL.Path, fmt.Sprintf(\u0026#34;%d\u0026#34;, wrapped.statusCode), ).Observe(duration) }) } type responseWriter struct { http.ResponseWriter statusCode int } func (rw *responseWriter) WriteHeader(code int) { rw.statusCode = code rw.ResponseWriter.WriteHeader(code) } // Runtime metrics collector func (pp *PerformanceProfiler) StartMetricsCollection(ctx context.Context) { ticker := time.NewTicker(10 * time.Second) defer ticker.Stop() for { select { case \u0026lt;-ctx.Done(): return case \u0026lt;-ticker.C: pp.collectRuntimeMetrics() } } } func (pp *PerformanceProfiler) collectRuntimeMetrics() { var m runtime.MemStats runtime.ReadMemStats(\u0026amp;m) // Memory metrics memoryUsage.WithLabelValues(\u0026#34;heap_alloc\u0026#34;).Set(float64(m.HeapAlloc)) memoryUsage.WithLabelValues(\u0026#34;heap_sys\u0026#34;).Set(float64(m.HeapSys)) memoryUsage.WithLabelValues(\u0026#34;heap_idle\u0026#34;).Set(float64(m.HeapIdle)) memoryUsage.WithLabelValues(\u0026#34;heap_inuse\u0026#34;).Set(float64(m.HeapInuse)) memoryUsage.WithLabelValues(\u0026#34;stack_inuse\u0026#34;).Set(float64(m.StackInuse)) memoryUsage.WithLabelValues(\u0026#34;stack_sys\u0026#34;).Set(float64(m.StackSys)) // Goroutine count goroutineCount.Set(float64(runtime.NumGoroutine())) // GC metrics gcDuration.Observe(float64(m.PauseTotalNs) / 1e9) } // Benchmark helper functions func BenchmarkCriticalPath(b *testing.B, fn func()) { b.ResetTimer() b.ReportAllocs() for i := 0; i \u0026lt; b.N; i++ { fn() } } func ProfileFunction(name string, fn func()) { start := time.Now() fn() duration := time.Since(start) log.Printf(\u0026#34;Function %s took %v\u0026#34;, name, duration) } // Example usage in main application func main() { profiler := NewPerformanceProfiler() // Enable block and mutex profiling runtime.SetBlockProfileRate(1) runtime.SetMutexProfileFraction(1) // Start CPU profiling if err := profiler.StartCPUProfiling(); err != nil { log.Printf(\u0026#34;Failed to start CPU profiling: %v\u0026#34;, err) } defer profiler.StopCPUProfiling() // Start metrics collection ctx, cancel := context.WithCancel(context.Background()) defer cancel() go profiler.StartMetricsCollection(ctx) // Set up HTTP server with profiling endpoints mux := http.NewServeMux() // Application endpoints mux.HandleFunc(\u0026#34;/api/users\u0026#34;, handleUsers) mux.HandleFunc(\u0026#34;/api/orders\u0026#34;, handleOrders) // Profiling endpoints (only in development) if os.Getenv(\u0026#34;ENVIRONMENT\u0026#34;) == \u0026#34;development\u0026#34; { mux.HandleFunc(\u0026#34;/debug/pprof/\u0026#34;, pprof.Index) mux.HandleFunc(\u0026#34;/debug/pprof/cmdline\u0026#34;, pprof.Cmdline) mux.HandleFunc(\u0026#34;/debug/pprof/profile\u0026#34;, pprof.Profile) mux.HandleFunc(\u0026#34;/debug/pprof/symbol\u0026#34;, pprof.Symbol) mux.HandleFunc(\u0026#34;/debug/pprof/trace\u0026#34;, pprof.Trace) } // Metrics endpoint mux.Handle(\u0026#34;/metrics\u0026#34;, promhttp.Handler()) // Apply performance middleware handler := profiler.PerformanceMiddleware(mux) log.Println(\u0026#34;Server starting on :8080\u0026#34;) log.Fatal(http.ListenAndServe(\u0026#34;:8080\u0026#34;, handler)) } func handleUsers(w http.ResponseWriter, r *http.Request) { // Simulate some work time.Sleep(50 * time.Millisecond) w.WriteHeader(http.StatusOK) w.Write([]byte(`{\u0026#34;users\u0026#34;: []}`)) } func handleOrders(w http.ResponseWriter, r *http.Request) { // Simulate database query time.Sleep(100 * time.Millisecond) w.WriteHeader(http.StatusOK) w.Write([]byte(`{\u0026#34;orders\u0026#34;: []}`)) } Database Query Optimization -- database/optimization.sql -- Index optimization for common query patterns CREATE INDEX CONCURRENTLY idx_users_email_active ON users (email) WHERE active = true; CREATE INDEX CONCURRENTLY idx_orders_user_created ON orders (user_id, created_at DESC) INCLUDE (total_amount, status); CREATE INDEX CONCURRENTLY idx_products_category_price ON products (category_id, price) WHERE available = true; -- Partial index for recent data CREATE INDEX CONCURRENTLY idx_logs_recent ON application_logs (created_at, level) WHERE created_at \u0026gt; NOW() - INTERVAL \u0026#39;7 days\u0026#39;; -- Query optimization examples -- Before: Inefficient query -- SELECT * FROM orders o -- JOIN users u ON o.user_id = u.id -- WHERE o.created_at \u0026gt; \u0026#39;2024-01-01\u0026#39; -- ORDER BY o.created_at DESC; -- After: Optimized query with specific columns and better indexing SELECT o.id, o.total_amount, o.status, o.created_at, u.email, u.name FROM orders o JOIN users u ON o.user_id = u.id WHERE o.created_at \u0026gt; \u0026#39;2024-01-01\u0026#39; AND o.status IN (\u0026#39;pending\u0026#39;, \u0026#39;processing\u0026#39;) ORDER BY o.created_at DESC LIMIT 100; -- Materialized view for complex aggregations CREATE MATERIALIZED VIEW daily_sales_summary AS SELECT DATE(created_at) as sale_date, COUNT(*) as order_count, SUM(total_amount) as total_revenue, AVG(total_amount) as avg_order_value, COUNT(DISTINCT user_id) as unique_customers FROM orders WHERE status = \u0026#39;completed\u0026#39; GROUP BY DATE(created_at) ORDER BY sale_date DESC; -- Refresh materialized view (can be automated) REFRESH MATERIALIZED VIEW CONCURRENTLY daily_sales_summary; -- Query performance monitoring CREATE OR REPLACE FUNCTION log_slow_queries() RETURNS event_trigger AS $$ BEGIN -- Log queries taking longer than 1 second IF current_setting(\u0026#39;log_min_duration_statement\u0026#39;)::int \u0026gt; 1000 THEN RAISE LOG \u0026#39;Slow query detected: %\u0026#39;, current_query(); END IF; END; $$ LANGUAGE plpgsql; -- Connection pooling configuration -- postgresql.conf optimizations /* # Connection settings max_connections = 200 shared_buffers = 256MB effective_cache_size = 1GB work_mem = 4MB maintenance_work_mem = 64MB # Checkpoint settings checkpoint_completion_target = 0.9 wal_buffers = 16MB default_statistics_target = 100 # Query planner settings random_page_cost = 1.1 effective_io_concurrency = 200 # Logging settings log_min_duration_statement = 1000 log_checkpoints = on log_connections = on log_disconnections = on log_lock_waits = on */ Caching Strategy Implementation // cache/redis_cache.go package cache import ( \u0026#34;context\u0026#34; \u0026#34;encoding/json\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; \u0026#34;github.com/go-redis/redis/v8\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus\u0026#34; \u0026#34;github.com/prometheus/client_golang/prometheus/promauto\u0026#34; ) var ( cacheHits = promauto.NewCounterVec( prometheus.CounterOpts{ Name: \u0026#34;cache_hits_total\u0026#34;, Help: \u0026#34;Total number of cache hits\u0026#34;, }, []string{\u0026#34;cache_type\u0026#34;, \u0026#34;key_pattern\u0026#34;}, ) cacheMisses = promauto.NewCounterVec( prometheus.CounterOpts{ Name: \u0026#34;cache_misses_total\u0026#34;, Help: \u0026#34;Total number of cache misses\u0026#34;, }, []string{\u0026#34;cache_type\u0026#34;, \u0026#34;key_pattern\u0026#34;}, ) cacheOperationDuration = promauto.NewHistogramVec( prometheus.HistogramOpts{ Name: \u0026#34;cache_operation_duration_seconds\u0026#34;, Help: \u0026#34;Duration of cache operations\u0026#34;, Buckets: []float64{0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0}, }, []string{\u0026#34;operation\u0026#34;, \u0026#34;cache_type\u0026#34;}, ) ) type CacheConfig struct { RedisAddr string RedisPassword string RedisDB int DefaultTTL time.Duration MaxRetries int } type Cache struct { client *redis.Client defaultTTL time.Duration } func NewCache(config CacheConfig) *Cache { rdb := redis.NewClient(\u0026amp;redis.Options{ Addr: config.RedisAddr, Password: config.RedisPassword, DB: config.RedisDB, MaxRetries: config.MaxRetries, PoolSize: 20, MinIdleConns: 5, }) return \u0026amp;Cache{ client: rdb, defaultTTL: config.DefaultTTL, } } func (c *Cache) Get(ctx context.Context, key string, dest interface{}) error { start := time.Now() defer func() { cacheOperationDuration.WithLabelValues(\u0026#34;get\u0026#34;, \u0026#34;redis\u0026#34;).Observe(time.Since(start).Seconds()) }() val, err := c.client.Get(ctx, key).Result() if err != nil { if err == redis.Nil { cacheMisses.WithLabelValues(\u0026#34;redis\u0026#34;, c.getKeyPattern(key)).Inc() return ErrCacheMiss } return fmt.Errorf(\u0026#34;cache get error: %w\u0026#34;, err) } cacheHits.WithLabelValues(\u0026#34;redis\u0026#34;, c.getKeyPattern(key)).Inc() if err := json.Unmarshal([]byte(val), dest); err != nil { return fmt.Errorf(\u0026#34;cache unmarshal error: %w\u0026#34;, err) } return nil } func (c *Cache) Set(ctx context.Context, key string, value interface{}, ttl time.Duration) error { start := time.Now() defer func() { cacheOperationDuration.WithLabelValues(\u0026#34;set\u0026#34;, \u0026#34;redis\u0026#34;).Observe(time.Since(start).Seconds()) }() if ttl == 0 { ttl = c.defaultTTL } data, err := json.Marshal(value) if err != nil { return fmt.Errorf(\u0026#34;cache marshal error: %w\u0026#34;, err) } if err := c.client.Set(ctx, key, data, ttl).Err(); err != nil { return fmt.Errorf(\u0026#34;cache set error: %w\u0026#34;, err) } return nil } func (c *Cache) Delete(ctx context.Context, key string) error { start := time.Now() defer func() { cacheOperationDuration.WithLabelValues(\u0026#34;delete\u0026#34;, \u0026#34;redis\u0026#34;).Observe(time.Since(start).Seconds()) }() if err := c.client.Del(ctx, key).Err(); err != nil { return fmt.Errorf(\u0026#34;cache delete error: %w\u0026#34;, err) } return nil } func (c *Cache) GetOrSet(ctx context.Context, key string, dest interface{}, ttl time.Duration, fetchFn func() (interface{}, error)) error { // Try to get from cache first err := c.Get(ctx, key, dest) if err == nil { return nil // Cache hit } if err != ErrCacheMiss { return err // Actual error } // Cache miss, fetch data data, err := fetchFn() if err != nil { return fmt.Errorf(\u0026#34;fetch function error: %w\u0026#34;, err) } // Set in cache if err := c.Set(ctx, key, data, ttl); err != nil { // Log error but don\u0026#39;t fail the request log.Printf(\u0026#34;Failed to set cache: %v\u0026#34;, err) } // Copy data to destination dataBytes, _ := json.Marshal(data) return json.Unmarshal(dataBytes, dest) } func (c *Cache) getKeyPattern(key string) string { // Extract pattern from key for metrics parts := strings.Split(key, \u0026#34;:\u0026#34;) if len(parts) \u0026gt;= 2 { return parts[0] + \u0026#34;:\u0026#34; + parts[1] } return \u0026#34;unknown\u0026#34; } var ErrCacheMiss = fmt.Errorf(\u0026#34;cache miss\u0026#34;) // Multi-level cache implementation type MultiLevelCache struct { l1Cache *sync.Map // In-memory cache l2Cache *Cache // Redis cache l1TTL time.Duration l2TTL time.Duration } func NewMultiLevelCache(redisCache *Cache, l1TTL, l2TTL time.Duration) *MultiLevelCache { return \u0026amp;MultiLevelCache{ l1Cache: \u0026amp;sync.Map{}, l2Cache: redisCache, l1TTL: l1TTL, l2TTL: l2TTL, } } type cacheItem struct { data interface{} expiresAt time.Time } func (mlc *MultiLevelCache) Get(ctx context.Context, key string, dest interface{}) error { // Try L1 cache first if item, ok := mlc.l1Cache.Load(key); ok { cacheItem := item.(*cacheItem) if time.Now().Before(cacheItem.expiresAt) { cacheHits.WithLabelValues(\u0026#34;memory\u0026#34;, \u0026#34;l1\u0026#34;).Inc() return mlc.copyData(cacheItem.data, dest) } // Expired, remove from L1 mlc.l1Cache.Delete(key) } // Try L2 cache err := mlc.l2Cache.Get(ctx, key, dest) if err == nil { // Store in L1 cache mlc.l1Cache.Store(key, \u0026amp;cacheItem{ data: dest, expiresAt: time.Now().Add(mlc.l1TTL), }) return nil } return err } func (mlc *MultiLevelCache) Set(ctx context.Context, key string, value interface{}, ttl time.Duration) error { // Set in L2 cache if err := mlc.l2Cache.Set(ctx, key, value, ttl); err != nil { return err } // Set in L1 cache mlc.l1Cache.Store(key, \u0026amp;cacheItem{ data: value, expiresAt: time.Now().Add(mlc.l1TTL), }) return nil } func (mlc *MultiLevelCache) copyData(src, dest interface{}) error { data, err := json.Marshal(src) if err != nil { return err } return json.Unmarshal(data, dest) } // Cache warming strategy type CacheWarmer struct { cache *Cache warmers map[string]WarmupFunction interval time.Duration } type WarmupFunction func(ctx context.Context) (map[string]interface{}, error) func NewCacheWarmer(cache *Cache, interval time.Duration) *CacheWarmer { return \u0026amp;CacheWarmer{ cache: cache, warmers: make(map[string]WarmupFunction), interval: interval, } } func (cw *CacheWarmer) RegisterWarmer(name string, fn WarmupFunction) { cw.warmers[name] = fn } func (cw *CacheWarmer) Start(ctx context.Context) { ticker := time.NewTicker(cw.interval) defer ticker.Stop() // Initial warmup cw.warmup(ctx) for { select { case \u0026lt;-ctx.Done(): return case \u0026lt;-ticker.C: cw.warmup(ctx) } } } func (cw *CacheWarmer) warmup(ctx context.Context) { for name, warmer := range cw.warmers { go func(name string, warmer WarmupFunction) { data, err := warmer(ctx) if err != nil { log.Printf(\u0026#34;Cache warmer %s failed: %v\u0026#34;, name, err) return } for key, value := range data { if err := cw.cache.Set(ctx, key, value, 30*time.Minute); err != nil { log.Printf(\u0026#34;Failed to warm cache key %s: %v\u0026#34;, key, err) } } log.Printf(\u0026#34;Cache warmer %s completed, warmed %d keys\u0026#34;, name, len(data)) }(name, warmer) } } Kubernetes Resource Optimization Resource Requests and Limits Optimization # k8s/resource-optimization.yaml apiVersion: v1 kind: ConfigMap metadata: name: resource-optimization-config namespace: production data: # Resource optimization guidelines guidelines.yaml: | resource_guidelines: cpu: request_ratio: 0.1 # 10% of limit limit_ratio: 1.0 # 100% of allocated burst_allowance: 2.0 # 200% burst capacity memory: request_ratio: 0.8 # 80% of limit limit_ratio: 1.0 # 100% of allocated oom_kill_threshold: 0.95 # 95% before OOM application_profiles: web_frontend: cpu_request: \u0026#34;100m\u0026#34; cpu_limit: \u0026#34;500m\u0026#34; memory_request: \u0026#34;128Mi\u0026#34; memory_limit: \u0026#34;256Mi\u0026#34; api_backend: cpu_request: \u0026#34;200m\u0026#34; cpu_limit: \u0026#34;1000m\u0026#34; memory_request: \u0026#34;256Mi\u0026#34; memory_limit: \u0026#34;512Mi\u0026#34; database: cpu_request: \u0026#34;500m\u0026#34; cpu_limit: \u0026#34;2000m\u0026#34; memory_request: \u0026#34;1Gi\u0026#34; memory_limit: \u0026#34;2Gi\u0026#34; cache: cpu_request: \u0026#34;100m\u0026#34; cpu_limit: \u0026#34;500m\u0026#34; memory_request: \u0026#34;512Mi\u0026#34; memory_limit: \u0026#34;1Gi\u0026#34; --- apiVersion: apps/v1 kind: Deployment metadata: name: optimized-application namespace: production spec: replicas: 3 selector: matchLabels: app: optimized-application template: metadata: labels: app: optimized-application annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/port: \u0026#34;8080\u0026#34; prometheus.io/path: \u0026#34;/metrics\u0026#34; spec: containers: - name: app image: myapp:optimized ports: - containerPort: 8080 name: http resources: requests: memory: \u0026#34;256Mi\u0026#34; cpu: \u0026#34;200m\u0026#34; ephemeral-storage: \u0026#34;1Gi\u0026#34; limits: memory: \u0026#34;512Mi\u0026#34; cpu: \u0026#34;1000m\u0026#34; ephemeral-storage: \u0026#34;2Gi\u0026#34; env: - name: GOMAXPROCS valueFrom: resourceFieldRef: resource: limits.cpu - name: GOMEMLIMIT valueFrom: resourceFieldRef: resource: limits.memory livenessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: httpGet: path: /ready port: 8080 initialDelaySeconds: 5 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 startupProbe: httpGet: path: /startup port: 8080 initialDelaySeconds: 10 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 30 securityContext: allowPrivilegeEscalation: false runAsNonRoot: true runAsUser: 1000 capabilities: drop: - ALL readOnlyRootFilesystem: true volumeMounts: - name: tmp mountPath: /tmp - name: cache mountPath: /app/cache volumes: - name: tmp emptyDir: {} - name: cache emptyDir: sizeLimit: 1Gi nodeSelector: node-type: \u0026#34;compute-optimized\u0026#34; tolerations: - key: \u0026#34;high-performance\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;true\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - optimized-application topologyKey: kubernetes.io/hostname --- apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: optimized-application-hpa namespace: production spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: optimized-application minReplicas: 3 maxReplicas: 20 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 - type: Pods pods: metric: name: http_requests_per_second target: type: AverageValue averageValue: \u0026#34;100\u0026#34; behavior: scaleDown: stabilizationWindowSeconds: 300 policies: - type: Percent value: 10 periodSeconds: 60 scaleUp: stabilizationWindowSeconds: 60 policies: - type: Percent value: 50 periodSeconds: 60 - type: Pods value: 2 periodSeconds: 60 selectPolicy: Max --- apiVersion: autoscaling/v1 kind: VerticalPodAutoscaler metadata: name: optimized-application-vpa namespace: production spec: targetRef: apiVersion: apps/v1 kind: Deployment name: optimized-application updatePolicy: updateMode: \u0026#34;Auto\u0026#34; resourcePolicy: containerPolicies: - containerName: app minAllowed: cpu: 100m memory: 128Mi maxAllowed: cpu: 2000m memory: 1Gi controlledResources: [\u0026#34;cpu\u0026#34;, \u0026#34;memory\u0026#34;] Performance Monitoring and Alerting # monitoring/performance-alerts.yaml apiVersion: monitoring.coreos.com/v1 kind: PrometheusRule metadata: name: performance-alerts namespace: monitoring spec: groups: - name: performance.rules rules: # High latency alerts - alert: HighResponseTime expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) \u0026gt; 0.5 for: 2m labels: severity: warning category: performance annotations: summary: \u0026#34;High response time detected\u0026#34; description: \u0026#34;95th percentile response time is {{ $value }}s for {{ $labels.endpoint }}\u0026#34; - alert: VeryHighResponseTime expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) \u0026gt; 1.0 for: 1m labels: severity: critical category: performance annotations: summary: \u0026#34;Very high response time detected\u0026#34; description: \u0026#34;95th percentile response time is {{ $value }}s for {{ $labels.endpoint }}\u0026#34; # Throughput alerts - alert: LowThroughput expr: rate(http_requests_total[5m]) \u0026lt; 10 for: 5m labels: severity: warning category: performance annotations: summary: \u0026#34;Low throughput detected\u0026#34; description: \u0026#34;Request rate is {{ $value }} requests/second\u0026#34; # Resource utilization alerts - alert: HighCPUUtilization expr: rate(container_cpu_usage_seconds_total[5m]) / container_spec_cpu_quota * container_spec_cpu_period \u0026gt; 0.8 for: 5m labels: severity: warning category: performance annotations: summary: \u0026#34;High CPU utilization\u0026#34; description: \u0026#34;CPU utilization is {{ $value | humanizePercentage }} for {{ $labels.pod }}\u0026#34; - alert: HighMemoryUtilization expr: container_memory_usage_bytes / container_spec_memory_limit_bytes \u0026gt; 0.9 for: 2m labels: severity: critical category: performance annotations: summary: \u0026#34;High memory utilization\u0026#34; description: \u0026#34;Memory utilization is {{ $value | humanizePercentage }} for {{ $labels.pod }}\u0026#34; # Cache performance alerts - alert: LowCacheHitRate expr: rate(cache_hits_total[5m]) / (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m])) \u0026lt; 0.8 for: 5m labels: severity: warning category: performance annotations: summary: \u0026#34;Low cache hit rate\u0026#34; description: \u0026#34;Cache hit rate is {{ $value | humanizePercentage }} for {{ $labels.cache_type }}\u0026#34; # Database performance alerts - alert: SlowDatabaseQueries expr: rate(postgresql_slow_queries_total[5m]) \u0026gt; 1 for: 2m labels: severity: warning category: performance annotations: summary: \u0026#34;Slow database queries detected\u0026#34; description: \u0026#34;{{ $value }} slow queries per second detected\u0026#34; # Garbage collection alerts - alert: HighGCPressure expr: rate(go_gc_duration_seconds_sum[5m]) / rate(go_gc_duration_seconds_count[5m]) \u0026gt; 0.1 for: 5m labels: severity: warning category: performance annotations: summary: \u0026#34;High garbage collection pressure\u0026#34; description: \u0026#34;Average GC duration is {{ $value }}s\u0026#34; --- apiVersion: v1 kind: ConfigMap metadata: name: grafana-performance-dashboard namespace: monitoring data: performance-dashboard.json: | { \u0026#34;dashboard\u0026#34;: { \u0026#34;id\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;Application Performance Dashboard\u0026#34;, \u0026#34;tags\u0026#34;: [\u0026#34;performance\u0026#34;, \u0026#34;monitoring\u0026#34;], \u0026#34;timezone\u0026#34;: \u0026#34;browser\u0026#34;, \u0026#34;panels\u0026#34;: [ { \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;Response Time Percentiles\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;50th percentile\u0026#34; }, { \u0026#34;expr\u0026#34;: \u0026#34;histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;95th percentile\u0026#34; }, { \u0026#34;expr\u0026#34;: \u0026#34;histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;99th percentile\u0026#34; } ], \u0026#34;yAxes\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;Response Time (seconds)\u0026#34;, \u0026#34;min\u0026#34;: 0 } ] }, { \u0026#34;id\u0026#34;: 2, \u0026#34;title\u0026#34;: \u0026#34;Request Rate\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;rate(http_requests_total[5m])\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{ method }} {{ endpoint }}\u0026#34; } ], \u0026#34;yAxes\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;Requests per second\u0026#34;, \u0026#34;min\u0026#34;: 0 } ] }, { \u0026#34;id\u0026#34;: 3, \u0026#34;title\u0026#34;: \u0026#34;Error Rate\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;rate(http_requests_total{status_code=~\\\u0026#34;5..\\\u0026#34;}[5m]) / rate(http_requests_total[5m])\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;Error rate\u0026#34; } ], \u0026#34;yAxes\u0026#34;: [ { \u0026#34;label\u0026#34;: \u0026#34;Error rate\u0026#34;, \u0026#34;min\u0026#34;: 0, \u0026#34;max\u0026#34;: 1 } ] }, { \u0026#34;id\u0026#34;: 4, \u0026#34;title\u0026#34;: \u0026#34;Resource Utilization\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;rate(container_cpu_usage_seconds_total[5m])\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;CPU usage - {{ pod }}\u0026#34; }, { \u0026#34;expr\u0026#34;: \u0026#34;container_memory_usage_bytes / 1024 / 1024\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;Memory usage (MB) - {{ pod }}\u0026#34; } ] }, { \u0026#34;id\u0026#34;: 5, \u0026#34;title\u0026#34;: \u0026#34;Cache Performance\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;rate(cache_hits_total[5m])\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;Cache hits/sec - {{ cache_type }}\u0026#34; }, { \u0026#34;expr\u0026#34;: \u0026#34;rate(cache_misses_total[5m])\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;Cache misses/sec - {{ cache_type }}\u0026#34; } ] } ], \u0026#34;time\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;now-1h\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;now\u0026#34; }, \u0026#34;refresh\u0026#34;: \u0026#34;30s\u0026#34; } } Load Testing and Performance Validation Comprehensive Load Testing Script #!/bin/bash # scripts/load-test.sh set -euo pipefail # Configuration TARGET_URL=\u0026#34;${1:-http://localhost:8080}\u0026#34; TEST_DURATION=\u0026#34;${2:-300}\u0026#34; # 5 minutes MAX_VUS=\u0026#34;${3:-100}\u0026#34; # Virtual users RAMP_UP_TIME=\u0026#34;${4:-60}\u0026#34; # Ramp up time in seconds TEST_TYPE=\u0026#34;${5:-load}\u0026#34; # load, stress, spike, volume # Colors for output RED=\u0026#39;\\033[0;31m\u0026#39; GREEN=\u0026#39;\\033[0;32m\u0026#39; YELLOW=\u0026#39;\\033[1;33m\u0026#39; BLUE=\u0026#39;\\033[0;34m\u0026#39; NC=\u0026#39;\\033[0m\u0026#39; log() { echo -e \u0026#34;[$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)] $1\u0026#34; } success() { log \u0026#34;${GREEN}✓ $1${NC}\u0026#34; } warning() { log \u0026#34;${YELLOW}⚠ $1${NC}\u0026#34; } error() { log \u0026#34;${RED}✗ $1${NC}\u0026#34; } info() { log \u0026#34;${BLUE}ℹ $1${NC}\u0026#34; } # Create k6 test script create_k6_script() { local test_type=\u0026#34;$1\u0026#34; local script_file=\u0026#34;load-test-${test_type}.js\u0026#34; cat \u0026gt; \u0026#34;${script_file}\u0026#34; \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; import http from \u0026#39;k6/http\u0026#39;; import { check, sleep } from \u0026#39;k6\u0026#39;; import { Rate, Trend, Counter } from \u0026#39;k6/metrics\u0026#39;; // Custom metrics const errorRate = new Rate(\u0026#39;error_rate\u0026#39;); const responseTime = new Trend(\u0026#39;response_time\u0026#39;); const requestCount = new Counter(\u0026#39;request_count\u0026#39;); // Test configuration export let options = { stages: [ { duration: \u0026#39;${RAMP_UP_TIME}s\u0026#39;, target: ${MAX_VUS} }, { duration: \u0026#39;${TEST_DURATION}s\u0026#39;, target: ${MAX_VUS} }, { duration: \u0026#39;60s\u0026#39;, target: 0 }, ], thresholds: { http_req_duration: [\u0026#39;p(95)\u0026lt;500\u0026#39;, \u0026#39;p(99)\u0026lt;1000\u0026#39;], http_req_failed: [\u0026#39;rate\u0026lt;0.01\u0026#39;], error_rate: [\u0026#39;rate\u0026lt;0.01\u0026#39;], }, }; // Test scenarios const scenarios = { load_test: { executor: \u0026#39;ramping-vus\u0026#39;, startVUs: 0, stages: [ { duration: \u0026#39;${RAMP_UP_TIME}s\u0026#39;, target: ${MAX_VUS} }, { duration: \u0026#39;${TEST_DURATION}s\u0026#39;, target: ${MAX_VUS} }, { duration: \u0026#39;60s\u0026#39;, target: 0 }, ], }, stress_test: { executor: \u0026#39;ramping-vus\u0026#39;, startVUs: 0, stages: [ { duration: \u0026#39;60s\u0026#39;, target: ${MAX_VUS} }, { duration: \u0026#39;120s\u0026#39;, target: ${MAX_VUS} * 2 }, { duration: \u0026#39;180s\u0026#39;, target: ${MAX_VUS} * 3 }, { duration: \u0026#39;60s\u0026#39;, target: 0 }, ], }, spike_test: { executor: \u0026#39;ramping-vus\u0026#39;, startVUs: 0, stages: [ { duration: \u0026#39;60s\u0026#39;, target: ${MAX_VUS} }, { duration: \u0026#39;10s\u0026#39;, target: ${MAX_VUS} * 5 }, { duration: \u0026#39;60s\u0026#39;, target: ${MAX_VUS} }, { duration: \u0026#39;60s\u0026#39;, target: 0 }, ], }, }; export default function() { const baseUrl = \u0026#39;${TARGET_URL}\u0026#39;; // Test different endpoints with realistic load distribution const endpoints = [ { url: `${baseUrl}/api/users`, weight: 30 }, { url: `${baseUrl}/api/orders`, weight: 25 }, { url: `${baseUrl}/api/products`, weight: 20 }, { url: `${baseUrl}/api/search`, weight: 15 }, { url: `${baseUrl}/api/analytics`, weight: 10 }, ]; // Select endpoint based on weight const random = Math.random() * 100; let cumulative = 0; let selectedEndpoint = endpoints[0].url; for (const endpoint of endpoints) { cumulative += endpoint.weight; if (random \u0026lt;= cumulative) { selectedEndpoint = endpoint.url; break; } } // Make request const response = http.get(selectedEndpoint, { headers: { \u0026#39;User-Agent\u0026#39;: \u0026#39;k6-load-test\u0026#39;, \u0026#39;Accept\u0026#39;: \u0026#39;application/json\u0026#39;, }, timeout: \u0026#39;30s\u0026#39;, }); // Record metrics requestCount.add(1); responseTime.add(response.timings.duration); // Validate response const success = check(response, { \u0026#39;status is 200\u0026#39;: (r) =\u0026gt; r.status === 200, \u0026#39;response time \u0026lt; 1000ms\u0026#39;: (r) =\u0026gt; r.timings.duration \u0026lt; 1000, \u0026#39;response has body\u0026#39;: (r) =\u0026gt; r.body \u0026amp;\u0026amp; r.body.length \u0026gt; 0, }); errorRate.add(!success); // Realistic user behavior - think time sleep(Math.random() * 2 + 1); // 1-3 seconds } export function handleSummary(data) { return { \u0026#39;load-test-results.json\u0026#39;: JSON.stringify(data, null, 2), \u0026#39;load-test-summary.txt\u0026#39;: textSummary(data, { indent: \u0026#39; \u0026#39;, enableColors: true }), }; } EOF # Replace placeholders sed -i.bak \u0026#34;s/\\${TARGET_URL}/${TARGET_URL//\\//\\\\/}/g\u0026#34; \u0026#34;${script_file}\u0026#34; sed -i.bak \u0026#34;s/\\${TEST_DURATION}/${TEST_DURATION}/g\u0026#34; \u0026#34;${script_file}\u0026#34; sed -i.bak \u0026#34;s/\\${MAX_VUS}/${MAX_VUS}/g\u0026#34; \u0026#34;${script_file}\u0026#34; sed -i.bak \u0026#34;s/\\${RAMP_UP_TIME}/${RAMP_UP_TIME}/g\u0026#34; \u0026#34;${script_file}\u0026#34; rm \u0026#34;${script_file}.bak\u0026#34; echo \u0026#34;${script_file}\u0026#34; } # Performance baseline measurement measure_baseline() { info \u0026#34;Measuring performance baseline...\u0026#34; # Single user test k6 run --vus 1 --duration 60s --quiet \u0026#34;$(create_k6_script baseline)\u0026#34; \u0026gt; baseline-results.txt local baseline_p95=$(grep \u0026#34;http_req_duration.*p(95)\u0026#34; baseline-results.txt | awk \u0026#39;{print $3}\u0026#39; | sed \u0026#39;s/ms//\u0026#39;) local baseline_rps=$(grep \u0026#34;http_reqs\u0026#34; baseline-results.txt | awk \u0026#39;{print $3}\u0026#39; | sed \u0026#39;s/\\/s//\u0026#39;) info \u0026#34;Baseline P95 response time: ${baseline_p95}ms\u0026#34; info \u0026#34;Baseline RPS: ${baseline_rps}\u0026#34; echo \u0026#34;${baseline_p95}\u0026#34; \u0026gt; baseline-p95.txt echo \u0026#34;${baseline_rps}\u0026#34; \u0026gt; baseline-rps.txt } # Run load test run_load_test() { local test_type=\u0026#34;$1\u0026#34; info \u0026#34;Running ${test_type} test...\u0026#34; info \u0026#34;Target: ${TARGET_URL}\u0026#34; info \u0026#34;Duration: ${TEST_DURATION}s\u0026#34; info \u0026#34;Max VUs: ${MAX_VUS}\u0026#34; info \u0026#34;Ramp up: ${RAMP_UP_TIME}s\u0026#34; local script_file script_file=$(create_k6_script \u0026#34;${test_type}\u0026#34;) # Run k6 test if k6 run \u0026#34;${script_file}\u0026#34;; then success \u0026#34;${test_type} test completed successfully\u0026#34; return 0 else error \u0026#34;${test_type} test failed\u0026#34; return 1 fi } # Analyze results analyze_results() { if [[ ! -f \u0026#34;load-test-results.json\u0026#34; ]]; then error \u0026#34;Results file not found\u0026#34; return 1 fi info \u0026#34;Analyzing test results...\u0026#34; # Extract key metrics local p95_response_time local p99_response_time local avg_rps local error_rate local max_vus p95_response_time=$(jq -r \u0026#39;.metrics.http_req_duration.values.p95\u0026#39; load-test-results.json) p99_response_time=$(jq -r \u0026#39;.metrics.http_req_duration.values.p99\u0026#39; load-test-results.json) avg_rps=$(jq -r \u0026#39;.metrics.http_reqs.values.rate\u0026#39; load-test-results.json) error_rate=$(jq -r \u0026#39;.metrics.http_req_failed.values.rate\u0026#39; load-test-results.json) max_vus=$(jq -r \u0026#39;.metrics.vus_max.values.max\u0026#39; load-test-results.json) # Performance report cat \u0026lt;\u0026lt; EOF \u0026gt; performance-report.txt Performance Test Report ====================== Test Type: ${TEST_TYPE} Target URL: ${TARGET_URL} Test Duration: ${TEST_DURATION}s Max Virtual Users: ${max_vus} Key Metrics: - P95 Response Time: ${p95_response_time}ms - P99 Response Time: ${p99_response_time}ms - Average RPS: ${avg_rps} - Error Rate: $(echo \u0026#34;${error_rate} * 100\u0026#34; | bc -l | cut -d. -f1)% Performance Thresholds: - P95 \u0026lt; 500ms: $(if (( $(echo \u0026#34;${p95_response_time} \u0026lt; 500\u0026#34; | bc -l) )); then echo \u0026#34;✓ PASS\u0026#34;; else echo \u0026#34;✗ FAIL\u0026#34;; fi) - P99 \u0026lt; 1000ms: $(if (( $(echo \u0026#34;${p99_response_time} \u0026lt; 1000\u0026#34; | bc -l) )); then echo \u0026#34;✓ PASS\u0026#34;; else echo \u0026#34;✗ FAIL\u0026#34;; fi) - Error Rate \u0026lt; 1%: $(if (( $(echo \u0026#34;${error_rate} \u0026lt; 0.01\u0026#34; | bc -l) )); then echo \u0026#34;✓ PASS\u0026#34;; else echo \u0026#34;✗ FAIL\u0026#34;; fi) EOF # Compare with baseline if available if [[ -f \u0026#34;baseline-p95.txt\u0026#34; ]]; then local baseline_p95 baseline_p95=$(cat baseline-p95.txt) local degradation degradation=$(echo \u0026#34;scale=2; (${p95_response_time} - ${baseline_p95}) / ${baseline_p95} * 100\u0026#34; | bc -l) echo \u0026#34;Baseline Comparison:\u0026#34; \u0026gt;\u0026gt; performance-report.txt echo \u0026#34;- P95 Degradation: ${degradation}%\u0026#34; \u0026gt;\u0026gt; performance-report.txt if (( $(echo \u0026#34;${degradation} \u0026gt; 20\u0026#34; | bc -l) )); then warning \u0026#34;Performance degradation detected: ${degradation}%\u0026#34; fi fi cat performance-report.txt } # Generate performance graphs generate_graphs() { if command -v gnuplot \u0026amp;\u0026gt; /dev/null; then info \u0026#34;Generating performance graphs...\u0026#34; # Extract time series data jq -r \u0026#39;.metrics.http_req_duration.values | to_entries[] | \u0026#34;\\(.key) \\(.value)\u0026#34;\u0026#39; load-test-results.json \u0026gt; response-time-data.txt # Create gnuplot script cat \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026gt; plot-performance.gp set terminal png size 1200,800 set output \u0026#39;performance-graph.png\u0026#39; set title \u0026#39;Load Test Performance Results\u0026#39; set xlabel \u0026#39;Time\u0026#39; set ylabel \u0026#39;Response Time (ms)\u0026#39; set grid plot \u0026#39;response-time-data.txt\u0026#39; using 1:2 with lines title \u0026#39;Response Time\u0026#39; EOF gnuplot plot-performance.gp success \u0026#34;Performance graph generated: performance-graph.png\u0026#34; fi } # Cleanup function cleanup() { rm -f load-test-*.js rm -f baseline-results.txt rm -f response-time-data.txt rm -f plot-performance.gp } # Main execution main() { info \u0026#34;Starting performance testing suite\u0026#34; # Check dependencies if ! command -v k6 \u0026amp;\u0026gt; /dev/null; then error \u0026#34;k6 is not installed. Please install k6 first.\u0026#34; exit 1 fi # Measure baseline if not exists if [[ ! -f \u0026#34;baseline-p95.txt\u0026#34; ]]; then measure_baseline fi # Run the specified test if run_load_test \u0026#34;${TEST_TYPE}\u0026#34;; then analyze_results generate_graphs success \u0026#34;Performance testing completed successfully\u0026#34; else error \u0026#34;Performance testing failed\u0026#34; exit 1 fi } # Set up signal handlers trap cleanup EXIT # Run main function main \u0026#34;$@\u0026#34; Conclusion Performance optimization for cloud-native applications requires a systematic approach that encompasses:\n","content":"Performance Optimization for Cloud-Native Applications: From Profiling to Production Tuning Performance optimization is a critical aspect of running successful cloud-native applications at scale. This comprehensive guide covers systematic approaches to identifying performance bottlenecks, implementing optimization strategies, and maintaining optimal performance in production environments.\nPerformance Optimization Methodology Performance Engineering Lifecycle graph TB subgraph \u0026amp;#34;Performance …","date":"2025-12-31","lastmod":"2025-12-31","tags":["Performance","Optimization","Profiling","Caching","Kubernetes","Monitoring","Tuning"],"categories":["DevOps"],"author":"Performance Engineering Team","readingTime":20,"wordCount":4199,"section":"posts","type":"posts","draft":false,"featured":false,"series":["Performance Engineering"]},{"title":"Privacy Policy","url":"https://www.dishuihengxin.com/en/privacy/","summary":"Privacy Policy Last Updated: December 31, 2025\nWelcome to this tech blog. We value your privacy and are committed to protecting your personal information. This Privacy Policy explains how we collect, use, and safeguard your information.\n1. Information Collection 1.1 Automatically Collected Information When you visit this website, we may automatically collect:\nAccess Logs: IP address, browser type, operating system, access time, pages visited Cookie Information: Technical cookies to improve user experience Analytics Data: Anonymous statistical data collected through tools like Google Analytics 1.2 Voluntarily Provided Information If you choose to:\n","content":"Privacy Policy Last Updated: December 31, 2025\nWelcome to this tech blog. We value your privacy and are committed to protecting your personal information. This Privacy Policy explains how we collect, use, and safeguard your information.\n1. Information Collection 1.1 Automatically Collected Information When you visit this website, we may automatically collect:\nAccess Logs: IP address, browser type, operating system, access time, pages visited Cookie Information: Technical cookies to improve user …","date":"2025-12-31","lastmod":"2025-12-31","tags":null,"categories":null,"author":"Author","readingTime":3,"wordCount":607,"section":"","type":"page","draft":false,"featured":false,"series":null},{"title":"Terms of Service","url":"https://www.dishuihengxin.com/en/terms/","summary":"Terms of Service Last Updated: December 31, 2024\nWelcome to this tech blog. By using this website, you agree to comply with the following Terms of Service. If you do not agree with these terms, please do not use this website.\n1. Service Description 1.1 Website Nature This website is a personal tech blog aimed at sharing technical knowledge, experience, and insights. Content includes but is not limited to:\nTechnical articles and tutorials Programming experience sharing Technical tool recommendations Industry commentary 1.2 Service Provision This website provides free access to content We reserve the right to modify, suspend, or terminate services at any time We do not guarantee continuous service availability 2. Intellectual Property 2.1 Original Content Original content on this website (including articles, images, code, etc.) is protected by copyright and owned by the author. Without permission, you may not:\n","content":"Terms of Service Last Updated: December 31, 2024\nWelcome to this tech blog. By using this website, you agree to comply with the following Terms of Service. If you do not agree with these terms, please do not use this website.\n1. Service Description 1.1 Website Nature This website is a personal tech blog aimed at sharing technical knowledge, experience, and insights. Content includes but is not limited to:\nTechnical articles and tutorials Programming experience sharing Technical tool …","date":"2025-12-31","lastmod":"2025-12-31","tags":null,"categories":null,"author":"Author","readingTime":5,"wordCount":856,"section":"","type":"page","draft":false,"featured":false,"series":null}]