[{"title":"ç½‘ç«™æ›´æ–°æ—¥å¿—","url":"https://www.dishuihengxin.com/changelog/changelog/","summary":"ğŸ‰ æŠ€æœ¯åšå®¢ v1.0.0 æ­£å¼å‘å¸ƒ å‘å¸ƒæ—¥æœŸ: 2025å¹´12æœˆ31æ—¥\nç»è¿‡ç²¾å¿ƒè®¾è®¡ä¸å¼€å‘ï¼ŒæŠ€æœ¯åšå®¢æ­£å¼ä¸Šçº¿ï¼æœ¬æ¬¡å‘å¸ƒå¸¦æ¥äº†å…¨æ–°çš„è§†è§‰ä½“éªŒå’Œå¼ºå¤§çš„åŠŸèƒ½ç‰¹æ€§ã€‚\nâœ¨ æ ¸å¿ƒåŠŸèƒ½äº®ç‚¹ å“åº”å¼è®¾è®¡ \u0026amp; æ·±è‰²æ¨¡å¼ å®Œç¾é€‚é…å„ç§è®¾å¤‡ï¼Œä»æ‰‹æœºåˆ°æ¡Œé¢éƒ½èƒ½è·å¾—æœ€ä½³ä½“éªŒã€‚æ”¯æŒæ·±è‰²æ¨¡å¼ï¼Œä¿æŠ¤æ‚¨çš„çœ¼ç›ã€‚\næ¶²æ€ç»ç’ƒè§†è§‰é£æ ¼ é‡‡ç”¨ç°ä»£åŒ–çš„æ¶²æ€ç»ç’ƒï¼ˆGlassmorphismï¼‰è®¾è®¡è¯­è¨€ï¼Œé…åˆç²¾å¿ƒè®¾è®¡çš„è¿‡æ¸¡åŠ¨ç”»å’Œäº¤äº’æ•ˆæœï¼Œå¸¦æ¥æ²‰æµ¸å¼çš„é˜…è¯»ä½“éªŒã€‚\næ™ºèƒ½åˆ†ç±»ä¸æœç´¢ å®Œå–„çš„åˆ†ç±»å’Œæ ‡ç­¾ç³»ç»Ÿï¼Œé…åˆå¼ºå¤§çš„å…¨ç«™æœç´¢åŠŸèƒ½ï¼Œè®©æ‚¨å¿«é€Ÿæ‰¾åˆ°æ‰€éœ€å†…å®¹ã€‚\nå†…å®¹ç®¡ç† ğŸ“ Markdown æ”¯æŒ - å®Œæ•´çš„ Markdown æ–‡ç« ç¼–å†™æ”¯æŒï¼ŒåŒ…æ‹¬ä»£ç é«˜äº®ã€è¡¨æ ¼ã€æ•°å­¦å…¬å¼ç­‰ ğŸ·ï¸ åˆ†ç±»æ ‡ç­¾ç³»ç»Ÿ - å®Œå–„çš„åˆ†ç±»å’Œæ ‡ç­¾ç³»ç»Ÿï¼Œæ–¹ä¾¿å†…å®¹ç»„ç»‡å’Œæ£€ç´¢ ğŸ” å…¨ç«™æœç´¢ - å¼ºå¤§çš„æœç´¢åŠŸèƒ½ï¼Œå¿«é€Ÿæ‰¾åˆ°æ‚¨éœ€è¦çš„å†…å®¹ ç”¨æˆ·ä½“éªŒ ğŸ¨ è§†å›¾åˆ‡æ¢ - æ–‡ç« åˆ—è¡¨æ”¯æŒå¡ç‰‡è§†å›¾å’Œåˆ—è¡¨è§†å›¾è‡ªç”±åˆ‡æ¢ ğŸ” å¢å¼ºç­›é€‰ - åˆ†ç±»ç­›é€‰åœ¨æ‰€æœ‰è§†å›¾æ¨¡å¼ä¸‹ç»Ÿä¸€å¯ç”¨ ğŸŒ“ æ·±è‰²æ¨¡å¼ - ä¸€é”®åˆ‡æ¢ï¼Œè‡ªåŠ¨ä¿å­˜åå¥½è®¾ç½® ğŸ“± å“åº”å¼è®¾è®¡ - å®Œç¾é€‚é…å„ç§è®¾å¤‡ è®¾è®¡ç‰¹è‰² æ¶²æ€ç»ç’ƒé£æ ¼ - ç°ä»£åŒ–çš„æ¯›ç»ç’ƒè§†è§‰æ•ˆæœ æµç•…åŠ¨ç”» - ç²¾å¿ƒè®¾è®¡çš„è¿‡æ¸¡åŠ¨ç”»å’Œäº¤äº’æ•ˆæœ ä¼˜é›…æ’ç‰ˆ - ä¸“ä¸šçš„æ–‡ç« æ’ç‰ˆï¼Œæä¾›æœ€ä½³é˜…è¯»ä½“éªŒ AI è§†è§‰å…ƒç´  - èå…¥ AI é£æ ¼çš„è§†è§‰è®¾è®¡å…ƒç´  æ€§èƒ½ä¼˜åŒ– ä¼˜åŒ–äº†å…¨å±€æ»šåŠ¨æ¡æ ·å¼ï¼Œæ›´ç»†æ›´ç¾è§‚ æ”¹è¿›äº†æ¶²æ€ç»ç’ƒæ•ˆæœçš„è§†è§‰å‘ˆç° ä¼˜åŒ–äº†å¡ç‰‡æ‚¬åœåŠ¨ç”»æ•ˆæœ é‡æ„äº†ç­›é€‰å’Œåˆ†é¡µçš„ JavaScript é€»è¾‘ ä¼˜åŒ–äº† DOM æ“ä½œæ€§èƒ½ ğŸš€ æŠ€æœ¯æ ˆ æŠ€æœ¯ ç”¨é€” Hugo é™æ€ç«™ç‚¹ç”Ÿæˆå™¨ CSS3 ç°ä»£åŒ–æ ·å¼è®¾è®¡ JavaScript äº¤äº’åŠŸèƒ½å®ç° Markdown å†…å®¹ç¼–å†™æ ¼å¼ æ•ˆæœé¢„è§ˆ ","content":"ğŸ‰ æŠ€æœ¯åšå®¢ v1.0.0 æ­£å¼å‘å¸ƒ å‘å¸ƒæ—¥æœŸ: 2025å¹´12æœˆ31æ—¥\nç»è¿‡ç²¾å¿ƒè®¾è®¡ä¸å¼€å‘ï¼ŒæŠ€æœ¯åšå®¢æ­£å¼ä¸Šçº¿ï¼æœ¬æ¬¡å‘å¸ƒå¸¦æ¥äº†å…¨æ–°çš„è§†è§‰ä½“éªŒå’Œå¼ºå¤§çš„åŠŸèƒ½ç‰¹æ€§ã€‚\nâœ¨ æ ¸å¿ƒåŠŸèƒ½äº®ç‚¹ å“åº”å¼è®¾è®¡ \u0026amp;amp; æ·±è‰²æ¨¡å¼ å®Œç¾é€‚é…å„ç§è®¾å¤‡ï¼Œä»æ‰‹æœºåˆ°æ¡Œé¢éƒ½èƒ½è·å¾—æœ€ä½³ä½“éªŒã€‚æ”¯æŒæ·±è‰²æ¨¡å¼ï¼Œä¿æŠ¤æ‚¨çš„çœ¼ç›ã€‚\næ¶²æ€ç»ç’ƒè§†è§‰é£æ ¼ é‡‡ç”¨ç°ä»£åŒ–çš„æ¶²æ€ç»ç’ƒï¼ˆGlassmorphismï¼‰è®¾è®¡è¯­è¨€ï¼Œé…åˆç²¾å¿ƒè®¾è®¡çš„è¿‡æ¸¡åŠ¨ç”»å’Œäº¤äº’æ•ˆæœï¼Œå¸¦æ¥æ²‰æµ¸å¼çš„é˜…è¯»ä½“éªŒã€‚\næ™ºèƒ½åˆ†ç±»ä¸æœç´¢ å®Œå–„çš„åˆ†ç±»å’Œæ ‡ç­¾ç³»ç»Ÿï¼Œé…åˆå¼ºå¤§çš„å…¨ç«™æœç´¢åŠŸèƒ½ï¼Œè®©æ‚¨å¿«é€Ÿæ‰¾åˆ°æ‰€éœ€å†…å®¹ã€‚\nå†…å®¹ç®¡ç† ğŸ“ Markdown æ”¯æŒ - å®Œæ•´çš„ Markdown æ–‡ç« ç¼–å†™æ”¯æŒï¼ŒåŒ…æ‹¬ä»£ç é«˜äº®ã€è¡¨æ ¼ã€æ•°å­¦å…¬å¼ç­‰ ğŸ·ï¸ åˆ†ç±»æ ‡ç­¾ç³»ç»Ÿ - å®Œå–„çš„åˆ†ç±»å’Œæ ‡ç­¾ç³»ç»Ÿï¼Œæ–¹ä¾¿å†…å®¹ç»„ç»‡å’Œæ£€ç´¢ ğŸ” å…¨ç«™æœç´¢ - å¼ºå¤§çš„æœç´¢åŠŸèƒ½ï¼Œå¿«é€Ÿæ‰¾åˆ°æ‚¨éœ€è¦çš„å†…å®¹ ç”¨æˆ·ä½“éªŒ ğŸ¨ è§†å›¾åˆ‡æ¢ - æ–‡ç« åˆ—è¡¨æ”¯æŒå¡ç‰‡è§†å›¾å’Œåˆ—è¡¨è§†å›¾è‡ªç”±åˆ‡æ¢ ğŸ” å¢å¼ºç­›é€‰ - åˆ†ç±»ç­›é€‰åœ¨æ‰€æœ‰è§†å›¾æ¨¡å¼ä¸‹ç»Ÿä¸€å¯ç”¨ ğŸŒ“ æ·±è‰²æ¨¡å¼ - ä¸€é”®åˆ‡æ¢ï¼Œè‡ªåŠ¨ä¿å­˜åå¥½è®¾ç½® ğŸ“± å“åº”å¼è®¾è®¡ - å®Œç¾é€‚é…å„ç§è®¾å¤‡ è®¾è®¡ç‰¹è‰² æ¶²æ€ç»ç’ƒé£æ ¼ - ç°ä»£åŒ–çš„æ¯›ç»ç’ƒè§†è§‰æ•ˆ â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":null,"categories":null,"author":"åšä¸»","readingTime":1,"wordCount":96,"section":"changelog","type":"changelog","draft":false,"featured":false,"series":null},{"title":"Database Performance Optimization Strategies: From Query Tuning to System Architecture","url":"https://www.dishuihengxin.com/posts/database-performance-optimization/","summary":"Database Performance Optimization Strategies: From Query Tuning to System Architecture Database performance optimization is a critical aspect of modern application development. As data volumes grow and user expectations increase, the ability to efficiently query and manipulate data becomes paramount. This comprehensive guide explores various strategies and techniques for optimizing database performance, from basic query tuning to advanced architectural considerations.\nTable of Contents Performance Analysis and Monitoring Query Optimization Techniques Indexing Strategies Database Schema Design System-Level Optimization Caching Strategies Partitioning and Sharding Monitoring and Alerting Performance Analysis and Monitoring 1. Performance Metrics Collection #!/usr/bin/env python3 # src/monitoring/performance_monitor.py import time import psutil import pymysql import psycopg2 import logging import json from typing import Dict, List, Any, Optional from dataclasses import dataclass, asdict from datetime import datetime, timedelta import threading from collections import defaultdict, deque @dataclass class QueryMetrics: query_id: str query_text: str execution_time: float rows_examined: int rows_returned: int cpu_usage: float memory_usage: int timestamp: datetime database_name: str user: str @dataclass class SystemMetrics: timestamp: datetime cpu_percent: float memory_percent: float disk_io_read: int disk_io_write: int network_io_sent: int network_io_recv: int active_connections: int slow_queries: int class DatabasePerformanceMonitor: def __init__(self, config: Dict[str, Any]): self.config = config self.logger = self._setup_logging() # Metrics storage self.query_metrics: deque = deque(maxlen=10000) self.system_metrics: deque = deque(maxlen=1000) # Performance thresholds self.slow_query_threshold = config.get(\u0026#39;slow_query_threshold\u0026#39;, 1.0) # seconds self.cpu_threshold = config.get(\u0026#39;cpu_threshold\u0026#39;, 80.0) # percent self.memory_threshold = config.get(\u0026#39;memory_threshold\u0026#39;, 85.0) # percent # Database connections self.db_connections = {} self._initialize_connections() # Monitoring state self.monitoring_active = False self.monitor_thread = None def _setup_logging(self) -\u0026gt; logging.Logger: \u0026#34;\u0026#34;\u0026#34;Setup logging configuration\u0026#34;\u0026#34;\u0026#34; logger = logging.getLogger(\u0026#39;PerformanceMonitor\u0026#39;) logger.setLevel(logging.INFO) handler = logging.FileHandler(\u0026#39;performance_monitor.log\u0026#39;) formatter = logging.Formatter( \u0026#39;%(asctime)s - %(name)s - %(levelname)s - %(message)s\u0026#39; ) handler.setFormatter(formatter) logger.addHandler(handler) return logger def _initialize_connections(self): \u0026#34;\u0026#34;\u0026#34;Initialize database connections\u0026#34;\u0026#34;\u0026#34; for db_config in self.config.get(\u0026#39;databases\u0026#39;, []): db_type = db_config[\u0026#39;type\u0026#39;] db_name = db_config[\u0026#39;name\u0026#39;] try: if db_type == \u0026#39;mysql\u0026#39;: conn = pymysql.connect( host=db_config[\u0026#39;host\u0026#39;], port=db_config[\u0026#39;port\u0026#39;], user=db_config[\u0026#39;user\u0026#39;], password=db_config[\u0026#39;password\u0026#39;], database=db_config[\u0026#39;database\u0026#39;], charset=\u0026#39;utf8mb4\u0026#39; ) elif db_type == \u0026#39;postgresql\u0026#39;: conn = psycopg2.connect( host=db_config[\u0026#39;host\u0026#39;], port=db_config[\u0026#39;port\u0026#39;], user=db_config[\u0026#39;user\u0026#39;], password=db_config[\u0026#39;password\u0026#39;], database=db_config[\u0026#39;database\u0026#39;] ) else: continue self.db_connections[db_name] = { \u0026#39;connection\u0026#39;: conn, \u0026#39;type\u0026#39;: db_type, \u0026#39;config\u0026#39;: db_config } self.logger.info(f\u0026#34;Connected to {db_type} database: {db_name}\u0026#34;) except Exception as e: self.logger.error(f\u0026#34;Failed to connect to {db_name}: {e}\u0026#34;) def start_monitoring(self): \u0026#34;\u0026#34;\u0026#34;Start performance monitoring\u0026#34;\u0026#34;\u0026#34; if self.monitoring_active: self.logger.warning(\u0026#34;Monitoring is already active\u0026#34;) return self.monitoring_active = True self.monitor_thread = threading.Thread(target=self._monitoring_loop) self.monitor_thread.daemon = True self.monitor_thread.start() self.logger.info(\u0026#34;Performance monitoring started\u0026#34;) def stop_monitoring(self): \u0026#34;\u0026#34;\u0026#34;Stop performance monitoring\u0026#34;\u0026#34;\u0026#34; self.monitoring_active = False if self.monitor_thread: self.monitor_thread.join() self.logger.info(\u0026#34;Performance monitoring stopped\u0026#34;) def _monitoring_loop(self): \u0026#34;\u0026#34;\u0026#34;Main monitoring loop\u0026#34;\u0026#34;\u0026#34; while self.monitoring_active: try: # Collect system metrics system_metrics = self._collect_system_metrics() self.system_metrics.append(system_metrics) # Collect database metrics for db_name, db_info in self.db_connections.items(): self._collect_database_metrics(db_name, db_info) # Check for performance issues self._check_performance_alerts() # Sleep for monitoring interval time.sleep(self.config.get(\u0026#39;monitoring_interval\u0026#39;, 30)) except Exception as e: self.logger.error(f\u0026#34;Error in monitoring loop: {e}\u0026#34;) time.sleep(5) def _collect_system_metrics(self) -\u0026gt; SystemMetrics: \u0026#34;\u0026#34;\u0026#34;Collect system-level metrics\u0026#34;\u0026#34;\u0026#34; # CPU and Memory cpu_percent = psutil.cpu_percent(interval=1) memory = psutil.virtual_memory() # Disk I/O disk_io = psutil.disk_io_counters() # Network I/O network_io = psutil.net_io_counters() # Database connections (simplified) active_connections = sum( self._get_active_connections(db_name, db_info) for db_name, db_info in self.db_connections.items() ) # Slow queries count slow_queries = self._count_recent_slow_queries() return SystemMetrics( timestamp=datetime.now(), cpu_percent=cpu_percent, memory_percent=memory.percent, disk_io_read=disk_io.read_bytes if disk_io else 0, disk_io_write=disk_io.write_bytes if disk_io else 0, network_io_sent=network_io.bytes_sent if network_io else 0, network_io_recv=network_io.bytes_recv if network_io else 0, active_connections=active_connections, slow_queries=slow_queries ) def _collect_database_metrics(self, db_name: str, db_info: Dict[str, Any]): \u0026#34;\u0026#34;\u0026#34;Collect database-specific metrics\u0026#34;\u0026#34;\u0026#34; try: conn = db_info[\u0026#39;connection\u0026#39;] db_type = db_info[\u0026#39;type\u0026#39;] if db_type == \u0026#39;mysql\u0026#39;: self._collect_mysql_metrics(db_name, conn) elif db_type == \u0026#39;postgresql\u0026#39;: self._collect_postgresql_metrics(db_name, conn) except Exception as e: self.logger.error(f\u0026#34;Error collecting metrics for {db_name}: {e}\u0026#34;) def _collect_mysql_metrics(self, db_name: str, conn): \u0026#34;\u0026#34;\u0026#34;Collect MySQL-specific metrics\u0026#34;\u0026#34;\u0026#34; cursor = conn.cursor() try: # Get slow query log entries cursor.execute(\u0026#34;\u0026#34;\u0026#34; SELECT sql_text, query_time, rows_examined, rows_sent, start_time, user_host FROM mysql.slow_log WHERE start_time \u0026gt; %s ORDER BY start_time DESC LIMIT 100 \u0026#34;\u0026#34;\u0026#34;, (datetime.now() - timedelta(minutes=5),)) for row in cursor.fetchall(): query_metrics = QueryMetrics( query_id=self._generate_query_id(row[0]), query_text=row[0][:500], # Truncate long queries execution_time=float(row[1]), rows_examined=int(row[2]), rows_returned=int(row[3]), cpu_usage=0.0, # Not available in slow log memory_usage=0, # Not available in slow log timestamp=row[4], database_name=db_name, user=row[5].split(\u0026#39;@\u0026#39;)[0] if \u0026#39;@\u0026#39; in row[5] else row[5] ) self.query_metrics.append(query_metrics) # Get current process list for active queries cursor.execute(\u0026#34;SHOW PROCESSLIST\u0026#34;) active_queries = cursor.fetchall() for process in active_queries: if process[4] and process[6] and process[6] != \u0026#39;Sleep\u0026#39;: # This is an active query query_metrics = QueryMetrics( query_id=self._generate_query_id(process[7] or \u0026#39;\u0026#39;), query_text=(process[7] or \u0026#39;\u0026#39;)[:500], execution_time=float(process[5] or 0), rows_examined=0, # Not available in process list rows_returned=0, # Not available in process list cpu_usage=0.0, memory_usage=0, timestamp=datetime.now(), database_name=db_name, user=process[1] or \u0026#39;\u0026#39; ) self.query_metrics.append(query_metrics) except Exception as e: self.logger.error(f\u0026#34;Error collecting MySQL metrics: {e}\u0026#34;) finally: cursor.close() def _collect_postgresql_metrics(self, db_name: str, conn): \u0026#34;\u0026#34;\u0026#34;Collect PostgreSQL-specific metrics\u0026#34;\u0026#34;\u0026#34; cursor = conn.cursor() try: # Get active queries cursor.execute(\u0026#34;\u0026#34;\u0026#34; SELECT query, state, query_start, usename, backend_start, state_change FROM pg_stat_activity WHERE state = \u0026#39;active\u0026#39; AND query NOT LIKE \u0026#39;%pg_stat_activity%\u0026#39; \u0026#34;\u0026#34;\u0026#34;) for row in cursor.fetchall(): execution_time = (datetime.now() - row[2]).total_seconds() if row[2] else 0 query_metrics = QueryMetrics( query_id=self._generate_query_id(row[0]), query_text=row[0][:500], execution_time=execution_time, rows_examined=0, # Not directly available rows_returned=0, # Not directly available cpu_usage=0.0, memory_usage=0, timestamp=datetime.now(), database_name=db_name, user=row[3] ) self.query_metrics.append(query_metrics) # Get query statistics from pg_stat_statements if available try: cursor.execute(\u0026#34;\u0026#34;\u0026#34; SELECT query, calls, total_time, mean_time, rows FROM pg_stat_statements WHERE last_exec \u0026gt; %s ORDER BY total_time DESC LIMIT 50 \u0026#34;\u0026#34;\u0026#34;, (datetime.now() - timedelta(minutes=5),)) for row in cursor.fetchall(): query_metrics = QueryMetrics( query_id=self._generate_query_id(row[0]), query_text=row[0][:500], execution_time=float(row[3]) / 1000.0, # Convert to seconds rows_examined=0, rows_returned=int(row[4]), cpu_usage=0.0, memory_usage=0, timestamp=datetime.now(), database_name=db_name, user=\u0026#39;system\u0026#39; ) self.query_metrics.append(query_metrics) except Exception: # pg_stat_statements extension not available pass except Exception as e: self.logger.error(f\u0026#34;Error collecting PostgreSQL metrics: {e}\u0026#34;) finally: cursor.close() def _get_active_connections(self, db_name: str, db_info: Dict[str, Any]) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Get number of active connections\u0026#34;\u0026#34;\u0026#34; try: conn = db_info[\u0026#39;connection\u0026#39;] db_type = db_info[\u0026#39;type\u0026#39;] cursor = conn.cursor() if db_type == \u0026#39;mysql\u0026#39;: cursor.execute(\u0026#34;SHOW STATUS LIKE \u0026#39;Threads_connected\u0026#39;\u0026#34;) result = cursor.fetchone() return int(result[1]) if result else 0 elif db_type == \u0026#39;postgresql\u0026#39;: cursor.execute(\u0026#34;SELECT count(*) FROM pg_stat_activity\u0026#34;) result = cursor.fetchone() return int(result[0]) if result else 0 except Exception as e: self.logger.error(f\u0026#34;Error getting active connections for {db_name}: {e}\u0026#34;) return 0 finally: if \u0026#39;cursor\u0026#39; in locals(): cursor.close() def _count_recent_slow_queries(self) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Count slow queries in recent time window\u0026#34;\u0026#34;\u0026#34; cutoff_time = datetime.now() - timedelta(minutes=5) return sum( 1 for metric in self.query_metrics if metric.timestamp \u0026gt; cutoff_time and metric.execution_time \u0026gt; self.slow_query_threshold ) def _check_performance_alerts(self): \u0026#34;\u0026#34;\u0026#34;Check for performance issues and generate alerts\u0026#34;\u0026#34;\u0026#34; if not self.system_metrics: return latest_metrics = self.system_metrics[-1] # CPU threshold alert if latest_metrics.cpu_percent \u0026gt; self.cpu_threshold: self._send_alert( \u0026#39;HIGH_CPU\u0026#39;, f\u0026#34;CPU usage is {latest_metrics.cpu_percent:.1f}%, \u0026#34; f\u0026#34;exceeding threshold of {self.cpu_threshold}%\u0026#34; ) # Memory threshold alert if latest_metrics.memory_percent \u0026gt; self.memory_threshold: self._send_alert( \u0026#39;HIGH_MEMORY\u0026#39;, f\u0026#34;Memory usage is {latest_metrics.memory_percent:.1f}%, \u0026#34; f\u0026#34;exceeding threshold of {self.memory_threshold}%\u0026#34; ) # Slow query alert if latest_metrics.slow_queries \u0026gt; 10: self._send_alert( \u0026#39;SLOW_QUERIES\u0026#39;, f\u0026#34;Detected {latest_metrics.slow_queries} slow queries in the last 5 minutes\u0026#34; ) def _send_alert(self, alert_type: str, message: str): \u0026#34;\u0026#34;\u0026#34;Send performance alert\u0026#34;\u0026#34;\u0026#34; alert = { \u0026#39;type\u0026#39;: alert_type, \u0026#39;message\u0026#39;: message, \u0026#39;timestamp\u0026#39;: datetime.now().isoformat(), \u0026#39;severity\u0026#39;: \u0026#39;WARNING\u0026#39; } self.logger.warning(f\u0026#34;ALERT [{alert_type}]: {message}\u0026#34;) # Here you could integrate with alerting systems like: # - Email notifications # - Slack/Teams webhooks # - PagerDuty # - Custom monitoring systems def _generate_query_id(self, query_text: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Generate unique ID for query\u0026#34;\u0026#34;\u0026#34; import hashlib return hashlib.md5(query_text.encode()).hexdigest()[:16] def get_performance_summary(self, hours: int = 24) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Get performance summary for specified time period\u0026#34;\u0026#34;\u0026#34; cutoff_time = datetime.now() - timedelta(hours=hours) # Filter metrics by time recent_queries = [ m for m in self.query_metrics if m.timestamp \u0026gt; cutoff_time ] recent_system = [ m for m in self.system_metrics if m.timestamp \u0026gt; cutoff_time ] if not recent_queries or not recent_system: return {\u0026#39;error\u0026#39;: \u0026#39;Insufficient data for analysis\u0026#39;} # Query analysis slow_queries = [q for q in recent_queries if q.execution_time \u0026gt; self.slow_query_threshold] avg_execution_time = sum(q.execution_time for q in recent_queries) / len(recent_queries) # System analysis avg_cpu = sum(s.cpu_percent for s in recent_system) / len(recent_system) avg_memory = sum(s.memory_percent for s in recent_system) / len(recent_system) max_connections = max(s.active_connections for s in recent_system) # Top slow queries top_slow_queries = sorted( slow_queries, key=lambda x: x.execution_time, reverse=True )[:10] return { \u0026#39;time_period_hours\u0026#39;: hours, \u0026#39;total_queries\u0026#39;: len(recent_queries), \u0026#39;slow_queries_count\u0026#39;: len(slow_queries), \u0026#39;slow_query_percentage\u0026#39;: (len(slow_queries) / len(recent_queries)) * 100, \u0026#39;average_execution_time\u0026#39;: avg_execution_time, \u0026#39;average_cpu_usage\u0026#39;: avg_cpu, \u0026#39;average_memory_usage\u0026#39;: avg_memory, \u0026#39;max_concurrent_connections\u0026#39;: max_connections, \u0026#39;top_slow_queries\u0026#39;: [ { \u0026#39;query_text\u0026#39;: q.query_text, \u0026#39;execution_time\u0026#39;: q.execution_time, \u0026#39;database\u0026#39;: q.database_name, \u0026#39;user\u0026#39;: q.user } for q in top_slow_queries ] } def export_metrics(self, filename: str, format: str = \u0026#39;json\u0026#39;): \u0026#34;\u0026#34;\u0026#34;Export collected metrics to file\u0026#34;\u0026#34;\u0026#34; data = { \u0026#39;query_metrics\u0026#39;: [asdict(m) for m in self.query_metrics], \u0026#39;system_metrics\u0026#39;: [asdict(m) for m in self.system_metrics], \u0026#39;export_timestamp\u0026#39;: datetime.now().isoformat() } if format == \u0026#39;json\u0026#39;: with open(filename, \u0026#39;w\u0026#39;) as f: json.dump(data, f, indent=2, default=str) else: raise ValueError(f\u0026#34;Unsupported format: {format}\u0026#34;) self.logger.info(f\u0026#34;Metrics exported to {filename}\u0026#34;) def main(): # Example configuration config = { \u0026#39;databases\u0026#39;: [ { \u0026#39;name\u0026#39;: \u0026#39;main_db\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;mysql\u0026#39;, \u0026#39;host\u0026#39;: \u0026#39;localhost\u0026#39;, \u0026#39;port\u0026#39;: 3306, \u0026#39;user\u0026#39;: \u0026#39;monitor_user\u0026#39;, \u0026#39;password\u0026#39;: \u0026#39;monitor_pass\u0026#39;, \u0026#39;database\u0026#39;: \u0026#39;production\u0026#39; }, { \u0026#39;name\u0026#39;: \u0026#39;analytics_db\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;postgresql\u0026#39;, \u0026#39;host\u0026#39;: \u0026#39;localhost\u0026#39;, \u0026#39;port\u0026#39;: 5432, \u0026#39;user\u0026#39;: \u0026#39;monitor_user\u0026#39;, \u0026#39;password\u0026#39;: \u0026#39;monitor_pass\u0026#39;, \u0026#39;database\u0026#39;: \u0026#39;analytics\u0026#39; } ], \u0026#39;slow_query_threshold\u0026#39;: 1.0, \u0026#39;cpu_threshold\u0026#39;: 80.0, \u0026#39;memory_threshold\u0026#39;: 85.0, \u0026#39;monitoring_interval\u0026#39;: 30 } # Create and start monitor monitor = DatabasePerformanceMonitor(config) try: monitor.start_monitoring() # Let it run for a while time.sleep(300) # 5 minutes # Get performance summary summary = monitor.get_performance_summary(hours=1) print(\u0026#34;Performance Summary:\u0026#34;) print(json.dumps(summary, indent=2)) # Export metrics monitor.export_metrics(\u0026#39;performance_metrics.json\u0026#39;) finally: monitor.stop_monitoring() if __name__ == \u0026#34;__main__\u0026#34;: main() Query Optimization Techniques 1. SQL Query Analyzer #!/usr/bin/env python3 # src/optimization/query_analyzer.py import re import sqlparse from sqlparse.sql import Statement, Token from sqlparse.tokens import Keyword, Name, Punctuation from typing import Dict, List, Any, Optional, Tuple from dataclasses import dataclass from enum import Enum class OptimizationLevel(Enum): LOW = \u0026#34;low\u0026#34; MEDIUM = \u0026#34;medium\u0026#34; HIGH = \u0026#34;high\u0026#34; CRITICAL = \u0026#34;critical\u0026#34; @dataclass class OptimizationSuggestion: level: OptimizationLevel category: str description: str original_query: str optimized_query: Optional[str] estimated_improvement: str explanation: str class SQLQueryAnalyzer: def __init__(self): self.optimization_rules = [ self._check_select_star, self._check_missing_where_clause, self._check_function_in_where, self._check_or_conditions, self._check_like_patterns, self._check_subquery_optimization, self._check_join_conditions, self._check_order_by_optimization, self._check_group_by_optimization, self._check_index_hints ] def analyze_query(self, query: str) -\u0026gt; List[OptimizationSuggestion]: \u0026#34;\u0026#34;\u0026#34;Analyze SQL query and provide optimization suggestions\u0026#34;\u0026#34;\u0026#34; suggestions = [] # Parse the query try: parsed = sqlparse.parse(query)[0] except Exception as e: return [OptimizationSuggestion( level=OptimizationLevel.CRITICAL, category=\u0026#34;Syntax Error\u0026#34;, description=f\u0026#34;Query parsing failed: {str(e)}\u0026#34;, original_query=query, optimized_query=None, estimated_improvement=\u0026#34;N/A\u0026#34;, explanation=\u0026#34;Fix syntax errors before optimization\u0026#34; )] # Apply optimization rules for rule in self.optimization_rules: try: rule_suggestions = rule(query, parsed) if rule_suggestions: suggestions.extend(rule_suggestions) except Exception as e: # Log error but continue with other rules print(f\u0026#34;Error in rule {rule.__name__}: {e}\u0026#34;) return suggestions def _check_select_star(self, query: str, parsed: Statement) -\u0026gt; List[OptimizationSuggestion]: \u0026#34;\u0026#34;\u0026#34;Check for SELECT * usage\u0026#34;\u0026#34;\u0026#34; suggestions = [] if re.search(r\u0026#39;\\bSELECT\\s+\\*\\b\u0026#39;, query, re.IGNORECASE): suggestions.append(OptimizationSuggestion( level=OptimizationLevel.MEDIUM, category=\u0026#34;Column Selection\u0026#34;, description=\u0026#34;Avoid SELECT * - specify only needed columns\u0026#34;, original_query=query, optimized_query=None, estimated_improvement=\u0026#34;10-50% reduction in I/O\u0026#34;, explanation=\u0026#34;SELECT * retrieves all columns, increasing network traffic and memory usage. \u0026#34; \u0026#34;Specify only the columns you actually need.\u0026#34; )) return suggestions def _check_missing_where_clause(self, query: str, parsed: Statement) -\u0026gt; List[OptimizationSuggestion]: \u0026#34;\u0026#34;\u0026#34;Check for missing WHERE clause in SELECT/UPDATE/DELETE\u0026#34;\u0026#34;\u0026#34; suggestions = [] query_upper = query.upper() # Check for SELECT without WHERE if (\u0026#39;SELECT\u0026#39; in query_upper and \u0026#39;WHERE\u0026#39; not in query_upper and \u0026#39;LIMIT\u0026#39; not in query_upper and \u0026#39;JOIN\u0026#39; not in query_upper): suggestions.append(OptimizationSuggestion( level=OptimizationLevel.HIGH, category=\u0026#34;Filtering\u0026#34;, description=\u0026#34;SELECT query without WHERE clause may scan entire table\u0026#34;, original_query=query, optimized_query=None, estimated_improvement=\u0026#34;90%+ reduction in execution time\u0026#34;, explanation=\u0026#34;Add WHERE clause to filter rows and use indexes effectively\u0026#34; )) # Check for UPDATE/DELETE without WHERE if ((\u0026#39;UPDATE\u0026#39; in query_upper or \u0026#39;DELETE\u0026#39; in query_upper) and \u0026#39;WHERE\u0026#39; not in query_upper): suggestions.append(OptimizationSuggestion( level=OptimizationLevel.CRITICAL, category=\u0026#34;Safety\u0026#34;, description=\u0026#34;UPDATE/DELETE without WHERE clause affects all rows\u0026#34;, original_query=query, optimized_query=None, estimated_improvement=\u0026#34;Prevents accidental data modification\u0026#34;, explanation=\u0026#34;Always use WHERE clause with UPDATE/DELETE to avoid modifying all rows\u0026#34; )) return suggestions def _check_function_in_where(self, query: str, parsed: Statement) -\u0026gt; List[OptimizationSuggestion]: \u0026#34;\u0026#34;\u0026#34;Check for functions applied to columns in WHERE clause\u0026#34;\u0026#34;\u0026#34; suggestions = [] # Common function patterns that prevent index usage function_patterns = [ r\u0026#39;\\bUPPER\\s*\\(\\s*\\w+\\s*\\)\u0026#39;, r\u0026#39;\\bLOWER\\s*\\(\\s*\\w+\\s*\\)\u0026#39;, r\u0026#39;\\bSUBSTRING\\s*\\(\\s*\\w+\u0026#39;, r\u0026#39;\\bDATE\\s*\\(\\s*\\w+\\s*\\)\u0026#39;, r\u0026#39;\\bYEAR\\s*\\(\\s*\\w+\\s*\\)\u0026#39;, r\u0026#39;\\bMONTH\\s*\\(\\s*\\w+\\s*\\)\u0026#39; ] for pattern in function_patterns: if re.search(pattern, query, re.IGNORECASE): suggestions.append(OptimizationSuggestion( level=OptimizationLevel.HIGH, category=\u0026#34;Index Usage\u0026#34;, description=\u0026#34;Functions on columns in WHERE clause prevent index usage\u0026#34;, original_query=query, optimized_query=None, estimated_improvement=\u0026#34;50-90% improvement with proper indexing\u0026#34;, explanation=\u0026#34;Move functions to the right side of comparison or use functional indexes\u0026#34; )) break return suggestions def _check_or_conditions(self, query: str, parsed: Statement) -\u0026gt; List[OptimizationSuggestion]: \u0026#34;\u0026#34;\u0026#34;Check for OR conditions that might benefit from UNION\u0026#34;\u0026#34;\u0026#34; suggestions = [] # Count OR conditions in WHERE clause where_match = re.search(r\u0026#39;WHERE\\s+(.*?)(?:\\s+GROUP\\s+BY|\\s+ORDER\\s+BY|\\s+LIMIT|$)\u0026#39;, query, re.IGNORECASE | re.DOTALL) if where_match: where_clause = where_match.group(1) or_count = len(re.findall(r\u0026#39;\\bOR\\b\u0026#39;, where_clause, re.IGNORECASE)) if or_count \u0026gt;= 3: suggestions.append(OptimizationSuggestion( level=OptimizationLevel.MEDIUM, category=\u0026#34;Query Structure\u0026#34;, description=f\u0026#34;Multiple OR conditions ({or_count}) may benefit from UNION\u0026#34;, original_query=query, optimized_query=None, estimated_improvement=\u0026#34;20-60% improvement with proper indexing\u0026#34;, explanation=\u0026#34;Consider rewriting multiple OR conditions as UNION queries \u0026#34; \u0026#34;to better utilize indexes\u0026#34; )) return suggestions def _check_like_patterns(self, query: str, parsed: Statement) -\u0026gt; List[OptimizationSuggestion]: \u0026#34;\u0026#34;\u0026#34;Check for inefficient LIKE patterns\u0026#34;\u0026#34;\u0026#34; suggestions = [] # Check for leading wildcard patterns leading_wildcard = re.findall(r\u0026#34;LIKE\\s+[\u0026#39;\\\u0026#34;]%[^\u0026#39;\\\u0026#34;]*[\u0026#39;\\\u0026#34;]\u0026#34;, query, re.IGNORECASE) if leading_wildcard: suggestions.append(OptimizationSuggestion( level=OptimizationLevel.HIGH, category=\u0026#34;Pattern Matching\u0026#34;, description=\u0026#34;LIKE patterns starting with % cannot use indexes\u0026#34;, original_query=query, optimized_query=None, estimated_improvement=\u0026#34;Consider full-text search or other alternatives\u0026#34;, explanation=\u0026#34;Leading wildcards force full table scans. \u0026#34; \u0026#34;Consider full-text indexes or alternative search methods\u0026#34; )) return suggestions def _check_subquery_optimization(self, query: str, parsed: Statement) -\u0026gt; List[OptimizationSuggestion]: \u0026#34;\u0026#34;\u0026#34;Check for subqueries that could be optimized\u0026#34;\u0026#34;\u0026#34; suggestions = [] # Check for correlated subqueries if re.search(r\u0026#39;WHERE\\s+\\w+\\s+IN\\s*\\(\\s*SELECT\u0026#39;, query, re.IGNORECASE): suggestions.append(OptimizationSuggestion( level=OptimizationLevel.MEDIUM, category=\u0026#34;Subquery Optimization\u0026#34;, description=\u0026#34;IN subquery might be optimized with JOIN\u0026#34;, original_query=query, optimized_query=None, estimated_improvement=\u0026#34;20-80% improvement depending on data size\u0026#34;, explanation=\u0026#34;Consider rewriting IN subqueries as JOINs for better performance\u0026#34; )) # Check for EXISTS subqueries if re.search(r\u0026#39;WHERE\\s+EXISTS\\s*\\(\\s*SELECT\u0026#39;, query, re.IGNORECASE): suggestions.append(OptimizationSuggestion( level=OptimizationLevel.LOW, category=\u0026#34;Subquery Optimization\u0026#34;, description=\u0026#34;EXISTS subquery detected - ensure proper indexing\u0026#34;, original_query=query, optimized_query=None, estimated_improvement=\u0026#34;Varies with indexing strategy\u0026#34;, explanation=\u0026#34;EXISTS subqueries can be efficient but require proper indexing \u0026#34; \u0026#34;on correlated columns\u0026#34; )) return suggestions def _check_join_conditions(self, query: str, parsed: Statement) -\u0026gt; List[OptimizationSuggestion]: \u0026#34;\u0026#34;\u0026#34;Check JOIN conditions and types\u0026#34;\u0026#34;\u0026#34; suggestions = [] # Check for CROSS JOINs or missing JOIN conditions if re.search(r\u0026#39;FROM\\s+\\w+\\s*,\\s*\\w+\u0026#39;, query, re.IGNORECASE): suggestions.append(OptimizationSuggestion( level=OptimizationLevel.HIGH, category=\u0026#34;JOIN Optimization\u0026#34;, description=\u0026#34;Comma-separated tables create CROSS JOIN\u0026#34;, original_query=query, optimized_query=None, estimated_improvement=\u0026#34;Significant - prevents cartesian products\u0026#34;, explanation=\u0026#34;Use explicit JOIN syntax with proper ON conditions \u0026#34; \u0026#34;to avoid cartesian products\u0026#34; )) # Check for JOIN without ON clause join_matches = re.findall(r\u0026#39;(\\w+\\s+)?JOIN\\s+\\w+(?!\\s+ON)\u0026#39;, query, re.IGNORECASE) if join_matches: suggestions.append(OptimizationSuggestion( level=OptimizationLevel.CRITICAL, category=\u0026#34;JOIN Optimization\u0026#34;, description=\u0026#34;JOIN without ON clause creates cartesian product\u0026#34;, original_query=query, optimized_query=None, estimated_improvement=\u0026#34;Critical - prevents query from running efficiently\u0026#34;, explanation=\u0026#34;Always specify ON conditions for JOINs to define the relationship\u0026#34; )) return suggestions def _check_order_by_optimization(self, query: str, parsed: Statement) -\u0026gt; List[OptimizationSuggestion]: \u0026#34;\u0026#34;\u0026#34;Check ORDER BY optimization opportunities\u0026#34;\u0026#34;\u0026#34; suggestions = [] # Check for ORDER BY without LIMIT if (\u0026#39;ORDER BY\u0026#39; in query.upper() and \u0026#39;LIMIT\u0026#39; not in query.upper() and \u0026#39;TOP\u0026#39; not in query.upper()): suggestions.append(OptimizationSuggestion( level=OptimizationLevel.MEDIUM, category=\u0026#34;Sorting\u0026#34;, description=\u0026#34;ORDER BY without LIMIT sorts entire result set\u0026#34;, original_query=query, optimized_query=None, estimated_improvement=\u0026#34;Consider adding LIMIT if appropriate\u0026#34;, explanation=\u0026#34;If you only need top N results, add LIMIT to avoid sorting \u0026#34; \u0026#34;the entire result set\u0026#34; )) return suggestions def _check_group_by_optimization(self, query: str, parsed: Statement) -\u0026gt; List[OptimizationSuggestion]: \u0026#34;\u0026#34;\u0026#34;Check GROUP BY optimization opportunities\u0026#34;\u0026#34;\u0026#34; suggestions = [] # Check for GROUP BY with ORDER BY on different columns group_match = re.search(r\u0026#39;GROUP\\s+BY\\s+([\\w\\s,]+)\u0026#39;, query, re.IGNORECASE) order_match = re.search(r\u0026#39;ORDER\\s+BY\\s+([\\w\\s,]+)\u0026#39;, query, re.IGNORECASE) if group_match and order_match: group_cols = [col.strip() for col in group_match.group(1).split(\u0026#39;,\u0026#39;)] order_cols = [col.strip().split()[0] for col in order_match.group(1).split(\u0026#39;,\u0026#39;)] if not any(col in group_cols for col in order_cols): suggestions.append(OptimizationSuggestion( level=OptimizationLevel.MEDIUM, category=\u0026#34;Grouping and Sorting\u0026#34;, description=\u0026#34;ORDER BY columns differ from GROUP BY columns\u0026#34;, original_query=query, optimized_query=None, estimated_improvement=\u0026#34;Consider ordering by GROUP BY columns\u0026#34;, explanation=\u0026#34;Ordering by GROUP BY columns can be more efficient \u0026#34; \u0026#34;as the data is already grouped\u0026#34; )) return suggestions def _check_index_hints(self, query: str, parsed: Statement) -\u0026gt; List[OptimizationSuggestion]: \u0026#34;\u0026#34;\u0026#34;Suggest potential indexing opportunities\u0026#34;\u0026#34;\u0026#34; suggestions = [] # Extract WHERE conditions where_match = re.search(r\u0026#39;WHERE\\s+(.*?)(?:\\s+GROUP\\s+BY|\\s+ORDER\\s+BY|\\s+LIMIT|$)\u0026#39;, query, re.IGNORECASE | re.DOTALL) if where_match: where_clause = where_match.group(1) # Find equality conditions eq_conditions = re.findall(r\u0026#39;(\\w+)\\s*=\\s*\u0026#39;, where_clause, re.IGNORECASE) if eq_conditions: suggestions.append(OptimizationSuggestion( level=OptimizationLevel.LOW, category=\u0026#34;Indexing\u0026#34;, description=f\u0026#34;Consider indexes on columns: {\u0026#39;, \u0026#39;.join(set(eq_conditions))}\u0026#34;, original_query=query, optimized_query=None, estimated_improvement=\u0026#34;Significant with proper indexing\u0026#34;, explanation=\u0026#34;Equality conditions in WHERE clause benefit from indexes\u0026#34; )) # Check JOIN conditions for indexing join_matches = re.findall(r\u0026#39;JOIN\\s+\\w+\\s+ON\\s+(\\w+\\.\\w+)\\s*=\\s*(\\w+\\.\\w+)\u0026#39;, query, re.IGNORECASE) if join_matches: join_columns = [] for match in join_matches: join_columns.extend([match[0], match[1]]) suggestions.append(OptimizationSuggestion( level=OptimizationLevel.LOW, category=\u0026#34;Indexing\u0026#34;, description=f\u0026#34;Ensure indexes on JOIN columns: {\u0026#39;, \u0026#39;.join(set(join_columns))}\u0026#34;, original_query=query, optimized_query=None, estimated_improvement=\u0026#34;Critical for JOIN performance\u0026#34;, explanation=\u0026#34;JOIN conditions require indexes on both sides for optimal performance\u0026#34; )) return suggestions def main(): # Example usage analyzer = SQLQueryAnalyzer() test_queries = [ \u0026#34;\u0026#34;\u0026#34; SELECT * FROM users u, orders o WHERE UPPER(u.email) = \u0026#39;TEST@EXAMPLE.COM\u0026#39; ORDER BY u.created_date \u0026#34;\u0026#34;\u0026#34;, \u0026#34;\u0026#34;\u0026#34; SELECT user_id, name, email FROM users WHERE status = \u0026#39;active\u0026#39; AND (category = \u0026#39;premium\u0026#39; OR category = \u0026#39;gold\u0026#39; OR category = \u0026#39;platinum\u0026#39;) ORDER BY created_date DESC LIMIT 10 \u0026#34;\u0026#34;\u0026#34;, \u0026#34;\u0026#34;\u0026#34; UPDATE users SET last_login = NOW() \u0026#34;\u0026#34;\u0026#34;, \u0026#34;\u0026#34;\u0026#34; SELECT u.name, COUNT(o.id) as order_count FROM users u LEFT JOIN orders o ON u.id = o.user_id WHERE u.email LIKE \u0026#39;%@gmail.com\u0026#39; GROUP BY u.id, u.name ORDER BY order_count DESC \u0026#34;\u0026#34;\u0026#34; ] for i, query in enumerate(test_queries, 1): print(f\u0026#34;\\n=== Query {i} Analysis ===\u0026#34;) print(f\u0026#34;Query: {query.strip()}\u0026#34;) suggestions = analyzer.analyze_query(query) if suggestions: print(f\u0026#34;\\nOptimization Suggestions ({len(suggestions)}):\u0026#34;) for j, suggestion in enumerate(suggestions, 1): print(f\u0026#34;\\n{j}. [{suggestion.level.value.upper()}] {suggestion.category}\u0026#34;) print(f\u0026#34; Description: {suggestion.description}\u0026#34;) print(f\u0026#34; Improvement: {suggestion.estimated_improvement}\u0026#34;) print(f\u0026#34; Explanation: {suggestion.explanation}\u0026#34;) else: print(\u0026#34;\\nNo optimization suggestions found.\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() Indexing Strategies 1. Index Recommendation Engine #!/usr/bin/env python3 # src/indexing/index_advisor.py import re import json import logging from typing import Dict, List, Any, Optional, Set, Tuple from dataclasses import dataclass, asdict from collections import defaultdict, Counter from enum import Enum class IndexType(Enum): BTREE = \u0026#34;btree\u0026#34; HASH = \u0026#34;hash\u0026#34; BITMAP = \u0026#34;bitmap\u0026#34; PARTIAL = \u0026#34;partial\u0026#34; COMPOSITE = \u0026#34;composite\u0026#34; COVERING = \u0026#34;covering\u0026#34; FUNCTIONAL = \u0026#34;functional\u0026#34; @dataclass class IndexRecommendation: table_name: str index_name: str index_type: IndexType columns: List[str] priority: int # 1-10, 10 being highest estimated_benefit: str creation_sql: str rationale: str query_patterns: List[str] estimated_size_mb: float @dataclass class QueryPattern: pattern_id: str query_template: str frequency: int avg_execution_time: float tables_accessed: List[str] where_columns: List[str] join_columns: List[str] order_by_columns: List[str] group_by_columns: List[str] class IndexAdvisor: def __init__(self, database_type: str = \u0026#39;mysql\u0026#39;): self.database_type = database_type.lower() self.logger = self._setup_logging() # Query patterns storage self.query_patterns: Dict[str, QueryPattern] = {} # Table metadata self.table_metadata: Dict[str, Dict[str, Any]] = {} # Existing indexes self.existing_indexes: Dict[str, List[Dict[str, Any]]] = {} # Configuration self.min_query_frequency = 10 self.min_execution_time = 0.1 # seconds self.max_index_columns = 5 def _setup_logging(self) -\u0026gt; logging.Logger: \u0026#34;\u0026#34;\u0026#34;Setup logging configuration\u0026#34;\u0026#34;\u0026#34; logger = logging.getLogger(\u0026#39;IndexAdvisor\u0026#39;) logger.setLevel(logging.INFO) handler = logging.StreamHandler() formatter = logging.Formatter( \u0026#39;%(asctime)s - %(name)s - %(levelname)s - %(message)s\u0026#39; ) handler.setFormatter(formatter) logger.addHandler(handler) return logger def analyze_query_workload(self, queries: List[Dict[str, Any]]) -\u0026gt; List[IndexRecommendation]: \u0026#34;\u0026#34;\u0026#34;Analyze query workload and generate index recommendations\u0026#34;\u0026#34;\u0026#34; # Step 1: Extract query patterns self._extract_query_patterns(queries) # Step 2: Analyze patterns for indexing opportunities recommendations = [] # Single column indexes recommendations.extend(self._recommend_single_column_indexes()) # Composite indexes recommendations.extend(self._recommend_composite_indexes()) # Covering indexes recommendations.extend(self._recommend_covering_indexes()) # Partial indexes recommendations.extend(self._recommend_partial_indexes()) # Step 3: Prioritize and filter recommendations recommendations = self._prioritize_recommendations(recommendations) return recommendations def _extract_query_patterns(self, queries: List[Dict[str, Any]]): \u0026#34;\u0026#34;\u0026#34;Extract patterns from query workload\u0026#34;\u0026#34;\u0026#34; pattern_counter = defaultdict(list) for query_data in queries: query_text = query_data.get(\u0026#39;query\u0026#39;, \u0026#39;\u0026#39;) execution_time = query_data.get(\u0026#39;execution_time\u0026#39;, 0) # Normalize query to create pattern normalized = self._normalize_query(query_text) pattern_id = self._generate_pattern_id(normalized) pattern_counter[pattern_id].append({ \u0026#39;query\u0026#39;: query_text, \u0026#39;normalized\u0026#39;: normalized, \u0026#39;execution_time\u0026#39;: execution_time }) # Create QueryPattern objects for pattern_id, query_list in pattern_counter.items(): if len(query_list) \u0026lt; self.min_query_frequency: continue representative_query = query_list[0][\u0026#39;normalized\u0026#39;] avg_execution_time = sum(q[\u0026#39;execution_time\u0026#39;] for q in query_list) / len(query_list) if avg_execution_time \u0026lt; self.min_execution_time: continue # Analyze query structure analysis = self._analyze_query_structure(representative_query) pattern = QueryPattern( pattern_id=pattern_id, query_template=representative_query, frequency=len(query_list), avg_execution_time=avg_execution_time, tables_accessed=analysis[\u0026#39;tables\u0026#39;], where_columns=analysis[\u0026#39;where_columns\u0026#39;], join_columns=analysis[\u0026#39;join_columns\u0026#39;], order_by_columns=analysis[\u0026#39;order_by_columns\u0026#39;], group_by_columns=analysis[\u0026#39;group_by_columns\u0026#39;] ) self.query_patterns[pattern_id] = pattern def _normalize_query(self, query: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Normalize query by replacing literals with placeholders\u0026#34;\u0026#34;\u0026#34; # Remove extra whitespace normalized = re.sub(r\u0026#39;\\s+\u0026#39;, \u0026#39; \u0026#39;, query.strip()) # Replace string literals normalized = re.sub(r\u0026#34;\u0026#39;[^\u0026#39;]*\u0026#39;\u0026#34;, \u0026#34;\u0026#39;?\u0026#39;\u0026#34;, normalized) # Replace numeric literals normalized = re.sub(r\u0026#39;\\b\\d+\\b\u0026#39;, \u0026#39;?\u0026#39;, normalized) # Convert to uppercase for consistency normalized = normalized.upper() return normalized def _generate_pattern_id(self, normalized_query: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Generate unique pattern ID\u0026#34;\u0026#34;\u0026#34; import hashlib return hashlib.md5(normalized_query.encode()).hexdigest()[:16] def _analyze_query_structure(self, query: str) -\u0026gt; Dict[str, List[str]]: \u0026#34;\u0026#34;\u0026#34;Analyze query structure to extract column usage\u0026#34;\u0026#34;\u0026#34; analysis = { \u0026#39;tables\u0026#39;: [], \u0026#39;where_columns\u0026#39;: [], \u0026#39;join_columns\u0026#39;: [], \u0026#39;order_by_columns\u0026#39;: [], \u0026#39;group_by_columns\u0026#39;: [] } # Extract tables table_matches = re.findall(r\u0026#39;FROM\\s+(\\w+)\u0026#39;, query, re.IGNORECASE) table_matches.extend(re.findall(r\u0026#39;JOIN\\s+(\\w+)\u0026#39;, query, re.IGNORECASE)) analysis[\u0026#39;tables\u0026#39;] = list(set(table_matches)) # Extract WHERE columns where_match = re.search(r\u0026#39;WHERE\\s+(.*?)(?:\\s+GROUP\\s+BY|\\s+ORDER\\s+BY|\\s+LIMIT|$)\u0026#39;, query, re.IGNORECASE | re.DOTALL) if where_match: where_clause = where_match.group(1) # Simple column extraction (can be improved) columns = re.findall(r\u0026#39;(\\w+)\\s*[=\u0026lt;\u0026gt;!]\u0026#39;, where_clause) analysis[\u0026#39;where_columns\u0026#39;] = list(set(columns)) # Extract JOIN columns join_matches = re.findall(r\u0026#39;ON\\s+(\\w+)\\.(\\w+)\\s*=\\s*(\\w+)\\.(\\w+)\u0026#39;, query, re.IGNORECASE) for match in join_matches: analysis[\u0026#39;join_columns\u0026#39;].extend([f\u0026#34;{match[0]}.{match[1]}\u0026#34;, f\u0026#34;{match[2]}.{match[3]}\u0026#34;]) # Extract ORDER BY columns order_match = re.search(r\u0026#39;ORDER\\s+BY\\s+(.*?)(?:\\s+LIMIT|$)\u0026#39;, query, re.IGNORECASE) if order_match: order_clause = order_match.group(1) columns = re.findall(r\u0026#39;(\\w+)\u0026#39;, order_clause) analysis[\u0026#39;order_by_columns\u0026#39;] = list(set(columns)) # Extract GROUP BY columns group_match = re.search(r\u0026#39;GROUP\\s+BY\\s+(.*?)(?:\\s+ORDER\\s+BY|\\s+LIMIT|$)\u0026#39;, query, re.IGNORECASE) if group_match: group_clause = group_match.group(1) columns = re.findall(r\u0026#39;(\\w+)\u0026#39;, group_clause) analysis[\u0026#39;group_by_columns\u0026#39;] = list(set(columns)) return analysis def _recommend_single_column_indexes(self) -\u0026gt; List[IndexRecommendation]: \u0026#34;\u0026#34;\u0026#34;Recommend single column indexes\u0026#34;\u0026#34;\u0026#34; recommendations = [] column_usage = defaultdict(list) # Collect column usage statistics for pattern in self.query_patterns.values(): for column in pattern.where_columns: column_usage[column].append({ \u0026#39;pattern\u0026#39;: pattern, \u0026#39;usage_type\u0026#39;: \u0026#39;where\u0026#39; }) for column in pattern.order_by_columns: column_usage[column].append({ \u0026#39;pattern\u0026#39;: pattern, \u0026#39;usage_type\u0026#39;: \u0026#39;order_by\u0026#39; }) # Generate recommendations for column, usages in column_usage.items(): if len(usages) \u0026lt; 2: # Skip columns used in only one pattern continue # Calculate priority based on usage frequency and execution time total_frequency = sum(usage[\u0026#39;pattern\u0026#39;].frequency for usage in usages) avg_execution_time = sum(usage[\u0026#39;pattern\u0026#39;].avg_execution_time for usage in usages) / len(usages) priority = min(10, int((total_frequency * avg_execution_time) / 10)) # Determine table name (simplified) table_name = self._infer_table_name(column, usages) if table_name: recommendation = IndexRecommendation( table_name=table_name, index_name=f\u0026#34;idx_{table_name}_{column}\u0026#34;, index_type=IndexType.BTREE, columns=[column], priority=priority, estimated_benefit=f\u0026#34;Improves {len(usages)} query patterns\u0026#34;, creation_sql=self._generate_index_sql(table_name, f\u0026#34;idx_{table_name}_{column}\u0026#34;, [column]), rationale=f\u0026#34;Column \u0026#39;{column}\u0026#39; is frequently used in WHERE/ORDER BY clauses\u0026#34;, query_patterns=[usage[\u0026#39;pattern\u0026#39;].pattern_id for usage in usages], estimated_size_mb=self._estimate_index_size(table_name, [column]) ) recommendations.append(recommendation) return recommendations def _recommend_composite_indexes(self) -\u0026gt; List[IndexRecommendation]: \u0026#34;\u0026#34;\u0026#34;Recommend composite indexes\u0026#34;\u0026#34;\u0026#34; recommendations = [] for pattern in self.query_patterns.values(): if len(pattern.where_columns) \u0026lt; 2: continue # Consider combinations of WHERE columns for table in pattern.tables_accessed: table_columns = [col for col in pattern.where_columns if not \u0026#39;.\u0026#39; in col or col.startswith(f\u0026#34;{table}.\u0026#34;)] if len(table_columns) \u0026gt;= 2: # Order columns by selectivity (simplified heuristic) ordered_columns = self._order_columns_by_selectivity(table_columns) if len(ordered_columns) \u0026lt;= self.max_index_columns: index_name = f\u0026#34;idx_{table}_composite_{\u0026#39;_\u0026#39;.join(ordered_columns[:3])}\u0026#34; recommendation = IndexRecommendation( table_name=table, index_name=index_name, index_type=IndexType.COMPOSITE, columns=ordered_columns, priority=min(10, pattern.frequency // 5), estimated_benefit=f\u0026#34;Optimizes multi-column WHERE conditions\u0026#34;, creation_sql=self._generate_index_sql(table, index_name, ordered_columns), rationale=f\u0026#34;Multiple columns used together in WHERE clause\u0026#34;, query_patterns=[pattern.pattern_id], estimated_size_mb=self._estimate_index_size(table, ordered_columns) ) recommendations.append(recommendation) return recommendations def _recommend_covering_indexes(self) -\u0026gt; List[IndexRecommendation]: \u0026#34;\u0026#34;\u0026#34;Recommend covering indexes\u0026#34;\u0026#34;\u0026#34; recommendations = [] for pattern in self.query_patterns.values(): if pattern.frequency \u0026lt; 50: # Only for high-frequency queries continue for table in pattern.tables_accessed: # Get all columns accessed for this table accessed_columns = set() accessed_columns.update(pattern.where_columns) accessed_columns.update(pattern.order_by_columns) # Remove table prefixes clean_columns = [col.split(\u0026#39;.\u0026#39;)[-1] for col in accessed_columns if not \u0026#39;.\u0026#39; in col or col.startswith(f\u0026#34;{table}.\u0026#34;)] if 2 \u0026lt;= len(clean_columns) \u0026lt;= self.max_index_columns: # Order: WHERE columns first, then ORDER BY where_cols = [col for col in clean_columns if col in pattern.where_columns] order_cols = [col for col in clean_columns if col in pattern.order_by_columns and col not in where_cols] covering_columns = where_cols + order_cols index_name = f\u0026#34;idx_{table}_covering_{\u0026#39;_\u0026#39;.join(covering_columns[:3])}\u0026#34; recommendation = IndexRecommendation( table_name=table, index_name=index_name, index_type=IndexType.COVERING, columns=covering_columns, priority=min(10, pattern.frequency // 10), estimated_benefit=\u0026#34;Eliminates table lookups (index-only scan)\u0026#34;, creation_sql=self._generate_index_sql(table, index_name, covering_columns), rationale=\u0026#34;Covers all columns needed for query execution\u0026#34;, query_patterns=[pattern.pattern_id], estimated_size_mb=self._estimate_index_size(table, covering_columns) ) recommendations.append(recommendation) return recommendations def _recommend_partial_indexes(self) -\u0026gt; List[IndexRecommendation]: \u0026#34;\u0026#34;\u0026#34;Recommend partial indexes for filtered queries\u0026#34;\u0026#34;\u0026#34; recommendations = [] if self.database_type not in [\u0026#39;postgresql\u0026#39;]: return recommendations # Partial indexes mainly supported in PostgreSQL for pattern in self.query_patterns.values(): # Look for queries with constant WHERE conditions query = pattern.query_template # Find constant conditions (simplified) constant_conditions = re.findall(r\u0026#34;(\\w+)\\s*=\\s*\u0026#39;[^\u0026#39;]*\u0026#39;\u0026#34;, query) if constant_conditions: for table in pattern.tables_accessed: for condition_col in constant_conditions: # Create partial index for other columns when this condition is met other_columns = [col for col in pattern.where_columns if col != condition_col] if other_columns: index_name = f\u0026#34;idx_{table}_{other_columns[0]}_partial\u0026#34; recommendation = IndexRecommendation( table_name=table, index_name=index_name, index_type=IndexType.PARTIAL, columns=other_columns[:2], # Limit to 2 columns priority=min(8, pattern.frequency // 20), estimated_benefit=\u0026#34;Smaller index size for filtered data\u0026#34;, creation_sql=self._generate_partial_index_sql( table, index_name, other_columns[:2], condition_col ), rationale=f\u0026#34;Frequent queries with constant {condition_col} condition\u0026#34;, query_patterns=[pattern.pattern_id], estimated_size_mb=self._estimate_index_size(table, other_columns[:2]) * 0.3 ) recommendations.append(recommendation) return recommendations def _prioritize_recommendations(self, recommendations: List[IndexRecommendation]) -\u0026gt; List[IndexRecommendation]: \u0026#34;\u0026#34;\u0026#34;Prioritize and filter recommendations\u0026#34;\u0026#34;\u0026#34; # Remove duplicates unique_recommendations = {} for rec in recommendations: key = (rec.table_name, tuple(rec.columns)) if key not in unique_recommendations or rec.priority \u0026gt; unique_recommendations[key].priority: unique_recommendations[key] = rec # Sort by priority sorted_recommendations = sorted( unique_recommendations.values(), key=lambda x: x.priority, reverse=True ) # Filter out low-priority recommendations filtered_recommendations = [rec for rec in sorted_recommendations if rec.priority \u0026gt;= 3] return filtered_recommendations[:20] # Limit to top 20 def _infer_table_name(self, column: str, usages: List[Dict[str, Any]]) -\u0026gt; Optional[str]: \u0026#34;\u0026#34;\u0026#34;Infer table name for a column\u0026#34;\u0026#34;\u0026#34; # Simple heuristic: use the most common table from patterns table_counter = Counter() for usage in usages: for table in usage[\u0026#39;pattern\u0026#39;].tables_accessed: table_counter[table] += 1 if table_counter: return table_counter.most_common(1)[0][0] return None def _order_columns_by_selectivity(self, columns: List[str]) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;Order columns by estimated selectivity (most selective first)\u0026#34;\u0026#34;\u0026#34; # Simplified heuristic: assume shorter column names are more selective # In practice, you\u0026#39;d use actual cardinality statistics return sorted(columns, key=len) def _generate_index_sql(self, table_name: str, index_name: str, columns: List[str]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Generate CREATE INDEX SQL\u0026#34;\u0026#34;\u0026#34; columns_str = \u0026#39;, \u0026#39;.join(columns) if self.database_type == \u0026#39;mysql\u0026#39;: return f\u0026#34;CREATE INDEX {index_name} ON {table_name} ({columns_str});\u0026#34; elif self.database_type == \u0026#39;postgresql\u0026#39;: return f\u0026#34;CREATE INDEX {index_name} ON {table_name} ({columns_str});\u0026#34; else: return f\u0026#34;CREATE INDEX {index_name} ON {table_name} ({columns_str});\u0026#34; def _generate_partial_index_sql(self, table_name: str, index_name: str, columns: List[str], condition_column: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Generate partial index SQL (PostgreSQL)\u0026#34;\u0026#34;\u0026#34; columns_str = \u0026#39;, \u0026#39;.join(columns) return f\u0026#34;CREATE INDEX {index_name} ON {table_name} ({columns_str}) WHERE {condition_column} IS NOT NULL;\u0026#34; def _estimate_index_size(self, table_name: str, columns: List[str]) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;Estimate index size in MB\u0026#34;\u0026#34;\u0026#34; # Simplified estimation base_size = 10.0 # Base size in MB column_factor = len(columns) * 2.0 return base_size + column_factor def set_table_metadata(self, metadata: Dict[str, Dict[str, Any]]): \u0026#34;\u0026#34;\u0026#34;Set table metadata for better recommendations\u0026#34;\u0026#34;\u0026#34; self.table_metadata = metadata def set_existing_indexes(self, indexes: Dict[str, List[Dict[str, Any]]]): \u0026#34;\u0026#34;\u0026#34;Set existing indexes to avoid duplicates\u0026#34;\u0026#34;\u0026#34; self.existing_indexes = indexes def export_recommendations(self, filename: str): \u0026#34;\u0026#34;\u0026#34;Export recommendations to JSON file\u0026#34;\u0026#34;\u0026#34; # This would be called after analyze_query_workload pass def main(): # Example usage advisor = IndexAdvisor(database_type=\u0026#39;mysql\u0026#39;) # Sample query workload sample_queries = [ { \u0026#39;query\u0026#39;: \u0026#34;SELECT * FROM users WHERE email = \u0026#39;test@example.com\u0026#39;\u0026#34;, \u0026#39;execution_time\u0026#39;: 0.5, \u0026#39;frequency\u0026#39;: 100 }, { \u0026#39;query\u0026#39;: \u0026#34;SELECT name, email FROM users WHERE status = \u0026#39;active\u0026#39; AND created_date \u0026gt; \u0026#39;2023-01-01\u0026#39;\u0026#34;, \u0026#39;execution_time\u0026#39;: 1.2, \u0026#39;frequency\u0026#39;: 50 }, { \u0026#39;query\u0026#39;: \u0026#34;SELECT u.name, COUNT(o.id) FROM users u JOIN orders o ON u.id = o.user_id WHERE u.status = \u0026#39;active\u0026#39; GROUP BY u.id ORDER BY COUNT(o.id) DESC\u0026#34;, \u0026#39;execution_time\u0026#39;: 2.5, \u0026#39;frequency\u0026#39;: 25 } ] * 10 # Simulate frequency # Analyze workload recommendations = advisor.analyze_query_workload(sample_queries) # Display recommendations print(f\u0026#34;Generated {len(recommendations)} index recommendations:\\n\u0026#34;) for i, rec in enumerate(recommendations, 1): print(f\u0026#34;{i}. {rec.index_name} (Priority: {rec.priority})\u0026#34;) print(f\u0026#34; Table: {rec.table_name}\u0026#34;) print(f\u0026#34; Columns: {\u0026#39;, \u0026#39;.join(rec.columns)}\u0026#34;) print(f\u0026#34; Type: {rec.index_type.value}\u0026#34;) print(f\u0026#34; Benefit: {rec.estimated_benefit}\u0026#34;) print(f\u0026#34; Size: {rec.estimated_size_mb:.1f} MB\u0026#34;) print(f\u0026#34; SQL: {rec.creation_sql}\u0026#34;) print(f\u0026#34; Rationale: {rec.rationale}\u0026#34;) print() if __name__ == \u0026#34;__main__\u0026#34;: main() Database Schema Design 1. Schema Optimization Analyzer #!/usr/bin/env python3 # src/schema/schema_analyzer.py import json import logging from typing import Dict, List, Any, Optional, Set from dataclasses import dataclass from enum import Enum class SchemaIssueType(Enum): NORMALIZATION = \u0026#34;normalization\u0026#34; DENORMALIZATION = \u0026#34;denormalization\u0026#34; DATA_TYPE = \u0026#34;data_type\u0026#34; CONSTRAINT = \u0026#34;constraint\u0026#34; RELATIONSHIP = \u0026#34;relationship\u0026#34; @dataclass class SchemaIssue: issue_type: SchemaIssueType severity: str # LOW, MEDIUM, HIGH, CRITICAL table_name: str column_name: Optional[str] description: str recommendation: str impact: str class SchemaAnalyzer: def __init__(self): self.logger = self._setup_logging() def _setup_logging(self) -\u0026gt; logging.Logger: logger = logging.getLogger(\u0026#39;SchemaAnalyzer\u0026#39;) logger.setLevel(logging.INFO) handler = logging.StreamHandler() formatter = logging.Formatter(\u0026#39;%(asctime)s - %(levelname)s - %(message)s\u0026#39;) handler.setFormatter(formatter) logger.addHandler(handler) return logger def analyze_schema(self, schema_metadata: Dict[str, Any]) -\u0026gt; List[SchemaIssue]: \u0026#34;\u0026#34;\u0026#34;Analyze database schema and identify optimization opportunities\u0026#34;\u0026#34;\u0026#34; issues = [] tables = schema_metadata.get(\u0026#39;tables\u0026#39;, {}) for table_name, table_info in tables.items(): # Check data types issues.extend(self._check_data_types(table_name, table_info)) # Check normalization issues.extend(self._check_normalization(table_name, table_info)) # Check constraints issues.extend(self._check_constraints(table_name, table_info)) # Check relationships issues.extend(self._check_relationships(table_name, table_info, tables)) return sorted(issues, key=lambda x: self._severity_weight(x.severity), reverse=True) def _check_data_types(self, table_name: str, table_info: Dict[str, Any]) -\u0026gt; List[SchemaIssue]: \u0026#34;\u0026#34;\u0026#34;Check for suboptimal data type choices\u0026#34;\u0026#34;\u0026#34; issues = [] columns = table_info.get(\u0026#39;columns\u0026#39;, {}) for column_name, column_info in columns.items(): data_type = column_info.get(\u0026#39;type\u0026#39;, \u0026#39;\u0026#39;).upper() # Check for oversized VARCHAR if \u0026#39;VARCHAR\u0026#39; in data_type: size_match = re.search(r\u0026#39;VARCHAR\\((\\d+)\\)\u0026#39;, data_type) if size_match: size = int(size_match.group(1)) if size \u0026gt; 1000: issues.append(SchemaIssue( issue_type=SchemaIssueType.DATA_TYPE, severity=\u0026#34;MEDIUM\u0026#34;, table_name=table_name, column_name=column_name, description=f\u0026#34;VARCHAR({size}) may be oversized\u0026#34;, recommendation=\u0026#34;Consider using TEXT for large text or smaller VARCHAR\u0026#34;, impact=\u0026#34;Increased storage and memory usage\u0026#34; )) # Check for CHAR vs VARCHAR if \u0026#39;CHAR(\u0026#39; in data_type and not column_name.endswith(\u0026#39;_code\u0026#39;): issues.append(SchemaIssue( issue_type=SchemaIssueType.DATA_TYPE, severity=\u0026#34;LOW\u0026#34;, table_name=table_name, column_name=column_name, description=\u0026#34;CHAR used for variable-length data\u0026#34;, recommendation=\u0026#34;Consider VARCHAR for variable-length strings\u0026#34;, impact=\u0026#34;Wasted storage space\u0026#34; )) # Check for inappropriate use of TEXT if data_type == \u0026#39;TEXT\u0026#39; and column_name in [\u0026#39;status\u0026#39;, \u0026#39;type\u0026#39;, \u0026#39;category\u0026#39;]: issues.append(SchemaIssue( issue_type=SchemaIssueType.DATA_TYPE, severity=\u0026#34;MEDIUM\u0026#34;, table_name=table_name, column_name=column_name, description=\u0026#34;TEXT used for short categorical data\u0026#34;, recommendation=\u0026#34;Use ENUM or small VARCHAR instead\u0026#34;, impact=\u0026#34;Inefficient storage and indexing\u0026#34; )) return issues def _check_normalization(self, table_name: str, table_info: Dict[str, Any]) -\u0026gt; List[SchemaIssue]: \u0026#34;\u0026#34;\u0026#34;Check for normalization issues\u0026#34;\u0026#34;\u0026#34; issues = [] columns = table_info.get(\u0026#39;columns\u0026#39;, {}) # Check for repeated column patterns (potential denormalization) column_names = list(columns.keys()) # Look for address fields that could be normalized address_fields = [col for col in column_names if any(addr in col.lower() for addr in [\u0026#39;address\u0026#39;, \u0026#39;street\u0026#39;, \u0026#39;city\u0026#39;, \u0026#39;state\u0026#39;, \u0026#39;zip\u0026#39;, \u0026#39;country\u0026#39;])] if len(address_fields) \u0026gt; 3: issues.append(SchemaIssue( issue_type=SchemaIssueType.NORMALIZATION, severity=\u0026#34;MEDIUM\u0026#34;, table_name=table_name, column_name=None, description=\u0026#34;Multiple address fields in single table\u0026#34;, recommendation=\u0026#34;Consider creating separate address table\u0026#34;, impact=\u0026#34;Data redundancy and maintenance complexity\u0026#34; )) # Check for JSON/TEXT columns that might need normalization json_columns = [col for col, info in columns.items() if info.get(\u0026#39;type\u0026#39;, \u0026#39;\u0026#39;).upper() in [\u0026#39;JSON\u0026#39;, \u0026#39;TEXT\u0026#39;] and col not in [\u0026#39;description\u0026#39;, \u0026#39;notes\u0026#39;, \u0026#39;content\u0026#39;]] if json_columns: issues.append(SchemaIssue( issue_type=SchemaIssueType.NORMALIZATION, severity=\u0026#34;LOW\u0026#34;, table_name=table_name, column_name=\u0026#39;, \u0026#39;.join(json_columns), description=\u0026#34;JSON/TEXT columns may contain structured data\u0026#34;, recommendation=\u0026#34;Consider normalizing if data has consistent structure\u0026#34;, impact=\u0026#34;Limited query capabilities and indexing\u0026#34; )) return issues def _check_constraints(self, table_name: str, table_info: Dict[str, Any]) -\u0026gt; List[SchemaIssue]: \u0026#34;\u0026#34;\u0026#34;Check for missing or inappropriate constraints\u0026#34;\u0026#34;\u0026#34; issues = [] columns = table_info.get(\u0026#39;columns\u0026#39;, {}) constraints = table_info.get(\u0026#39;constraints\u0026#39;, []) # Check for missing primary key has_primary_key = any(constraint.get(\u0026#39;type\u0026#39;) == \u0026#39;PRIMARY KEY\u0026#39; for constraint in constraints) if not has_primary_key: issues.append(SchemaIssue( issue_type=SchemaIssueType.CONSTRAINT, severity=\u0026#34;HIGH\u0026#34;, table_name=table_name, column_name=None, description=\u0026#34;Table lacks primary key\u0026#34;, recommendation=\u0026#34;Add primary key constraint\u0026#34;, impact=\u0026#34;Poor performance and replication issues\u0026#34; )) # Check for email columns without format constraints email_columns = [col for col in columns.keys() if \u0026#39;email\u0026#39; in col.lower()] for email_col in email_columns: has_email_constraint = any( constraint.get(\u0026#39;column\u0026#39;) == email_col and \u0026#39;email\u0026#39; in constraint.get(\u0026#39;definition\u0026#39;, \u0026#39;\u0026#39;).lower() for constraint in constraints ) if not has_email_constraint: issues.append(SchemaIssue( issue_type=SchemaIssueType.CONSTRAINT, severity=\u0026#34;MEDIUM\u0026#34;, table_name=table_name, column_name=email_col, description=\u0026#34;Email column lacks format validation\u0026#34;, recommendation=\u0026#34;Add CHECK constraint for email format\u0026#34;, impact=\u0026#34;Data quality issues\u0026#34; )) return issues def _check_relationships(self, table_name: str, table_info: Dict[str, Any], all_tables: Dict[str, Any]) -\u0026gt; List[SchemaIssue]: \u0026#34;\u0026#34;\u0026#34;Check for relationship issues\u0026#34;\u0026#34;\u0026#34; issues = [] columns = table_info.get(\u0026#39;columns\u0026#39;, {}) foreign_keys = table_info.get(\u0026#39;foreign_keys\u0026#39;, []) # Check for potential foreign key columns without constraints potential_fk_columns = [col for col in columns.keys() if col.endswith(\u0026#39;_id\u0026#39;) and col != \u0026#39;id\u0026#39;] existing_fk_columns = [fk.get(\u0026#39;column\u0026#39;) for fk in foreign_keys] for fk_col in potential_fk_columns: if fk_col not in existing_fk_columns: # Check if referenced table exists referenced_table = fk_col.replace(\u0026#39;_id\u0026#39;, \u0026#39;\u0026#39;) + \u0026#39;s\u0026#39; # Simple heuristic if referenced_table in all_tables: issues.append(SchemaIssue( issue_type=SchemaIssueType.RELATIONSHIP, severity=\u0026#34;MEDIUM\u0026#34;, table_name=table_name, column_name=fk_col, description=f\u0026#34;Potential foreign key {fk_col} lacks constraint\u0026#34;, recommendation=f\u0026#34;Add foreign key constraint to {referenced_table}\u0026#34;, impact=\u0026#34;Referential integrity not enforced\u0026#34; )) return issues def _severity_weight(self, severity: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Convert severity to numeric weight for sorting\u0026#34;\u0026#34;\u0026#34; weights = {\u0026#39;CRITICAL\u0026#39;: 4, \u0026#39;HIGH\u0026#39;: 3, \u0026#39;MEDIUM\u0026#39;: 2, \u0026#39;LOW\u0026#39;: 1} return weights.get(severity, 0) # Example usage def main(): analyzer = SchemaAnalyzer() # Sample schema metadata schema_metadata = { \u0026#39;tables\u0026#39;: { \u0026#39;users\u0026#39;: { \u0026#39;columns\u0026#39;: { \u0026#39;id\u0026#39;: {\u0026#39;type\u0026#39;: \u0026#39;INT\u0026#39;, \u0026#39;nullable\u0026#39;: False}, \u0026#39;email\u0026#39;: {\u0026#39;type\u0026#39;: \u0026#39;VARCHAR(255)\u0026#39;, \u0026#39;nullable\u0026#39;: False}, \u0026#39;name\u0026#39;: {\u0026#39;type\u0026#39;: \u0026#39;VARCHAR(1000)\u0026#39;, \u0026#39;nullable\u0026#39;: True}, \u0026#39;address\u0026#39;: {\u0026#39;type\u0026#39;: \u0026#39;TEXT\u0026#39;, \u0026#39;nullable\u0026#39;: True}, \u0026#39;city\u0026#39;: {\u0026#39;type\u0026#39;: \u0026#39;VARCHAR(100)\u0026#39;, \u0026#39;nullable\u0026#39;: True}, \u0026#39;state\u0026#39;: {\u0026#39;type\u0026#39;: \u0026#39;VARCHAR(50)\u0026#39;, \u0026#39;nullable\u0026#39;: True}, \u0026#39;zip\u0026#39;: {\u0026#39;type\u0026#39;: \u0026#39;VARCHAR(20)\u0026#39;, \u0026#39;nullable\u0026#39;: True}, \u0026#39;status\u0026#39;: {\u0026#39;type\u0026#39;: \u0026#39;TEXT\u0026#39;, \u0026#39;nullable\u0026#39;: True} }, \u0026#39;constraints\u0026#39;: [ {\u0026#39;type\u0026#39;: \u0026#39;PRIMARY KEY\u0026#39;, \u0026#39;column\u0026#39;: \u0026#39;id\u0026#39;} ], \u0026#39;foreign_keys\u0026#39;: [] }, \u0026#39;orders\u0026#39;: { \u0026#39;columns\u0026#39;: { \u0026#39;order_id\u0026#39;: {\u0026#39;type\u0026#39;: \u0026#39;INT\u0026#39;, \u0026#39;nullable\u0026#39;: False}, \u0026#39;user_id\u0026#39;: {\u0026#39;type\u0026#39;: \u0026#39;INT\u0026#39;, \u0026#39;nullable\u0026#39;: False}, \u0026#39;total\u0026#39;: {\u0026#39;type\u0026#39;: \u0026#39;DECIMAL(10,2)\u0026#39;, \u0026#39;nullable\u0026#39;: False} }, \u0026#39;constraints\u0026#39;: [], \u0026#39;foreign_keys\u0026#39;: [] } } } issues = analyzer.analyze_schema(schema_metadata) print(f\u0026#34;Found {len(issues)} schema issues:\\n\u0026#34;) for issue in issues: print(f\u0026#34;[{issue.severity}] {issue.table_name}.{issue.column_name or \u0026#39;TABLE\u0026#39;}\u0026#34;) print(f\u0026#34; Issue: {issue.description}\u0026#34;) print(f\u0026#34; Recommendation: {issue.recommendation}\u0026#34;) print(f\u0026#34; Impact: {issue.impact}\u0026#34;) print() if __name__ == \u0026#34;__main__\u0026#34;: main() System-Level Optimization 1. Database Configuration Tuner #!/bin/bash # src/tuning/db_config_tuner.sh # Database Configuration Tuner # Analyzes system resources and generates optimized database configurations set -euo pipefail # Configuration SCRIPT_DIR=\u0026#34;$(cd \u0026#34;$(dirname \u0026#34;${BASH_SOURCE[0]}\u0026#34;)\u0026#34; \u0026amp;\u0026amp; pwd)\u0026#34; LOG_FILE=\u0026#34;${SCRIPT_DIR}/tuning.log\u0026#34; CONFIG_OUTPUT_DIR=\u0026#34;${SCRIPT_DIR}/optimized_configs\u0026#34; # Logging function log() { echo \u0026#34;[$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)] $*\u0026#34; | tee -a \u0026#34;$LOG_FILE\u0026#34; } # System information gathering get_system_info() { log \u0026#34;Gathering system information...\u0026#34; # Memory information TOTAL_MEMORY_KB=$(grep MemTotal /proc/meminfo | awk \u0026#39;{print $2}\u0026#39;) TOTAL_MEMORY_GB=$((TOTAL_MEMORY_KB / 1024 / 1024)) # CPU information CPU_CORES=$(nproc) CPU_MODEL=$(grep \u0026#34;model name\u0026#34; /proc/cpuinfo | head -1 | cut -d: -f2 | xargs) # Storage information STORAGE_INFO=$(df -h / | tail -1) # Network information NETWORK_INTERFACES=$(ip link show | grep -E \u0026#34;^[0-9]+\u0026#34; | awk -F: \u0026#39;{print $2}\u0026#39; | xargs) log \u0026#34;System Information:\u0026#34; log \u0026#34; Memory: ${TOTAL_MEMORY_GB}GB\u0026#34; log \u0026#34; CPU Cores: ${CPU_CORES}\u0026#34; log \u0026#34; CPU Model: ${CPU_MODEL}\u0026#34; log \u0026#34; Storage: ${STORAGE_INFO}\u0026#34; log \u0026#34; Network Interfaces: ${NETWORK_INTERFACES}\u0026#34; } # MySQL configuration optimization optimize_mysql_config() { local memory_gb=$1 local cpu_cores=$2 local config_file=\u0026#34;${CONFIG_OUTPUT_DIR}/mysql_optimized.cnf\u0026#34; log \u0026#34;Generating optimized MySQL configuration...\u0026#34; # Calculate memory allocations (conservative approach) local innodb_buffer_pool=$((memory_gb * 70 / 100)) # 70% of total memory local query_cache_size=$((memory_gb * 5 / 100)) # 5% of total memory local tmp_table_size=$((memory_gb * 2 / 100)) # 2% of total memory # Ensure minimum values [[ $innodb_buffer_pool -lt 1 ]] \u0026amp;\u0026amp; innodb_buffer_pool=1 [[ $query_cache_size -lt 1 ]] \u0026amp;\u0026amp; query_cache_size=1 [[ $tmp_table_size -lt 1 ]] \u0026amp;\u0026amp; tmp_table_size=1 cat \u0026gt; \u0026#34;$config_file\u0026#34; \u0026lt;\u0026lt; EOF # Optimized MySQL Configuration # Generated on $(date) # System: ${memory_gb}GB RAM, ${cpu_cores} CPU cores [mysqld] # Basic Settings user = mysql pid-file = /var/run/mysqld/mysqld.pid socket = /var/run/mysqld/mysqld.sock port = 3306 basedir = /usr datadir = /var/lib/mysql tmpdir = /tmp # Memory Settings innodb_buffer_pool_size = ${innodb_buffer_pool}G innodb_buffer_pool_instances = $((cpu_cores \u0026gt; 8 ? 8 : cpu_cores)) query_cache_size = ${query_cache_size}G query_cache_type = 1 tmp_table_size = ${tmp_table_size}G max_heap_table_size = ${tmp_table_size}G # Connection Settings max_connections = $((cpu_cores * 50)) max_connect_errors = 1000000 thread_cache_size = $((cpu_cores * 2)) table_open_cache = $((cpu_cores * 200)) # InnoDB Settings innodb_file_per_table = 1 innodb_flush_log_at_trx_commit = 2 innodb_log_file_size = 256M innodb_log_buffer_size = 64M innodb_flush_method = O_DIRECT innodb_io_capacity = 1000 innodb_io_capacity_max = 2000 innodb_read_io_threads = $((cpu_cores / 2 \u0026gt; 4 ? 4 : cpu_cores / 2)) innodb_write_io_threads = $((cpu_cores / 2 \u0026gt; 4 ? 4 : cpu_cores / 2)) # Query Cache query_cache_limit = 2M query_cache_min_res_unit = 2k # Slow Query Log slow_query_log = 1 slow_query_log_file = /var/log/mysql/slow.log long_query_time = 1 # Binary Logging log_bin = /var/log/mysql/mysql-bin.log binlog_format = ROW expire_logs_days = 7 max_binlog_size = 100M # Performance Schema performance_schema = ON performance_schema_max_table_instances = 400 performance_schema_max_table_handles = 4000 # Security local_infile = 0 skip_show_database [mysql] no-auto-rehash [mysqldump] quick quote-names max_allowed_packet = 16M EOF log \u0026#34;MySQL configuration saved to: $config_file\u0026#34; } # PostgreSQL configuration optimization optimize_postgresql_config() { local memory_gb=$1 local cpu_cores=$2 local config_file=\u0026#34;${CONFIG_OUTPUT_DIR}/postgresql_optimized.conf\u0026#34; log \u0026#34;Generating optimized PostgreSQL configuration...\u0026#34; # Calculate memory allocations local shared_buffers=$((memory_gb * 25 / 100)) # 25% of total memory local effective_cache_size=$((memory_gb * 75 / 100)) # 75% of total memory local work_mem=$((memory_gb * 1024 / cpu_cores / 4)) # Conservative work_mem # Ensure minimum values [[ $shared_buffers -lt 1 ]] \u0026amp;\u0026amp; shared_buffers=1 [[ $effective_cache_size -lt 1 ]] \u0026amp;\u0026amp; effective_cache_size=1 [[ $work_mem -lt 4 ]] \u0026amp;\u0026amp; work_mem=4 cat \u0026gt; \u0026#34;$config_file\u0026#34; \u0026lt;\u0026lt; EOF # Optimized PostgreSQL Configuration # Generated on $(date) # System: ${memory_gb}GB RAM, ${cpu_cores} CPU cores # Memory Settings shared_buffers = ${shared_buffers}GB effective_cache_size = ${effective_cache_size}GB work_mem = ${work_mem}MB maintenance_work_mem = $((work_mem * 10))MB # Connection Settings max_connections = $((cpu_cores * 25)) shared_preload_libraries = \u0026#39;pg_stat_statements\u0026#39; # WAL Settings wal_buffers = 64MB checkpoint_completion_target = 0.9 wal_writer_delay = 200ms commit_delay = 0 commit_siblings = 5 # Query Planner random_page_cost = 1.1 effective_io_concurrency = $((cpu_cores * 2)) # Logging log_destination = \u0026#39;stderr\u0026#39; logging_collector = on log_directory = \u0026#39;pg_log\u0026#39; log_filename = \u0026#39;postgresql-%Y-%m-%d_%H%M%S.log\u0026#39; log_min_duration_statement = 1000 log_checkpoints = on log_connections = on log_disconnections = on log_lock_waits = on # Statistics track_activities = on track_counts = on track_io_timing = on track_functions = pl stats_temp_directory = \u0026#39;/var/run/postgresql/stats_temp\u0026#39; # Autovacuum autovacuum = on autovacuum_max_workers = $((cpu_cores / 4 \u0026gt; 1 ? cpu_cores / 4 : 1)) autovacuum_naptime = 1min autovacuum_vacuum_threshold = 50 autovacuum_analyze_threshold = 50 # Background Writer bgwriter_delay = 200ms bgwriter_lru_maxpages = 100 bgwriter_lru_multiplier = 2.0 # Parallel Query max_worker_processes = $cpu_cores max_parallel_workers_per_gather = $((cpu_cores / 4 \u0026gt; 1 ? cpu_cores / 4 : 1)) max_parallel_workers = $cpu_cores EOF log \u0026#34;PostgreSQL configuration saved to: $config_file\u0026#34; } # Operating system optimization optimize_os_settings() { local config_file=\u0026#34;${CONFIG_OUTPUT_DIR}/os_optimizations.sh\u0026#34; log \u0026#34;Generating OS optimization script...\u0026#34; cat \u0026gt; \u0026#34;$config_file\u0026#34; \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; #!/bin/bash # Operating System Optimizations for Database Performance # Kernel parameters for database workloads cat \u0026gt;\u0026gt; /etc/sysctl.conf \u0026lt;\u0026lt; \u0026#39;SYSCTL_EOF\u0026#39; # Memory management vm.swappiness = 1 vm.dirty_ratio = 15 vm.dirty_background_ratio = 5 vm.dirty_expire_centisecs = 500 vm.dirty_writeback_centisecs = 100 # Network settings net.core.rmem_default = 262144 net.core.rmem_max = 16777216 net.core.wmem_default = 262144 net.core.wmem_max = 16777216 net.ipv4.tcp_rmem = 4096 65536 16777216 net.ipv4.tcp_wmem = 4096 65536 16777216 net.core.netdev_max_backlog = 5000 # File system fs.file-max = 2097152 SYSCTL_EOF # Apply kernel parameters sysctl -p # I/O scheduler optimization echo \u0026#34;Setting I/O scheduler to deadline for database disks...\u0026#34; for disk in /sys/block/sd*; do if [[ -f \u0026#34;$disk/queue/scheduler\u0026#34; ]]; then echo deadline \u0026gt; \u0026#34;$disk/queue/scheduler\u0026#34; echo \u0026#34;Set deadline scheduler for $(basename $disk)\u0026#34; fi done # Disable transparent huge pages (often problematic for databases) echo \u0026#34;Disabling transparent huge pages...\u0026#34; echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/defrag # Add to startup script cat \u0026gt;\u0026gt; /etc/rc.local \u0026lt;\u0026lt; \u0026#39;RC_EOF\u0026#39; # Disable transparent huge pages echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/enabled echo never \u0026gt; /sys/kernel/mm/transparent_hugepage/defrag RC_EOF echo \u0026#34;OS optimizations applied. Reboot recommended.\u0026#34; EOF chmod +x \u0026#34;$config_file\u0026#34; log \u0026#34;OS optimization script saved to: $config_file\u0026#34; } # Storage optimization recommendations analyze_storage() { log \u0026#34;Analyzing storage configuration...\u0026#34; local report_file=\u0026#34;${CONFIG_OUTPUT_DIR}/storage_analysis.txt\u0026#34; { echo \u0026#34;Storage Analysis Report\u0026#34; echo \u0026#34;======================\u0026#34; echo \u0026#34;Generated on $(date)\u0026#34; echo echo \u0026#34;Disk Usage:\u0026#34; df -h echo echo \u0026#34;Mount Options:\u0026#34; mount | grep -E \u0026#34;(ext4|xfs|btrfs)\u0026#34; echo echo \u0026#34;I/O Statistics:\u0026#34; iostat -x 1 3 2\u0026gt;/dev/null || echo \u0026#34;iostat not available\u0026#34; echo echo \u0026#34;Recommendations:\u0026#34; echo \u0026#34;1. Use separate disks for data, logs, and temp files\u0026#34; echo \u0026#34;2. Consider RAID 10 for data files\u0026#34; echo \u0026#34;3. Use SSD for transaction logs\u0026#34; echo \u0026#34;4. Mount options: noatime,nodiratime for data partitions\u0026#34; echo \u0026#34;5. Consider XFS or ext4 with appropriate block size\u0026#34; echo echo \u0026#34;Suggested mount options for database partitions:\u0026#34; echo \u0026#34;/dev/sdb1 /var/lib/mysql ext4 defaults,noatime,nodiratime 0 2\u0026#34; echo \u0026#34;/dev/sdc1 /var/lib/mysql-logs ext4 defaults,noatime,nodiratime 0 2\u0026#34; } \u0026gt; \u0026#34;$report_file\u0026#34; log \u0026#34;Storage analysis saved to: $report_file\u0026#34; } # Main execution main() { log \u0026#34;Starting database configuration tuning...\u0026#34; # Create output directory mkdir -p \u0026#34;$CONFIG_OUTPUT_DIR\u0026#34; # Gather system information get_system_info # Generate optimized configurations optimize_mysql_config \u0026#34;$TOTAL_MEMORY_GB\u0026#34; \u0026#34;$CPU_CORES\u0026#34; optimize_postgresql_config \u0026#34;$TOTAL_MEMORY_GB\u0026#34; \u0026#34;$CPU_CORES\u0026#34; optimize_os_settings analyze_storage log \u0026#34;Configuration tuning completed!\u0026#34; log \u0026#34;Generated files in: $CONFIG_OUTPUT_DIR\u0026#34; log \u0026#34;Review configurations before applying to production systems\u0026#34; } # Run main function main \u0026#34;$@\u0026#34; Caching Strategies 1. Multi-Level Cache Manager #!/usr/bin/env python3 # src/caching/cache_manager.py import redis import memcache import json import hashlib import time import logging from typing import Any, Optional, Dict, List, Union from dataclasses import dataclass from enum import Enum import threading from functools import wraps class CacheLevel(Enum): L1_MEMORY = \u0026#34;l1_memory\u0026#34; L2_REDIS = \u0026#34;l2_redis\u0026#34; L3_MEMCACHED = \u0026#34;l3_memcached\u0026#34; @dataclass class CacheConfig: l1_max_size: int = 1000 l1_ttl: int = 300 # 5 minutes l2_host: str = \u0026#39;localhost\u0026#39; l2_port: int = 6379 l2_ttl: int = 3600 # 1 hour l3_servers: List[str] = None l3_ttl: int = 7200 # 2 hours class MultiLevelCacheManager: def __init__(self, config: CacheConfig): self.config = config self.logger = self._setup_logging() # L1 Cache: In-memory dictionary with LRU eviction self.l1_cache: Dict[str, Dict[str, Any]] = {} self.l1_access_order: List[str] = [] self.l1_lock = threading.RLock() # L2 Cache: Redis try: self.redis_client = redis.Redis( host=config.l2_host, port=config.l2_port, decode_responses=True, socket_connect_timeout=5, socket_timeout=5 ) self.redis_client.ping() self.l2_available = True except Exception as e: self.logger.warning(f\u0026#34;Redis not available: {e}\u0026#34;) self.l2_available = False # L3 Cache: Memcached try: if config.l3_servers: self.memcached_client = memcache.Client(config.l3_servers) # Test connection self.memcached_client.set(\u0026#39;test\u0026#39;, \u0026#39;test\u0026#39;, time=1) self.l3_available = True else: self.l3_available = False except Exception as e: self.logger.warning(f\u0026#34;Memcached not available: {e}\u0026#34;) self.l3_available = False # Statistics self.stats = { \u0026#39;l1_hits\u0026#39;: 0, \u0026#39;l1_misses\u0026#39;: 0, \u0026#39;l2_hits\u0026#39;: 0, \u0026#39;l2_misses\u0026#39;: 0, \u0026#39;l3_hits\u0026#39;: 0, \u0026#39;l3_misses\u0026#39;: 0, \u0026#39;total_requests\u0026#39;: 0 } self.stats_lock = threading.Lock() def _setup_logging(self) -\u0026gt; logging.Logger: logger = logging.getLogger(\u0026#39;CacheManager\u0026#39;) logger.setLevel(logging.INFO) handler = logging.StreamHandler() formatter = logging.Formatter(\u0026#39;%(asctime)s - %(levelname)s - %(message)s\u0026#39;) handler.setFormatter(formatter) logger.addHandler(handler) return logger def _generate_key(self, key: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Generate consistent cache key\u0026#34;\u0026#34;\u0026#34; return hashlib.md5(key.encode()).hexdigest() def _update_stats(self, level: str, hit: bool): \u0026#34;\u0026#34;\u0026#34;Update cache statistics\u0026#34;\u0026#34;\u0026#34; with self.stats_lock: self.stats[\u0026#39;total_requests\u0026#39;] += 1 if hit: self.stats[f\u0026#39;{level}_hits\u0026#39;] += 1 else: self.stats[f\u0026#39;{level}_misses\u0026#39;] += 1 def _l1_get(self, key: str) -\u0026gt; Optional[Any]: \u0026#34;\u0026#34;\u0026#34;Get from L1 cache\u0026#34;\u0026#34;\u0026#34; with self.l1_lock: if key in self.l1_cache: entry = self.l1_cache[key] if time.time() \u0026lt; entry[\u0026#39;expires\u0026#39;]: # Move to end (most recently used) self.l1_access_order.remove(key) self.l1_access_order.append(key) self._update_stats(\u0026#39;l1\u0026#39;, True) return entry[\u0026#39;value\u0026#39;] else: # Expired del self.l1_cache[key] self.l1_access_order.remove(key) self._update_stats(\u0026#39;l1\u0026#39;, False) return None def _l1_set(self, key: str, value: Any, ttl: int): \u0026#34;\u0026#34;\u0026#34;Set in L1 cache\u0026#34;\u0026#34;\u0026#34; with self.l1_lock: # Evict if at capacity while len(self.l1_cache) \u0026gt;= self.config.l1_max_size: oldest_key = self.l1_access_order.pop(0) del self.l1_cache[oldest_key] self.l1_cache[key] = { \u0026#39;value\u0026#39;: value, \u0026#39;expires\u0026#39;: time.time() + ttl } if key in self.l1_access_order: self.l1_access_order.remove(key) self.l1_access_order.append(key) def _l2_get(self, key: str) -\u0026gt; Optional[Any]: \u0026#34;\u0026#34;\u0026#34;Get from L2 cache (Redis)\u0026#34;\u0026#34;\u0026#34; if not self.l2_available: self._update_stats(\u0026#39;l2\u0026#39;, False) return None try: value = self.redis_client.get(key) if value is not None: self._update_stats(\u0026#39;l2\u0026#39;, True) return json.loads(value) else: self._update_stats(\u0026#39;l2\u0026#39;, False) return None except Exception as e: self.logger.error(f\u0026#34;Redis get error: {e}\u0026#34;) self._update_stats(\u0026#39;l2\u0026#39;, False) return None def _l2_set(self, key: str, value: Any, ttl: int): \u0026#34;\u0026#34;\u0026#34;Set in L2 cache (Redis)\u0026#34;\u0026#34;\u0026#34; if not self.l2_available: return try: self.redis_client.setex(key, ttl, json.dumps(value, default=str)) except Exception as e: self.logger.error(f\u0026#34;Redis set error: {e}\u0026#34;) def _l3_get(self, key: str) -\u0026gt; Optional[Any]: \u0026#34;\u0026#34;\u0026#34;Get from L3 cache (Memcached)\u0026#34;\u0026#34;\u0026#34; if not self.l3_available: self._update_stats(\u0026#39;l3\u0026#39;, False) return None try: value = self.memcached_client.get(key) if value is not None: self._update_stats(\u0026#39;l3\u0026#39;, True) return json.loads(value) else: self._update_stats(\u0026#39;l3\u0026#39;, False) return None except Exception as e: self.logger.error(f\u0026#34;Memcached get error: {e}\u0026#34;) self._update_stats(\u0026#39;l3\u0026#39;, False) return None def _l3_set(self, key: str, value: Any, ttl: int): \u0026#34;\u0026#34;\u0026#34;Set in L3 cache (Memcached)\u0026#34;\u0026#34;\u0026#34; if not self.l3_available: return try: self.memcached_client.set(key, json.dumps(value, default=str), time=ttl) except Exception as e: self.logger.error(f\u0026#34;Memcached set error: {e}\u0026#34;) def get(self, key: str) -\u0026gt; Optional[Any]: \u0026#34;\u0026#34;\u0026#34;Get value from cache (tries all levels)\u0026#34;\u0026#34;\u0026#34; cache_key = self._generate_key(key) # Try L1 first value = self._l1_get(cache_key) if value is not None: return value # Try L2 value = self._l2_get(cache_key) if value is not None: # Populate L1 self._l1_set(cache_key, value, self.config.l1_ttl) return value # Try L3 value = self._l3_get(cache_key) if value is not None: # Populate L1 and L2 self._l1_set(cache_key, value, self.config.l1_ttl) self._l2_set(cache_key, value, self.config.l2_ttl) return value return None def set(self, key: str, value: Any, ttl: Optional[int] = None): \u0026#34;\u0026#34;\u0026#34;Set value in all cache levels\u0026#34;\u0026#34;\u0026#34; cache_key = self._generate_key(key) # Use default TTLs if not specified l1_ttl = ttl or self.config.l1_ttl l2_ttl = ttl or self.config.l2_ttl l3_ttl = ttl or self.config.l3_ttl # Set in all levels self._l1_set(cache_key, value, l1_ttl) self._l2_set(cache_key, value, l2_ttl) self._l3_set(cache_key, value, l3_ttl) def delete(self, key: str): \u0026#34;\u0026#34;\u0026#34;Delete from all cache levels\u0026#34;\u0026#34;\u0026#34; cache_key = self._generate_key(key) # Delete from L1 with self.l1_lock: if cache_key in self.l1_cache: del self.l1_cache[cache_key] self.l1_access_order.remove(cache_key) # Delete from L2 if self.l2_available: try: self.redis_client.delete(cache_key) except Exception as e: self.logger.error(f\u0026#34;Redis delete error: {e}\u0026#34;) # Delete from L3 if self.l3_available: try: self.memcached_client.delete(cache_key) except Exception as e: self.logger.error(f\u0026#34;Memcached delete error: {e}\u0026#34;) def clear_all(self): \u0026#34;\u0026#34;\u0026#34;Clear all cache levels\u0026#34;\u0026#34;\u0026#34; # Clear L1 with self.l1_lock: self.l1_cache.clear() self.l1_access_order.clear() # Clear L2 if self.l2_available: try: self.redis_client.flushdb() except Exception as e: self.logger.error(f\u0026#34;Redis clear error: {e}\u0026#34;) # Clear L3 if self.l3_available: try: self.memcached_client.flush_all() except Exception as e: self.logger.error(f\u0026#34;Memcached clear error: {e}\u0026#34;) def get_stats(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Get cache statistics\u0026#34;\u0026#34;\u0026#34; with self.stats_lock: total_hits = self.stats[\u0026#39;l1_hits\u0026#39;] + self.stats[\u0026#39;l2_hits\u0026#39;] + self.stats[\u0026#39;l3_hits\u0026#39;] total_misses = self.stats[\u0026#39;l1_misses\u0026#39;] + self.stats[\u0026#39;l2_misses\u0026#39;] + self.stats[\u0026#39;l3_misses\u0026#39;] hit_rate = (total_hits / (total_hits + total_misses)) * 100 if (total_hits + total_misses) \u0026gt; 0 else 0 return { **self.stats, \u0026#39;total_hits\u0026#39;: total_hits, \u0026#39;total_misses\u0026#39;: total_misses, \u0026#39;hit_rate_percent\u0026#39;: round(hit_rate, 2), \u0026#39;l1_size\u0026#39;: len(self.l1_cache), \u0026#39;l2_available\u0026#39;: self.l2_available, \u0026#39;l3_available\u0026#39;: self.l3_available } # Cache decorator def cached(cache_manager: MultiLevelCacheManager, ttl: Optional[int] = None, key_prefix: str = \u0026#34;\u0026#34;): \u0026#34;\u0026#34;\u0026#34;Decorator for caching function results\u0026#34;\u0026#34;\u0026#34; def decorator(func): @wraps(func) def wrapper(*args, **kwargs): # Generate cache key from function name and arguments key_parts = [key_prefix, func.__name__] key_parts.extend(str(arg) for arg in args) key_parts.extend(f\u0026#34;{k}={v}\u0026#34; for k, v in sorted(kwargs.items())) cache_key = \u0026#34;:\u0026#34;.join(key_parts) # Try to get from cache result = cache_manager.get(cache_key) if result is not None: return result # Execute function and cache result result = func(*args, **kwargs) cache_manager.set(cache_key, result, ttl) return result return wrapper return decorator # Example usage def main(): # Configure cache config = CacheConfig( l1_max_size=500, l1_ttl=300, l2_host=\u0026#39;localhost\u0026#39;, l2_port=6379, l2_ttl=3600, l3_servers=[\u0026#39;localhost:11211\u0026#39;], l3_ttl=7200 ) # Create cache manager cache = MultiLevelCacheManager(config) # Example cached function @cached(cache, ttl=600, key_prefix=\u0026#34;user_data\u0026#34;) def get_user_data(user_id: int) -\u0026gt; Dict[str, Any]: # Simulate database query time.sleep(0.1) # Simulate query time return { \u0026#39;id\u0026#39;: user_id, \u0026#39;name\u0026#39;: f\u0026#39;User {user_id}\u0026#39;, \u0026#39;email\u0026#39;: f\u0026#39;user{user_id}@example.com\u0026#39;, \u0026#39;created_at\u0026#39;: time.time() } # Test caching print(\u0026#34;Testing multi-level cache...\u0026#34;) # First call - cache miss start_time = time.time() user_data = get_user_data(123) first_call_time = time.time() - start_time print(f\u0026#34;First call (cache miss): {first_call_time:.3f}s\u0026#34;) print(f\u0026#34;User data: {user_data}\u0026#34;) # Second call - cache hit start_time = time.time() user_data = get_user_data(123) second_call_time = time.time() - start_time print(f\u0026#34;Second call (cache hit): {second_call_time:.3f}s\u0026#34;) # Show cache statistics stats = cache.get_stats() print(f\u0026#34;\\nCache Statistics:\u0026#34;) for key, value in stats.items(): print(f\u0026#34; {key}: {value}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() Summary This comprehensive database performance optimization guide provides practical strategies and tools for improving database performance across multiple dimensions:\n","content":"Database Performance Optimization Strategies: From Query Tuning to System Architecture Database performance optimization is a critical aspect of modern application development. As data volumes grow and user expectations increase, the ability to efficiently query and manipulate data becomes paramount. This comprehensive guide explores various strategies and techniques for optimizing database performance, from basic query tuning to advanced architectural considerations.\nTable of Contents â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["Database","Performance","Optimization","SQL","Indexing"],"categories":["Database"],"author":"Database Expert","readingTime":35,"wordCount":7314,"section":"posts","type":"posts","draft":false,"featured":false,"series":null},{"title":"Database Security and Access Control: A Comprehensive Guide to Protecting Your Data","url":"https://www.dishuihengxin.com/posts/database-security-access-control/","summary":"Database Security and Access Control: A Comprehensive Guide to Protecting Your Data Database security is a critical aspect of modern data management, encompassing multiple layers of protection to safeguard sensitive information from unauthorized access, data breaches, and malicious attacks. This comprehensive guide explores essential security strategies, implementation techniques, and best practices for securing database systems.\nTable of Contents Security Architecture Overview Authentication and Authorization Data Encryption Access Control Implementation Auditing and Monitoring Compliance and Governance Security Testing and Validation Security Architecture Overview 1. Multi-Layer Security Model graph TB A[Application Layer] --\u0026gt; B[Authentication Layer] B --\u0026gt; C[Authorization Layer] C --\u0026gt; D[Network Security Layer] D --\u0026gt; E[Database Security Layer] E --\u0026gt; F[Storage Encryption Layer] F --\u0026gt; G[Physical Security Layer] subgraph \u0026#34;Security Controls\u0026#34; H[Access Control] I[Encryption] J[Auditing] K[Monitoring] end B -.-\u0026gt; H C -.-\u0026gt; H E -.-\u0026gt; I F -.-\u0026gt; I D -.-\u0026gt; J E -.-\u0026gt; J A -.-\u0026gt; K E -.-\u0026gt; K 2. Security Configuration Framework # Database Security Configuration database_security: global_settings: security_level: \u0026#34;high\u0026#34; compliance_standards: [\u0026#34;SOX\u0026#34;, \u0026#34;GDPR\u0026#34;, \u0026#34;HIPAA\u0026#34;, \u0026#34;PCI-DSS\u0026#34;] audit_enabled: true encryption_required: true authentication: methods: - type: \u0026#34;multi_factor\u0026#34; providers: [\u0026#34;ldap\u0026#34;, \u0026#34;oauth2\u0026#34;, \u0026#34;saml\u0026#34;] - type: \u0026#34;certificate\u0026#34; ca_validation: true - type: \u0026#34;kerberos\u0026#34; realm: \u0026#34;COMPANY.COM\u0026#34; password_policy: min_length: 12 complexity_required: true expiration_days: 90 history_count: 12 lockout_attempts: 3 lockout_duration: 30 authorization: rbac_enabled: true principle_of_least_privilege: true role_hierarchy: true dynamic_permissions: true encryption: at_rest: algorithm: \u0026#34;AES-256\u0026#34; key_management: \u0026#34;external_hsm\u0026#34; transparent_encryption: true in_transit: tls_version: \u0026#34;1.3\u0026#34; certificate_validation: true cipher_suites: [\u0026#34;ECDHE-RSA-AES256-GCM-SHA384\u0026#34;] column_level: sensitive_data_types: [\u0026#34;ssn\u0026#34;, \u0026#34;credit_card\u0026#34;, \u0026#34;email\u0026#34;, \u0026#34;phone\u0026#34;] format_preserving: true network_security: firewall_rules: - source: \u0026#34;application_tier\u0026#34; destination: \u0026#34;database_tier\u0026#34; ports: [3306, 5432, 1521] protocol: \u0026#34;tcp\u0026#34; vpc_isolation: true private_subnets: true bastion_host_required: true monitoring: real_time_alerts: true anomaly_detection: true failed_login_tracking: true privilege_escalation_detection: true retention: audit_logs: \u0026#34;7_years\u0026#34; security_events: \u0026#34;2_years\u0026#34; access_logs: \u0026#34;1_year\u0026#34; Authentication and Authorization 1. Multi-Factor Authentication System #!/usr/bin/env python3 # src/security/mfa_auth.py import hashlib import hmac import time import base64 import qrcode import logging from typing import Dict, List, Optional, Tuple, Any from dataclasses import dataclass from enum import Enum import secrets import pyotp import ldap3 from cryptography.fernet import Fernet from cryptography.hazmat.primitives import hashes from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC class AuthMethod(Enum): PASSWORD = \u0026#34;password\u0026#34; TOTP = \u0026#34;totp\u0026#34; SMS = \u0026#34;sms\u0026#34; EMAIL = \u0026#34;email\u0026#34; CERTIFICATE = \u0026#34;certificate\u0026#34; BIOMETRIC = \u0026#34;biometric\u0026#34; @dataclass class AuthResult: success: bool user_id: Optional[str] session_token: Optional[str] required_methods: List[AuthMethod] error_message: Optional[str] risk_score: float @dataclass class UserCredentials: user_id: str password_hash: str salt: str totp_secret: Optional[str] certificate_thumbprint: Optional[str] failed_attempts: int locked_until: Optional[float] last_login: Optional[float] class DatabaseAuthenticator: def __init__(self, config: Dict[str, Any]): self.config = config self.logger = self._setup_logging() self.encryption_key = self._derive_encryption_key() self.fernet = Fernet(self.encryption_key) # Initialize LDAP connection if configured self.ldap_connection = None if config.get(\u0026#39;ldap\u0026#39;, {}).get(\u0026#39;enabled\u0026#39;): self._init_ldap() def _setup_logging(self) -\u0026gt; logging.Logger: logger = logging.getLogger(\u0026#39;DatabaseAuth\u0026#39;) logger.setLevel(logging.INFO) handler = logging.StreamHandler() formatter = logging.Formatter(\u0026#39;%(asctime)s - %(levelname)s - %(message)s\u0026#39;) handler.setFormatter(formatter) logger.addHandler(handler) return logger def _derive_encryption_key(self) -\u0026gt; bytes: \u0026#34;\u0026#34;\u0026#34;Derive encryption key from master password\u0026#34;\u0026#34;\u0026#34; master_password = self.config.get(\u0026#39;master_password\u0026#39;, \u0026#39;default_key\u0026#39;).encode() salt = self.config.get(\u0026#39;encryption_salt\u0026#39;, \u0026#39;default_salt\u0026#39;).encode() kdf = PBKDF2HMAC( algorithm=hashes.SHA256(), length=32, salt=salt, iterations=100000, ) return base64.urlsafe_b64encode(kdf.derive(master_password)) def _init_ldap(self): \u0026#34;\u0026#34;\u0026#34;Initialize LDAP connection\u0026#34;\u0026#34;\u0026#34; ldap_config = self.config[\u0026#39;ldap\u0026#39;] try: server = ldap3.Server( ldap_config[\u0026#39;server\u0026#39;], port=ldap_config.get(\u0026#39;port\u0026#39;, 389), use_ssl=ldap_config.get(\u0026#39;use_ssl\u0026#39;, False) ) self.ldap_connection = ldap3.Connection( server, user=ldap_config[\u0026#39;bind_dn\u0026#39;], password=ldap_config[\u0026#39;bind_password\u0026#39;], auto_bind=True ) self.logger.info(\u0026#34;LDAP connection established\u0026#34;) except Exception as e: self.logger.error(f\u0026#34;Failed to connect to LDAP: {e}\u0026#34;) def hash_password(self, password: str, salt: Optional[str] = None) -\u0026gt; Tuple[str, str]: \u0026#34;\u0026#34;\u0026#34;Hash password with salt\u0026#34;\u0026#34;\u0026#34; if salt is None: salt = secrets.token_hex(32) # Use PBKDF2 with SHA-256 password_hash = hashlib.pbkdf2_hmac( \u0026#39;sha256\u0026#39;, password.encode(\u0026#39;utf-8\u0026#39;), salt.encode(\u0026#39;utf-8\u0026#39;), 100000 # iterations ) return base64.b64encode(password_hash).decode(\u0026#39;utf-8\u0026#39;), salt def verify_password(self, password: str, stored_hash: str, salt: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Verify password against stored hash\u0026#34;\u0026#34;\u0026#34; computed_hash, _ = self.hash_password(password, salt) return hmac.compare_digest(computed_hash, stored_hash) def generate_totp_secret(self, user_id: str) -\u0026gt; Tuple[str, str]: \u0026#34;\u0026#34;\u0026#34;Generate TOTP secret and QR code\u0026#34;\u0026#34;\u0026#34; secret = pyotp.random_base32() # Create TOTP URI totp_uri = pyotp.totp.TOTP(secret).provisioning_uri( name=user_id, issuer_name=self.config.get(\u0026#39;app_name\u0026#39;, \u0026#39;Database System\u0026#39;) ) # Generate QR code qr = qrcode.QRCode(version=1, box_size=10, border=5) qr.add_data(totp_uri) qr.make(fit=True) return secret, totp_uri def verify_totp(self, secret: str, token: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Verify TOTP token\u0026#34;\u0026#34;\u0026#34; totp = pyotp.TOTP(secret) return totp.verify(token, valid_window=1) # Allow 30-second window def authenticate_ldap(self, username: str, password: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Authenticate against LDAP\u0026#34;\u0026#34;\u0026#34; if not self.ldap_connection: return False try: user_dn = f\u0026#34;uid={username},{self.config[\u0026#39;ldap\u0026#39;][\u0026#39;user_base\u0026#39;]}\u0026#34; return self.ldap_connection.authenticate(user_dn, password) except Exception as e: self.logger.error(f\u0026#34;LDAP authentication failed: {e}\u0026#34;) return False def calculate_risk_score(self, user_id: str, request_info: Dict[str, Any]) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;Calculate authentication risk score\u0026#34;\u0026#34;\u0026#34; risk_score = 0.0 # Check for unusual login time current_hour = time.localtime().tm_hour if current_hour \u0026lt; 6 or current_hour \u0026gt; 22: risk_score += 0.2 # Check for new IP address if request_info.get(\u0026#39;ip_address\u0026#39;) not in self._get_known_ips(user_id): risk_score += 0.3 # Check for new device/user agent if request_info.get(\u0026#39;user_agent\u0026#39;) not in self._get_known_devices(user_id): risk_score += 0.2 # Check for geographic anomaly if self._is_geographic_anomaly(user_id, request_info.get(\u0026#39;ip_address\u0026#39;)): risk_score += 0.4 # Check for recent failed attempts failed_attempts = self._get_recent_failed_attempts(user_id) if failed_attempts \u0026gt; 0: risk_score += min(failed_attempts * 0.1, 0.3) return min(risk_score, 1.0) def _get_known_ips(self, user_id: str) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;Get list of known IP addresses for user\u0026#34;\u0026#34;\u0026#34; # This would typically query a database return [] def _get_known_devices(self, user_id: str) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;Get list of known devices for user\u0026#34;\u0026#34;\u0026#34; # This would typically query a database return [] def _is_geographic_anomaly(self, user_id: str, ip_address: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Check if login location is anomalous\u0026#34;\u0026#34;\u0026#34; # This would typically use IP geolocation services return False def _get_recent_failed_attempts(self, user_id: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;Get count of recent failed login attempts\u0026#34;\u0026#34;\u0026#34; # This would typically query audit logs return 0 def authenticate(self, user_id: str, credentials: Dict[str, Any], request_info: Dict[str, Any]) -\u0026gt; AuthResult: \u0026#34;\u0026#34;\u0026#34;Main authentication method\u0026#34;\u0026#34;\u0026#34; try: # Load user credentials user_creds = self._load_user_credentials(user_id) if not user_creds: return AuthResult( success=False, user_id=None, session_token=None, required_methods=[], error_message=\u0026#34;User not found\u0026#34;, risk_score=1.0 ) # Check if account is locked if user_creds.locked_until and time.time() \u0026lt; user_creds.locked_until: return AuthResult( success=False, user_id=None, session_token=None, required_methods=[], error_message=\u0026#34;Account locked\u0026#34;, risk_score=1.0 ) # Calculate risk score risk_score = self.calculate_risk_score(user_id, request_info) # Determine required authentication methods based on risk required_methods = self._determine_required_methods(risk_score) # Verify provided credentials auth_success = True # Password authentication if AuthMethod.PASSWORD in required_methods: password = credentials.get(\u0026#39;password\u0026#39;) if not password or not self.verify_password( password, user_creds.password_hash, user_creds.salt ): auth_success = False # TOTP authentication if AuthMethod.TOTP in required_methods: totp_token = credentials.get(\u0026#39;totp_token\u0026#39;) if not totp_token or not self.verify_totp( user_creds.totp_secret, totp_token ): auth_success = False # LDAP authentication if self.config.get(\u0026#39;ldap\u0026#39;, {}).get(\u0026#39;enabled\u0026#39;): password = credentials.get(\u0026#39;password\u0026#39;) if password and not self.authenticate_ldap(user_id, password): auth_success = False if auth_success: # Generate session token session_token = self._generate_session_token(user_id) # Update last login time self._update_last_login(user_id) # Reset failed attempts self._reset_failed_attempts(user_id) self.logger.info(f\u0026#34;Successful authentication for user: {user_id}\u0026#34;) return AuthResult( success=True, user_id=user_id, session_token=session_token, required_methods=required_methods, error_message=None, risk_score=risk_score ) else: # Increment failed attempts self._increment_failed_attempts(user_id) self.logger.warning(f\u0026#34;Failed authentication for user: {user_id}\u0026#34;) return AuthResult( success=False, user_id=None, session_token=None, required_methods=required_methods, error_message=\u0026#34;Authentication failed\u0026#34;, risk_score=risk_score ) except Exception as e: self.logger.error(f\u0026#34;Authentication error: {e}\u0026#34;) return AuthResult( success=False, user_id=None, session_token=None, required_methods=[], error_message=\u0026#34;Internal error\u0026#34;, risk_score=1.0 ) def _determine_required_methods(self, risk_score: float) -\u0026gt; List[AuthMethod]: \u0026#34;\u0026#34;\u0026#34;Determine required authentication methods based on risk score\u0026#34;\u0026#34;\u0026#34; methods = [AuthMethod.PASSWORD] if risk_score \u0026gt; 0.3: methods.append(AuthMethod.TOTP) if risk_score \u0026gt; 0.7: methods.append(AuthMethod.EMAIL) return methods def _load_user_credentials(self, user_id: str) -\u0026gt; Optional[UserCredentials]: \u0026#34;\u0026#34;\u0026#34;Load user credentials from database\u0026#34;\u0026#34;\u0026#34; # This would typically query the database # For demo purposes, return a sample user if user_id == \u0026#34;demo_user\u0026#34;: password_hash, salt = self.hash_password(\u0026#34;demo_password\u0026#34;) return UserCredentials( user_id=user_id, password_hash=password_hash, salt=salt, totp_secret=\u0026#34;JBSWY3DPEHPK3PXP\u0026#34;, certificate_thumbprint=None, failed_attempts=0, locked_until=None, last_login=None ) return None def _generate_session_token(self, user_id: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Generate secure session token\u0026#34;\u0026#34;\u0026#34; token_data = { \u0026#39;user_id\u0026#39;: user_id, \u0026#39;timestamp\u0026#39;: time.time(), \u0026#39;random\u0026#39;: secrets.token_hex(16) } token_json = str(token_data).encode() encrypted_token = self.fernet.encrypt(token_json) return base64.urlsafe_b64encode(encrypted_token).decode() def _update_last_login(self, user_id: str): \u0026#34;\u0026#34;\u0026#34;Update last login timestamp\u0026#34;\u0026#34;\u0026#34; # This would typically update the database pass def _reset_failed_attempts(self, user_id: str): \u0026#34;\u0026#34;\u0026#34;Reset failed login attempts counter\u0026#34;\u0026#34;\u0026#34; # This would typically update the database pass def _increment_failed_attempts(self, user_id: str): \u0026#34;\u0026#34;\u0026#34;Increment failed login attempts counter\u0026#34;\u0026#34;\u0026#34; # This would typically update the database pass # Example usage def main(): config = { \u0026#39;master_password\u0026#39;: \u0026#39;secure_master_key_2024\u0026#39;, \u0026#39;encryption_salt\u0026#39;: \u0026#39;database_security_salt\u0026#39;, \u0026#39;app_name\u0026#39;: \u0026#39;Secure Database System\u0026#39;, \u0026#39;ldap\u0026#39;: { \u0026#39;enabled\u0026#39;: False, \u0026#39;server\u0026#39;: \u0026#39;ldap.company.com\u0026#39;, \u0026#39;port\u0026#39;: 389, \u0026#39;bind_dn\u0026#39;: \u0026#39;cn=admin,dc=company,dc=com\u0026#39;, \u0026#39;bind_password\u0026#39;: \u0026#39;admin_password\u0026#39;, \u0026#39;user_base\u0026#39;: \u0026#39;ou=users,dc=company,dc=com\u0026#39; } } authenticator = DatabaseAuthenticator(config) # Test authentication credentials = { \u0026#39;password\u0026#39;: \u0026#39;demo_password\u0026#39;, \u0026#39;totp_token\u0026#39;: \u0026#39;123456\u0026#39; } request_info = { \u0026#39;ip_address\u0026#39;: \u0026#39;192.168.1.100\u0026#39;, \u0026#39;user_agent\u0026#39;: \u0026#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64)\u0026#39;, \u0026#39;timestamp\u0026#39;: time.time() } result = authenticator.authenticate(\u0026#39;demo_user\u0026#39;, credentials, request_info) print(f\u0026#34;Authentication Result:\u0026#34;) print(f\u0026#34; Success: {result.success}\u0026#34;) print(f\u0026#34; User ID: {result.user_id}\u0026#34;) print(f\u0026#34; Session Token: {result.session_token}\u0026#34;) print(f\u0026#34; Required Methods: {[method.value for method in result.required_methods]}\u0026#34;) print(f\u0026#34; Risk Score: {result.risk_score}\u0026#34;) print(f\u0026#34; Error: {result.error_message}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() 2. Role-Based Access Control (RBAC) System #!/usr/bin/env python3 # src/security/rbac_system.py import json import logging from typing import Dict, List, Set, Optional, Any from dataclasses import dataclass, field from enum import Enum import time from datetime import datetime, timedelta class Permission(Enum): READ = \u0026#34;read\u0026#34; WRITE = \u0026#34;write\u0026#34; DELETE = \u0026#34;delete\u0026#34; EXECUTE = \u0026#34;execute\u0026#34; ADMIN = \u0026#34;admin\u0026#34; GRANT = \u0026#34;grant\u0026#34; class ResourceType(Enum): TABLE = \u0026#34;table\u0026#34; VIEW = \u0026#34;view\u0026#34; PROCEDURE = \u0026#34;procedure\u0026#34; FUNCTION = \u0026#34;function\u0026#34; SCHEMA = \u0026#34;schema\u0026#34; DATABASE = \u0026#34;database\u0026#34; @dataclass class Resource: resource_type: ResourceType name: str schema: Optional[str] = None database: Optional[str] = None def __str__(self) -\u0026gt; str: parts = [] if self.database: parts.append(self.database) if self.schema: parts.append(self.schema) parts.append(self.name) return \u0026#34;.\u0026#34;.join(parts) @dataclass class Role: name: str description: str permissions: Dict[str, Set[Permission]] = field(default_factory=dict) parent_roles: Set[str] = field(default_factory=set) created_at: datetime = field(default_factory=datetime.now) is_active: bool = True @dataclass class User: user_id: str username: str email: str roles: Set[str] = field(default_factory=set) direct_permissions: Dict[str, Set[Permission]] = field(default_factory=dict) created_at: datetime = field(default_factory=datetime.now) last_login: Optional[datetime] = None is_active: bool = True session_timeout: int = 3600 # seconds @dataclass class AccessRequest: user_id: str resource: Resource permission: Permission context: Dict[str, Any] = field(default_factory=dict) timestamp: datetime = field(default_factory=datetime.now) @dataclass class AccessResult: granted: bool reason: str effective_permissions: Set[Permission] applied_policies: List[str] class RBACManager: def __init__(self): self.logger = self._setup_logging() self.users: Dict[str, User] = {} self.roles: Dict[str, Role] = {} self.access_policies: List[Dict[str, Any]] = [] self.audit_log: List[Dict[str, Any]] = [] # Initialize default roles self._create_default_roles() def _setup_logging(self) -\u0026gt; logging.Logger: logger = logging.getLogger(\u0026#39;RBACManager\u0026#39;) logger.setLevel(logging.INFO) handler = logging.StreamHandler() formatter = logging.Formatter(\u0026#39;%(asctime)s - %(levelname)s - %(message)s\u0026#39;) handler.setFormatter(formatter) logger.addHandler(handler) return logger def _create_default_roles(self): \u0026#34;\u0026#34;\u0026#34;Create default system roles\u0026#34;\u0026#34;\u0026#34; # Database Administrator dba_role = Role( name=\u0026#34;dba\u0026#34;, description=\u0026#34;Database Administrator with full access\u0026#34; ) dba_role.permissions[\u0026#34;*\u0026#34;] = {Permission.READ, Permission.WRITE, Permission.DELETE, Permission.EXECUTE, Permission.ADMIN, Permission.GRANT} self.roles[\u0026#34;dba\u0026#34;] = dba_role # Data Analyst analyst_role = Role( name=\u0026#34;data_analyst\u0026#34;, description=\u0026#34;Read-only access to data tables\u0026#34; ) analyst_role.permissions[\u0026#34;*.data.*\u0026#34;] = {Permission.READ} self.roles[\u0026#34;data_analyst\u0026#34;] = analyst_role # Application User app_user_role = Role( name=\u0026#34;app_user\u0026#34;, description=\u0026#34;Standard application user\u0026#34; ) app_user_role.permissions[\u0026#34;app_db.public.*\u0026#34;] = {Permission.READ, Permission.WRITE} self.roles[\u0026#34;app_user\u0026#34;] = app_user_role # Read-only User readonly_role = Role( name=\u0026#34;readonly\u0026#34;, description=\u0026#34;Read-only access to specific schemas\u0026#34; ) readonly_role.permissions[\u0026#34;*.public.*\u0026#34;] = {Permission.READ} self.roles[\u0026#34;readonly\u0026#34;] = readonly_role def create_user(self, user_id: str, username: str, email: str, roles: Optional[List[str]] = None) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Create a new user\u0026#34;\u0026#34;\u0026#34; try: if user_id in self.users: self.logger.warning(f\u0026#34;User {user_id} already exists\u0026#34;) return False user = User( user_id=user_id, username=username, email=email, roles=set(roles or []) ) # Validate roles exist for role_name in user.roles: if role_name not in self.roles: self.logger.error(f\u0026#34;Role {role_name} does not exist\u0026#34;) return False self.users[user_id] = user self._audit_log(\u0026#34;USER_CREATED\u0026#34;, {\u0026#34;user_id\u0026#34;: user_id, \u0026#34;username\u0026#34;: username}) self.logger.info(f\u0026#34;Created user: {username} ({user_id})\u0026#34;) return True except Exception as e: self.logger.error(f\u0026#34;Failed to create user {user_id}: {e}\u0026#34;) return False def create_role(self, name: str, description: str, parent_roles: Optional[List[str]] = None) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Create a new role\u0026#34;\u0026#34;\u0026#34; try: if name in self.roles: self.logger.warning(f\u0026#34;Role {name} already exists\u0026#34;) return False role = Role( name=name, description=description, parent_roles=set(parent_roles or []) ) # Validate parent roles exist for parent_role in role.parent_roles: if parent_role not in self.roles: self.logger.error(f\u0026#34;Parent role {parent_role} does not exist\u0026#34;) return False self.roles[name] = role self._audit_log(\u0026#34;ROLE_CREATED\u0026#34;, {\u0026#34;role_name\u0026#34;: name, \u0026#34;description\u0026#34;: description}) self.logger.info(f\u0026#34;Created role: {name}\u0026#34;) return True except Exception as e: self.logger.error(f\u0026#34;Failed to create role {name}: {e}\u0026#34;) return False def grant_permission(self, role_name: str, resource_pattern: str, permissions: List[Permission]) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Grant permissions to a role\u0026#34;\u0026#34;\u0026#34; try: if role_name not in self.roles: self.logger.error(f\u0026#34;Role {role_name} does not exist\u0026#34;) return False role = self.roles[role_name] if resource_pattern not in role.permissions: role.permissions[resource_pattern] = set() role.permissions[resource_pattern].update(permissions) self._audit_log(\u0026#34;PERMISSION_GRANTED\u0026#34;, { \u0026#34;role_name\u0026#34;: role_name, \u0026#34;resource_pattern\u0026#34;: resource_pattern, \u0026#34;permissions\u0026#34;: [p.value for p in permissions] }) self.logger.info(f\u0026#34;Granted permissions to role {role_name}: {resource_pattern}\u0026#34;) return True except Exception as e: self.logger.error(f\u0026#34;Failed to grant permission: {e}\u0026#34;) return False def revoke_permission(self, role_name: str, resource_pattern: str, permissions: List[Permission]) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Revoke permissions from a role\u0026#34;\u0026#34;\u0026#34; try: if role_name not in self.roles: self.logger.error(f\u0026#34;Role {role_name} does not exist\u0026#34;) return False role = self.roles[role_name] if resource_pattern in role.permissions: role.permissions[resource_pattern] -= set(permissions) # Remove empty permission sets if not role.permissions[resource_pattern]: del role.permissions[resource_pattern] self._audit_log(\u0026#34;PERMISSION_REVOKED\u0026#34;, { \u0026#34;role_name\u0026#34;: role_name, \u0026#34;resource_pattern\u0026#34;: resource_pattern, \u0026#34;permissions\u0026#34;: [p.value for p in permissions] }) self.logger.info(f\u0026#34;Revoked permissions from role {role_name}: {resource_pattern}\u0026#34;) return True except Exception as e: self.logger.error(f\u0026#34;Failed to revoke permission: {e}\u0026#34;) return False def assign_role(self, user_id: str, role_name: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Assign a role to a user\u0026#34;\u0026#34;\u0026#34; try: if user_id not in self.users: self.logger.error(f\u0026#34;User {user_id} does not exist\u0026#34;) return False if role_name not in self.roles: self.logger.error(f\u0026#34;Role {role_name} does not exist\u0026#34;) return False self.users[user_id].roles.add(role_name) self._audit_log(\u0026#34;ROLE_ASSIGNED\u0026#34;, { \u0026#34;user_id\u0026#34;: user_id, \u0026#34;role_name\u0026#34;: role_name }) self.logger.info(f\u0026#34;Assigned role {role_name} to user {user_id}\u0026#34;) return True except Exception as e: self.logger.error(f\u0026#34;Failed to assign role: {e}\u0026#34;) return False def remove_role(self, user_id: str, role_name: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Remove a role from a user\u0026#34;\u0026#34;\u0026#34; try: if user_id not in self.users: self.logger.error(f\u0026#34;User {user_id} does not exist\u0026#34;) return False self.users[user_id].roles.discard(role_name) self._audit_log(\u0026#34;ROLE_REMOVED\u0026#34;, { \u0026#34;user_id\u0026#34;: user_id, \u0026#34;role_name\u0026#34;: role_name }) self.logger.info(f\u0026#34;Removed role {role_name} from user {user_id}\u0026#34;) return True except Exception as e: self.logger.error(f\u0026#34;Failed to remove role: {e}\u0026#34;) return False def _get_effective_permissions(self, user_id: str) -\u0026gt; Dict[str, Set[Permission]]: \u0026#34;\u0026#34;\u0026#34;Get all effective permissions for a user\u0026#34;\u0026#34;\u0026#34; if user_id not in self.users: return {} user = self.users[user_id] effective_permissions = {} # Start with direct permissions for resource_pattern, perms in user.direct_permissions.items(): effective_permissions[resource_pattern] = perms.copy() # Add permissions from roles (including inherited roles) processed_roles = set() roles_to_process = list(user.roles) while roles_to_process: role_name = roles_to_process.pop(0) if role_name in processed_roles or role_name not in self.roles: continue processed_roles.add(role_name) role = self.roles[role_name] # Add parent roles to processing queue roles_to_process.extend(role.parent_roles) # Merge role permissions for resource_pattern, perms in role.permissions.items(): if resource_pattern not in effective_permissions: effective_permissions[resource_pattern] = set() effective_permissions[resource_pattern].update(perms) return effective_permissions def _match_resource_pattern(self, pattern: str, resource: Resource) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Check if a resource matches a pattern\u0026#34;\u0026#34;\u0026#34; resource_str = str(resource) # Handle wildcard patterns if pattern == \u0026#34;*\u0026#34;: return True # Split pattern and resource into parts pattern_parts = pattern.split(\u0026#34;.\u0026#34;) resource_parts = resource_str.split(\u0026#34;.\u0026#34;) # Ensure same number of parts or pattern has wildcards if len(pattern_parts) != len(resource_parts): return False # Check each part for pattern_part, resource_part in zip(pattern_parts, resource_parts): if pattern_part != \u0026#34;*\u0026#34; and pattern_part != resource_part: return False return True def check_access(self, request: AccessRequest) -\u0026gt; AccessResult: \u0026#34;\u0026#34;\u0026#34;Check if a user has access to a resource\u0026#34;\u0026#34;\u0026#34; try: if request.user_id not in self.users: return AccessResult( granted=False, reason=\u0026#34;User not found\u0026#34;, effective_permissions=set(), applied_policies=[] ) user = self.users[request.user_id] # Check if user is active if not user.is_active: return AccessResult( granted=False, reason=\u0026#34;User account is inactive\u0026#34;, effective_permissions=set(), applied_policies=[] ) # Get effective permissions effective_permissions = self._get_effective_permissions(request.user_id) # Find matching permissions granted_permissions = set() applied_policies = [] for pattern, permissions in effective_permissions.items(): if self._match_resource_pattern(pattern, request.resource): granted_permissions.update(permissions) applied_policies.append(f\u0026#34;Pattern: {pattern}\u0026#34;) # Check if requested permission is granted access_granted = request.permission in granted_permissions # Apply additional policies policy_result = self._apply_access_policies(request, granted_permissions) if not policy_result[\u0026#39;allowed\u0026#39;]: access_granted = False applied_policies.extend(policy_result[\u0026#39;policies\u0026#39;]) # Log access attempt self._audit_log(\u0026#34;ACCESS_CHECK\u0026#34;, { \u0026#34;user_id\u0026#34;: request.user_id, \u0026#34;resource\u0026#34;: str(request.resource), \u0026#34;permission\u0026#34;: request.permission.value, \u0026#34;granted\u0026#34;: access_granted, \u0026#34;reason\u0026#34;: \u0026#34;Permission granted\u0026#34; if access_granted else \u0026#34;Permission denied\u0026#34; }) return AccessResult( granted=access_granted, reason=\u0026#34;Permission granted\u0026#34; if access_granted else \u0026#34;Permission denied\u0026#34;, effective_permissions=granted_permissions, applied_policies=applied_policies ) except Exception as e: self.logger.error(f\u0026#34;Access check failed: {e}\u0026#34;) return AccessResult( granted=False, reason=f\u0026#34;Internal error: {e}\u0026#34;, effective_permissions=set(), applied_policies=[] ) def _apply_access_policies(self, request: AccessRequest, granted_permissions: Set[Permission]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Apply additional access policies\u0026#34;\u0026#34;\u0026#34; policies_applied = [] allowed = True # Time-based access policy current_hour = datetime.now().hour if current_hour \u0026lt; 6 or current_hour \u0026gt; 22: if Permission.ADMIN in granted_permissions: # Admin operations restricted during off-hours allowed = False policies_applied.append(\u0026#34;Off-hours admin restriction\u0026#34;) # Sensitive data access policy if \u0026#34;sensitive\u0026#34; in str(request.resource).lower(): if Permission.READ in granted_permissions: # Additional logging for sensitive data access policies_applied.append(\u0026#34;Sensitive data access logged\u0026#34;) # High-privilege operation policy if request.permission in [Permission.DELETE, Permission.ADMIN]: # Require additional verification for high-privilege operations if not request.context.get(\u0026#39;verified\u0026#39;, False): allowed = False policies_applied.append(\u0026#34;High-privilege operation requires verification\u0026#34;) return { \u0026#39;allowed\u0026#39;: allowed, \u0026#39;policies\u0026#39;: policies_applied } def _audit_log(self, action: str, details: Dict[str, Any]): \u0026#34;\u0026#34;\u0026#34;Log audit event\u0026#34;\u0026#34;\u0026#34; audit_entry = { \u0026#39;timestamp\u0026#39;: datetime.now().isoformat(), \u0026#39;action\u0026#39;: action, \u0026#39;details\u0026#39;: details } self.audit_log.append(audit_entry) def get_user_permissions(self, user_id: str) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Get comprehensive permission report for a user\u0026#34;\u0026#34;\u0026#34; if user_id not in self.users: return {} user = self.users[user_id] effective_permissions = self._get_effective_permissions(user_id) return { \u0026#39;user_id\u0026#39;: user_id, \u0026#39;username\u0026#39;: user.username, \u0026#39;roles\u0026#39;: list(user.roles), \u0026#39;effective_permissions\u0026#39;: { pattern: [p.value for p in perms] for pattern, perms in effective_permissions.items() }, \u0026#39;direct_permissions\u0026#39;: { pattern: [p.value for p in perms] for pattern, perms in user.direct_permissions.items() } } def export_configuration(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Export RBAC configuration\u0026#34;\u0026#34;\u0026#34; return { \u0026#39;users\u0026#39;: { user_id: { \u0026#39;username\u0026#39;: user.username, \u0026#39;email\u0026#39;: user.email, \u0026#39;roles\u0026#39;: list(user.roles), \u0026#39;is_active\u0026#39;: user.is_active } for user_id, user in self.users.items() }, \u0026#39;roles\u0026#39;: { role_name: { \u0026#39;description\u0026#39;: role.description, \u0026#39;permissions\u0026#39;: { pattern: [p.value for p in perms] for pattern, perms in role.permissions.items() }, \u0026#39;parent_roles\u0026#39;: list(role.parent_roles), \u0026#39;is_active\u0026#39;: role.is_active } for role_name, role in self.roles.items() } } # Example usage def main(): rbac = RBACManager() # Create users rbac.create_user(\u0026#34;alice\u0026#34;, \u0026#34;Alice Smith\u0026#34;, \u0026#34;alice@company.com\u0026#34;, [\u0026#34;dba\u0026#34;]) rbac.create_user(\u0026#34;bob\u0026#34;, \u0026#34;Bob Johnson\u0026#34;, \u0026#34;bob@company.com\u0026#34;, [\u0026#34;data_analyst\u0026#34;]) rbac.create_user(\u0026#34;charlie\u0026#34;, \u0026#34;Charlie Brown\u0026#34;, \u0026#34;charlie@company.com\u0026#34;, [\u0026#34;app_user\u0026#34;]) # Create custom role rbac.create_role(\u0026#34;report_viewer\u0026#34;, \u0026#34;Can view reports\u0026#34;, [\u0026#34;readonly\u0026#34;]) rbac.grant_permission(\u0026#34;report_viewer\u0026#34;, \u0026#34;reports.*\u0026#34;, [Permission.READ]) # Test access resource = Resource(ResourceType.TABLE, \u0026#34;users\u0026#34;, \u0026#34;public\u0026#34;, \u0026#34;app_db\u0026#34;) request = AccessRequest(\u0026#34;alice\u0026#34;, resource, Permission.READ) result = rbac.check_access(request) print(f\u0026#34;Access Result for Alice:\u0026#34;) print(f\u0026#34; Granted: {result.granted}\u0026#34;) print(f\u0026#34; Reason: {result.reason}\u0026#34;) print(f\u0026#34; Effective Permissions: {[p.value for p in result.effective_permissions]}\u0026#34;) # Get user permissions report permissions_report = rbac.get_user_permissions(\u0026#34;alice\u0026#34;) print(f\u0026#34;\\nAlice\u0026#39;s Permissions:\u0026#34;) print(json.dumps(permissions_report, indent=2)) if __name__ == \u0026#34;__main__\u0026#34;: main() Data Encryption 1. Comprehensive Encryption Manager #!/usr/bin/env python3 # src/security/encryption_manager.py import os import base64 import json import logging from typing import Dict, List, Optional, Any, Tuple from dataclasses import dataclass from enum import Enum import secrets from cryptography.fernet import Fernet from cryptography.hazmat.primitives import hashes, serialization from cryptography.hazmat.primitives.asymmetric import rsa, padding from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC from cryptography.hazmat.primitives.kdf.scrypt import Scrypt import hashlib import hmac class EncryptionAlgorithm(Enum): AES_256_GCM = \u0026#34;aes_256_gcm\u0026#34; AES_256_CBC = \u0026#34;aes_256_cbc\u0026#34; CHACHA20_POLY1305 = \u0026#34;chacha20_poly1305\u0026#34; RSA_OAEP = \u0026#34;rsa_oaep\u0026#34; FERNET = \u0026#34;fernet\u0026#34; class KeyDerivationFunction(Enum): PBKDF2 = \u0026#34;pbkdf2\u0026#34; SCRYPT = \u0026#34;scrypt\u0026#34; ARGON2 = \u0026#34;argon2\u0026#34; @dataclass class EncryptionConfig: algorithm: EncryptionAlgorithm key_size: int kdf: KeyDerivationFunction iterations: int salt_size: int iv_size: int @dataclass class EncryptedData: ciphertext: bytes iv: Optional[bytes] salt: Optional[bytes] tag: Optional[bytes] algorithm: EncryptionAlgorithm metadata: Dict[str, Any] class EncryptionManager: def __init__(self, config: Dict[str, Any]): self.config = config self.logger = self._setup_logging() self.master_key = self._derive_master_key() self.key_cache: Dict[str, bytes] = {} # Default encryption configurations self.encryption_configs = { \u0026#39;default\u0026#39;: EncryptionConfig( algorithm=EncryptionAlgorithm.AES_256_GCM, key_size=32, kdf=KeyDerivationFunction.PBKDF2, iterations=100000, salt_size=16, iv_size=12 ), \u0026#39;high_security\u0026#39;: EncryptionConfig( algorithm=EncryptionAlgorithm.AES_256_GCM, key_size=32, kdf=KeyDerivationFunction.SCRYPT, iterations=32768, salt_size=32, iv_size=12 ), \u0026#39;performance\u0026#39;: EncryptionConfig( algorithm=EncryptionAlgorithm.CHACHA20_POLY1305, key_size=32, kdf=KeyDerivationFunction.PBKDF2, iterations=50000, salt_size=16, iv_size=12 ) } def _setup_logging(self) -\u0026gt; logging.Logger: logger = logging.getLogger(\u0026#39;EncryptionManager\u0026#39;) logger.setLevel(logging.INFO) handler = logging.StreamHandler() formatter = logging.Formatter(\u0026#39;%(asctime)s - %(levelname)s - %(message)s\u0026#39;) handler.setFormatter(formatter) logger.addHandler(handler) return logger def _derive_master_key(self) -\u0026gt; bytes: \u0026#34;\u0026#34;\u0026#34;Derive master key from configuration\u0026#34;\u0026#34;\u0026#34; master_password = self.config.get(\u0026#39;master_password\u0026#39;, \u0026#39;\u0026#39;).encode() salt = self.config.get(\u0026#39;master_salt\u0026#39;, \u0026#39;default_salt\u0026#39;).encode() if not master_password: # Generate random master key if no password provided return secrets.token_bytes(32) kdf = PBKDF2HMAC( algorithm=hashes.SHA256(), length=32, salt=salt, iterations=100000, ) return kdf.derive(master_password) def derive_key(self, password: str, salt: bytes, config: EncryptionConfig) -\u0026gt; bytes: \u0026#34;\u0026#34;\u0026#34;Derive encryption key from password\u0026#34;\u0026#34;\u0026#34; password_bytes = password.encode(\u0026#39;utf-8\u0026#39;) if config.kdf == KeyDerivationFunction.PBKDF2: kdf = PBKDF2HMAC( algorithm=hashes.SHA256(), length=config.key_size, salt=salt, iterations=config.iterations, ) return kdf.derive(password_bytes) elif config.kdf == KeyDerivationFunction.SCRYPT: kdf = Scrypt( algorithm=hashes.SHA256(), length=config.key_size, salt=salt, n=config.iterations, r=8, p=1, ) return kdf.derive(password_bytes) else: raise ValueError(f\u0026#34;Unsupported KDF: {config.kdf}\u0026#34;) def generate_key(self, key_id: str, config_name: str = \u0026#39;default\u0026#39;) -\u0026gt; bytes: \u0026#34;\u0026#34;\u0026#34;Generate and store a new encryption key\u0026#34;\u0026#34;\u0026#34; config = self.encryption_configs[config_name] key = secrets.token_bytes(config.key_size) # Encrypt key with master key for storage encrypted_key = self._encrypt_key(key) # Store encrypted key (in production, this would be in a secure key store) self._store_key(key_id, encrypted_key) # Cache the key self.key_cache[key_id] = key self.logger.info(f\u0026#34;Generated new key: {key_id}\u0026#34;) return key def get_key(self, key_id: str) -\u0026gt; Optional[bytes]: \u0026#34;\u0026#34;\u0026#34;Retrieve encryption key\u0026#34;\u0026#34;\u0026#34; if key_id in self.key_cache: return self.key_cache[key_id] # Load from storage encrypted_key = self._load_key(key_id) if encrypted_key: key = self._decrypt_key(encrypted_key) self.key_cache[key_id] = key return key return None def _encrypt_key(self, key: bytes) -\u0026gt; bytes: \u0026#34;\u0026#34;\u0026#34;Encrypt key with master key\u0026#34;\u0026#34;\u0026#34; fernet = Fernet(base64.urlsafe_b64encode(self.master_key)) return fernet.encrypt(key) def _decrypt_key(self, encrypted_key: bytes) -\u0026gt; bytes: \u0026#34;\u0026#34;\u0026#34;Decrypt key with master key\u0026#34;\u0026#34;\u0026#34; fernet = Fernet(base64.urlsafe_b64encode(self.master_key)) return fernet.decrypt(encrypted_key) def _store_key(self, key_id: str, encrypted_key: bytes): \u0026#34;\u0026#34;\u0026#34;Store encrypted key (placeholder implementation)\u0026#34;\u0026#34;\u0026#34; # In production, this would store in a secure key management system key_file = f\u0026#34;/tmp/keys/{key_id}.key\u0026#34; os.makedirs(os.path.dirname(key_file), exist_ok=True) with open(key_file, \u0026#39;wb\u0026#39;) as f: f.write(encrypted_key) def _load_key(self, key_id: str) -\u0026gt; Optional[bytes]: \u0026#34;\u0026#34;\u0026#34;Load encrypted key (placeholder implementation)\u0026#34;\u0026#34;\u0026#34; key_file = f\u0026#34;/tmp/keys/{key_id}.key\u0026#34; try: with open(key_file, \u0026#39;rb\u0026#39;) as f: return f.read() except FileNotFoundError: return None def encrypt_data(self, data: bytes, key_id: str, config_name: str = \u0026#39;default\u0026#39;) -\u0026gt; EncryptedData: \u0026#34;\u0026#34;\u0026#34;Encrypt data using specified configuration\u0026#34;\u0026#34;\u0026#34; config = self.encryption_configs[config_name] key = self.get_key(key_id) if not key: raise ValueError(f\u0026#34;Key not found: {key_id}\u0026#34;) if config.algorithm == EncryptionAlgorithm.AES_256_GCM: return self._encrypt_aes_gcm(data, key, config) elif config.algorithm == EncryptionAlgorithm.AES_256_CBC: return self._encrypt_aes_cbc(data, key, config) elif config.algorithm == EncryptionAlgorithm.CHACHA20_POLY1305: return self._encrypt_chacha20(data, key, config) elif config.algorithm == EncryptionAlgorithm.FERNET: return self._encrypt_fernet(data, key, config) else: raise ValueError(f\u0026#34;Unsupported algorithm: {config.algorithm}\u0026#34;) def decrypt_data(self, encrypted_data: EncryptedData, key_id: str) -\u0026gt; bytes: \u0026#34;\u0026#34;\u0026#34;Decrypt data\u0026#34;\u0026#34;\u0026#34; key = self.get_key(key_id) if not key: raise ValueError(f\u0026#34;Key not found: {key_id}\u0026#34;) if encrypted_data.algorithm == EncryptionAlgorithm.AES_256_GCM: return self._decrypt_aes_gcm(encrypted_data, key) elif encrypted_data.algorithm == EncryptionAlgorithm.AES_256_CBC: return self._decrypt_aes_cbc(encrypted_data, key) elif encrypted_data.algorithm == EncryptionAlgorithm.CHACHA20_POLY1305: return self._decrypt_chacha20(encrypted_data, key) elif encrypted_data.algorithm == EncryptionAlgorithm.FERNET: return self._decrypt_fernet(encrypted_data, key) else: raise ValueError(f\u0026#34;Unsupported algorithm: {encrypted_data.algorithm}\u0026#34;) def _encrypt_aes_gcm(self, data: bytes, key: bytes, config: EncryptionConfig) -\u0026gt; EncryptedData: \u0026#34;\u0026#34;\u0026#34;Encrypt using AES-256-GCM\u0026#34;\u0026#34;\u0026#34; iv = secrets.token_bytes(config.iv_size) cipher = Cipher(algorithms.AES(key), modes.GCM(iv)) encryptor = cipher.encryptor() ciphertext = encryptor.update(data) + encryptor.finalize() return EncryptedData( ciphertext=ciphertext, iv=iv, salt=None, tag=encryptor.tag, algorithm=config.algorithm, metadata={\u0026#39;key_size\u0026#39;: len(key)} ) def _decrypt_aes_gcm(self, encrypted_data: EncryptedData, key: bytes) -\u0026gt; bytes: \u0026#34;\u0026#34;\u0026#34;Decrypt using AES-256-GCM\u0026#34;\u0026#34;\u0026#34; cipher = Cipher( algorithms.AES(key), modes.GCM(encrypted_data.iv, encrypted_data.tag) ) decryptor = cipher.decryptor() return decryptor.update(encrypted_data.ciphertext) + decryptor.finalize() def _encrypt_aes_cbc(self, data: bytes, key: bytes, config: EncryptionConfig) -\u0026gt; EncryptedData: \u0026#34;\u0026#34;\u0026#34;Encrypt using AES-256-CBC\u0026#34;\u0026#34;\u0026#34; iv = secrets.token_bytes(16) # AES block size # Pad data to block size padding_length = 16 - (len(data) % 16) padded_data = data + bytes([padding_length] * padding_length) cipher = Cipher(algorithms.AES(key), modes.CBC(iv)) encryptor = cipher.encryptor() ciphertext = encryptor.update(padded_data) + encryptor.finalize() return EncryptedData( ciphertext=ciphertext, iv=iv, salt=None, tag=None, algorithm=config.algorithm, metadata={\u0026#39;key_size\u0026#39;: len(key)} ) def _decrypt_aes_cbc(self, encrypted_data: EncryptedData, key: bytes) -\u0026gt; bytes: \u0026#34;\u0026#34;\u0026#34;Decrypt using AES-256-CBC\u0026#34;\u0026#34;\u0026#34; cipher = Cipher(algorithms.AES(key), modes.CBC(encrypted_data.iv)) decryptor = cipher.decryptor() padded_data = decryptor.update(encrypted_data.ciphertext) + decryptor.finalize() # Remove padding padding_length = padded_data[-1] return padded_data[:-padding_length] def _encrypt_chacha20(self, data: bytes, key: bytes, config: EncryptionConfig) -\u0026gt; EncryptedData: \u0026#34;\u0026#34;\u0026#34;Encrypt using ChaCha20-Poly1305\u0026#34;\u0026#34;\u0026#34; nonce = secrets.token_bytes(12) # ChaCha20 nonce size cipher = Cipher(algorithms.ChaCha20(key, nonce), mode=None) encryptor = cipher.encryptor() ciphertext = encryptor.update(data) + encryptor.finalize() return EncryptedData( ciphertext=ciphertext, iv=nonce, salt=None, tag=None, algorithm=config.algorithm, metadata={\u0026#39;key_size\u0026#39;: len(key)} ) def _decrypt_chacha20(self, encrypted_data: EncryptedData, key: bytes) -\u0026gt; bytes: \u0026#34;\u0026#34;\u0026#34;Decrypt using ChaCha20-Poly1305\u0026#34;\u0026#34;\u0026#34; cipher = Cipher(algorithms.ChaCha20(key, encrypted_data.iv), mode=None) decryptor = cipher.decryptor() return decryptor.update(encrypted_data.ciphertext) + decryptor.finalize() def _encrypt_fernet(self, data: bytes, key: bytes, config: EncryptionConfig) -\u0026gt; EncryptedData: \u0026#34;\u0026#34;\u0026#34;Encrypt using Fernet\u0026#34;\u0026#34;\u0026#34; fernet = Fernet(base64.urlsafe_b64encode(key)) ciphertext = fernet.encrypt(data) return EncryptedData( ciphertext=ciphertext, iv=None, salt=None, tag=None, algorithm=config.algorithm, metadata={\u0026#39;key_size\u0026#39;: len(key)} ) def _decrypt_fernet(self, encrypted_data: EncryptedData, key: bytes) -\u0026gt; bytes: \u0026#34;\u0026#34;\u0026#34;Decrypt using Fernet\u0026#34;\u0026#34;\u0026#34; fernet = Fernet(base64.urlsafe_b64encode(key)) return fernet.decrypt(encrypted_data.ciphertext) def encrypt_column_data(self, table_name: str, column_name: str, data: Any, data_type: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Encrypt column data with format preservation\u0026#34;\u0026#34;\u0026#34; # Convert data to bytes if isinstance(data, str): data_bytes = data.encode(\u0026#39;utf-8\u0026#39;) elif isinstance(data, (int, float)): data_bytes = str(data).encode(\u0026#39;utf-8\u0026#39;) else: data_bytes = str(data).encode(\u0026#39;utf-8\u0026#39;) # Use column-specific key key_id = f\u0026#34;{table_name}.{column_name}\u0026#34; # Ensure key exists if not self.get_key(key_id): self.generate_key(key_id) # Encrypt data encrypted_data = self.encrypt_data(data_bytes, key_id) # Return base64-encoded ciphertext for storage return base64.b64encode(encrypted_data.ciphertext).decode(\u0026#39;utf-8\u0026#39;) def decrypt_column_data(self, table_name: str, column_name: str, encrypted_value: str, data_type: str) -\u0026gt; Any: \u0026#34;\u0026#34;\u0026#34;Decrypt column data and convert back to original type\u0026#34;\u0026#34;\u0026#34; key_id = f\u0026#34;{table_name}.{column_name}\u0026#34; # Decode base64 ciphertext = base64.b64decode(encrypted_value.encode(\u0026#39;utf-8\u0026#39;)) # Create EncryptedData object (simplified - in practice, store metadata) encrypted_data = EncryptedData( ciphertext=ciphertext, iv=None, salt=None, tag=None, algorithm=EncryptionAlgorithm.FERNET, metadata={} ) # Decrypt decrypted_bytes = self.decrypt_data(encrypted_data, key_id) decrypted_str = decrypted_bytes.decode(\u0026#39;utf-8\u0026#39;) # Convert back to original type if data_type.lower() in [\u0026#39;int\u0026#39;, \u0026#39;integer\u0026#39;, \u0026#39;bigint\u0026#39;]: return int(decrypted_str) elif data_type.lower() in [\u0026#39;float\u0026#39;, \u0026#39;double\u0026#39;, \u0026#39;decimal\u0026#39;]: return float(decrypted_str) else: return decrypted_str def rotate_key(self, key_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Rotate encryption key\u0026#34;\u0026#34;\u0026#34; try: # Generate new key old_key = self.get_key(key_id) new_key = secrets.token_bytes(32) # Store new key with versioned ID new_key_id = f\u0026#34;{key_id}_v{int(time.time())}\u0026#34; encrypted_new_key = self._encrypt_key(new_key) self._store_key(new_key_id, encrypted_new_key) # Update key cache self.key_cache[key_id] = new_key self.logger.info(f\u0026#34;Rotated key: {key_id}\u0026#34;) return True except Exception as e: self.logger.error(f\u0026#34;Key rotation failed for {key_id}: {e}\u0026#34;) return False def generate_hash(self, data: str, salt: Optional[str] = None) -\u0026gt; Tuple[str, str]: \u0026#34;\u0026#34;\u0026#34;Generate secure hash for data\u0026#34;\u0026#34;\u0026#34; if salt is None: salt = secrets.token_hex(16) # Use PBKDF2 for password-like data hash_value = hashlib.pbkdf2_hmac( \u0026#39;sha256\u0026#39;, data.encode(\u0026#39;utf-8\u0026#39;), salt.encode(\u0026#39;utf-8\u0026#39;), 100000 ) return base64.b64encode(hash_value).decode(\u0026#39;utf-8\u0026#39;), salt def verify_hash(self, data: str, hash_value: str, salt: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Verify data against hash\u0026#34;\u0026#34;\u0026#34; computed_hash, _ = self.generate_hash(data, salt) return hmac.compare_digest(computed_hash, hash_value) # Example usage def main(): config = { \u0026#39;master_password\u0026#39;: \u0026#39;secure_master_password_2024\u0026#39;, \u0026#39;master_salt\u0026#39;: \u0026#39;encryption_salt_2024\u0026#39; } encryption_manager = EncryptionManager(config) # Generate a key for sensitive data key_id = \u0026#34;users.ssn\u0026#34; encryption_manager.generate_key(key_id) # Encrypt sensitive data sensitive_data = \u0026#34;123-45-6789\u0026#34; encrypted_ssn = encryption_manager.encrypt_column_data( \u0026#34;users\u0026#34;, \u0026#34;ssn\u0026#34;, sensitive_data, \u0026#34;varchar\u0026#34; ) print(f\u0026#34;Original SSN: {sensitive_data}\u0026#34;) print(f\u0026#34;Encrypted SSN: {encrypted_ssn}\u0026#34;) # Decrypt data decrypted_ssn = encryption_manager.decrypt_column_data( \u0026#34;users\u0026#34;, \u0026#34;ssn\u0026#34;, encrypted_ssn, \u0026#34;varchar\u0026#34; ) print(f\u0026#34;Decrypted SSN: {decrypted_ssn}\u0026#34;) # Generate hash for password password = \u0026#34;user_password_123\u0026#34; hash_value, salt = encryption_manager.generate_hash(password) print(f\u0026#34;Password hash: {hash_value}\u0026#34;) print(f\u0026#34;Salt: {salt}\u0026#34;) # Verify password is_valid = encryption_manager.verify_hash(password, hash_value, salt) print(f\u0026#34;Password verification: {is_valid}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() Access Control Implementation 1. Database-Level Access Control import sqlite3 import mysql.connector import psycopg2 from typing import Dict, List, Optional, Any import logging from datetime import datetime, timedelta class DatabaseAccessController: \u0026#34;\u0026#34;\u0026#34;Unified database access control implementation\u0026#34;\u0026#34;\u0026#34; def __init__(self, config: Dict[str, Any]): self.config = config self.logger = logging.getLogger(__name__) self.connections = {} self.access_policies = {} def create_connection(self, db_type: str, connection_params: Dict[str, Any]) -\u0026gt; Any: \u0026#34;\u0026#34;\u0026#34;Create secure database connection\u0026#34;\u0026#34;\u0026#34; try: if db_type == \u0026#39;mysql\u0026#39;: connection = mysql.connector.connect( **connection_params, ssl_disabled=False, ssl_verify_cert=True, ssl_verify_identity=True, autocommit=False ) elif db_type == \u0026#39;postgresql\u0026#39;: connection = psycopg2.connect( **connection_params, sslmode=\u0026#39;require\u0026#39;, connect_timeout=10 ) elif db_type == \u0026#39;sqlite\u0026#39;: connection = sqlite3.connect( connection_params[\u0026#39;database\u0026#39;], timeout=10.0, check_same_thread=False ) else: raise ValueError(f\u0026#34;Unsupported database type: {db_type}\u0026#34;) self.connections[db_type] = connection self.logger.info(f\u0026#34;Secure connection established for {db_type}\u0026#34;) return connection except Exception as e: self.logger.error(f\u0026#34;Connection failed for {db_type}: {e}\u0026#34;) raise def create_user_with_privileges(self, db_type: str, username: str, password: str, privileges: List[str], databases: List[str] = None) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Create database user with specific privileges\u0026#34;\u0026#34;\u0026#34; try: connection = self.connections.get(db_type) if not connection: raise ValueError(f\u0026#34;No connection available for {db_type}\u0026#34;) cursor = connection.cursor() if db_type == \u0026#39;mysql\u0026#39;: # Create user cursor.execute(f\u0026#34;CREATE USER \u0026#39;{username}\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY %s\u0026#34;, (password,)) # Grant privileges for db_name in (databases or [\u0026#39;*\u0026#39;]): for privilege in privileges: if privilege.upper() == \u0026#39;ALL\u0026#39;: cursor.execute(f\u0026#34;GRANT ALL PRIVILEGES ON {db_name}.* TO \u0026#39;{username}\u0026#39;@\u0026#39;%\u0026#39;\u0026#34;) else: cursor.execute(f\u0026#34;GRANT {privilege} ON {db_name}.* TO \u0026#39;{username}\u0026#39;@\u0026#39;%\u0026#39;\u0026#34;) cursor.execute(\u0026#34;FLUSH PRIVILEGES\u0026#34;) elif db_type == \u0026#39;postgresql\u0026#39;: # Create user cursor.execute(f\u0026#34;CREATE USER {username} WITH PASSWORD %s\u0026#34;, (password,)) # Grant privileges for db_name in (databases or []): cursor.execute(f\u0026#34;GRANT CONNECT ON DATABASE {db_name} TO {username}\u0026#34;) for privilege in privileges: if privilege.upper() == \u0026#39;ALL\u0026#39;: cursor.execute(f\u0026#34;GRANT ALL PRIVILEGES ON DATABASE {db_name} TO {username}\u0026#34;) else: cursor.execute(f\u0026#34;GRANT {privilege} ON DATABASE {db_name} TO {username}\u0026#34;) connection.commit() self.logger.info(f\u0026#34;User {username} created with privileges: {privileges}\u0026#34;) return True except Exception as e: self.logger.error(f\u0026#34;Failed to create user {username}: {e}\u0026#34;) connection.rollback() return False def implement_row_level_security(self, db_type: str, table_name: str, policy_name: str, policy_condition: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Implement row-level security policies\u0026#34;\u0026#34;\u0026#34; try: connection = self.connections.get(db_type) cursor = connection.cursor() if db_type == \u0026#39;postgresql\u0026#39;: # Enable RLS on table cursor.execute(f\u0026#34;ALTER TABLE {table_name} ENABLE ROW LEVEL SECURITY\u0026#34;) # Create policy cursor.execute(f\u0026#34;\u0026#34;\u0026#34; CREATE POLICY {policy_name} ON {table_name} FOR ALL TO PUBLIC USING ({policy_condition}) \u0026#34;\u0026#34;\u0026#34;) elif db_type == \u0026#39;mysql\u0026#39;: # MySQL doesn\u0026#39;t have native RLS, implement with views view_name = f\u0026#34;{table_name}_secure\u0026#34; cursor.execute(f\u0026#34;\u0026#34;\u0026#34; CREATE VIEW {view_name} AS SELECT * FROM {table_name} WHERE {policy_condition} \u0026#34;\u0026#34;\u0026#34;) connection.commit() self.logger.info(f\u0026#34;Row-level security implemented for {table_name}\u0026#34;) return True except Exception as e: self.logger.error(f\u0026#34;Failed to implement RLS for {table_name}: {e}\u0026#34;) return False def create_security_definer_function(self, db_type: str, function_name: str, function_body: str, owner: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Create security definer functions for controlled access\u0026#34;\u0026#34;\u0026#34; try: connection = self.connections.get(db_type) cursor = connection.cursor() if db_type == \u0026#39;postgresql\u0026#39;: cursor.execute(f\u0026#34;\u0026#34;\u0026#34; CREATE OR REPLACE FUNCTION {function_name}() RETURNS TABLE(result_data TEXT) LANGUAGE plpgsql SECURITY DEFINER SET search_path = public AS $$ BEGIN {function_body} END; $$; \u0026#34;\u0026#34;\u0026#34;) # Set owner cursor.execute(f\u0026#34;ALTER FUNCTION {function_name}() OWNER TO {owner}\u0026#34;) connection.commit() self.logger.info(f\u0026#34;Security definer function {function_name} created\u0026#34;) return True except Exception as e: self.logger.error(f\u0026#34;Failed to create function {function_name}: {e}\u0026#34;) return False ### 2. Application-Level Access Control class ApplicationAccessControl: \u0026#34;\u0026#34;\u0026#34;Application-level access control and session management\u0026#34;\u0026#34;\u0026#34; def __init__(self, config: Dict[str, Any]): self.config = config self.logger = logging.getLogger(__name__) self.active_sessions = {} self.failed_attempts = {} def validate_session(self, session_id: str, user_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Validate user session\u0026#34;\u0026#34;\u0026#34; try: session = self.active_sessions.get(session_id) if not session: return False # Check session expiry if datetime.now() \u0026gt; session[\u0026#39;expires_at\u0026#39;]: del self.active_sessions[session_id] return False # Check user match if session[\u0026#39;user_id\u0026#39;] != user_id: return False # Update last activity session[\u0026#39;last_activity\u0026#39;] = datetime.now() return True except Exception as e: self.logger.error(f\u0026#34;Session validation failed: {e}\u0026#34;) return False def check_rate_limiting(self, user_id: str, action: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Implement rate limiting for database operations\u0026#34;\u0026#34;\u0026#34; try: current_time = datetime.now() key = f\u0026#34;{user_id}:{action}\u0026#34; if key not in self.failed_attempts: self.failed_attempts[key] = [] # Clean old attempts self.failed_attempts[key] = [ attempt for attempt in self.failed_attempts[key] if current_time - attempt \u0026lt; timedelta(minutes=15) ] # Check rate limit max_attempts = self.config.get(\u0026#39;rate_limit\u0026#39;, {}).get(action, 10) if len(self.failed_attempts[key]) \u0026gt;= max_attempts: self.logger.warning(f\u0026#34;Rate limit exceeded for {user_id}:{action}\u0026#34;) return False return True except Exception as e: self.logger.error(f\u0026#34;Rate limiting check failed: {e}\u0026#34;) return False def log_access_attempt(self, user_id: str, action: str, success: bool, details: Dict[str, Any] = None): \u0026#34;\u0026#34;\u0026#34;Log access attempts for auditing\u0026#34;\u0026#34;\u0026#34; try: log_entry = { \u0026#39;timestamp\u0026#39;: datetime.now().isoformat(), \u0026#39;user_id\u0026#39;: user_id, \u0026#39;action\u0026#39;: action, \u0026#39;success\u0026#39;: success, \u0026#39;ip_address\u0026#39;: details.get(\u0026#39;ip_address\u0026#39;) if details else None, \u0026#39;user_agent\u0026#39;: details.get(\u0026#39;user_agent\u0026#39;) if details else None, \u0026#39;resource\u0026#39;: details.get(\u0026#39;resource\u0026#39;) if details else None } # Log to file or database self.logger.info(f\u0026#34;Access attempt: {log_entry}\u0026#34;) # Track failed attempts for rate limiting if not success: key = f\u0026#34;{user_id}:{action}\u0026#34; if key not in self.failed_attempts: self.failed_attempts[key] = [] self.failed_attempts[key].append(datetime.now()) except Exception as e: self.logger.error(f\u0026#34;Failed to log access attempt: {e}\u0026#34;) ## Auditing and Monitoring ### 1. Comprehensive Audit System ```python import json import hashlib from datetime import datetime from typing import Dict, List, Any, Optional from dataclasses import dataclass, asdict from enum import Enum class AuditEventType(Enum): LOGIN = \u0026#34;login\u0026#34; LOGOUT = \u0026#34;logout\u0026#34; QUERY = \u0026#34;query\u0026#34; INSERT = \u0026#34;insert\u0026#34; UPDATE = \u0026#34;update\u0026#34; DELETE = \u0026#34;delete\u0026#34; SCHEMA_CHANGE = \u0026#34;schema_change\u0026#34; PRIVILEGE_CHANGE = \u0026#34;privilege_change\u0026#34; BACKUP = \u0026#34;backup\u0026#34; RESTORE = \u0026#34;restore\u0026#34; @dataclass class AuditEvent: event_id: str timestamp: datetime event_type: AuditEventType user_id: str session_id: str database_name: str table_name: Optional[str] query: Optional[str] affected_rows: Optional[int] ip_address: str user_agent: str success: bool error_message: Optional[str] data_before: Optional[Dict[str, Any]] data_after: Optional[Dict[str, Any]] class DatabaseAuditSystem: \u0026#34;\u0026#34;\u0026#34;Comprehensive database auditing system\u0026#34;\u0026#34;\u0026#34; def __init__(self, config: Dict[str, Any]): self.config = config self.logger = logging.getLogger(__name__) self.audit_storage = config.get(\u0026#39;audit_storage\u0026#39;, \u0026#39;file\u0026#39;) self.retention_days = config.get(\u0026#39;retention_days\u0026#39;, 365) def generate_event_id(self, event_data: Dict[str, Any]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Generate unique event ID\u0026#34;\u0026#34;\u0026#34; event_string = json.dumps(event_data, sort_keys=True, default=str) return hashlib.sha256(event_string.encode()).hexdigest()[:16] def log_audit_event(self, event: AuditEvent) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Log audit event to storage\u0026#34;\u0026#34;\u0026#34; try: event_dict = asdict(event) event_dict[\u0026#39;timestamp\u0026#39;] = event.timestamp.isoformat() event_dict[\u0026#39;event_type\u0026#39;] = event.event_type.value if self.audit_storage == \u0026#39;file\u0026#39;: self._log_to_file(event_dict) elif self.audit_storage == \u0026#39;database\u0026#39;: self._log_to_database(event_dict) elif self.audit_storage == \u0026#39;syslog\u0026#39;: self._log_to_syslog(event_dict) return True except Exception as e: self.logger.error(f\u0026#34;Failed to log audit event: {e}\u0026#34;) return False def _log_to_file(self, event_dict: Dict[str, Any]): \u0026#34;\u0026#34;\u0026#34;Log audit event to file\u0026#34;\u0026#34;\u0026#34; audit_file = f\u0026#34;audit_{datetime.now().strftime(\u0026#39;%Y%m%d\u0026#39;)}.log\u0026#34; with open(audit_file, \u0026#39;a\u0026#39;) as f: f.write(json.dumps(event_dict) + \u0026#39;\\n\u0026#39;) def _log_to_database(self, event_dict: Dict[str, Any]): \u0026#34;\u0026#34;\u0026#34;Log audit event to database\u0026#34;\u0026#34;\u0026#34; # Implementation for database logging pass def _log_to_syslog(self, event_dict: Dict[str, Any]): \u0026#34;\u0026#34;\u0026#34;Log audit event to syslog\u0026#34;\u0026#34;\u0026#34; import syslog syslog.openlog(\u0026#34;database_audit\u0026#34;) syslog.syslog(syslog.LOG_INFO, json.dumps(event_dict)) def create_audit_triggers(self, db_type: str, table_name: str) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;Create database triggers for automatic auditing\u0026#34;\u0026#34;\u0026#34; triggers = [] if db_type == \u0026#39;postgresql\u0026#39;: # Create audit table audit_table = f\u0026#34;{table_name}_audit\u0026#34; triggers.append(f\u0026#34;\u0026#34;\u0026#34; CREATE TABLE IF NOT EXISTS {audit_table} ( audit_id SERIAL PRIMARY KEY, operation CHAR(1) NOT NULL, stamp TIMESTAMP NOT NULL DEFAULT NOW(), userid TEXT NOT NULL DEFAULT USER, old_values JSONB, new_values JSONB ); \u0026#34;\u0026#34;\u0026#34;) # Create trigger function triggers.append(f\u0026#34;\u0026#34;\u0026#34; CREATE OR REPLACE FUNCTION {table_name}_audit_trigger() RETURNS TRIGGER AS $$ BEGIN IF TG_OP = \u0026#39;DELETE\u0026#39; THEN INSERT INTO {audit_table} (operation, old_values) VALUES (\u0026#39;D\u0026#39;, row_to_json(OLD)); RETURN OLD; ELSIF TG_OP = \u0026#39;UPDATE\u0026#39; THEN INSERT INTO {audit_table} (operation, old_values, new_values) VALUES (\u0026#39;U\u0026#39;, row_to_json(OLD), row_to_json(NEW)); RETURN NEW; ELSIF TG_OP = \u0026#39;INSERT\u0026#39; THEN INSERT INTO {audit_table} (operation, new_values) VALUES (\u0026#39;I\u0026#39;, row_to_json(NEW)); RETURN NEW; END IF; RETURN NULL; END; $$ LANGUAGE plpgsql; \u0026#34;\u0026#34;\u0026#34;) # Create triggers for operation in [\u0026#39;INSERT\u0026#39;, \u0026#39;UPDATE\u0026#39;, \u0026#39;DELETE\u0026#39;]: triggers.append(f\u0026#34;\u0026#34;\u0026#34; CREATE TRIGGER {table_name}_{operation.lower()}_audit AFTER {operation} ON {table_name} FOR EACH ROW EXECUTE FUNCTION {table_name}_audit_trigger(); \u0026#34;\u0026#34;\u0026#34;) elif db_type == \u0026#39;mysql\u0026#39;: # MySQL trigger implementation audit_table = f\u0026#34;{table_name}_audit\u0026#34; triggers.append(f\u0026#34;\u0026#34;\u0026#34; CREATE TABLE IF NOT EXISTS {audit_table} ( audit_id INT AUTO_INCREMENT PRIMARY KEY, operation CHAR(1) NOT NULL, stamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP, userid VARCHAR(255) DEFAULT USER(), old_values JSON, new_values JSON ); \u0026#34;\u0026#34;\u0026#34;) # Create triggers for each operation triggers.append(f\u0026#34;\u0026#34;\u0026#34; CREATE TRIGGER {table_name}_insert_audit AFTER INSERT ON {table_name} FOR EACH ROW INSERT INTO {audit_table} (operation, new_values) VALUES (\u0026#39;I\u0026#39;, JSON_OBJECT({self._get_column_list(table_name, \u0026#39;NEW\u0026#39;)})); \u0026#34;\u0026#34;\u0026#34;) return triggers def analyze_audit_logs(self, start_date: datetime, end_date: datetime) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Analyze audit logs for security insights\u0026#34;\u0026#34;\u0026#34; try: analysis = { \u0026#39;total_events\u0026#39;: 0, \u0026#39;events_by_type\u0026#39;: {}, \u0026#39;events_by_user\u0026#39;: {}, \u0026#39;failed_attempts\u0026#39;: 0, \u0026#39;suspicious_activities\u0026#39;: [], \u0026#39;top_queries\u0026#39;: [], \u0026#39;peak_hours\u0026#39;: {} } # Read and analyze audit logs # Implementation depends on storage type return analysis except Exception as e: self.logger.error(f\u0026#34;Audit log analysis failed: {e}\u0026#34;) return {} ### 2. Real-time Security Monitoring ```bash #!/bin/bash # Database Security Monitoring Script # Monitors database security events and generates alerts # Configuration DB_HOST=\u0026#34;localhost\u0026#34; DB_PORT=\u0026#34;3306\u0026#34; DB_USER=\u0026#34;monitor_user\u0026#34; DB_PASS=\u0026#34;monitor_password\u0026#34; ALERT_EMAIL=\u0026#34;security@company.com\u0026#34; LOG_FILE=\u0026#34;/var/log/db_security_monitor.log\u0026#34; THRESHOLD_FAILED_LOGINS=5 THRESHOLD_QUERY_TIME=30 # Logging function log_message() { echo \u0026#34;[$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)] $1\u0026#34; | tee -a \u0026#34;$LOG_FILE\u0026#34; } # Check for failed login attempts check_failed_logins() { log_message \u0026#34;Checking for failed login attempts...\u0026#34; failed_logins=$(mysql -h \u0026#34;$DB_HOST\u0026#34; -P \u0026#34;$DB_PORT\u0026#34; -u \u0026#34;$DB_USER\u0026#34; -p\u0026#34;$DB_PASS\u0026#34; \\ -e \u0026#34;SELECT COUNT(*) FROM mysql.general_log WHERE command_type = \u0026#39;Connect\u0026#39; AND argument LIKE \u0026#39;%Access denied%\u0026#39; AND event_time \u0026gt; DATE_SUB(NOW(), INTERVAL 1 HOUR);\u0026#34; \\ -s -N 2\u0026gt;/dev/null) if [ \u0026#34;$failed_logins\u0026#34; -gt \u0026#34;$THRESHOLD_FAILED_LOGINS\u0026#34; ]; then alert_message=\u0026#34;SECURITY ALERT: $failed_logins failed login attempts in the last hour\u0026#34; log_message \u0026#34;$alert_message\u0026#34; send_alert \u0026#34;$alert_message\u0026#34; fi } # Check for suspicious queries check_suspicious_queries() { log_message \u0026#34;Checking for suspicious queries...\u0026#34; # Check for potential SQL injection patterns suspicious_patterns=( \u0026#34;UNION.*SELECT\u0026#34; \u0026#34;OR.*1=1\u0026#34; \u0026#34;DROP.*TABLE\u0026#34; \u0026#34;DELETE.*FROM.*WHERE.*1=1\u0026#34; \u0026#34;INSERT.*INTO.*VALUES.*SELECT\u0026#34; ) for pattern in \u0026#34;${suspicious_patterns[@]}\u0026#34;; do count=$(mysql -h \u0026#34;$DB_HOST\u0026#34; -P \u0026#34;$DB_PORT\u0026#34; -u \u0026#34;$DB_USER\u0026#34; -p\u0026#34;$DB_PASS\u0026#34; \\ -e \u0026#34;SELECT COUNT(*) FROM mysql.general_log WHERE argument REGEXP \u0026#39;$pattern\u0026#39; AND event_time \u0026gt; DATE_SUB(NOW(), INTERVAL 1 HOUR);\u0026#34; \\ -s -N 2\u0026gt;/dev/null) if [ \u0026#34;$count\u0026#34; -gt 0 ]; then alert_message=\u0026#34;SECURITY ALERT: $count suspicious queries detected with pattern: $pattern\u0026#34; log_message \u0026#34;$alert_message\u0026#34; send_alert \u0026#34;$alert_message\u0026#34; fi done } # Check for long-running queries check_long_queries() { log_message \u0026#34;Checking for long-running queries...\u0026#34; long_queries=$(mysql -h \u0026#34;$DB_HOST\u0026#34; -P \u0026#34;$DB_PORT\u0026#34; -u \u0026#34;$DB_USER\u0026#34; -p\u0026#34;$DB_PASS\u0026#34; \\ -e \u0026#34;SELECT COUNT(*) FROM information_schema.processlist WHERE time \u0026gt; $THRESHOLD_QUERY_TIME AND command != \u0026#39;Sleep\u0026#39;;\u0026#34; \\ -s -N 2\u0026gt;/dev/null) if [ \u0026#34;$long_queries\u0026#34; -gt 0 ]; then alert_message=\u0026#34;PERFORMANCE ALERT: $long_queries queries running longer than $THRESHOLD_QUERY_TIME seconds\u0026#34; log_message \u0026#34;$alert_message\u0026#34; send_alert \u0026#34;$alert_message\u0026#34; fi } # Check database connections check_connections() { log_message \u0026#34;Checking database connections...\u0026#34; max_connections=$(mysql -h \u0026#34;$DB_HOST\u0026#34; -P \u0026#34;$DB_PORT\u0026#34; -u \u0026#34;$DB_USER\u0026#34; -p\u0026#34;$DB_PASS\u0026#34; \\ -e \u0026#34;SHOW VARIABLES LIKE \u0026#39;max_connections\u0026#39;;\u0026#34; -s -N | cut -f2) current_connections=$(mysql -h \u0026#34;$DB_HOST\u0026#34; -P \u0026#34;$DB_PORT\u0026#34; -u \u0026#34;$DB_USER\u0026#34; -p\u0026#34;$DB_PASS\u0026#34; \\ -e \u0026#34;SHOW STATUS LIKE \u0026#39;Threads_connected\u0026#39;;\u0026#34; -s -N | cut -f2) connection_percentage=$((current_connections * 100 / max_connections)) if [ \u0026#34;$connection_percentage\u0026#34; -gt 80 ]; then alert_message=\u0026#34;CONNECTION ALERT: Database connections at $connection_percentage% capacity ($current_connections/$max_connections)\u0026#34; log_message \u0026#34;$alert_message\u0026#34; send_alert \u0026#34;$alert_message\u0026#34; fi } # Check for privilege escalation attempts check_privilege_escalation() { log_message \u0026#34;Checking for privilege escalation attempts...\u0026#34; privilege_changes=$(mysql -h \u0026#34;$DB_HOST\u0026#34; -P \u0026#34;$DB_PORT\u0026#34; -u \u0026#34;$DB_USER\u0026#34; -p\u0026#34;$DB_PASS\u0026#34; \\ -e \u0026#34;SELECT COUNT(*) FROM mysql.general_log WHERE argument REGEXP \u0026#39;GRANT|REVOKE|CREATE USER|DROP USER\u0026#39; AND event_time \u0026gt; DATE_SUB(NOW(), INTERVAL 1 HOUR);\u0026#34; \\ -s -N 2\u0026gt;/dev/null) if [ \u0026#34;$privilege_changes\u0026#34; -gt 0 ]; then alert_message=\u0026#34;SECURITY ALERT: $privilege_changes privilege changes detected in the last hour\u0026#34; log_message \u0026#34;$alert_message\u0026#34; send_alert \u0026#34;$alert_message\u0026#34; fi } # Send alert notification send_alert() { local message=\u0026#34;$1\u0026#34; # Send email alert echo \u0026#34;$message\u0026#34; | mail -s \u0026#34;Database Security Alert - $(hostname)\u0026#34; \u0026#34;$ALERT_EMAIL\u0026#34; # Send to syslog logger -p local0.alert \u0026#34;$message\u0026#34; # Send to monitoring system (example: Prometheus Alertmanager) curl -X POST http://alertmanager:9093/api/v1/alerts \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#34;[{ \\\u0026#34;labels\\\u0026#34;: { \\\u0026#34;alertname\\\u0026#34;: \\\u0026#34;DatabaseSecurityAlert\\\u0026#34;, \\\u0026#34;severity\\\u0026#34;: \\\u0026#34;critical\\\u0026#34;, \\\u0026#34;instance\\\u0026#34;: \\\u0026#34;$(hostname)\\\u0026#34; }, \\\u0026#34;annotations\\\u0026#34;: { \\\u0026#34;summary\\\u0026#34;: \\\u0026#34;$message\\\u0026#34; } }]\u0026#34; 2\u0026gt;/dev/null } # Generate security report generate_security_report() { log_message \u0026#34;Generating security report...\u0026#34; report_file=\u0026#34;/tmp/db_security_report_$(date +%Y%m%d_%H%M%S).txt\u0026#34; cat \u0026gt; \u0026#34;$report_file\u0026#34; \u0026lt;\u0026lt; EOF Database Security Report - $(date) ===================================== System Information: - Database Host: $DB_HOST:$DB_PORT - Report Generated: $(date) Security Metrics: EOF # Add failed login statistics echo \u0026#34;- Failed Logins (last 24h): $(mysql -h \u0026#34;$DB_HOST\u0026#34; -P \u0026#34;$DB_PORT\u0026#34; -u \u0026#34;$DB_USER\u0026#34; -p\u0026#34;$DB_PASS\u0026#34; \\ -e \u0026#34;SELECT COUNT(*) FROM mysql.general_log WHERE command_type = \u0026#39;Connect\u0026#39; AND argument LIKE \u0026#39;%Access denied%\u0026#39; AND event_time \u0026gt; DATE_SUB(NOW(), INTERVAL 24 HOUR);\u0026#34; \\ -s -N 2\u0026gt;/dev/null)\u0026#34; \u0026gt;\u0026gt; \u0026#34;$report_file\u0026#34; # Add connection statistics echo \u0026#34;- Current Connections: $(mysql -h \u0026#34;$DB_HOST\u0026#34; -P \u0026#34;$DB_PORT\u0026#34; -u \u0026#34;$DB_USER\u0026#34; -p\u0026#34;$DB_PASS\u0026#34; \\ -e \u0026#34;SHOW STATUS LIKE \u0026#39;Threads_connected\u0026#39;;\u0026#34; -s -N | cut -f2)\u0026#34; \u0026gt;\u0026gt; \u0026#34;$report_file\u0026#34; # Add query statistics echo \u0026#34;- Queries (last 24h): $(mysql -h \u0026#34;$DB_HOST\u0026#34; -P \u0026#34;$DB_PORT\u0026#34; -u \u0026#34;$DB_USER\u0026#34; -p\u0026#34;$DB_PASS\u0026#34; \\ -e \u0026#34;SELECT COUNT(*) FROM mysql.general_log WHERE event_time \u0026gt; DATE_SUB(NOW(), INTERVAL 24 HOUR);\u0026#34; \\ -s -N 2\u0026gt;/dev/null)\u0026#34; \u0026gt;\u0026gt; \u0026#34;$report_file\u0026#34; log_message \u0026#34;Security report generated: $report_file\u0026#34; # Email the report mail -s \u0026#34;Daily Database Security Report - $(hostname)\u0026#34; \u0026#34;$ALERT_EMAIL\u0026#34; \u0026lt; \u0026#34;$report_file\u0026#34; } # Main monitoring loop main() { log_message \u0026#34;Starting database security monitoring...\u0026#34; while true; do check_failed_logins check_suspicious_queries check_long_queries check_connections check_privilege_escalation # Generate daily report at midnight if [ \u0026#34;$(date +%H%M)\u0026#34; = \u0026#34;0000\u0026#34; ]; then generate_security_report fi # Wait 5 minutes before next check sleep 300 done } # Run monitoring if script is executed directly if [ \u0026#34;${BASH_SOURCE[0]}\u0026#34; = \u0026#34;${0}\u0026#34; ]; then main \u0026#34;$@\u0026#34; fi Compliance and Governance 1. Compliance Framework Implementation from typing import Dict, List, Any, Optional from dataclasses import dataclass from enum import Enum import json import re from datetime import datetime, timedelta class ComplianceStandard(Enum): GDPR = \u0026#34;gdpr\u0026#34; HIPAA = \u0026#34;hipaa\u0026#34; PCI_DSS = \u0026#34;pci_dss\u0026#34; SOX = \u0026#34;sox\u0026#34; ISO27001 = \u0026#34;iso27001\u0026#34; @dataclass class ComplianceRule: rule_id: str standard: ComplianceStandard category: str description: str severity: str check_function: str remediation: str class DatabaseComplianceManager: \u0026#34;\u0026#34;\u0026#34;Database compliance management and validation\u0026#34;\u0026#34;\u0026#34; def __init__(self, config: Dict[str, Any]): self.config = config self.logger = logging.getLogger(__name__) self.compliance_rules = self._load_compliance_rules() self.scan_results = {} def _load_compliance_rules(self) -\u0026gt; List[ComplianceRule]: \u0026#34;\u0026#34;\u0026#34;Load compliance rules for different standards\u0026#34;\u0026#34;\u0026#34; rules = [] # GDPR Rules rules.extend([ ComplianceRule( rule_id=\u0026#34;GDPR-001\u0026#34;, standard=ComplianceStandard.GDPR, category=\u0026#34;Data Protection\u0026#34;, description=\u0026#34;Personal data must be encrypted at rest\u0026#34;, severity=\u0026#34;high\u0026#34;, check_function=\u0026#34;check_encryption_at_rest\u0026#34;, remediation=\u0026#34;Enable transparent data encryption (TDE) or column-level encryption\u0026#34; ), ComplianceRule( rule_id=\u0026#34;GDPR-002\u0026#34;, standard=ComplianceStandard.GDPR, category=\u0026#34;Access Control\u0026#34;, description=\u0026#34;Access to personal data must be logged and auditable\u0026#34;, severity=\u0026#34;high\u0026#34;, check_function=\u0026#34;check_audit_logging\u0026#34;, remediation=\u0026#34;Enable comprehensive audit logging for all data access\u0026#34; ), ComplianceRule( rule_id=\u0026#34;GDPR-003\u0026#34;, standard=ComplianceStandard.GDPR, category=\u0026#34;Data Retention\u0026#34;, description=\u0026#34;Personal data retention policies must be enforced\u0026#34;, severity=\u0026#34;medium\u0026#34;, check_function=\u0026#34;check_data_retention_policies\u0026#34;, remediation=\u0026#34;Implement automated data retention and deletion policies\u0026#34; ) ]) # HIPAA Rules rules.extend([ ComplianceRule( rule_id=\u0026#34;HIPAA-001\u0026#34;, standard=ComplianceStandard.HIPAA, category=\u0026#34;Access Control\u0026#34;, description=\u0026#34;Unique user identification required\u0026#34;, severity=\u0026#34;high\u0026#34;, check_function=\u0026#34;check_unique_user_identification\u0026#34;, remediation=\u0026#34;Ensure each user has a unique identifier and disable shared accounts\u0026#34; ), ComplianceRule( rule_id=\u0026#34;HIPAA-002\u0026#34;, standard=ComplianceStandard.HIPAA, category=\u0026#34;Audit Controls\u0026#34;, description=\u0026#34;Audit logs must capture access to PHI\u0026#34;, severity=\u0026#34;high\u0026#34;, check_function=\u0026#34;check_phi_audit_logging\u0026#34;, remediation=\u0026#34;Enable detailed audit logging for all PHI access\u0026#34; ) ]) # PCI DSS Rules rules.extend([ ComplianceRule( rule_id=\u0026#34;PCI-001\u0026#34;, standard=ComplianceStandard.PCI_DSS, category=\u0026#34;Data Protection\u0026#34;, description=\u0026#34;Cardholder data must be encrypted\u0026#34;, severity=\u0026#34;critical\u0026#34;, check_function=\u0026#34;check_cardholder_data_encryption\u0026#34;, remediation=\u0026#34;Encrypt all stored cardholder data using strong cryptography\u0026#34; ), ComplianceRule( rule_id=\u0026#34;PCI-002\u0026#34;, standard=ComplianceStandard.PCI_DSS, category=\u0026#34;Access Control\u0026#34;, description=\u0026#34;Restrict access to cardholder data by business need-to-know\u0026#34;, severity=\u0026#34;high\u0026#34;, check_function=\u0026#34;check_cardholder_data_access\u0026#34;, remediation=\u0026#34;Implement role-based access control for cardholder data\u0026#34; ) ]) return rules def run_compliance_scan(self, standards: List[ComplianceStandard] = None) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Run comprehensive compliance scan\u0026#34;\u0026#34;\u0026#34; try: if standards is None: standards = [standard for standard in ComplianceStandard] scan_results = { \u0026#39;scan_id\u0026#39;: f\u0026#34;scan_{datetime.now().strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;)}\u0026#34;, \u0026#39;timestamp\u0026#39;: datetime.now().isoformat(), \u0026#39;standards_checked\u0026#39;: [s.value for s in standards], \u0026#39;results\u0026#39;: {}, \u0026#39;summary\u0026#39;: { \u0026#39;total_rules\u0026#39;: 0, \u0026#39;passed\u0026#39;: 0, \u0026#39;failed\u0026#39;: 0, \u0026#39;warnings\u0026#39;: 0, \u0026#39;compliance_score\u0026#39;: 0 } } for standard in standards: standard_results = self._scan_standard(standard) scan_results[\u0026#39;results\u0026#39;][standard.value] = standard_results # Update summary scan_results[\u0026#39;summary\u0026#39;][\u0026#39;total_rules\u0026#39;] += len(standard_results) scan_results[\u0026#39;summary\u0026#39;][\u0026#39;passed\u0026#39;] += sum(1 for r in standard_results if r[\u0026#39;status\u0026#39;] == \u0026#39;passed\u0026#39;) scan_results[\u0026#39;summary\u0026#39;][\u0026#39;failed\u0026#39;] += sum(1 for r in standard_results if r[\u0026#39;status\u0026#39;] == \u0026#39;failed\u0026#39;) scan_results[\u0026#39;summary\u0026#39;][\u0026#39;warnings\u0026#39;] += sum(1 for r in standard_results if r[\u0026#39;status\u0026#39;] == \u0026#39;warning\u0026#39;) # Calculate compliance score total_rules = scan_results[\u0026#39;summary\u0026#39;][\u0026#39;total_rules\u0026#39;] if total_rules \u0026gt; 0: scan_results[\u0026#39;summary\u0026#39;][\u0026#39;compliance_score\u0026#39;] = ( scan_results[\u0026#39;summary\u0026#39;][\u0026#39;passed\u0026#39;] / total_rules * 100 ) self.scan_results = scan_results return scan_results except Exception as e: self.logger.error(f\u0026#34;Compliance scan failed: {e}\u0026#34;) return {} def _scan_standard(self, standard: ComplianceStandard) -\u0026gt; List[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;Scan for specific compliance standard\u0026#34;\u0026#34;\u0026#34; results = [] for rule in self.compliance_rules: if rule.standard == standard: try: # Execute check function check_method = getattr(self, rule.check_function, None) if check_method: check_result = check_method() status = \u0026#39;passed\u0026#39; if check_result else \u0026#39;failed\u0026#39; else: status = \u0026#39;warning\u0026#39; check_result = False results.append({ \u0026#39;rule_id\u0026#39;: rule.rule_id, \u0026#39;description\u0026#39;: rule.description, \u0026#39;category\u0026#39;: rule.category, \u0026#39;severity\u0026#39;: rule.severity, \u0026#39;status\u0026#39;: status, \u0026#39;details\u0026#39;: check_result, \u0026#39;remediation\u0026#39;: rule.remediation if status == \u0026#39;failed\u0026#39; else None }) except Exception as e: self.logger.error(f\u0026#34;Check failed for rule {rule.rule_id}: {e}\u0026#34;) results.append({ \u0026#39;rule_id\u0026#39;: rule.rule_id, \u0026#39;description\u0026#39;: rule.description, \u0026#39;category\u0026#39;: rule.category, \u0026#39;severity\u0026#39;: rule.severity, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;remediation\u0026#39;: rule.remediation }) return results # Compliance check functions def check_encryption_at_rest(self) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Check if data encryption at rest is enabled\u0026#34;\u0026#34;\u0026#34; # Implementation depends on database type return True # Placeholder def check_audit_logging(self) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Check if comprehensive audit logging is enabled\u0026#34;\u0026#34;\u0026#34; # Implementation depends on database type return True # Placeholder def check_data_retention_policies(self) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Check if data retention policies are implemented\u0026#34;\u0026#34;\u0026#34; # Check for automated data retention mechanisms return False # Placeholder def check_unique_user_identification(self) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Check for unique user identification\u0026#34;\u0026#34;\u0026#34; # Verify no shared accounts exist return True # Placeholder def check_phi_audit_logging(self) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Check PHI access audit logging\u0026#34;\u0026#34;\u0026#34; # Verify PHI access is logged return True # Placeholder def check_cardholder_data_encryption(self) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Check cardholder data encryption\u0026#34;\u0026#34;\u0026#34; # Verify cardholder data is encrypted return False # Placeholder def check_cardholder_data_access(self) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;Check cardholder data access controls\u0026#34;\u0026#34;\u0026#34; # Verify access controls for cardholder data return True # Placeholder def generate_compliance_report(self, output_format: str = \u0026#39;json\u0026#39;) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Generate compliance report\u0026#34;\u0026#34;\u0026#34; try: if not self.scan_results: raise ValueError(\u0026#34;No scan results available. Run compliance scan first.\u0026#34;) if output_format == \u0026#39;json\u0026#39;: return json.dumps(self.scan_results, indent=2) elif output_format == \u0026#39;html\u0026#39;: return self._generate_html_report() elif output_format == \u0026#39;csv\u0026#39;: return self._generate_csv_report() else: raise ValueError(f\u0026#34;Unsupported output format: {output_format}\u0026#34;) except Exception as e: self.logger.error(f\u0026#34;Report generation failed: {e}\u0026#34;) return \u0026#34;\u0026#34; def _generate_html_report(self) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;Generate HTML compliance report\u0026#34;\u0026#34;\u0026#34; html_template = \u0026#34;\u0026#34;\u0026#34; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Database Compliance Report\u0026lt;/title\u0026gt; \u0026lt;style\u0026gt; body { font-family: Arial, sans-serif; margin: 20px; } .header { background-color: #f0f0f0; padding: 20px; border-radius: 5px; } .summary { margin: 20px 0; } .standard { margin: 20px 0; border: 1px solid #ddd; border-radius: 5px; } .standard-header { background-color: #e0e0e0; padding: 10px; font-weight: bold; } .rule { padding: 10px; border-bottom: 1px solid #eee; } .passed { color: green; } .failed { color: red; } .warning { color: orange; } .error { color: purple; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;header\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;Database Compliance Report\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Generated: {timestamp}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Scan ID: {scan_id}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;summary\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;Summary\u0026lt;/h2\u0026gt; \u0026lt;p\u0026gt;Compliance Score: {compliance_score:.1f}%\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Total Rules: {total_rules}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;Passed: {passed} | Failed: {failed} | Warnings: {warnings}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; {standards_content} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34; # Generate content for each standard standards_content = \u0026#34;\u0026#34; for standard, results in self.scan_results[\u0026#39;results\u0026#39;].items(): standards_content += f\u0026#39;\u0026lt;div class=\u0026#34;standard\u0026#34;\u0026gt;\u0026#39; standards_content += f\u0026#39;\u0026lt;div class=\u0026#34;standard-header\u0026#34;\u0026gt;{standard.upper()}\u0026lt;/div\u0026gt;\u0026#39; for rule in results: status_class = rule[\u0026#39;status\u0026#39;] standards_content += f\u0026#39;\u0026#39;\u0026#39; \u0026lt;div class=\u0026#34;rule\u0026#34;\u0026gt; \u0026lt;strong class=\u0026#34;{status_class}\u0026#34;\u0026gt;[{rule[\u0026#39;status\u0026#39;].upper()}]\u0026lt;/strong\u0026gt; {rule[\u0026#39;rule_id\u0026#39;]}: {rule[\u0026#39;description\u0026#39;]} {f\u0026#34;\u0026lt;br\u0026gt;\u0026lt;em\u0026gt;Remediation: {rule[\u0026#39;remediation\u0026#39;]}\u0026lt;/em\u0026gt;\u0026#34; if rule.get(\u0026#39;remediation\u0026#39;) else \u0026#34;\u0026#34;} \u0026lt;/div\u0026gt; \u0026#39;\u0026#39;\u0026#39; standards_content += \u0026#39;\u0026lt;/div\u0026gt;\u0026#39; return html_template.format( timestamp=self.scan_results[\u0026#39;timestamp\u0026#39;], scan_id=self.scan_results[\u0026#39;scan_id\u0026#39;], compliance_score=self.scan_results[\u0026#39;summary\u0026#39;][\u0026#39;compliance_score\u0026#39;], total_rules=self.scan_results[\u0026#39;summary\u0026#39;][\u0026#39;total_rules\u0026#39;], passed=self.scan_results[\u0026#39;summary\u0026#39;][\u0026#39;passed\u0026#39;], failed=self.scan_results[\u0026#39;summary\u0026#39;][\u0026#39;failed\u0026#39;], warnings=self.scan_results[\u0026#39;summary\u0026#39;][\u0026#39;warnings\u0026#39;], standards_content=standards_content ) ## Security Testing and Validation ### 1. Automated Security Testing ```python import subprocess import socket import ssl import requests from typing import Dict, List, Any, Tuple import time import random import string class DatabaseSecurityTester: \u0026#34;\u0026#34;\u0026#34;Automated database security testing framework\u0026#34;\u0026#34;\u0026#34; def __init__(self, config: Dict[str, Any]): self.config = config self.logger = logging.getLogger(__name__) self.test_results = {} def run_security_tests(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Run comprehensive security test suite\u0026#34;\u0026#34;\u0026#34; try: test_results = { \u0026#39;test_id\u0026#39;: f\u0026#34;test_{datetime.now().strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;)}\u0026#34;, \u0026#39;timestamp\u0026#39;: datetime.now().isoformat(), \u0026#39;tests\u0026#39;: {}, \u0026#39;summary\u0026#39;: { \u0026#39;total_tests\u0026#39;: 0, \u0026#39;passed\u0026#39;: 0, \u0026#39;failed\u0026#39;: 0, \u0026#39;security_score\u0026#39;: 0 } } # Run different test categories test_categories = [ (\u0026#39;authentication\u0026#39;, self.test_authentication_security), (\u0026#39;authorization\u0026#39;, self.test_authorization_controls), (\u0026#39;encryption\u0026#39;, self.test_encryption_implementation), (\u0026#39;injection\u0026#39;, self.test_sql_injection_protection), (\u0026#39;network\u0026#39;, self.test_network_security), (\u0026#39;configuration\u0026#39;, self.test_security_configuration) ] for category, test_function in test_categories: category_results = test_function() test_results[\u0026#39;tests\u0026#39;][category] = category_results # Update summary test_results[\u0026#39;summary\u0026#39;][\u0026#39;total_tests\u0026#39;] += len(category_results) test_results[\u0026#39;summary\u0026#39;][\u0026#39;passed\u0026#39;] += sum(1 for r in category_results if r[\u0026#39;status\u0026#39;] == \u0026#39;passed\u0026#39;) test_results[\u0026#39;summary\u0026#39;][\u0026#39;failed\u0026#39;] += sum(1 for r in category_results if r[\u0026#39;status\u0026#39;] == \u0026#39;failed\u0026#39;) # Calculate security score total_tests = test_results[\u0026#39;summary\u0026#39;][\u0026#39;total_tests\u0026#39;] if total_tests \u0026gt; 0: test_results[\u0026#39;summary\u0026#39;][\u0026#39;security_score\u0026#39;] = ( test_results[\u0026#39;summary\u0026#39;][\u0026#39;passed\u0026#39;] / total_tests * 100 ) self.test_results = test_results return test_results except Exception as e: self.logger.error(f\u0026#34;Security testing failed: {e}\u0026#34;) return {} def test_authentication_security(self) -\u0026gt; List[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;Test authentication security controls\u0026#34;\u0026#34;\u0026#34; tests = [] # Test 1: Weak password policy tests.append(self._test_weak_passwords()) # Test 2: Account lockout policy tests.append(self._test_account_lockout()) # Test 3: Multi-factor authentication tests.append(self._test_mfa_enforcement()) # Test 4: Session management tests.append(self._test_session_security()) return tests def test_authorization_controls(self) -\u0026gt; List[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;Test authorization and access controls\u0026#34;\u0026#34;\u0026#34; tests = [] # Test 1: Privilege escalation tests.append(self._test_privilege_escalation()) # Test 2: Role-based access control tests.append(self._test_rbac_implementation()) # Test 3: Data access controls tests.append(self._test_data_access_controls()) return tests def test_encryption_implementation(self) -\u0026gt; List[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;Test encryption implementation\u0026#34;\u0026#34;\u0026#34; tests = [] # Test 1: Data at rest encryption tests.append(self._test_encryption_at_rest()) # Test 2: Data in transit encryption tests.append(self._test_encryption_in_transit()) # Test 3: Key management tests.append(self._test_key_management()) return tests def test_sql_injection_protection(self) -\u0026gt; List[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;Test SQL injection protection\u0026#34;\u0026#34;\u0026#34; tests = [] # Test various SQL injection patterns injection_patterns = [ \u0026#34;\u0026#39; OR \u0026#39;1\u0026#39;=\u0026#39;1\u0026#34;, \u0026#34;\u0026#39;; DROP TABLE users; --\u0026#34;, \u0026#34;\u0026#39; UNION SELECT * FROM information_schema.tables --\u0026#34;, \u0026#34;\u0026#39; AND 1=1 --\u0026#34;, \u0026#34;admin\u0026#39;--\u0026#34;, \u0026#34;\u0026#39; OR 1=1#\u0026#34; ] for pattern in injection_patterns: tests.append(self._test_sql_injection_pattern(pattern)) return tests def test_network_security(self) -\u0026gt; List[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;Test network security controls\u0026#34;\u0026#34;\u0026#34; tests = [] # Test 1: SSL/TLS configuration tests.append(self._test_ssl_configuration()) # Test 2: Port security tests.append(self._test_port_security()) # Test 3: Firewall rules tests.append(self._test_firewall_rules()) return tests def test_security_configuration(self) -\u0026gt; List[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;Test security configuration\u0026#34;\u0026#34;\u0026#34; tests = [] # Test 1: Default accounts tests.append(self._test_default_accounts()) # Test 2: Security parameters tests.append(self._test_security_parameters()) # Test 3: Audit configuration tests.append(self._test_audit_configuration()) return tests def _test_weak_passwords(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Test for weak password policies\u0026#34;\u0026#34;\u0026#34; try: # Attempt to create user with weak password weak_passwords = [\u0026#39;123456\u0026#39;, \u0026#39;password\u0026#39;, \u0026#39;admin\u0026#39;, \u0026#39;test\u0026#39;] for password in weak_passwords: # Test password creation (implementation depends on database) # This is a simulation if len(password) \u0026lt; 8: return { \u0026#39;test_name\u0026#39;: \u0026#39;Weak Password Policy\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;failed\u0026#39;, \u0026#39;details\u0026#39;: f\u0026#39;Weak password \u0026#34;{password}\u0026#34; was accepted\u0026#39;, \u0026#39;recommendation\u0026#39;: \u0026#39;Implement strong password policy with minimum length, complexity requirements\u0026#39; } return { \u0026#39;test_name\u0026#39;: \u0026#39;Weak Password Policy\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;passed\u0026#39;, \u0026#39;details\u0026#39;: \u0026#39;Strong password policy is enforced\u0026#39;, \u0026#39;recommendation\u0026#39;: None } except Exception as e: return { \u0026#39;test_name\u0026#39;: \u0026#39;Weak Password Policy\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;recommendation\u0026#39;: \u0026#39;Review password policy configuration\u0026#39; } def _test_account_lockout(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Test account lockout policy\u0026#34;\u0026#34;\u0026#34; try: # Simulate multiple failed login attempts # Implementation depends on database and authentication system return { \u0026#39;test_name\u0026#39;: \u0026#39;Account Lockout Policy\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;passed\u0026#39;, \u0026#39;details\u0026#39;: \u0026#39;Account lockout policy is properly configured\u0026#39;, \u0026#39;recommendation\u0026#39;: None } except Exception as e: return { \u0026#39;test_name\u0026#39;: \u0026#39;Account Lockout Policy\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;recommendation\u0026#39;: \u0026#39;Configure account lockout after failed attempts\u0026#39; } def _test_mfa_enforcement(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Test multi-factor authentication enforcement\u0026#34;\u0026#34;\u0026#34; try: # Check if MFA is enforced for privileged accounts # Implementation depends on authentication system return { \u0026#39;test_name\u0026#39;: \u0026#39;Multi-Factor Authentication\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;warning\u0026#39;, \u0026#39;details\u0026#39;: \u0026#39;MFA enforcement could not be verified\u0026#39;, \u0026#39;recommendation\u0026#39;: \u0026#39;Implement MFA for all privileged accounts\u0026#39; } except Exception as e: return { \u0026#39;test_name\u0026#39;: \u0026#39;Multi-Factor Authentication\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;recommendation\u0026#39;: \u0026#39;Review MFA configuration\u0026#39; } def _test_session_security(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Test session security controls\u0026#34;\u0026#34;\u0026#34; try: # Test session timeout, secure cookies, etc. # Implementation depends on application framework return { \u0026#39;test_name\u0026#39;: \u0026#39;Session Security\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;passed\u0026#39;, \u0026#39;details\u0026#39;: \u0026#39;Session security controls are properly implemented\u0026#39;, \u0026#39;recommendation\u0026#39;: None } except Exception as e: return { \u0026#39;test_name\u0026#39;: \u0026#39;Session Security\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;recommendation\u0026#39;: \u0026#39;Review session management configuration\u0026#39; } def _test_privilege_escalation(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Test for privilege escalation vulnerabilities\u0026#34;\u0026#34;\u0026#34; try: # Test various privilege escalation scenarios # Implementation depends on database type and configuration return { \u0026#39;test_name\u0026#39;: \u0026#39;Privilege Escalation\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;passed\u0026#39;, \u0026#39;details\u0026#39;: \u0026#39;No privilege escalation vulnerabilities detected\u0026#39;, \u0026#39;recommendation\u0026#39;: None } except Exception as e: return { \u0026#39;test_name\u0026#39;: \u0026#39;Privilege Escalation\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;recommendation\u0026#39;: \u0026#39;Review user privileges and access controls\u0026#39; } def _test_rbac_implementation(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Test role-based access control implementation\u0026#34;\u0026#34;\u0026#34; try: # Verify RBAC is properly implemented # Check role assignments, permissions, etc. return { \u0026#39;test_name\u0026#39;: \u0026#39;Role-Based Access Control\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;passed\u0026#39;, \u0026#39;details\u0026#39;: \u0026#39;RBAC is properly implemented\u0026#39;, \u0026#39;recommendation\u0026#39;: None } except Exception as e: return { \u0026#39;test_name\u0026#39;: \u0026#39;Role-Based Access Control\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;recommendation\u0026#39;: \u0026#39;Implement proper RBAC system\u0026#39; } def _test_data_access_controls(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Test data access controls\u0026#34;\u0026#34;\u0026#34; try: # Test row-level security, column-level security, etc. # Implementation depends on database features return { \u0026#39;test_name\u0026#39;: \u0026#39;Data Access Controls\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;warning\u0026#39;, \u0026#39;details\u0026#39;: \u0026#39;Data access controls need review\u0026#39;, \u0026#39;recommendation\u0026#39;: \u0026#39;Implement row-level and column-level security\u0026#39; } except Exception as e: return { \u0026#39;test_name\u0026#39;: \u0026#39;Data Access Controls\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;recommendation\u0026#39;: \u0026#39;Review data access control implementation\u0026#39; } def _test_encryption_at_rest(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Test encryption at rest\u0026#34;\u0026#34;\u0026#34; try: # Check if data is encrypted at rest # Implementation depends on database and storage configuration return { \u0026#39;test_name\u0026#39;: \u0026#39;Encryption at Rest\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;passed\u0026#39;, \u0026#39;details\u0026#39;: \u0026#39;Data encryption at rest is enabled\u0026#39;, \u0026#39;recommendation\u0026#39;: None } except Exception as e: return { \u0026#39;test_name\u0026#39;: \u0026#39;Encryption at Rest\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;recommendation\u0026#39;: \u0026#39;Enable transparent data encryption (TDE)\u0026#39; } def _test_encryption_in_transit(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Test encryption in transit\u0026#34;\u0026#34;\u0026#34; try: # Check SSL/TLS configuration host = self.config.get(\u0026#39;host\u0026#39;, \u0026#39;localhost\u0026#39;) port = self.config.get(\u0026#39;port\u0026#39;, 3306) # Test SSL connection context = ssl.create_default_context() with socket.create_connection((host, port), timeout=10) as sock: with context.wrap_socket(sock, server_hostname=host) as ssock: cipher = ssock.cipher() if cipher: return { \u0026#39;test_name\u0026#39;: \u0026#39;Encryption in Transit\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;passed\u0026#39;, \u0026#39;details\u0026#39;: f\u0026#39;SSL/TLS enabled with cipher: {cipher[0]}\u0026#39;, \u0026#39;recommendation\u0026#39;: None } return { \u0026#39;test_name\u0026#39;: \u0026#39;Encryption in Transit\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;failed\u0026#39;, \u0026#39;details\u0026#39;: \u0026#39;SSL/TLS not properly configured\u0026#39;, \u0026#39;recommendation\u0026#39;: \u0026#39;Enable SSL/TLS for all database connections\u0026#39; } except Exception as e: return { \u0026#39;test_name\u0026#39;: \u0026#39;Encryption in Transit\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;recommendation\u0026#39;: \u0026#39;Configure SSL/TLS for database connections\u0026#39; } def _test_key_management(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Test key management practices\u0026#34;\u0026#34;\u0026#34; try: # Test key rotation, storage, access controls # Implementation depends on key management system return { \u0026#39;test_name\u0026#39;: \u0026#39;Key Management\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;warning\u0026#39;, \u0026#39;details\u0026#39;: \u0026#39;Key management practices need review\u0026#39;, \u0026#39;recommendation\u0026#39;: \u0026#39;Implement proper key management with rotation and secure storage\u0026#39; } except Exception as e: return { \u0026#39;test_name\u0026#39;: \u0026#39;Key Management\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;recommendation\u0026#39;: \u0026#39;Review key management implementation\u0026#39; } def _test_sql_injection_pattern(self, pattern: str) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Test specific SQL injection pattern\u0026#34;\u0026#34;\u0026#34; try: # Test SQL injection pattern # This would typically involve sending the pattern to the application # and checking if it\u0026#39;s properly sanitized test_name = f\u0026#34;SQL Injection: {pattern[:20]}...\u0026#34; # Simulate testing (in real implementation, this would test actual endpoints) if \u0026#34;DROP\u0026#34; in pattern.upper(): return { \u0026#39;test_name\u0026#39;: test_name, \u0026#39;status\u0026#39;: \u0026#39;passed\u0026#39;, \u0026#39;details\u0026#39;: \u0026#39;SQL injection attempt was blocked\u0026#39;, \u0026#39;recommendation\u0026#39;: None } return { \u0026#39;test_name\u0026#39;: test_name, \u0026#39;status\u0026#39;: \u0026#39;passed\u0026#39;, \u0026#39;details\u0026#39;: \u0026#39;SQL injection protection is working\u0026#39;, \u0026#39;recommendation\u0026#39;: None } except Exception as e: return { \u0026#39;test_name\u0026#39;: f\u0026#34;SQL Injection: {pattern[:20]}...\u0026#34;, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;recommendation\u0026#39;: \u0026#39;Implement proper input validation and parameterized queries\u0026#39; } def _test_ssl_configuration(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Test SSL/TLS configuration\u0026#34;\u0026#34;\u0026#34; try: # Test SSL configuration strength host = self.config.get(\u0026#39;host\u0026#39;, \u0026#39;localhost\u0026#39;) port = self.config.get(\u0026#39;ssl_port\u0026#39;, 443) context = ssl.create_default_context() with socket.create_connection((host, port), timeout=10) as sock: with context.wrap_socket(sock, server_hostname=host) as ssock: protocol = ssock.version() cipher = ssock.cipher() # Check for strong protocols and ciphers if protocol in [\u0026#39;TLSv1.2\u0026#39;, \u0026#39;TLSv1.3\u0026#39;] and cipher: return { \u0026#39;test_name\u0026#39;: \u0026#39;SSL/TLS Configuration\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;passed\u0026#39;, \u0026#39;details\u0026#39;: f\u0026#39;Strong SSL/TLS configuration: {protocol}, {cipher[0]}\u0026#39;, \u0026#39;recommendation\u0026#39;: None } else: return { \u0026#39;test_name\u0026#39;: \u0026#39;SSL/TLS Configuration\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;failed\u0026#39;, \u0026#39;details\u0026#39;: f\u0026#39;Weak SSL/TLS configuration: {protocol}\u0026#39;, \u0026#39;recommendation\u0026#39;: \u0026#39;Upgrade to TLS 1.2 or higher with strong ciphers\u0026#39; } except Exception as e: return { \u0026#39;test_name\u0026#39;: \u0026#39;SSL/TLS Configuration\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;recommendation\u0026#39;: \u0026#39;Configure SSL/TLS properly\u0026#39; } def _test_port_security(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Test port security\u0026#34;\u0026#34;\u0026#34; try: # Test for unnecessary open ports host = self.config.get(\u0026#39;host\u0026#39;, \u0026#39;localhost\u0026#39;) common_ports = [22, 23, 80, 443, 3306, 5432, 1521, 1433] open_ports = [] for port in common_ports: sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM) sock.settimeout(1) result = sock.connect_ex((host, port)) if result == 0: open_ports.append(port) sock.close() # Check if only necessary ports are open necessary_ports = self.config.get(\u0026#39;necessary_ports\u0026#39;, []) unnecessary_ports = [p for p in open_ports if p not in necessary_ports] if unnecessary_ports: return { \u0026#39;test_name\u0026#39;: \u0026#39;Port Security\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;warning\u0026#39;, \u0026#39;details\u0026#39;: f\u0026#39;Unnecessary ports open: {unnecessary_ports}\u0026#39;, \u0026#39;recommendation\u0026#39;: \u0026#39;Close unnecessary ports and use firewall rules\u0026#39; } else: return { \u0026#39;test_name\u0026#39;: \u0026#39;Port Security\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;passed\u0026#39;, \u0026#39;details\u0026#39;: \u0026#39;Only necessary ports are open\u0026#39;, \u0026#39;recommendation\u0026#39;: None } except Exception as e: return { \u0026#39;test_name\u0026#39;: \u0026#39;Port Security\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;recommendation\u0026#39;: \u0026#39;Review port configuration and firewall rules\u0026#39; } def _test_firewall_rules(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Test firewall rules\u0026#34;\u0026#34;\u0026#34; try: # Test firewall configuration # This would typically involve checking iptables, ufw, or cloud security groups return { \u0026#39;test_name\u0026#39;: \u0026#39;Firewall Rules\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;warning\u0026#39;, \u0026#39;details\u0026#39;: \u0026#39;Firewall configuration needs manual review\u0026#39;, \u0026#39;recommendation\u0026#39;: \u0026#39;Ensure proper firewall rules are in place\u0026#39; } except Exception as e: return { \u0026#39;test_name\u0026#39;: \u0026#39;Firewall Rules\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;recommendation\u0026#39;: \u0026#39;Configure proper firewall rules\u0026#39; } def _test_default_accounts(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Test for default accounts\u0026#34;\u0026#34;\u0026#34; try: # Check for default database accounts default_accounts = [\u0026#39;root\u0026#39;, \u0026#39;admin\u0026#39;, \u0026#39;sa\u0026#39;, \u0026#39;postgres\u0026#39;, \u0026#39;mysql\u0026#39;] # This would typically query the database for user accounts # Implementation depends on database type return { \u0026#39;test_name\u0026#39;: \u0026#39;Default Accounts\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;passed\u0026#39;, \u0026#39;details\u0026#39;: \u0026#39;No default accounts with default passwords found\u0026#39;, \u0026#39;recommendation\u0026#39;: None } except Exception as e: return { \u0026#39;test_name\u0026#39;: \u0026#39;Default Accounts\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;recommendation\u0026#39;: \u0026#39;Remove or secure default accounts\u0026#39; } def _test_security_parameters(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Test security parameters\u0026#34;\u0026#34;\u0026#34; try: # Check database security parameters # Implementation depends on database type return { \u0026#39;test_name\u0026#39;: \u0026#39;Security Parameters\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;warning\u0026#39;, \u0026#39;details\u0026#39;: \u0026#39;Security parameters need review\u0026#39;, \u0026#39;recommendation\u0026#39;: \u0026#39;Review and harden database security parameters\u0026#39; } except Exception as e: return { \u0026#39;test_name\u0026#39;: \u0026#39;Security Parameters\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;recommendation\u0026#39;: \u0026#39;Configure security parameters properly\u0026#39; } def _test_audit_configuration(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Test audit configuration\u0026#34;\u0026#34;\u0026#34; try: # Check audit logging configuration # Implementation depends on database type return { \u0026#39;test_name\u0026#39;: \u0026#39;Audit Configuration\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;passed\u0026#39;, \u0026#39;details\u0026#39;: \u0026#39;Audit logging is properly configured\u0026#39;, \u0026#39;recommendation\u0026#39;: None } except Exception as e: return { \u0026#39;test_name\u0026#39;: \u0026#39;Audit Configuration\u0026#39;, \u0026#39;status\u0026#39;: \u0026#39;error\u0026#39;, \u0026#39;details\u0026#39;: str(e), \u0026#39;recommendation\u0026#39;: \u0026#39;Configure comprehensive audit logging\u0026#39; } # Example usage def main(): config = { \u0026#39;host\u0026#39;: \u0026#39;localhost\u0026#39;, \u0026#39;port\u0026#39;: 3306, \u0026#39;ssl_port\u0026#39;: 443, \u0026#39;necessary_ports\u0026#39;: [22, 3306, 443], \u0026#39;database_type\u0026#39;: \u0026#39;mysql\u0026#39; } # Initialize security tester security_tester = DatabaseSecurityTester(config) # Run security tests test_results = security_tester.run_security_tests() print(\u0026#34;Database Security Test Results:\u0026#34;) print(f\u0026#34;Security Score: {test_results[\u0026#39;summary\u0026#39;][\u0026#39;security_score\u0026#39;]:.1f}%\u0026#34;) print(f\u0026#34;Tests Passed: {test_results[\u0026#39;summary\u0026#39;][\u0026#39;passed\u0026#39;]}\u0026#34;) print(f\u0026#34;Tests Failed: {test_results[\u0026#39;summary\u0026#39;][\u0026#39;failed\u0026#39;]}\u0026#34;) # Initialize compliance manager compliance_manager = DatabaseComplianceManager(config) # Run compliance scan compliance_results = compliance_manager.run_compliance_scan() print(\u0026#34;\\nCompliance Scan Results:\u0026#34;) print(f\u0026#34;Compliance Score: {compliance_results[\u0026#39;summary\u0026#39;][\u0026#39;compliance_score\u0026#39;]:.1f}%\u0026#34;) print(f\u0026#34;Rules Passed: {compliance_results[\u0026#39;summary\u0026#39;][\u0026#39;passed\u0026#39;]}\u0026#34;) print(f\u0026#34;Rules Failed: {compliance_results[\u0026#39;summary\u0026#39;][\u0026#39;failed\u0026#39;]}\u0026#34;) # Generate reports security_report = json.dumps(test_results, indent=2) compliance_report = compliance_manager.generate_compliance_report(\u0026#39;html\u0026#39;) print(\u0026#34;\\nReports generated successfully!\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() Summary This comprehensive database security and access control guide covers essential aspects of protecting database systems in modern environments. The implementation includes:\n","content":"Database Security and Access Control: A Comprehensive Guide to Protecting Your Data Database security is a critical aspect of modern data management, encompassing multiple layers of protection to safeguard sensitive information from unauthorized access, data breaches, and malicious attacks. This comprehensive guide explores essential security strategies, implementation techniques, and best practices for securing database systems.\nTable of Contents Security Architecture Overview Authentication â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["Database","Security","Access Control","Authentication","Authorization","Encryption"],"categories":["Database"],"author":"Database Security Team","readingTime":43,"wordCount":8974,"section":"posts","type":"posts","draft":false,"featured":false,"series":null},{"title":"Elasticsearchæœç´¢å¼•æ“ä¼˜åŒ–ï¼šä»ç´¢å¼•è®¾è®¡åˆ°æŸ¥è¯¢æ€§èƒ½è°ƒä¼˜çš„å®Œæ•´æŒ‡å—","url":"https://www.dishuihengxin.com/posts/database-elasticsearch-optimization/","summary":"Elasticsearchæœç´¢å¼•æ“ä¼˜åŒ–ï¼šä»ç´¢å¼•è®¾è®¡åˆ°æŸ¥è¯¢æ€§èƒ½è°ƒä¼˜çš„å®Œæ•´æŒ‡å— Elasticsearchä½œä¸ºå½“ä»Šæœ€æµè¡Œçš„æœç´¢å¼•æ“ï¼Œåœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æœç´¢ã€æ—¥å¿—åˆ†æã€å®æ—¶åˆ†æç­‰åœºæ™¯ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨Elasticsearchçš„ä¼˜åŒ–ç­–ç•¥ï¼Œä»ç´¢å¼•è®¾è®¡åˆ°æŸ¥è¯¢æ€§èƒ½è°ƒä¼˜ï¼Œæä¾›å®Œæ•´çš„ä¼˜åŒ–æŒ‡å—ã€‚\nElasticsearchæ¶æ„æ¦‚è¿° æ ¸å¿ƒæ¦‚å¿µ graph TB subgraph \u0026#34;Elasticsearché›†ç¾¤\u0026#34; subgraph \u0026#34;MasterèŠ‚ç‚¹\u0026#34; M1[Master Node 1] M2[Master Node 2] M3[Master Node 3] end subgraph \u0026#34;æ•°æ®èŠ‚ç‚¹\u0026#34; D1[Data Node 1] D2[Data Node 2] D3[Data Node 3] D4[Data Node 4] end subgraph \u0026#34;åè°ƒèŠ‚ç‚¹\u0026#34; C1[Coordinating Node 1] C2[Coordinating Node 2] end subgraph \u0026#34;æ‘„å–èŠ‚ç‚¹\u0026#34; I1[Ingest Node 1] I2[Ingest Node 2] end end Client[å®¢æˆ·ç«¯åº”ç”¨] --\u0026gt; C1 Client --\u0026gt; C2 C1 --\u0026gt; D1 C1 --\u0026gt; D2 C2 --\u0026gt; D3 C2 --\u0026gt; D4 I1 --\u0026gt; D1 I2 --\u0026gt; D2 M1 -.-\u0026gt; M2 M2 -.-\u0026gt; M3 M3 -.-\u0026gt; M1 èŠ‚ç‚¹è§’è‰²é…ç½® # elasticsearch.yml - MasterèŠ‚ç‚¹é…ç½® cluster.name: production-cluster node.name: master-node-1 node.roles: [master] discovery.seed_hosts: [\u0026#34;master-node-1\u0026#34;, \u0026#34;master-node-2\u0026#34;, \u0026#34;master-node-3\u0026#34;] cluster.initial_master_nodes: [\u0026#34;master-node-1\u0026#34;, \u0026#34;master-node-2\u0026#34;, \u0026#34;master-node-3\u0026#34;] # å†…å­˜é…ç½® bootstrap.memory_lock: true indices.memory.index_buffer_size: 10% # ç½‘ç»œé…ç½® network.host: 0.0.0.0 http.port: 9200 transport.port: 9300 # å®‰å…¨é…ç½® xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.http.ssl.enabled: true --- # elasticsearch.yml - æ•°æ®èŠ‚ç‚¹é…ç½® cluster.name: production-cluster node.name: data-node-1 node.roles: [data, data_content, data_hot, data_warm, data_cold] # æ•°æ®è·¯å¾„é…ç½® path.data: [\u0026#34;/data1/elasticsearch\u0026#34;, \u0026#34;/data2/elasticsearch\u0026#34;] path.logs: \u0026#34;/var/log/elasticsearch\u0026#34; # å†…å­˜é…ç½® bootstrap.memory_lock: true indices.memory.index_buffer_size: 20% indices.memory.min_index_buffer_size: 96mb # çº¿ç¨‹æ± é…ç½® thread_pool: write: size: 8 queue_size: 1000 search: size: 13 queue_size: 1000 get: size: 8 queue_size: 1000 --- # elasticsearch.yml - åè°ƒèŠ‚ç‚¹é…ç½® cluster.name: production-cluster node.name: coordinating-node-1 node.roles: [] # å†…å­˜é…ç½® bootstrap.memory_lock: true indices.memory.index_buffer_size: 5% # æœç´¢é…ç½® search.max_buckets: 65536 search.allow_expensive_queries: false ç´¢å¼•è®¾è®¡ä¼˜åŒ– æ˜ å°„ï¼ˆMappingï¼‰è®¾è®¡æœ€ä½³å®è·µ { \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;store\u0026#34;: true }, \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_max_word\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;keyword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;ignore_above\u0026#34;: 256 }, \u0026#34;suggest\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;completion\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;simple\u0026#34;, \u0026#34;preserve_separators\u0026#34;: true, \u0026#34;preserve_position_increments\u0026#34;: true, \u0026#34;max_input_length\u0026#34;: 50 } } }, \u0026#34;content\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_max_word\u0026#34;, \u0026#34;search_analyzer\u0026#34;: \u0026#34;ik_smart\u0026#34;, \u0026#34;index_options\u0026#34;: \u0026#34;positions\u0026#34; }, \u0026#34;category\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;text\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34; } } }, \u0026#34;tags\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;publish_date\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\u0026#34; }, \u0026#34;view_count\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;, \u0026#34;index\u0026#34;: false }, \u0026#34;author\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;name\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;ik_max_word\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;keyword\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;ignore_above\u0026#34;: 64 } } }, \u0026#34;email\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;index\u0026#34;: false } } }, \u0026#34;location\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;geo_point\u0026#34; }, \u0026#34;metadata\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;object\u0026#34;, \u0026#34;enabled\u0026#34;: false } } }, \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 3, \u0026#34;number_of_replicas\u0026#34;: 1, \u0026#34;refresh_interval\u0026#34;: \u0026#34;30s\u0026#34;, \u0026#34;max_result_window\u0026#34;: 50000, \u0026#34;analysis\u0026#34;: { \u0026#34;analyzer\u0026#34;: { \u0026#34;ik_max_word\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;ik_max_word\u0026#34; }, \u0026#34;ik_smart\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;ik_smart\u0026#34; }, \u0026#34;custom_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;stop\u0026#34;, \u0026#34;snowball\u0026#34; ] } } }, \u0026#34;index\u0026#34;: { \u0026#34;sort.field\u0026#34;: [\u0026#34;publish_date\u0026#34;, \u0026#34;_score\u0026#34;], \u0026#34;sort.order\u0026#34;: [\u0026#34;desc\u0026#34;, \u0026#34;desc\u0026#34;] } } } ç´¢å¼•æ¨¡æ¿é…ç½® { \u0026#34;index_patterns\u0026#34;: [\u0026#34;logs-*\u0026#34;, \u0026#34;metrics-*\u0026#34;], \u0026#34;template\u0026#34;: { \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 1, \u0026#34;number_of_replicas\u0026#34;: 1, \u0026#34;refresh_interval\u0026#34;: \u0026#34;5s\u0026#34;, \u0026#34;index.lifecycle.name\u0026#34;: \u0026#34;logs-policy\u0026#34;, \u0026#34;index.lifecycle.rollover_alias\u0026#34;: \u0026#34;logs-write\u0026#34; }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;@timestamp\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;date\u0026#34; }, \u0026#34;level\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;message\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34; }, \u0026#34;service\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;host\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; }, \u0026#34;tags\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34; } } } }, \u0026#34;composed_of\u0026#34;: [\u0026#34;component-template-mappings\u0026#34;, \u0026#34;component-template-settings\u0026#34;], \u0026#34;priority\u0026#34;: 200, \u0026#34;version\u0026#34;: 1, \u0026#34;_meta\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;Template for application logs\u0026#34; } } ç´¢å¼•ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼ˆILMï¼‰ { \u0026#34;policy\u0026#34;: { \u0026#34;phases\u0026#34;: { \u0026#34;hot\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;0ms\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;rollover\u0026#34;: { \u0026#34;max_size\u0026#34;: \u0026#34;10gb\u0026#34;, \u0026#34;max_age\u0026#34;: \u0026#34;1d\u0026#34;, \u0026#34;max_docs\u0026#34;: 10000000 }, \u0026#34;set_priority\u0026#34;: { \u0026#34;priority\u0026#34;: 100 } } }, \u0026#34;warm\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;1d\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;allocate\u0026#34;: { \u0026#34;number_of_replicas\u0026#34;: 0, \u0026#34;include\u0026#34;: { \u0026#34;data_tier\u0026#34;: \u0026#34;data_warm\u0026#34; } }, \u0026#34;forcemerge\u0026#34;: { \u0026#34;max_num_segments\u0026#34;: 1 }, \u0026#34;set_priority\u0026#34;: { \u0026#34;priority\u0026#34;: 50 } } }, \u0026#34;cold\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;7d\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;allocate\u0026#34;: { \u0026#34;number_of_replicas\u0026#34;: 0, \u0026#34;include\u0026#34;: { \u0026#34;data_tier\u0026#34;: \u0026#34;data_cold\u0026#34; } }, \u0026#34;set_priority\u0026#34;: { \u0026#34;priority\u0026#34;: 0 } } }, \u0026#34;delete\u0026#34;: { \u0026#34;min_age\u0026#34;: \u0026#34;30d\u0026#34;, \u0026#34;actions\u0026#34;: { \u0026#34;delete\u0026#34;: {} } } } } } æŸ¥è¯¢æ€§èƒ½ä¼˜åŒ– æŸ¥è¯¢DSLä¼˜åŒ–ç­–ç•¥ // 1. ä½¿ç”¨è¿‡æ»¤å™¨è€ŒéæŸ¥è¯¢ï¼ˆFilter vs Queryï¼‰ { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;elasticsearch optimization\u0026#34; } } ], \u0026#34;filter\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;published\u0026#34; } }, { \u0026#34;range\u0026#34;: { \u0026#34;publish_date\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2024-01-01\u0026#34; } } } ] } } } // 2. ä½¿ç”¨constant_scoreæŸ¥è¯¢é¿å…è¯„åˆ†è®¡ç®— { \u0026#34;query\u0026#34;: { \u0026#34;constant_score\u0026#34;: { \u0026#34;filter\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;term\u0026#34;: { \u0026#34;category\u0026#34;: \u0026#34;technology\u0026#34; } }, { \u0026#34;range\u0026#34;: { \u0026#34;view_count\u0026#34;: { \u0026#34;gte\u0026#34;: 1000 } } } ] } }, \u0026#34;boost\u0026#34;: 1.0 } } } // 3. ä½¿ç”¨multi_matchæŸ¥è¯¢ä¼˜åŒ– { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;elasticsearch performance\u0026#34;, \u0026#34;fields\u0026#34;: [ \u0026#34;title^3\u0026#34;, \u0026#34;content^1\u0026#34;, \u0026#34;tags^2\u0026#34; ], \u0026#34;type\u0026#34;: \u0026#34;best_fields\u0026#34;, \u0026#34;tie_breaker\u0026#34;: 0.3, \u0026#34;minimum_should_match\u0026#34;: \u0026#34;75%\u0026#34; } } } // 4. ä½¿ç”¨function_scoreè‡ªå®šä¹‰è¯„åˆ† { \u0026#34;query\u0026#34;: { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: { \u0026#34;match\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;elasticsearch\u0026#34; } }, \u0026#34;functions\u0026#34;: [ { \u0026#34;filter\u0026#34;: { \u0026#34;range\u0026#34;: { \u0026#34;publish_date\u0026#34;: { \u0026#34;gte\u0026#34;: \u0026#34;2024-01-01\u0026#34; } } }, \u0026#34;weight\u0026#34;: 2 }, { \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;view_count\u0026#34;, \u0026#34;factor\u0026#34;: 0.1, \u0026#34;modifier\u0026#34;: \u0026#34;log1p\u0026#34;, \u0026#34;missing\u0026#34;: 1 } } ], \u0026#34;score_mode\u0026#34;: \u0026#34;multiply\u0026#34;, \u0026#34;boost_mode\u0026#34;: \u0026#34;multiply\u0026#34; } } } // 5. ä½¿ç”¨èšåˆä¼˜åŒ– { \u0026#34;size\u0026#34;: 0, \u0026#34;aggs\u0026#34;: { \u0026#34;categories\u0026#34;: { \u0026#34;terms\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;category\u0026#34;, \u0026#34;size\u0026#34;: 10, \u0026#34;order\u0026#34;: { \u0026#34;_count\u0026#34;: \u0026#34;desc\u0026#34; } }, \u0026#34;aggs\u0026#34;: { \u0026#34;avg_views\u0026#34;: { \u0026#34;avg\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;view_count\u0026#34; } }, \u0026#34;top_articles\u0026#34;: { \u0026#34;top_hits\u0026#34;: { \u0026#34;size\u0026#34;: 3, \u0026#34;_source\u0026#34;: [\u0026#34;title\u0026#34;, \u0026#34;author.name\u0026#34;], \u0026#34;sort\u0026#34;: [ { \u0026#34;view_count\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ] } } } }, \u0026#34;date_histogram\u0026#34;: { \u0026#34;date_histogram\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;publish_date\u0026#34;, \u0026#34;calendar_interval\u0026#34;: \u0026#34;month\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;yyyy-MM\u0026#34; }, \u0026#34;aggs\u0026#34;: { \u0026#34;article_count\u0026#34;: { \u0026#34;value_count\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;id\u0026#34; } } } } } } æŸ¥è¯¢æ€§èƒ½ç›‘æ§è„šæœ¬ #!/usr/bin/env python3 # scripts/elasticsearch_query_monitor.py import json import time import requests import argparse from datetime import datetime, timedelta from collections import defaultdict import statistics class ElasticsearchQueryMonitor: def __init__(self, es_host, username=None, password=None): self.es_host = es_host.rstrip(\u0026#39;/\u0026#39;) self.session = requests.Session() if username and password: self.session.auth = (username, password) self.session.headers.update({ \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; }) def get_cluster_stats(self): \u0026#34;\u0026#34;\u0026#34;è·å–é›†ç¾¤ç»Ÿè®¡ä¿¡æ¯\u0026#34;\u0026#34;\u0026#34; try: response = self.session.get(f\u0026#34;{self.es_host}/_cluster/stats\u0026#34;) response.raise_for_status() return response.json() except Exception as e: print(f\u0026#34;Error getting cluster stats: {e}\u0026#34;) return {} def get_node_stats(self): \u0026#34;\u0026#34;\u0026#34;è·å–èŠ‚ç‚¹ç»Ÿè®¡ä¿¡æ¯\u0026#34;\u0026#34;\u0026#34; try: response = self.session.get(f\u0026#34;{self.es_host}/_nodes/stats\u0026#34;) response.raise_for_status() return response.json() except Exception as e: print(f\u0026#34;Error getting node stats: {e}\u0026#34;) return {} def get_index_stats(self, index_pattern=\u0026#34;*\u0026#34;): \u0026#34;\u0026#34;\u0026#34;è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯\u0026#34;\u0026#34;\u0026#34; try: response = self.session.get(f\u0026#34;{self.es_host}/{index_pattern}/_stats\u0026#34;) response.raise_for_status() return response.json() except Exception as e: print(f\u0026#34;Error getting index stats: {e}\u0026#34;) return {} def get_slow_queries(self, index_pattern=\u0026#34;*\u0026#34;, time_range=\u0026#34;1h\u0026#34;): \u0026#34;\u0026#34;\u0026#34;è·å–æ…¢æŸ¥è¯¢æ—¥å¿—\u0026#34;\u0026#34;\u0026#34; query = { \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;must\u0026#34;: [ { \u0026#34;range\u0026#34;: { \u0026#34;@timestamp\u0026#34;: { \u0026#34;gte\u0026#34;: f\u0026#34;now-{time_range}\u0026#34; } } }, { \u0026#34;exists\u0026#34;: { \u0026#34;field\u0026#34;: \u0026#34;elasticsearch.slowlog.took\u0026#34; } } ] } }, \u0026#34;sort\u0026#34;: [ { \u0026#34;elasticsearch.slowlog.took\u0026#34;: { \u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34; } } ], \u0026#34;size\u0026#34;: 100 } try: response = self.session.post( f\u0026#34;{self.es_host}/{index_pattern}/_search\u0026#34;, json=query ) response.raise_for_status() return response.json() except Exception as e: print(f\u0026#34;Error getting slow queries: {e}\u0026#34;) return {} def analyze_query_performance(self, index_name, query_body): \u0026#34;\u0026#34;\u0026#34;åˆ†ææŸ¥è¯¢æ€§èƒ½\u0026#34;\u0026#34;\u0026#34; try: # æ‰§è¡ŒæŸ¥è¯¢å¹¶è·å–è¯¦ç»†ä¿¡æ¯ response = self.session.post( f\u0026#34;{self.es_host}/{index_name}/_search?explain=true\u0026amp;profile=true\u0026#34;, json=query_body ) response.raise_for_status() result = response.json() analysis = { \u0026#39;took_ms\u0026#39;: result.get(\u0026#39;took\u0026#39;, 0), \u0026#39;total_hits\u0026#39;: result.get(\u0026#39;hits\u0026#39;, {}).get(\u0026#39;total\u0026#39;, {}).get(\u0026#39;value\u0026#39;, 0), \u0026#39;max_score\u0026#39;: result.get(\u0026#39;hits\u0026#39;, {}).get(\u0026#39;max_score\u0026#39;, 0), \u0026#39;profile\u0026#39;: result.get(\u0026#39;profile\u0026#39;, {}), \u0026#39;suggestions\u0026#39;: [] } # åˆ†æprofileä¿¡æ¯ if \u0026#39;shards\u0026#39; in result.get(\u0026#39;profile\u0026#39;, {}): for shard in result[\u0026#39;profile\u0026#39;][\u0026#39;shards\u0026#39;]: for search in shard.get(\u0026#39;searches\u0026#39;, []): for query in search.get(\u0026#39;query\u0026#39;, []): query_time = query.get(\u0026#39;time_in_nanos\u0026#39;, 0) / 1000000 # è½¬æ¢ä¸ºæ¯«ç§’ if query_time \u0026gt; 100: # è¶…è¿‡100msçš„æŸ¥è¯¢ analysis[\u0026#39;suggestions\u0026#39;].append({ \u0026#39;type\u0026#39;: \u0026#39;slow_query_component\u0026#39;, \u0026#39;component\u0026#39;: query.get(\u0026#39;type\u0026#39;, \u0026#39;unknown\u0026#39;), \u0026#39;time_ms\u0026#39;: query_time, \u0026#39;suggestion\u0026#39;: f\u0026#34;æŸ¥è¯¢ç»„ä»¶ {query.get(\u0026#39;type\u0026#39;)} è€—æ—¶ {query_time:.2f}msï¼Œè€ƒè™‘ä¼˜åŒ–\u0026#34; }) return analysis except Exception as e: print(f\u0026#34;Error analyzing query performance: {e}\u0026#34;) return {} def get_cache_stats(self): \u0026#34;\u0026#34;\u0026#34;è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯\u0026#34;\u0026#34;\u0026#34; try: response = self.session.get(f\u0026#34;{self.es_host}/_nodes/stats/indices/query_cache,request_cache,fielddata\u0026#34;) response.raise_for_status() stats = response.json() cache_analysis = { \u0026#39;query_cache\u0026#39;: {}, \u0026#39;request_cache\u0026#39;: {}, \u0026#39;fielddata_cache\u0026#39;: {} } for node_id, node_stats in stats.get(\u0026#39;nodes\u0026#39;, {}).items(): indices_stats = node_stats.get(\u0026#39;indices\u0026#39;, {}) # Query Cache query_cache = indices_stats.get(\u0026#39;query_cache\u0026#39;, {}) cache_analysis[\u0026#39;query_cache\u0026#39;][node_id] = { \u0026#39;memory_size_bytes\u0026#39;: query_cache.get(\u0026#39;memory_size_in_bytes\u0026#39;, 0), \u0026#39;total_count\u0026#39;: query_cache.get(\u0026#39;total_count\u0026#39;, 0), \u0026#39;hit_count\u0026#39;: query_cache.get(\u0026#39;hit_count\u0026#39;, 0), \u0026#39;miss_count\u0026#39;: query_cache.get(\u0026#39;miss_count\u0026#39;, 0), \u0026#39;cache_size\u0026#39;: query_cache.get(\u0026#39;cache_size\u0026#39;, 0), \u0026#39;evictions\u0026#39;: query_cache.get(\u0026#39;evictions\u0026#39;, 0) } # Request Cache request_cache = indices_stats.get(\u0026#39;request_cache\u0026#39;, {}) cache_analysis[\u0026#39;request_cache\u0026#39;][node_id] = { \u0026#39;memory_size_bytes\u0026#39;: request_cache.get(\u0026#39;memory_size_in_bytes\u0026#39;, 0), \u0026#39;hit_count\u0026#39;: request_cache.get(\u0026#39;hit_count\u0026#39;, 0), \u0026#39;miss_count\u0026#39;: request_cache.get(\u0026#39;miss_count\u0026#39;, 0), \u0026#39;evictions\u0026#39;: request_cache.get(\u0026#39;evictions\u0026#39;, 0) } # Fielddata Cache fielddata = indices_stats.get(\u0026#39;fielddata\u0026#39;, {}) cache_analysis[\u0026#39;fielddata_cache\u0026#39;][node_id] = { \u0026#39;memory_size_bytes\u0026#39;: fielddata.get(\u0026#39;memory_size_in_bytes\u0026#39;, 0), \u0026#39;evictions\u0026#39;: fielddata.get(\u0026#39;evictions\u0026#39;, 0) } return cache_analysis except Exception as e: print(f\u0026#34;Error getting cache stats: {e}\u0026#34;) return {} def generate_performance_report(self, indices=None): \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š\u0026#34;\u0026#34;\u0026#34; report = { \u0026#39;timestamp\u0026#39;: datetime.now().isoformat(), \u0026#39;cluster_stats\u0026#39;: self.get_cluster_stats(), \u0026#39;node_stats\u0026#39;: self.get_node_stats(), \u0026#39;cache_stats\u0026#39;: self.get_cache_stats(), \u0026#39;index_analysis\u0026#39;: {} } # åˆ†ææŒ‡å®šç´¢å¼• if indices: for index_name in indices: print(f\u0026#34;åˆ†æç´¢å¼•: {index_name}\u0026#34;) index_stats = self.get_index_stats(index_name) report[\u0026#39;index_analysis\u0026#39;][index_name] = index_stats return report def print_performance_summary(self, report): \u0026#34;\u0026#34;\u0026#34;æ‰“å°æ€§èƒ½æ‘˜è¦\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;\\n\u0026#34; + \u0026#34;=\u0026#34;*80) print(\u0026#34;Elasticsearchæ€§èƒ½ç›‘æ§æŠ¥å‘Š\u0026#34;) print(\u0026#34;=\u0026#34;*80) print(f\u0026#34;ç”Ÿæˆæ—¶é—´: {report[\u0026#39;timestamp\u0026#39;]}\u0026#34;) # é›†ç¾¤æ¦‚è§ˆ cluster_stats = report.get(\u0026#39;cluster_stats\u0026#39;, {}) if cluster_stats: indices_stats = cluster_stats.get(\u0026#39;indices\u0026#39;, {}) nodes_stats = cluster_stats.get(\u0026#39;nodes\u0026#39;, {}) print(f\u0026#34;\\né›†ç¾¤æ¦‚è§ˆ:\u0026#34;) print(f\u0026#34; èŠ‚ç‚¹æ•°é‡: {nodes_stats.get(\u0026#39;count\u0026#39;, {}).get(\u0026#39;total\u0026#39;, 0)}\u0026#34;) print(f\u0026#34; ç´¢å¼•æ•°é‡: {indices_stats.get(\u0026#39;count\u0026#39;, 0)}\u0026#34;) print(f\u0026#34; æ–‡æ¡£æ€»æ•°: {indices_stats.get(\u0026#39;docs\u0026#39;, {}).get(\u0026#39;count\u0026#39;, 0):,}\u0026#34;) print(f\u0026#34; å­˜å‚¨å¤§å°: {self._format_bytes(indices_stats.get(\u0026#39;store\u0026#39;, {}).get(\u0026#39;size_in_bytes\u0026#39;, 0))}\u0026#34;) # ç¼“å­˜åˆ†æ cache_stats = report.get(\u0026#39;cache_stats\u0026#39;, {}) if cache_stats: print(f\u0026#34;\\nç¼“å­˜åˆ†æ:\u0026#34;) # Query Cache query_cache_total_memory = sum( node_cache.get(\u0026#39;memory_size_bytes\u0026#39;, 0) for node_cache in cache_stats.get(\u0026#39;query_cache\u0026#39;, {}).values() ) query_cache_total_hits = sum( node_cache.get(\u0026#39;hit_count\u0026#39;, 0) for node_cache in cache_stats.get(\u0026#39;query_cache\u0026#39;, {}).values() ) query_cache_total_misses = sum( node_cache.get(\u0026#39;miss_count\u0026#39;, 0) for node_cache in cache_stats.get(\u0026#39;query_cache\u0026#39;, {}).values() ) if query_cache_total_hits + query_cache_total_misses \u0026gt; 0: hit_rate = query_cache_total_hits / (query_cache_total_hits + query_cache_total_misses) * 100 print(f\u0026#34; Query Cacheå‘½ä¸­ç‡: {hit_rate:.1f}%\u0026#34;) print(f\u0026#34; Query Cacheå†…å­˜ä½¿ç”¨: {self._format_bytes(query_cache_total_memory)}\u0026#34;) # Request Cache request_cache_total_hits = sum( node_cache.get(\u0026#39;hit_count\u0026#39;, 0) for node_cache in cache_stats.get(\u0026#39;request_cache\u0026#39;, {}).values() ) request_cache_total_misses = sum( node_cache.get(\u0026#39;miss_count\u0026#39;, 0) for node_cache in cache_stats.get(\u0026#39;request_cache\u0026#39;, {}).values() ) if request_cache_total_hits + request_cache_total_misses \u0026gt; 0: hit_rate = request_cache_total_hits / (request_cache_total_hits + request_cache_total_misses) * 100 print(f\u0026#34; Request Cacheå‘½ä¸­ç‡: {hit_rate:.1f}%\u0026#34;) # èŠ‚ç‚¹æ€§èƒ½åˆ†æ node_stats = report.get(\u0026#39;node_stats\u0026#39;, {}) if node_stats and \u0026#39;nodes\u0026#39; in node_stats: print(f\u0026#34;\\nèŠ‚ç‚¹æ€§èƒ½:\u0026#34;) for node_id, node_data in node_stats[\u0026#39;nodes\u0026#39;].items(): node_name = node_data.get(\u0026#39;name\u0026#39;, node_id) jvm_stats = node_data.get(\u0026#39;jvm\u0026#39;, {}) indices_stats = node_data.get(\u0026#39;indices\u0026#39;, {}) print(f\u0026#34; èŠ‚ç‚¹: {node_name}\u0026#34;) # JVMå†…å­˜ä½¿ç”¨ if \u0026#39;mem\u0026#39; in jvm_stats: heap_used_percent = jvm_stats[\u0026#39;mem\u0026#39;].get(\u0026#39;heap_used_percent\u0026#39;, 0) heap_max = jvm_stats[\u0026#39;mem\u0026#39;].get(\u0026#39;heap_max_in_bytes\u0026#39;, 0) print(f\u0026#34; JVMå †å†…å­˜ä½¿ç”¨: {heap_used_percent}% / {self._format_bytes(heap_max)}\u0026#34;) # æœç´¢æ€§èƒ½ if \u0026#39;search\u0026#39; in indices_stats: search_stats = indices_stats[\u0026#39;search\u0026#39;] query_total = search_stats.get(\u0026#39;query_total\u0026#39;, 0) query_time_ms = search_stats.get(\u0026#39;query_time_in_millis\u0026#39;, 0) avg_query_time = query_time_ms / query_total if query_total \u0026gt; 0 else 0 print(f\u0026#34; å¹³å‡æŸ¥è¯¢æ—¶é—´: {avg_query_time:.2f}ms\u0026#34;) print(f\u0026#34; æŸ¥è¯¢æ€»æ•°: {query_total:,}\u0026#34;) # ç´¢å¼•æ€§èƒ½ if \u0026#39;indexing\u0026#39; in indices_stats: indexing_stats = indices_stats[\u0026#39;indexing\u0026#39;] index_total = indexing_stats.get(\u0026#39;index_total\u0026#39;, 0) index_time_ms = indexing_stats.get(\u0026#39;index_time_in_millis\u0026#39;, 0) avg_index_time = index_time_ms / index_total if index_total \u0026gt; 0 else 0 print(f\u0026#34; å¹³å‡ç´¢å¼•æ—¶é—´: {avg_index_time:.2f}ms\u0026#34;) def _format_bytes(self, bytes_value): \u0026#34;\u0026#34;\u0026#34;æ ¼å¼åŒ–å­—èŠ‚æ•°\u0026#34;\u0026#34;\u0026#34; if bytes_value == 0: return \u0026#34;0 B\u0026#34; units = [\u0026#39;B\u0026#39;, \u0026#39;KB\u0026#39;, \u0026#39;MB\u0026#39;, \u0026#39;GB\u0026#39;, \u0026#39;TB\u0026#39;] unit_index = 0 while bytes_value \u0026gt;= 1024 and unit_index \u0026lt; len(units) - 1: bytes_value /= 1024 unit_index += 1 return f\u0026#34;{bytes_value:.1f} {units[unit_index]}\u0026#34; def monitor_queries(self, duration_minutes=60, interval_seconds=30): \u0026#34;\u0026#34;\u0026#34;æŒç»­ç›‘æ§æŸ¥è¯¢æ€§èƒ½\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;å¼€å§‹ç›‘æ§æŸ¥è¯¢æ€§èƒ½ï¼ŒæŒç»­æ—¶é—´: {duration_minutes} åˆ†é’Ÿ\u0026#34;) end_time = datetime.now() + timedelta(minutes=duration_minutes) query_times = [] while datetime.now() \u0026lt; end_time: try: # è·å–å½“å‰æ€§èƒ½æŒ‡æ ‡ node_stats = self.get_node_stats() current_metrics = {} for node_id, node_data in node_stats.get(\u0026#39;nodes\u0026#39;, {}).items(): search_stats = node_data.get(\u0026#39;indices\u0026#39;, {}).get(\u0026#39;search\u0026#39;, {}) query_total = search_stats.get(\u0026#39;query_total\u0026#39;, 0) query_time_ms = search_stats.get(\u0026#39;query_time_in_millis\u0026#39;, 0) if query_total \u0026gt; 0: avg_query_time = query_time_ms / query_total current_metrics[node_id] = avg_query_time query_times.append(avg_query_time) # æ‰“å°å½“å‰çŠ¶æ€ if current_metrics: avg_time = statistics.mean(current_metrics.values()) print(f\u0026#34;[{datetime.now().strftime(\u0026#39;%H:%M:%S\u0026#39;)}] å¹³å‡æŸ¥è¯¢æ—¶é—´: {avg_time:.2f}ms\u0026#34;) time.sleep(interval_seconds) except KeyboardInterrupt: print(\u0026#34;\\nç›‘æ§å·²åœæ­¢\u0026#34;) break except Exception as e: print(f\u0026#34;ç›‘æ§è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\u0026#34;) time.sleep(interval_seconds) # ç”Ÿæˆç›‘æ§æ‘˜è¦ if query_times: print(f\u0026#34;\\nç›‘æ§æ‘˜è¦:\u0026#34;) print(f\u0026#34; å¹³å‡æŸ¥è¯¢æ—¶é—´: {statistics.mean(query_times):.2f}ms\u0026#34;) print(f\u0026#34; æœ€å¤§æŸ¥è¯¢æ—¶é—´: {max(query_times):.2f}ms\u0026#34;) print(f\u0026#34; æœ€å°æŸ¥è¯¢æ—¶é—´: {min(query_times):.2f}ms\u0026#34;) print(f\u0026#34; æŸ¥è¯¢æ—¶é—´æ ‡å‡†å·®: {statistics.stdev(query_times):.2f}ms\u0026#34;) def main(): parser = argparse.ArgumentParser(description=\u0026#39;ElasticsearchæŸ¥è¯¢æ€§èƒ½ç›‘æ§å·¥å…·\u0026#39;) parser.add_argument(\u0026#39;--host\u0026#39;, required=True, help=\u0026#39;Elasticsearchä¸»æœºåœ°å€\u0026#39;) parser.add_argument(\u0026#39;--username\u0026#39;, help=\u0026#39;ç”¨æˆ·å\u0026#39;) parser.add_argument(\u0026#39;--password\u0026#39;, help=\u0026#39;å¯†ç \u0026#39;) parser.add_argument(\u0026#39;--action\u0026#39;, choices=[\u0026#39;report\u0026#39;, \u0026#39;monitor\u0026#39;], default=\u0026#39;report\u0026#39;, help=\u0026#39;æ‰§è¡ŒåŠ¨ä½œ\u0026#39;) parser.add_argument(\u0026#39;--duration\u0026#39;, type=int, default=60, help=\u0026#39;ç›‘æ§æŒç»­æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰\u0026#39;) parser.add_argument(\u0026#39;--interval\u0026#39;, type=int, default=30, help=\u0026#39;ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰\u0026#39;) parser.add_argument(\u0026#39;--indices\u0026#39;, help=\u0026#39;è¦åˆ†æçš„ç´¢å¼•æ¨¡å¼ï¼Œç”¨é€—å·åˆ†éš”\u0026#39;) args = parser.parse_args() monitor = ElasticsearchQueryMonitor(args.host, args.username, args.password) try: if args.action == \u0026#39;report\u0026#39;: indices = args.indices.split(\u0026#39;,\u0026#39;) if args.indices else None report = monitor.generate_performance_report(indices) monitor.print_performance_summary(report) # ä¿å­˜æŠ¥å‘Š timestamp = datetime.now().strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;) filename = f\u0026#34;elasticsearch_performance_report_{timestamp}.json\u0026#34; with open(filename, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: json.dump(report, f, indent=2, ensure_ascii=False, default=str) print(f\u0026#34;\\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}\u0026#34;) elif args.action == \u0026#39;monitor\u0026#39;: monitor.monitor_queries(args.duration, args.interval) except Exception as e: print(f\u0026#34;æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() é›†ç¾¤é…ç½®ä¼˜åŒ– JVMé…ç½®ä¼˜åŒ– #!/bin/bash # scripts/elasticsearch_jvm_tuning.sh # JVMå †å†…å­˜é…ç½® # è®¾ç½®ä¸ºç‰©ç†å†…å­˜çš„50%ï¼Œä½†ä¸è¶…è¿‡32GB PHYSICAL_MEMORY_GB=$(free -g | awk \u0026#39;/^Mem:/{print $2}\u0026#39;) HEAP_SIZE_GB=$((PHYSICAL_MEMORY_GB / 2)) if [ $HEAP_SIZE_GB -gt 32 ]; then HEAP_SIZE_GB=32 fi # ç”Ÿæˆjvm.optionsé…ç½® cat \u0026gt; /etc/elasticsearch/jvm.options.d/heap.options \u0026lt;\u0026lt; EOF # å †å†…å­˜é…ç½® -Xms${HEAP_SIZE_GB}g -Xmx${HEAP_SIZE_GB}g # GCé…ç½® -XX:+UseG1GC -XX:G1HeapRegionSize=16m -XX:+UseG1GC -XX:+UnlockExperimentalVMOptions -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:+DisableExplicitGC # å†…å­˜é…ç½® -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djna.nosys=true -XX:-OmitStackTraceInFastThrow -Dio.netty.noUnsafe=true -Dio.netty.noKeySetOptimization=true -Dio.netty.recycler.maxCapacityPerThread=0 -Dlog4j.shutdownHookEnabled=false -Dlog4j2.disable.jmx=true # é”™è¯¯å¤„ç† -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/var/lib/elasticsearch -XX:ErrorFile=/var/log/elasticsearch/hs_err_pid%p.log # GCæ—¥å¿— -Xlog:gc*,gc+age=trace,safepoint:gc.log:time,level,tags -XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=32 -XX:GCLogFileSize=64m EOF echo \u0026#34;JVMé…ç½®å·²ç”Ÿæˆ: /etc/elasticsearch/jvm.options.d/heap.options\u0026#34; echo \u0026#34;å»ºè®®çš„å †å†…å­˜å¤§å°: ${HEAP_SIZE_GB}GB\u0026#34; ç³»ç»Ÿçº§ä¼˜åŒ–è„šæœ¬ #!/bin/bash # scripts/elasticsearch_system_optimization.sh set -euo pipefail # æ—¥å¿—å‡½æ•° log() { echo \u0026#34;[$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)] $1\u0026#34; \u0026gt;\u0026amp;2 } # ä¼˜åŒ–æ–‡ä»¶æè¿°ç¬¦é™åˆ¶ optimize_file_descriptors() { log \u0026#34;ä¼˜åŒ–æ–‡ä»¶æè¿°ç¬¦é™åˆ¶...\u0026#34; # è®¾ç½®ç³»ç»Ÿçº§é™åˆ¶ cat \u0026gt;\u0026gt; /etc/security/limits.conf \u0026lt;\u0026lt; EOF elasticsearch soft nofile 65536 elasticsearch hard nofile 65536 elasticsearch soft nproc 4096 elasticsearch hard nproc 4096 elasticsearch soft memlock unlimited elasticsearch hard memlock unlimited EOF # è®¾ç½®systemdæœåŠ¡é™åˆ¶ mkdir -p /etc/systemd/system/elasticsearch.service.d cat \u0026gt; /etc/systemd/system/elasticsearch.service.d/override.conf \u0026lt;\u0026lt; EOF [Service] LimitNOFILE=65536 LimitNPROC=4096 LimitMEMLOCK=infinity EOF systemctl daemon-reload log \u0026#34;æ–‡ä»¶æè¿°ç¬¦é™åˆ¶ä¼˜åŒ–å®Œæˆ\u0026#34; } # ä¼˜åŒ–è™šæ‹Ÿå†…å­˜ optimize_virtual_memory() { log \u0026#34;ä¼˜åŒ–è™šæ‹Ÿå†…å­˜è®¾ç½®...\u0026#34; # è®¾ç½®vm.max_map_count echo \u0026#39;vm.max_map_count=262144\u0026#39; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p # è®¾ç½®swappiness echo \u0026#39;vm.swappiness=1\u0026#39; \u0026gt;\u0026gt; /etc/sysctl.conf sysctl -p log \u0026#34;è™šæ‹Ÿå†…å­˜ä¼˜åŒ–å®Œæˆ\u0026#34; } # ä¼˜åŒ–ç£ç›˜I/O optimize_disk_io() { log \u0026#34;ä¼˜åŒ–ç£ç›˜I/Oè®¾ç½®...\u0026#34; # è·å–æ•°æ®ç£ç›˜è®¾å¤‡ DATA_DEVICES=$(lsblk -no NAME,MOUNTPOINT | grep \u0026#39;/data\u0026#39; | awk \u0026#39;{print $1}\u0026#39; | sed \u0026#39;s/[0-9]*$//\u0026#39;) for device in $DATA_DEVICES; do if [ -b \u0026#34;/dev/$device\u0026#34; ]; then # è®¾ç½®I/Oè°ƒåº¦å™¨ä¸ºdeadlineæˆ–noop echo deadline \u0026gt; /sys/block/$device/queue/scheduler # è®¾ç½®è¯»å–é¢„è¯» echo 128 \u0026gt; /sys/block/$device/queue/read_ahead_kb # è®¾ç½®é˜Ÿåˆ—æ·±åº¦ echo 32 \u0026gt; /sys/block/$device/queue/nr_requests log \u0026#34;å·²ä¼˜åŒ–è®¾å¤‡ /dev/$device çš„I/Oè®¾ç½®\u0026#34; fi done } # ä¼˜åŒ–ç½‘ç»œè®¾ç½® optimize_network() { log \u0026#34;ä¼˜åŒ–ç½‘ç»œè®¾ç½®...\u0026#34; cat \u0026gt;\u0026gt; /etc/sysctl.conf \u0026lt;\u0026lt; EOF # ç½‘ç»œä¼˜åŒ– net.core.rmem_default = 262144 net.core.rmem_max = 16777216 net.core.wmem_default = 262144 net.core.wmem_max = 16777216 net.ipv4.tcp_rmem = 4096 65536 16777216 net.ipv4.tcp_wmem = 4096 65536 16777216 net.core.netdev_max_backlog = 5000 net.ipv4.tcp_congestion_control = bbr EOF sysctl -p log \u0026#34;ç½‘ç»œä¼˜åŒ–å®Œæˆ\u0026#34; } # åˆ›å»ºç›‘æ§è„šæœ¬ create_monitoring_script() { log \u0026#34;åˆ›å»ºç³»ç»Ÿç›‘æ§è„šæœ¬...\u0026#34; cat \u0026gt; /usr/local/bin/elasticsearch_system_monitor.sh \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; #!/bin/bash # Elasticsearchç³»ç»Ÿç›‘æ§è„šæœ¬ # æ£€æŸ¥JVMå †å†…å­˜ä½¿ç”¨ check_jvm_memory() { local es_host=\u0026#34;${1:-localhost:9200}\u0026#34; curl -s \u0026#34;$es_host/_nodes/stats/jvm\u0026#34; | jq -r \u0026#39; .nodes[] | \u0026#34;èŠ‚ç‚¹: \u0026#34; + .name + \u0026#34; | å †å†…å­˜ä½¿ç”¨: \u0026#34; + (.jvm.mem.heap_used_percent | tostring) + \u0026#34;%\u0026#34; + \u0026#34; | GCæ¬¡æ•°: \u0026#34; + (.jvm.gc.collectors.young.collection_count | tostring) \u0026#39; } # æ£€æŸ¥ç£ç›˜ä½¿ç”¨æƒ…å†µ check_disk_usage() { local es_host=\u0026#34;${1:-localhost:9200}\u0026#34; curl -s \u0026#34;$es_host/_nodes/stats/fs\u0026#34; | jq -r \u0026#39; .nodes[] | \u0026#34;èŠ‚ç‚¹: \u0026#34; + .name + \u0026#34; | ç£ç›˜ä½¿ç”¨: \u0026#34; + ((.fs.total.total_in_bytes - .fs.total.available_in_bytes) * 100 / .fs.total.total_in_bytes | floor | tostring) + \u0026#34;%\u0026#34; \u0026#39; } # æ£€æŸ¥æŸ¥è¯¢æ€§èƒ½ check_query_performance() { local es_host=\u0026#34;${1:-localhost:9200}\u0026#34; curl -s \u0026#34;$es_host/_nodes/stats/indices/search\u0026#34; | jq -r \u0026#39; .nodes[] | \u0026#34;èŠ‚ç‚¹: \u0026#34; + .name + \u0026#34; | æŸ¥è¯¢æ€»æ•°: \u0026#34; + (.indices.search.query_total | tostring) + \u0026#34; | å¹³å‡æŸ¥è¯¢æ—¶é—´: \u0026#34; + ((.indices.search.query_time_in_millis / .indices.search.query_total) | floor | tostring) + \u0026#34;ms\u0026#34; \u0026#39; } # æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€ check_cluster_health() { local es_host=\u0026#34;${1:-localhost:9200}\u0026#34; curl -s \u0026#34;$es_host/_cluster/health\u0026#34; | jq -r \u0026#39; \u0026#34;é›†ç¾¤çŠ¶æ€: \u0026#34; + .status + \u0026#34; | èŠ‚ç‚¹æ•°: \u0026#34; + (.number_of_nodes | tostring) + \u0026#34; | æ•°æ®èŠ‚ç‚¹æ•°: \u0026#34; + (.number_of_data_nodes | tostring) + \u0026#34; | æ´»è·ƒåˆ†ç‰‡: \u0026#34; + (.active_shards | tostring) + \u0026#34; | é‡å®šä½åˆ†ç‰‡: \u0026#34; + (.relocating_shards | tostring) + \u0026#34; | åˆå§‹åŒ–åˆ†ç‰‡: \u0026#34; + (.initializing_shards | tostring) + \u0026#34; | æœªåˆ†é…åˆ†ç‰‡: \u0026#34; + (.unassigned_shards | tostring) \u0026#39; } # ä¸»å‡½æ•° main() { local es_host=\u0026#34;${1:-localhost:9200}\u0026#34; echo \u0026#34;Elasticsearchç³»ç»Ÿç›‘æ§æŠ¥å‘Š - $(date)\u0026#34; echo \u0026#34;==================================\u0026#34; echo -e \u0026#34;\\né›†ç¾¤å¥åº·çŠ¶æ€:\u0026#34; check_cluster_health \u0026#34;$es_host\u0026#34; echo -e \u0026#34;\\nJVMå†…å­˜ä½¿ç”¨:\u0026#34; check_jvm_memory \u0026#34;$es_host\u0026#34; echo -e \u0026#34;\\nç£ç›˜ä½¿ç”¨æƒ…å†µ:\u0026#34; check_disk_usage \u0026#34;$es_host\u0026#34; echo -e \u0026#34;\\næŸ¥è¯¢æ€§èƒ½:\u0026#34; check_query_performance \u0026#34;$es_host\u0026#34; } main \u0026#34;$@\u0026#34; EOF chmod +x /usr/local/bin/elasticsearch_system_monitor.sh log \u0026#34;ç³»ç»Ÿç›‘æ§è„šæœ¬å·²åˆ›å»º: /usr/local/bin/elasticsearch_system_monitor.sh\u0026#34; } # ä¸»å‡½æ•° main() { local action=\u0026#34;${1:-all}\u0026#34; case \u0026#34;$action\u0026#34; in \u0026#34;fd\u0026#34;) optimize_file_descriptors ;; \u0026#34;vm\u0026#34;) optimize_virtual_memory ;; \u0026#34;io\u0026#34;) optimize_disk_io ;; \u0026#34;network\u0026#34;) optimize_network ;; \u0026#34;monitor\u0026#34;) create_monitoring_script ;; \u0026#34;all\u0026#34;) optimize_file_descriptors optimize_virtual_memory optimize_disk_io optimize_network create_monitoring_script log \u0026#34;æ‰€æœ‰ç³»ç»Ÿä¼˜åŒ–å®Œæˆï¼Œè¯·é‡å¯ElasticsearchæœåŠ¡\u0026#34; ;; *) echo \u0026#34;ç”¨æ³•: $0 {fd|vm|io|network|monitor|all}\u0026#34; echo \u0026#34; fd - ä¼˜åŒ–æ–‡ä»¶æè¿°ç¬¦é™åˆ¶\u0026#34; echo \u0026#34; vm - ä¼˜åŒ–è™šæ‹Ÿå†…å­˜è®¾ç½®\u0026#34; echo \u0026#34; io - ä¼˜åŒ–ç£ç›˜I/Oè®¾ç½®\u0026#34; echo \u0026#34; network - ä¼˜åŒ–ç½‘ç»œè®¾ç½®\u0026#34; echo \u0026#34; monitor - åˆ›å»ºç›‘æ§è„šæœ¬\u0026#34; echo \u0026#34; all - æ‰§è¡Œæ‰€æœ‰ä¼˜åŒ–\u0026#34; exit 1 ;; esac } # æ£€æŸ¥rootæƒé™ if [[ $EUID -ne 0 ]]; then echo \u0026#34;æ­¤è„šæœ¬éœ€è¦rootæƒé™è¿è¡Œ\u0026#34; exit 1 fi # æ‰§è¡Œä¸»å‡½æ•° main \u0026#34;$@\u0026#34; æœç´¢ç›¸å…³æ€§ä¼˜åŒ– è‡ªå®šä¹‰åˆ†æå™¨é…ç½® { \u0026#34;settings\u0026#34;: { \u0026#34;analysis\u0026#34;: { \u0026#34;char_filter\u0026#34;: { \u0026#34;html_strip_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;html_strip\u0026#34; }, \u0026#34;mapping_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mapping\u0026#34;, \u0026#34;mappings\u0026#34;: [ \u0026#34;\u0026amp; =\u0026gt; and\u0026#34;, \u0026#34;| =\u0026gt; or\u0026#34; ] } }, \u0026#34;tokenizer\u0026#34;: { \u0026#34;custom_keyword_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;buffer_size\u0026#34;: 256 }, \u0026#34;custom_pattern_tokenizer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;pattern\u0026#34;, \u0026#34;pattern\u0026#34;: \u0026#34;\\\\W+\u0026#34;, \u0026#34;lowercase\u0026#34;: true } }, \u0026#34;filter\u0026#34;: { \u0026#34;chinese_stop\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;stopwords\u0026#34;: [\u0026#34;çš„\u0026#34;, \u0026#34;äº†\u0026#34;, \u0026#34;åœ¨\u0026#34;, \u0026#34;æ˜¯\u0026#34;, \u0026#34;æˆ‘\u0026#34;, \u0026#34;æœ‰\u0026#34;, \u0026#34;å’Œ\u0026#34;, \u0026#34;å°±\u0026#34;, \u0026#34;ä¸\u0026#34;, \u0026#34;äºº\u0026#34;, \u0026#34;éƒ½\u0026#34;, \u0026#34;ä¸€\u0026#34;, \u0026#34;ä¸€ä¸ª\u0026#34;, \u0026#34;ä¸Š\u0026#34;, \u0026#34;ä¹Ÿ\u0026#34;, \u0026#34;å¾ˆ\u0026#34;, \u0026#34;åˆ°\u0026#34;, \u0026#34;è¯´\u0026#34;, \u0026#34;è¦\u0026#34;, \u0026#34;å»\u0026#34;, \u0026#34;ä½ \u0026#34;, \u0026#34;ä¼š\u0026#34;, \u0026#34;ç€\u0026#34;, \u0026#34;æ²¡æœ‰\u0026#34;, \u0026#34;çœ‹\u0026#34;, \u0026#34;å¥½\u0026#34;, \u0026#34;è‡ªå·±\u0026#34;, \u0026#34;è¿™\u0026#34;] }, \u0026#34;english_stop\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stop\u0026#34;, \u0026#34;stopwords\u0026#34;: \u0026#34;_english_\u0026#34; }, \u0026#34;synonym_filter\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;synonym\u0026#34;, \u0026#34;synonyms\u0026#34;: [ \u0026#34;elasticsearch,es,elastic search\u0026#34;, \u0026#34;database,db,æ•°æ®åº“\u0026#34;, \u0026#34;optimization,optimisation,ä¼˜åŒ–\u0026#34;, \u0026#34;performance,æ€§èƒ½\u0026#34; ] }, \u0026#34;custom_stemmer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;stemmer\u0026#34;, \u0026#34;language\u0026#34;: \u0026#34;english\u0026#34; } }, \u0026#34;analyzer\u0026#34;: { \u0026#34;chinese_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;html_strip_filter\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;ik_max_word\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;chinese_stop\u0026#34;, \u0026#34;synonym_filter\u0026#34; ] }, \u0026#34;english_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;char_filter\u0026#34;: [\u0026#34;html_strip_filter\u0026#34;, \u0026#34;mapping_filter\u0026#34;], \u0026#34;tokenizer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;english_stop\u0026#34;, \u0026#34;synonym_filter\u0026#34;, \u0026#34;custom_stemmer\u0026#34; ] }, \u0026#34;search_analyzer\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;custom\u0026#34;, \u0026#34;tokenizer\u0026#34;: \u0026#34;ik_smart\u0026#34;, \u0026#34;filter\u0026#34;: [ \u0026#34;lowercase\u0026#34;, \u0026#34;synonym_filter\u0026#34; ] } } } } } æœç´¢ç›¸å…³æ€§è°ƒä¼˜è„šæœ¬ #!/usr/bin/env python3 # scripts/elasticsearch_relevance_tuning.py import json import requests import argparse from datetime import datetime import math class ElasticsearchRelevanceTuner: def __init__(self, es_host, username=None, password=None): self.es_host = es_host.rstrip(\u0026#39;/\u0026#39;) self.session = requests.Session() if username and password: self.session.auth = (username, password) self.session.headers.update({ \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39; }) def analyze_search_results(self, index_name, query, expected_results=None): \u0026#34;\u0026#34;\u0026#34;åˆ†ææœç´¢ç»“æœç›¸å…³æ€§\u0026#34;\u0026#34;\u0026#34; search_body = { \u0026#34;query\u0026#34;: query, \u0026#34;size\u0026#34;: 20, \u0026#34;explain\u0026#34;: True } try: response = self.session.post( f\u0026#34;{self.es_host}/{index_name}/_search\u0026#34;, json=search_body ) response.raise_for_status() results = response.json() analysis = { \u0026#39;total_hits\u0026#39;: results.get(\u0026#39;hits\u0026#39;, {}).get(\u0026#39;total\u0026#39;, {}).get(\u0026#39;value\u0026#39;, 0), \u0026#39;max_score\u0026#39;: results.get(\u0026#39;hits\u0026#39;, {}).get(\u0026#39;max_score\u0026#39;, 0), \u0026#39;results\u0026#39;: [], \u0026#39;score_distribution\u0026#39;: {}, \u0026#39;relevance_metrics\u0026#39;: {} } scores = [] for hit in results.get(\u0026#39;hits\u0026#39;, {}).get(\u0026#39;hits\u0026#39;, []): score = hit.get(\u0026#39;_score\u0026#39;, 0) scores.append(score) result_info = { \u0026#39;id\u0026#39;: hit.get(\u0026#39;_id\u0026#39;), \u0026#39;score\u0026#39;: score, \u0026#39;source\u0026#39;: hit.get(\u0026#39;_source\u0026#39;, {}), \u0026#39;explanation\u0026#39;: hit.get(\u0026#39;_explanation\u0026#39;, {}) } analysis[\u0026#39;results\u0026#39;].append(result_info) # è®¡ç®—åˆ†æ•°åˆ†å¸ƒ if scores: analysis[\u0026#39;score_distribution\u0026#39;] = { \u0026#39;min\u0026#39;: min(scores), \u0026#39;max\u0026#39;: max(scores), \u0026#39;avg\u0026#39;: sum(scores) / len(scores), \u0026#39;std_dev\u0026#39;: math.sqrt(sum((x - sum(scores) / len(scores)) ** 2 for x in scores) / len(scores)) } # è®¡ç®—ç›¸å…³æ€§æŒ‡æ ‡ if expected_results: analysis[\u0026#39;relevance_metrics\u0026#39;] = self._calculate_relevance_metrics( [r[\u0026#39;id\u0026#39;] for r in analysis[\u0026#39;results\u0026#39;][:10]], expected_results ) return analysis except Exception as e: print(f\u0026#34;Error analyzing search results: {e}\u0026#34;) return {} def _calculate_relevance_metrics(self, actual_results, expected_results): \u0026#34;\u0026#34;\u0026#34;è®¡ç®—ç›¸å…³æ€§æŒ‡æ ‡ï¼ˆç²¾ç¡®ç‡ã€å¬å›ç‡ã€NDCGç­‰ï¼‰\u0026#34;\u0026#34;\u0026#34; # è®¡ç®—ç²¾ç¡®ç‡@K def precision_at_k(actual, expected, k): actual_k = actual[:k] relevant_retrieved = len(set(actual_k) \u0026amp; set(expected)) return relevant_retrieved / k if k \u0026gt; 0 else 0 # è®¡ç®—å¬å›ç‡@K def recall_at_k(actual, expected, k): actual_k = actual[:k] relevant_retrieved = len(set(actual_k) \u0026amp; set(expected)) return relevant_retrieved / len(expected) if len(expected) \u0026gt; 0 else 0 # è®¡ç®—NDCG@K def ndcg_at_k(actual, expected, k): actual_k = actual[:k] dcg = 0 for i, doc_id in enumerate(actual_k): if doc_id in expected: relevance = 1 # ç®€åŒ–çš„ç›¸å…³æ€§è¯„åˆ† dcg += relevance / math.log2(i + 2) # ç†æƒ³DCG idcg = sum(1 / math.log2(i + 2) for i in range(min(k, len(expected)))) return dcg / idcg if idcg \u0026gt; 0 else 0 metrics = {} for k in [1, 3, 5, 10]: metrics[f\u0026#39;precision@{k}\u0026#39;] = precision_at_k(actual_results, expected_results, k) metrics[f\u0026#39;recall@{k}\u0026#39;] = recall_at_k(actual_results, expected_results, k) metrics[f\u0026#39;ndcg@{k}\u0026#39;] = ndcg_at_k(actual_results, expected_results, k) return metrics def optimize_query_weights(self, index_name, test_queries): \u0026#34;\u0026#34;\u0026#34;ä¼˜åŒ–æŸ¥è¯¢æƒé‡\u0026#34;\u0026#34;\u0026#34; optimization_results = [] for test_case in test_queries: query_text = test_case[\u0026#39;query\u0026#39;] expected_results = test_case.get(\u0026#39;expected_results\u0026#39;, []) print(f\u0026#34;ä¼˜åŒ–æŸ¥è¯¢: {query_text}\u0026#34;) # æµ‹è¯•ä¸åŒçš„å­—æ®µæƒé‡ç»„åˆ weight_combinations = [ {\u0026#34;title\u0026#34;: 3, \u0026#34;content\u0026#34;: 1, \u0026#34;tags\u0026#34;: 2}, {\u0026#34;title\u0026#34;: 5, \u0026#34;content\u0026#34;: 1, \u0026#34;tags\u0026#34;: 3}, {\u0026#34;title\u0026#34;: 2, \u0026#34;content\u0026#34;: 1, \u0026#34;tags\u0026#34;: 1}, {\u0026#34;title\u0026#34;: 4, \u0026#34;content\u0026#34;: 2, \u0026#34;tags\u0026#34;: 2} ] best_combination = None best_score = 0 for weights in weight_combinations: query = { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: query_text, \u0026#34;fields\u0026#34;: [f\u0026#34;{field}^{weight}\u0026#34; for field, weight in weights.items()], \u0026#34;type\u0026#34;: \u0026#34;best_fields\u0026#34; } } analysis = self.analyze_search_results(index_name, query, expected_results) if analysis and \u0026#39;relevance_metrics\u0026#39; in analysis: # ä½¿ç”¨NDCG@10ä½œä¸ºä¸»è¦è¯„ä¼°æŒ‡æ ‡ ndcg_score = analysis[\u0026#39;relevance_metrics\u0026#39;].get(\u0026#39;ndcg@10\u0026#39;, 0) if ndcg_score \u0026gt; best_score: best_score = ndcg_score best_combination = weights optimization_results.append({ \u0026#39;query\u0026#39;: query_text, \u0026#39;best_weights\u0026#39;: best_combination, \u0026#39;best_ndcg_score\u0026#39;: best_score }) return optimization_results def create_custom_scoring_query(self, base_query, boost_factors): \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºè‡ªå®šä¹‰è¯„åˆ†æŸ¥è¯¢\u0026#34;\u0026#34;\u0026#34; function_score_query = { \u0026#34;function_score\u0026#34;: { \u0026#34;query\u0026#34;: base_query, \u0026#34;functions\u0026#34;: [], \u0026#34;score_mode\u0026#34;: \u0026#34;multiply\u0026#34;, \u0026#34;boost_mode\u0026#34;: \u0026#34;multiply\u0026#34; } } # æ·»åŠ æ—¶é—´è¡°å‡å‡½æ•° if \u0026#39;time_decay\u0026#39; in boost_factors: function_score_query[\u0026#34;function_score\u0026#34;][\u0026#34;functions\u0026#34;].append({ \u0026#34;gauss\u0026#34;: { \u0026#34;publish_date\u0026#34;: { \u0026#34;origin\u0026#34;: \u0026#34;now\u0026#34;, \u0026#34;scale\u0026#34;: boost_factors[\u0026#39;time_decay\u0026#39;].get(\u0026#39;scale\u0026#39;, \u0026#39;30d\u0026#39;), \u0026#34;decay\u0026#34;: boost_factors[\u0026#39;time_decay\u0026#39;].get(\u0026#39;decay\u0026#39;, 0.5) } } }) # æ·»åŠ å­—æ®µå€¼å› å­ if \u0026#39;field_factors\u0026#39; in boost_factors: for field, config in boost_factors[\u0026#39;field_factors\u0026#39;].items(): function_score_query[\u0026#34;function_score\u0026#34;][\u0026#34;functions\u0026#34;].append({ \u0026#34;field_value_factor\u0026#34;: { \u0026#34;field\u0026#34;: field, \u0026#34;factor\u0026#34;: config.get(\u0026#39;factor\u0026#39;, 1.0), \u0026#34;modifier\u0026#34;: config.get(\u0026#39;modifier\u0026#39;, \u0026#39;none\u0026#39;), \u0026#34;missing\u0026#34;: config.get(\u0026#39;missing\u0026#39;, 1) } }) # æ·»åŠ è„šæœ¬è¯„åˆ† if \u0026#39;script_score\u0026#39; in boost_factors: function_score_query[\u0026#34;function_score\u0026#34;][\u0026#34;functions\u0026#34;].append({ \u0026#34;script_score\u0026#34;: { \u0026#34;script\u0026#34;: { \u0026#34;source\u0026#34;: boost_factors[\u0026#39;script_score\u0026#39;] } } }) return function_score_query def benchmark_queries(self, index_name, queries, iterations=10): \u0026#34;\u0026#34;\u0026#34;åŸºå‡†æµ‹è¯•æŸ¥è¯¢æ€§èƒ½\u0026#34;\u0026#34;\u0026#34; benchmark_results = [] for query_name, query_body in queries.items(): print(f\u0026#34;åŸºå‡†æµ‹è¯•æŸ¥è¯¢: {query_name}\u0026#34;) times = [] for i in range(iterations): start_time = datetime.now() try: response = self.session.post( f\u0026#34;{self.es_host}/{index_name}/_search\u0026#34;, json=query_body ) response.raise_for_status() end_time = datetime.now() duration_ms = (end_time - start_time).total_seconds() * 1000 times.append(duration_ms) except Exception as e: print(f\u0026#34;æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}\u0026#34;) continue if times: benchmark_results.append({ \u0026#39;query_name\u0026#39;: query_name, \u0026#39;avg_time_ms\u0026#39;: sum(times) / len(times), \u0026#39;min_time_ms\u0026#39;: min(times), \u0026#39;max_time_ms\u0026#39;: max(times), \u0026#39;std_dev_ms\u0026#39;: math.sqrt(sum((x - sum(times) / len(times)) ** 2 for x in times) / len(times)), \u0026#39;iterations\u0026#39;: len(times) }) return benchmark_results def generate_optimization_recommendations(self, index_name): \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆä¼˜åŒ–å»ºè®®\u0026#34;\u0026#34;\u0026#34; recommendations = [] try: # è·å–ç´¢å¼•è®¾ç½®å’Œæ˜ å°„ response = self.session.get(f\u0026#34;{self.es_host}/{index_name}\u0026#34;) response.raise_for_status() index_info = response.json() index_settings = index_info[index_name][\u0026#39;settings\u0026#39;][\u0026#39;index\u0026#39;] index_mappings = index_info[index_name][\u0026#39;mappings\u0026#39;] # åˆ†æåˆ†ç‰‡é…ç½® num_shards = int(index_settings.get(\u0026#39;number_of_shards\u0026#39;, 1)) num_replicas = int(index_settings.get(\u0026#39;number_of_replicas\u0026#39;, 1)) # è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯ stats_response = self.session.get(f\u0026#34;{self.es_host}/{index_name}/_stats\u0026#34;) stats_response.raise_for_status() index_stats = stats_response.json() total_docs = index_stats[\u0026#39;indices\u0026#39;][index_name][\u0026#39;total\u0026#39;][\u0026#39;docs\u0026#39;][\u0026#39;count\u0026#39;] index_size_bytes = index_stats[\u0026#39;indices\u0026#39;][index_name][\u0026#39;total\u0026#39;][\u0026#39;store\u0026#39;][\u0026#39;size_in_bytes\u0026#39;] # åˆ†ç‰‡å¤§å°å»ºè®® avg_shard_size_gb = (index_size_bytes / num_shards) / (1024**3) if avg_shard_size_gb \u0026gt; 50: recommendations.append({ \u0026#39;type\u0026#39;: \u0026#39;shard_size\u0026#39;, \u0026#39;priority\u0026#39;: \u0026#39;high\u0026#39;, \u0026#39;message\u0026#39;: f\u0026#34;åˆ†ç‰‡è¿‡å¤§ ({avg_shard_size_gb:.1f}GB)ï¼Œå»ºè®®å¢åŠ åˆ†ç‰‡æ•°é‡\u0026#34; }) elif avg_shard_size_gb \u0026lt; 1: recommendations.append({ \u0026#39;type\u0026#39;: \u0026#39;shard_size\u0026#39;, \u0026#39;priority\u0026#39;: \u0026#39;medium\u0026#39;, \u0026#39;message\u0026#39;: f\u0026#34;åˆ†ç‰‡è¿‡å° ({avg_shard_size_gb:.1f}GB)ï¼Œå»ºè®®å‡å°‘åˆ†ç‰‡æ•°é‡\u0026#34; }) # åˆ·æ–°é—´éš”å»ºè®® refresh_interval = index_settings.get(\u0026#39;refresh_interval\u0026#39;, \u0026#39;1s\u0026#39;) if refresh_interval == \u0026#39;1s\u0026#39; and total_docs \u0026gt; 1000000: recommendations.append({ \u0026#39;type\u0026#39;: \u0026#39;refresh_interval\u0026#39;, \u0026#39;priority\u0026#39;: \u0026#39;medium\u0026#39;, \u0026#39;message\u0026#39;: \u0026#34;å¤§ç´¢å¼•å»ºè®®å¢åŠ refresh_intervalä»¥æé«˜ç´¢å¼•æ€§èƒ½\u0026#34; }) # æ˜ å°„åˆ†æ properties = index_mappings.get(\u0026#39;properties\u0026#39;, {}) # æ£€æŸ¥æœªä½¿ç”¨çš„å­—æ®µ for field_name, field_config in properties.items(): if field_config.get(\u0026#39;index\u0026#39;) is False and field_config.get(\u0026#39;type\u0026#39;) not in [\u0026#39;geo_point\u0026#39;, \u0026#39;geo_shape\u0026#39;]: recommendations.append({ \u0026#39;type\u0026#39;: \u0026#39;unused_field\u0026#39;, \u0026#39;priority\u0026#39;: \u0026#39;low\u0026#39;, \u0026#39;message\u0026#39;: f\u0026#34;å­—æ®µ {field_name} æœªè¢«ç´¢å¼•ï¼Œå¦‚æœä¸éœ€è¦æœç´¢å¯ä»¥è€ƒè™‘ç§»é™¤\u0026#34; }) # æ£€æŸ¥textå­—æ®µçš„åˆ†æå™¨é…ç½® for field_name, field_config in properties.items(): if field_config.get(\u0026#39;type\u0026#39;) == \u0026#39;text\u0026#39;: if \u0026#39;analyzer\u0026#39; not in field_config: recommendations.append({ \u0026#39;type\u0026#39;: \u0026#39;analyzer_missing\u0026#39;, \u0026#39;priority\u0026#39;: \u0026#39;medium\u0026#39;, \u0026#39;message\u0026#39;: f\u0026#34;textå­—æ®µ {field_name} æœªæŒ‡å®šåˆ†æå™¨ï¼Œå»ºè®®é…ç½®åˆé€‚çš„åˆ†æå™¨\u0026#34; }) return recommendations except Exception as e: print(f\u0026#34;Error generating recommendations: {e}\u0026#34;) return [] def print_optimization_report(self, index_name, benchmark_results, recommendations): \u0026#34;\u0026#34;\u0026#34;æ‰“å°ä¼˜åŒ–æŠ¥å‘Š\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;\\n\u0026#34; + \u0026#34;=\u0026#34;*80) print(\u0026#34;Elasticsearchæœç´¢ç›¸å…³æ€§ä¼˜åŒ–æŠ¥å‘Š\u0026#34;) print(\u0026#34;=\u0026#34;*80) print(f\u0026#34;ç´¢å¼•: {index_name}\u0026#34;) print(f\u0026#34;ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}\u0026#34;) # åŸºå‡†æµ‹è¯•ç»“æœ if benchmark_results: print(f\u0026#34;\\næŸ¥è¯¢æ€§èƒ½åŸºå‡†æµ‹è¯•:\u0026#34;) print(\u0026#34;-\u0026#34; * 40) for result in benchmark_results: print(f\u0026#34;æŸ¥è¯¢: {result[\u0026#39;query_name\u0026#39;]}\u0026#34;) print(f\u0026#34; å¹³å‡è€—æ—¶: {result[\u0026#39;avg_time_ms\u0026#39;]:.2f}ms\u0026#34;) print(f\u0026#34; æœ€å°è€—æ—¶: {result[\u0026#39;min_time_ms\u0026#39;]:.2f}ms\u0026#34;) print(f\u0026#34; æœ€å¤§è€—æ—¶: {result[\u0026#39;max_time_ms\u0026#39;]:.2f}ms\u0026#34;) print(f\u0026#34; æ ‡å‡†å·®: {result[\u0026#39;std_dev_ms\u0026#39;]:.2f}ms\u0026#34;) print(f\u0026#34; æµ‹è¯•æ¬¡æ•°: {result[\u0026#39;iterations\u0026#39;]}\u0026#34;) print() # ä¼˜åŒ–å»ºè®® if recommendations: print(f\u0026#34;ä¼˜åŒ–å»ºè®®:\u0026#34;) print(\u0026#34;-\u0026#34; * 40) high_priority = [r for r in recommendations if r[\u0026#39;priority\u0026#39;] == \u0026#39;high\u0026#39;] medium_priority = [r for r in recommendations if r[\u0026#39;priority\u0026#39;] == \u0026#39;medium\u0026#39;] low_priority = [r for r in recommendations if r[\u0026#39;priority\u0026#39;] == \u0026#39;low\u0026#39;] if high_priority: print(\u0026#34;é«˜ä¼˜å…ˆçº§:\u0026#34;) for rec in high_priority: print(f\u0026#34; ğŸ”´ {rec[\u0026#39;message\u0026#39;]}\u0026#34;) if medium_priority: print(\u0026#34;ä¸­ä¼˜å…ˆçº§:\u0026#34;) for rec in medium_priority: print(f\u0026#34; ğŸŸ¡ {rec[\u0026#39;message\u0026#39;]}\u0026#34;) if low_priority: print(\u0026#34;ä½ä¼˜å…ˆçº§:\u0026#34;) for rec in low_priority: print(f\u0026#34; ğŸŸ¢ {rec[\u0026#39;message\u0026#39;]}\u0026#34;) def main(): parser = argparse.ArgumentParser(description=\u0026#39;Elasticsearchæœç´¢ç›¸å…³æ€§è°ƒä¼˜å·¥å…·\u0026#39;) parser.add_argument(\u0026#39;--host\u0026#39;, required=True, help=\u0026#39;Elasticsearchä¸»æœºåœ°å€\u0026#39;) parser.add_argument(\u0026#39;--username\u0026#39;, help=\u0026#39;ç”¨æˆ·å\u0026#39;) parser.add_argument(\u0026#39;--password\u0026#39;, help=\u0026#39;å¯†ç \u0026#39;) parser.add_argument(\u0026#39;--index\u0026#39;, required=True, help=\u0026#39;è¦åˆ†æçš„ç´¢å¼•åç§°\u0026#39;) parser.add_argument(\u0026#39;--queries-file\u0026#39;, help=\u0026#39;åŒ…å«æµ‹è¯•æŸ¥è¯¢çš„JSONæ–‡ä»¶\u0026#39;) parser.add_argument(\u0026#39;--benchmark\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;æ‰§è¡ŒæŸ¥è¯¢åŸºå‡†æµ‹è¯•\u0026#39;) parser.add_argument(\u0026#39;--recommendations\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;ç”Ÿæˆä¼˜åŒ–å»ºè®®\u0026#39;) args = parser.parse_args() tuner = ElasticsearchRelevanceTuner(args.host, args.username, args.password) try: benchmark_results = [] recommendations = [] if args.benchmark and args.queries_file: # åŠ è½½æµ‹è¯•æŸ¥è¯¢ with open(args.queries_file, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: test_queries = json.load(f) benchmark_results = tuner.benchmark_queries(args.index, test_queries) if args.recommendations: recommendations = tuner.generate_optimization_recommendations(args.index) # ç”ŸæˆæŠ¥å‘Š tuner.print_optimization_report(args.index, benchmark_results, recommendations) except Exception as e: print(f\u0026#34;æ‰§è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() æ€§èƒ½æµ‹è¯•ä¸åŸºå‡†æµ‹è¯• è´Ÿè½½æµ‹è¯•è„šæœ¬ #!/bin/bash # scripts/elasticsearch_load_test.sh set -euo pipefail # é…ç½®å‚æ•° ES_HOST=\u0026#34;${ES_HOST:-http://localhost:9200}\u0026#34; INDEX_NAME=\u0026#34;${INDEX_NAME:-test-index}\u0026#34; CONCURRENT_USERS=\u0026#34;${CONCURRENT_USERS:-10}\u0026#34; TEST_DURATION=\u0026#34;${TEST_DURATION:-300}\u0026#34; RAMP_UP_TIME=\u0026#34;${RAMP_UP_TIME:-60}\u0026#34; # æ—¥å¿—å‡½æ•° log() { echo \u0026#34;[$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)] $1\u0026#34; \u0026gt;\u0026amp;2 } # åˆ›å»ºæµ‹è¯•æ•°æ® create_test_data() { local num_docs=\u0026#34;${1:-10000}\u0026#34; log \u0026#34;åˆ›å»ºæµ‹è¯•æ•°æ®ï¼Œæ–‡æ¡£æ•°é‡: $num_docs\u0026#34; # åˆ›å»ºç´¢å¼•æ˜ å°„ curl -X PUT \u0026#34;$ES_HOST/$INDEX_NAME\u0026#34; -H \u0026#39;Content-Type: application/json\u0026#39; -d \u0026#39;{ \u0026#34;settings\u0026#34;: { \u0026#34;number_of_shards\u0026#34;: 3, \u0026#34;number_of_replicas\u0026#34;: 1, \u0026#34;refresh_interval\u0026#34;: \u0026#34;30s\u0026#34; }, \u0026#34;mappings\u0026#34;: { \u0026#34;properties\u0026#34;: { \u0026#34;id\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;}, \u0026#34;title\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;fields\u0026#34;: { \u0026#34;keyword\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;, \u0026#34;ignore_above\u0026#34;: 256} } }, \u0026#34;content\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;analyzer\u0026#34;: \u0026#34;standard\u0026#34;}, \u0026#34;category\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;}, \u0026#34;tags\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;keyword\u0026#34;}, \u0026#34;publish_date\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;date\u0026#34;}, \u0026#34;view_count\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;integer\u0026#34;}, \u0026#34;rating\u0026#34;: {\u0026#34;type\u0026#34;: \u0026#34;float\u0026#34;} } } }\u0026#39; # ç”Ÿæˆæ‰¹é‡æ’å…¥æ•°æ® python3 \u0026lt;\u0026lt; EOF import json import random from datetime import datetime, timedelta categories = [\u0026#34;technology\u0026#34;, \u0026#34;science\u0026#34;, \u0026#34;business\u0026#34;, \u0026#34;sports\u0026#34;, \u0026#34;entertainment\u0026#34;] tags_pool = [\u0026#34;elasticsearch\u0026#34;, \u0026#34;database\u0026#34;, \u0026#34;search\u0026#34;, \u0026#34;performance\u0026#34;, \u0026#34;optimization\u0026#34;, \u0026#34;big data\u0026#34;, \u0026#34;analytics\u0026#34;] bulk_data = [] for i in range($num_docs): doc_id = f\u0026#34;doc_{i:06d}\u0026#34; # åˆ›å»ºç´¢å¼•æ“ä½œ bulk_data.append(json.dumps({\u0026#34;index\u0026#34;: {\u0026#34;_id\u0026#34;: doc_id}})) # åˆ›å»ºæ–‡æ¡£æ•°æ® doc = { \u0026#34;id\u0026#34;: doc_id, \u0026#34;title\u0026#34;: f\u0026#34;Test Document {i} - Sample Title with Keywords\u0026#34;, \u0026#34;content\u0026#34;: f\u0026#34;This is test content for document {i}. It contains various keywords for testing search functionality. Lorem ipsum dolor sit amet, consectetur adipiscing elit.\u0026#34;, \u0026#34;category\u0026#34;: random.choice(categories), \u0026#34;tags\u0026#34;: random.sample(tags_pool, random.randint(1, 3)), \u0026#34;publish_date\u0026#34;: (datetime.now() - timedelta(days=random.randint(0, 365))).isoformat(), \u0026#34;view_count\u0026#34;: random.randint(0, 10000), \u0026#34;rating\u0026#34;: round(random.uniform(1.0, 5.0), 1) } bulk_data.append(json.dumps(doc)) # å†™å…¥æ–‡ä»¶ with open(\u0026#39;/tmp/bulk_data.json\u0026#39;, \u0026#39;w\u0026#39;) as f: f.write(\u0026#39;\\n\u0026#39;.join(bulk_data) + \u0026#39;\\n\u0026#39;) EOF # æ‰¹é‡æ’å…¥æ•°æ® curl -X POST \u0026#34;$ES_HOST/$INDEX_NAME/_bulk\u0026#34; \\ -H \u0026#39;Content-Type: application/x-ndjson\u0026#39; \\ --data-binary @/tmp/bulk_data.json # åˆ·æ–°ç´¢å¼• curl -X POST \u0026#34;$ES_HOST/$INDEX_NAME/_refresh\u0026#34; log \u0026#34;æµ‹è¯•æ•°æ®åˆ›å»ºå®Œæˆ\u0026#34; } # æ‰§è¡Œæœç´¢è´Ÿè½½æµ‹è¯• run_search_load_test() { log \u0026#34;å¼€å§‹æœç´¢è´Ÿè½½æµ‹è¯•\u0026#34; # åˆ›å»ºJMeteræµ‹è¯•è®¡åˆ’ cat \u0026gt; /tmp/elasticsearch_test_plan.jmx \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;jmeterTestPlan version=\u0026#34;1.2\u0026#34; properties=\u0026#34;5.0\u0026#34; jmeter=\u0026#34;5.4.1\u0026#34;\u0026gt; \u0026lt;hashTree\u0026gt; \u0026lt;TestPlan guiclass=\u0026#34;TestPlanGui\u0026#34; testclass=\u0026#34;TestPlan\u0026#34; testname=\u0026#34;Elasticsearch Load Test\u0026#34; enabled=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;stringProp name=\u0026#34;TestPlan.comments\u0026#34;\u0026gt;\u0026lt;/stringProp\u0026gt; \u0026lt;boolProp name=\u0026#34;TestPlan.functional_mode\u0026#34;\u0026gt;false\u0026lt;/boolProp\u0026gt; \u0026lt;boolProp name=\u0026#34;TestPlan.tearDown_on_shutdown\u0026#34;\u0026gt;true\u0026lt;/boolProp\u0026gt; \u0026lt;boolProp name=\u0026#34;TestPlan.serialize_threadgroups\u0026#34;\u0026gt;false\u0026lt;/boolProp\u0026gt; \u0026lt;elementProp name=\u0026#34;TestPlan.arguments\u0026#34; elementType=\u0026#34;Arguments\u0026#34; guiclass=\u0026#34;ArgumentsPanel\u0026#34; testclass=\u0026#34;Arguments\u0026#34; testname=\u0026#34;User Defined Variables\u0026#34; enabled=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;collectionProp name=\u0026#34;Arguments.arguments\u0026#34;/\u0026gt; \u0026lt;/elementProp\u0026gt; \u0026lt;stringProp name=\u0026#34;TestPlan.user_define_classpath\u0026#34;\u0026gt;\u0026lt;/stringProp\u0026gt; \u0026lt;/TestPlan\u0026gt; \u0026lt;hashTree\u0026gt; \u0026lt;ThreadGroup guiclass=\u0026#34;ThreadGroupGui\u0026#34; testclass=\u0026#34;ThreadGroup\u0026#34; testname=\u0026#34;Search Thread Group\u0026#34; enabled=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;stringProp name=\u0026#34;ThreadGroup.on_sample_error\u0026#34;\u0026gt;continue\u0026lt;/stringProp\u0026gt; \u0026lt;elementProp name=\u0026#34;ThreadGroup.main_controller\u0026#34; elementType=\u0026#34;LoopController\u0026#34; guiclass=\u0026#34;LoopControllerGui\u0026#34; testclass=\u0026#34;LoopController\u0026#34; testname=\u0026#34;Loop Controller\u0026#34; enabled=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;boolProp name=\u0026#34;LoopController.continue_forever\u0026#34;\u0026gt;false\u0026lt;/boolProp\u0026gt; \u0026lt;intProp name=\u0026#34;LoopController.loops\u0026#34;\u0026gt;-1\u0026lt;/intProp\u0026gt; \u0026lt;/elementProp\u0026gt; \u0026lt;stringProp name=\u0026#34;ThreadGroup.num_threads\u0026#34;\u0026gt;${CONCURRENT_USERS}\u0026lt;/stringProp\u0026gt; \u0026lt;stringProp name=\u0026#34;ThreadGroup.ramp_time\u0026#34;\u0026gt;${RAMP_UP_TIME}\u0026lt;/stringProp\u0026gt; \u0026lt;boolProp name=\u0026#34;ThreadGroup.scheduler\u0026#34;\u0026gt;true\u0026lt;/boolProp\u0026gt; \u0026lt;stringProp name=\u0026#34;ThreadGroup.duration\u0026#34;\u0026gt;${TEST_DURATION}\u0026lt;/stringProp\u0026gt; \u0026lt;stringProp name=\u0026#34;ThreadGroup.delay\u0026#34;\u0026gt;\u0026lt;/stringProp\u0026gt; \u0026lt;/ThreadGroup\u0026gt; \u0026lt;hashTree\u0026gt; \u0026lt;HTTPSamplerProxy guiclass=\u0026#34;HttpTestSampleGui\u0026#34; testclass=\u0026#34;HTTPSamplerProxy\u0026#34; testname=\u0026#34;Search Request\u0026#34; enabled=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;elementProp name=\u0026#34;HTTPsampler.Arguments\u0026#34; elementType=\u0026#34;Arguments\u0026#34; guiclass=\u0026#34;HTTPArgumentsPanel\u0026#34; testclass=\u0026#34;Arguments\u0026#34; testname=\u0026#34;User Defined Variables\u0026#34; enabled=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;collectionProp name=\u0026#34;Arguments.arguments\u0026#34;\u0026gt; \u0026lt;elementProp name=\u0026#34;\u0026#34; elementType=\u0026#34;HTTPArgument\u0026#34;\u0026gt; \u0026lt;boolProp name=\u0026#34;HTTPArgument.always_encode\u0026#34;\u0026gt;false\u0026lt;/boolProp\u0026gt; \u0026lt;stringProp name=\u0026#34;Argument.value\u0026#34;\u0026gt;{ \u0026#34;query\u0026#34;: { \u0026#34;bool\u0026#34;: { \u0026#34;should\u0026#34;: [ {\u0026#34;match\u0026#34;: {\u0026#34;title\u0026#34;: \u0026#34;${__RandomString(5,abcdefghijklmnopqrstuvwxyz)}\u0026#34;}}, {\u0026#34;match\u0026#34;: {\u0026#34;content\u0026#34;: \u0026#34;${__RandomString(8,abcdefghijklmnopqrstuvwxyz)}\u0026#34;}}, {\u0026#34;term\u0026#34;: {\u0026#34;category\u0026#34;: \u0026#34;${__RandomFromMultipleVars(technology|science|business|sports|entertainment)}\u0026#34;}} ] } }, \u0026#34;size\u0026#34;: 10, \u0026#34;sort\u0026#34;: [{\u0026#34;_score\u0026#34;: {\u0026#34;order\u0026#34;: \u0026#34;desc\u0026#34;}}] }\u0026lt;/stringProp\u0026gt; \u0026lt;stringProp name=\u0026#34;Argument.metadata\u0026#34;\u0026gt;=\u0026lt;/stringProp\u0026gt; \u0026lt;boolProp name=\u0026#34;HTTPArgument.use_equals\u0026#34;\u0026gt;true\u0026lt;/boolProp\u0026gt; \u0026lt;stringProp name=\u0026#34;Argument.name\u0026#34;\u0026gt;\u0026lt;/stringProp\u0026gt; \u0026lt;/elementProp\u0026gt; \u0026lt;/collectionProp\u0026gt; \u0026lt;/elementProp\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.domain\u0026#34;\u0026gt;localhost\u0026lt;/stringProp\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.port\u0026#34;\u0026gt;9200\u0026lt;/stringProp\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.protocol\u0026#34;\u0026gt;http\u0026lt;/stringProp\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.contentEncoding\u0026#34;\u0026gt;\u0026lt;/stringProp\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.path\u0026#34;\u0026gt;/${INDEX_NAME}/_search\u0026lt;/stringProp\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.method\u0026#34;\u0026gt;POST\u0026lt;/stringProp\u0026gt; \u0026lt;boolProp name=\u0026#34;HTTPSampler.follow_redirects\u0026#34;\u0026gt;true\u0026lt;/boolProp\u0026gt; \u0026lt;boolProp name=\u0026#34;HTTPSampler.auto_redirects\u0026#34;\u0026gt;false\u0026lt;/boolProp\u0026gt; \u0026lt;boolProp name=\u0026#34;HTTPSampler.use_keepalive\u0026#34;\u0026gt;true\u0026lt;/boolProp\u0026gt; \u0026lt;boolProp name=\u0026#34;HTTPSampler.DO_MULTIPART_POST\u0026#34;\u0026gt;false\u0026lt;/boolProp\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.embedded_url_re\u0026#34;\u0026gt;\u0026lt;/stringProp\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.connect_timeout\u0026#34;\u0026gt;\u0026lt;/stringProp\u0026gt; \u0026lt;stringProp name=\u0026#34;HTTPSampler.response_timeout\u0026#34;\u0026gt;\u0026lt;/stringProp\u0026gt; \u0026lt;/HTTPSamplerProxy\u0026gt; \u0026lt;hashTree\u0026gt; \u0026lt;HeaderManager guiclass=\u0026#34;HeaderPanel\u0026#34; testclass=\u0026#34;HeaderManager\u0026#34; testname=\u0026#34;HTTP Header Manager\u0026#34; enabled=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;collectionProp name=\u0026#34;HeaderManager.headers\u0026#34;\u0026gt; \u0026lt;elementProp name=\u0026#34;\u0026#34; elementType=\u0026#34;Header\u0026#34;\u0026gt; \u0026lt;stringProp name=\u0026#34;Header.name\u0026#34;\u0026gt;Content-Type\u0026lt;/stringProp\u0026gt; \u0026lt;stringProp name=\u0026#34;Header.value\u0026#34;\u0026gt;application/json\u0026lt;/stringProp\u0026gt; \u0026lt;/elementProp\u0026gt; \u0026lt;/collectionProp\u0026gt; \u0026lt;/HeaderManager\u0026gt; \u0026lt;hashTree/\u0026gt; \u0026lt;/hashTree\u0026gt; \u0026lt;/hashTree\u0026gt; \u0026lt;/hashTree\u0026gt; \u0026lt;/hashTree\u0026gt; \u0026lt;/jmeterTestPlan\u0026gt; EOF # ä½¿ç”¨Apache Benchè¿›è¡Œç®€å•è´Ÿè½½æµ‹è¯• log \u0026#34;ä½¿ç”¨Apache Benchæ‰§è¡Œè´Ÿè½½æµ‹è¯•\u0026#34; # åˆ›å»ºæœç´¢è¯·æ±‚æ–‡ä»¶ cat \u0026gt; /tmp/search_request.json \u0026lt;\u0026lt; EOF { \u0026#34;query\u0026#34;: { \u0026#34;multi_match\u0026#34;: { \u0026#34;query\u0026#34;: \u0026#34;test document\u0026#34;, \u0026#34;fields\u0026#34;: [\u0026#34;title^2\u0026#34;, \u0026#34;content\u0026#34;] } }, \u0026#34;size\u0026#34;: 10 } EOF # æ‰§è¡Œè´Ÿè½½æµ‹è¯• ab -n 1000 -c $CONCURRENT_USERS -T \u0026#39;application/json\u0026#39; -p /tmp/search_request.json \\ \u0026#34;$ES_HOST/$INDEX_NAME/_search\u0026#34; \u0026gt; /tmp/ab_results.txt log \u0026#34;è´Ÿè½½æµ‹è¯•å®Œæˆï¼Œç»“æœä¿å­˜åœ¨ /tmp/ab_results.txt\u0026#34; } # ç›‘æ§é›†ç¾¤æ€§èƒ½ monitor_cluster_performance() { local duration=\u0026#34;${1:-300}\u0026#34; local interval=\u0026#34;${2:-10}\u0026#34; log \u0026#34;å¼€å§‹ç›‘æ§é›†ç¾¤æ€§èƒ½ï¼ŒæŒç»­æ—¶é—´: ${duration}ç§’\u0026#34; local end_time=$(($(date +%s) + duration)) local output_file=\u0026#34;/tmp/cluster_performance_$(date +%Y%m%d_%H%M%S).csv\u0026#34; # å†™å…¥CSVå¤´éƒ¨ echo \u0026#34;timestamp,heap_used_percent,query_total,query_time_avg_ms,index_total,index_time_avg_ms,search_current,index_current\u0026#34; \u0026gt; \u0026#34;$output_file\u0026#34; while [ $(date +%s) -lt $end_time ]; do local timestamp=$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) # è·å–èŠ‚ç‚¹ç»Ÿè®¡ä¿¡æ¯ local stats=$(curl -s \u0026#34;$ES_HOST/_nodes/stats/jvm,indices\u0026#34;) # è§£æç»Ÿè®¡ä¿¡æ¯ local heap_used_percent=$(echo \u0026#34;$stats\u0026#34; | jq -r \u0026#39;.nodes | to_entries[0].value.jvm.mem.heap_used_percent\u0026#39;) local query_total=$(echo \u0026#34;$stats\u0026#34; | jq -r \u0026#39;.nodes | to_entries[0].value.indices.search.query_total\u0026#39;) local query_time_ms=$(echo \u0026#34;$stats\u0026#34; | jq -r \u0026#39;.nodes | to_entries[0].value.indices.search.query_time_in_millis\u0026#39;) local index_total=$(echo \u0026#34;$stats\u0026#34; | jq -r \u0026#39;.nodes | to_entries[0].value.indices.indexing.index_total\u0026#39;) local index_time_ms=$(echo \u0026#34;$stats\u0026#34; | jq -r \u0026#39;.nodes | to_entries[0].value.indices.indexing.index_time_in_millis\u0026#39;) local search_current=$(echo \u0026#34;$stats\u0026#34; | jq -r \u0026#39;.nodes | to_entries[0].value.indices.search.query_current\u0026#39;) local index_current=$(echo \u0026#34;$stats\u0026#34; | jq -r \u0026#39;.nodes | to_entries[0].value.indices.indexing.index_current\u0026#39;) # è®¡ç®—å¹³å‡æ—¶é—´ local query_time_avg=0 local index_time_avg=0 if [ \u0026#34;$query_total\u0026#34; != \u0026#34;null\u0026#34; ] \u0026amp;\u0026amp; [ \u0026#34;$query_total\u0026#34; -gt 0 ]; then query_time_avg=$(echo \u0026#34;scale=2; $query_time_ms / $query_total\u0026#34; | bc) fi if [ \u0026#34;$index_total\u0026#34; != \u0026#34;null\u0026#34; ] \u0026amp;\u0026amp; [ \u0026#34;$index_total\u0026#34; -gt 0 ]; then index_time_avg=$(echo \u0026#34;scale=2; $index_time_ms / $index_total\u0026#34; | bc) fi # å†™å…¥CSV echo \u0026#34;$timestamp,$heap_used_percent,$query_total,$query_time_avg,$index_total,$index_time_avg,$search_current,$index_current\u0026#34; \u0026gt;\u0026gt; \u0026#34;$output_file\u0026#34; # æ‰“å°å½“å‰çŠ¶æ€ echo \u0026#34;[$timestamp] å †å†…å­˜: ${heap_used_percent}%, æŸ¥è¯¢: ${search_current}, ç´¢å¼•: ${index_current}\u0026#34; sleep $interval done log \u0026#34;æ€§èƒ½ç›‘æ§å®Œæˆï¼Œæ•°æ®ä¿å­˜åœ¨: $output_file\u0026#34; } # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š generate_performance_report() { log \u0026#34;ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š\u0026#34; local report_file=\u0026#34;/tmp/elasticsearch_performance_report_$(date +%Y%m%d_%H%M%S).html\u0026#34; cat \u0026gt; \u0026#34;$report_file\u0026#34; \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Elasticsearchæ€§èƒ½æµ‹è¯•æŠ¥å‘Š\u0026lt;/title\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;style\u0026gt; body { font-family: Arial, sans-serif; margin: 20px; } .header { background-color: #f0f0f0; padding: 20px; border-radius: 5px; } .section { margin: 20px 0; } .metric { display: inline-block; margin: 10px; padding: 15px; background-color: #e8f4f8; border-radius: 5px; } .chart { width: 100%; height: 400px; border: 1px solid #ccc; margin: 10px 0; } table { border-collapse: collapse; width: 100%; } th, td { border: 1px solid #ddd; padding: 8px; text-align: left; } th { background-color: #f2f2f2; } \u0026lt;/style\u0026gt; \u0026lt;script src=\u0026#34;https://cdn.jsdelivr.net/npm/chart.js\u0026#34;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;header\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;Elasticsearchæ€§èƒ½æµ‹è¯•æŠ¥å‘Š\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;ç”Ÿæˆæ—¶é—´: $(date)\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;æµ‹è¯•ç´¢å¼•: $INDEX_NAME\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;å¹¶å‘ç”¨æˆ·: $CONCURRENT_USERS\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;æµ‹è¯•æŒç»­æ—¶é—´: $TEST_DURATION ç§’\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;section\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;è´Ÿè½½æµ‹è¯•ç»“æœ\u0026lt;/h2\u0026gt; \u0026lt;div id=\u0026#34;load-test-results\u0026#34;\u0026gt; \u0026lt;!-- è¿™é‡Œä¼šæ’å…¥è´Ÿè½½æµ‹è¯•ç»“æœ --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;section\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;é›†ç¾¤æ€§èƒ½æŒ‡æ ‡\u0026lt;/h2\u0026gt; \u0026lt;canvas id=\u0026#34;performanceChart\u0026#34; class=\u0026#34;chart\u0026#34;\u0026gt;\u0026lt;/canvas\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;section\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;ä¼˜åŒ–å»ºè®®\u0026lt;/h2\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;æ ¹æ®æŸ¥è¯¢æ¨¡å¼ä¼˜åŒ–ç´¢å¼•æ˜ å°„\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;è°ƒæ•´JVMå †å†…å­˜å¤§å°\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;ä¼˜åŒ–åˆ†ç‰‡å’Œå‰¯æœ¬é…ç½®\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;ä½¿ç”¨é€‚å½“çš„åˆ·æ–°é—´éš”\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;å¯ç”¨æŸ¥è¯¢ç¼“å­˜å’Œè¯·æ±‚ç¼“å­˜\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;script\u0026gt; // è¿™é‡Œå¯ä»¥æ·»åŠ å›¾è¡¨æ¸²æŸ“ä»£ç  const ctx = document.getElementById(\u0026#39;performanceChart\u0026#39;).getContext(\u0026#39;2d\u0026#39;); const chart = new Chart(ctx, { type: \u0026#39;line\u0026#39;, data: { labels: [\u0026#39;æ—¶é—´1\u0026#39;, \u0026#39;æ—¶é—´2\u0026#39;, \u0026#39;æ—¶é—´3\u0026#39;, \u0026#39;æ—¶é—´4\u0026#39;, \u0026#39;æ—¶é—´5\u0026#39;], datasets: [{ label: \u0026#39;æŸ¥è¯¢å“åº”æ—¶é—´(ms)\u0026#39;, data: [12, 19, 3, 5, 2], borderColor: \u0026#39;rgb(75, 192, 192)\u0026#39;, tension: 0.1 }] }, options: { responsive: true, scales: { y: { beginAtZero: true } } } }); \u0026lt;/script\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; EOF log \u0026#34;æ€§èƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: $report_file\u0026#34; } # æ¸…ç†æµ‹è¯•æ•°æ® cleanup_test_data() { log \u0026#34;æ¸…ç†æµ‹è¯•æ•°æ®\u0026#34; curl -X DELETE \u0026#34;$ES_HOST/$INDEX_NAME\u0026#34; rm -f /tmp/bulk_data.json /tmp/search_request.json /tmp/ab_results.txt log \u0026#34;æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ\u0026#34; } # ä¸»å‡½æ•° main() { local action=\u0026#34;${1:-all}\u0026#34; case \u0026#34;$action\u0026#34; in \u0026#34;create-data\u0026#34;) create_test_data \u0026#34;${2:-10000}\u0026#34; ;; \u0026#34;load-test\u0026#34;) run_search_load_test ;; \u0026#34;monitor\u0026#34;) monitor_cluster_performance \u0026#34;${2:-300}\u0026#34; \u0026#34;${3:-10}\u0026#34; ;; \u0026#34;report\u0026#34;) generate_performance_report ;; \u0026#34;cleanup\u0026#34;) cleanup_test_data ;; \u0026#34;all\u0026#34;) create_test_data 10000 run_search_load_test \u0026amp; LOAD_TEST_PID=$! monitor_cluster_performance 300 10 \u0026amp; MONITOR_PID=$! wait $LOAD_TEST_PID wait $MONITOR_PID generate_performance_report ;; *) echo \u0026#34;ç”¨æ³•: $0 {create-data|load-test|monitor|report|cleanup|all}\u0026#34; echo \u0026#34; create-data [æ•°é‡] - åˆ›å»ºæµ‹è¯•æ•°æ®\u0026#34; echo \u0026#34; load-test - æ‰§è¡Œè´Ÿè½½æµ‹è¯•\u0026#34; echo \u0026#34; monitor [æ—¶é•¿] [é—´éš”] - ç›‘æ§é›†ç¾¤æ€§èƒ½\u0026#34; echo \u0026#34; report - ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š\u0026#34; echo \u0026#34; cleanup - æ¸…ç†æµ‹è¯•æ•°æ®\u0026#34; echo \u0026#34; all - æ‰§è¡Œå®Œæ•´æµ‹è¯•æµç¨‹\u0026#34; exit 1 ;; esac } # æ£€æŸ¥ä¾èµ– check_dependencies() { local missing_deps=() if ! command -v curl \u0026amp;\u0026gt; /dev/null; then missing_deps+=(\u0026#34;curl\u0026#34;) fi if ! command -v jq \u0026amp;\u0026gt; /dev/null; then missing_deps+=(\u0026#34;jq\u0026#34;) fi if ! command -v bc \u0026amp;\u0026gt; /dev/null; then missing_deps+=(\u0026#34;bc\u0026#34;) fi if [ ${#missing_deps[@]} -gt 0 ]; then echo \u0026#34;ç¼ºå°‘ä¾èµ–: ${missing_deps[*]}\u0026#34; echo \u0026#34;è¯·å®‰è£…ç¼ºå°‘çš„ä¾èµ–åé‡è¯•\u0026#34; exit 1 fi } # æ£€æŸ¥ä¾èµ–å¹¶æ‰§è¡Œä¸»å‡½æ•° check_dependencies main \u0026#34;$@\u0026#34; æ€»ç»“ æœ¬æ–‡æ·±å…¥æ¢è®¨äº†Elasticsearchæœç´¢å¼•æ“çš„å…¨é¢ä¼˜åŒ–ç­–ç•¥ï¼Œæ¶µç›–äº†ä»¥ä¸‹å…³é”®é¢†åŸŸï¼š\n","content":"Elasticsearchæœç´¢å¼•æ“ä¼˜åŒ–ï¼šä»ç´¢å¼•è®¾è®¡åˆ°æŸ¥è¯¢æ€§èƒ½è°ƒä¼˜çš„å®Œæ•´æŒ‡å— Elasticsearchä½œä¸ºå½“ä»Šæœ€æµè¡Œçš„æœç´¢å¼•æ“ï¼Œåœ¨å¤„ç†å¤§è§„æ¨¡æ•°æ®æœç´¢ã€æ—¥å¿—åˆ†æã€å®æ—¶åˆ†æç­‰åœºæ™¯ä¸­å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨Elasticsearchçš„ä¼˜åŒ–ç­–ç•¥ï¼Œä»ç´¢å¼•è®¾è®¡åˆ°æŸ¥è¯¢æ€§èƒ½è°ƒä¼˜ï¼Œæä¾›å®Œæ•´çš„ä¼˜åŒ–æŒ‡å—ã€‚\nElasticsearchæ¶æ„æ¦‚è¿° æ ¸å¿ƒæ¦‚å¿µ graph TB subgraph \u0026amp;#34;Elasticsearché›†ç¾¤\u0026amp;#34; subgraph \u0026amp;#34;MasterèŠ‚ç‚¹\u0026amp;#34; M1[Master Node 1] M2[Master Node 2] M3[Master Node 3] end subgraph \u0026amp;#34;æ•°æ®èŠ‚ç‚¹\u0026amp;#34; D1[Data Node 1] D2[Data Node 2] D3[Data Node 3] D4[Data Node 4] end subgraph \u0026amp;#34;åè°ƒèŠ‚ç‚¹\u0026amp;#34; C1[Coordinating Node 1] C2[Coordinating Node 2] end subgraph \u0026amp;#34;æ‘„å–èŠ‚ç‚¹\u0026amp;#34; I1[Ingest â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["Elasticsearch","æœç´¢å¼•æ“","æ€§èƒ½ä¼˜åŒ–","ç´¢å¼•è®¾è®¡","æŸ¥è¯¢è°ƒä¼˜"],"categories":["æ•°æ®åº“"],"author":"æŠ€æœ¯å›¢é˜Ÿ","readingTime":22,"wordCount":4559,"section":"posts","type":"posts","draft":false,"featured":false,"series":null},{"title":"InfluxDBæ—¶åºæ•°æ®åº“è®¾è®¡ä¸å®è·µï¼šä»æ•°æ®å»ºæ¨¡åˆ°é«˜æ€§èƒ½æŸ¥è¯¢ä¼˜åŒ–","url":"https://www.dishuihengxin.com/posts/database-influxdb-timeseries/","summary":"InfluxDBæ—¶åºæ•°æ®åº“è®¾è®¡ä¸å®è·µï¼šä»æ•°æ®å»ºæ¨¡åˆ°é«˜æ€§èƒ½æŸ¥è¯¢ä¼˜åŒ– å¼•è¨€ æ—¶åºæ•°æ®åº“åœ¨ç°ä»£æ•°æ®æ¶æ„ä¸­æ‰®æ¼”ç€è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨IoTã€ç›‘æ§ã€é‡‘èäº¤æ˜“ç­‰åœºæ™¯ä¸­ã€‚InfluxDBä½œä¸ºé¢†å…ˆçš„æ—¶åºæ•°æ®åº“ï¼Œä»¥å…¶é«˜æ€§èƒ½ã€æ˜“ç”¨æ€§å’Œä¸°å¯Œçš„ç”Ÿæ€ç³»ç»Ÿè€Œå¹¿å—æ¬¢è¿ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨InfluxDBçš„æ¶æ„è®¾è®¡ã€æ•°æ®å»ºæ¨¡æœ€ä½³å®è·µã€æŸ¥è¯¢ä¼˜åŒ–ç­–ç•¥å’Œä¼ä¸šçº§éƒ¨ç½²æ–¹æ¡ˆã€‚\nInfluxDBæ¶æ„æ¦‚è¿° æ ¸å¿ƒæ¦‚å¿µ graph TB subgraph \u0026#34;InfluxDBæ¶æ„\u0026#34; A[Database] --\u0026gt; B[Measurement] B --\u0026gt; C[Field] B --\u0026gt; D[Tag] B --\u0026gt; E[Timestamp] F[Series] --\u0026gt; G[Tag Set] F --\u0026gt; H[Measurement] I[Point] --\u0026gt; J[Measurement] I --\u0026gt; K[Tag Set] I --\u0026gt; L[Field Set] I --\u0026gt; M[Timestamp] end subgraph \u0026#34;å­˜å‚¨å¼•æ“\u0026#34; N[TSM Engine] --\u0026gt; O[WAL] N --\u0026gt; P[Cache] N --\u0026gt; Q[TSM Files] N --\u0026gt; R[Index] end æ•°æ®æ¨¡å‹è®¾è®¡ 1. åŸºç¡€æ•°æ®ç»“æ„ -- åˆ›å»ºæ•°æ®åº“ CREATE DATABASE \u0026#34;monitoring\u0026#34; -- åˆ›å»ºä¿ç•™ç­–ç•¥ CREATE RETENTION POLICY \u0026#34;one_week\u0026#34; ON \u0026#34;monitoring\u0026#34; DURATION 7d REPLICATION 1 DEFAULT CREATE RETENTION POLICY \u0026#34;one_month\u0026#34; ON \u0026#34;monitoring\u0026#34; DURATION 30d REPLICATION 1 CREATE RETENTION POLICY \u0026#34;one_year\u0026#34; ON \u0026#34;monitoring\u0026#34; DURATION 365d REPLICATION 1 -- åˆ›å»ºè¿ç»­æŸ¥è¯¢è¿›è¡Œæ•°æ®é™é‡‡æ · CREATE CONTINUOUS QUERY \u0026#34;cq_mean_1h\u0026#34; ON \u0026#34;monitoring\u0026#34; BEGIN SELECT mean(*) INTO \u0026#34;monitoring\u0026#34;.\u0026#34;one_month\u0026#34;.\u0026#34;cpu_usage_1h\u0026#34; FROM \u0026#34;monitoring\u0026#34;.\u0026#34;one_week\u0026#34;.\u0026#34;cpu_usage\u0026#34; GROUP BY time(1h), * END CREATE CONTINUOUS QUERY \u0026#34;cq_mean_1d\u0026#34; ON \u0026#34;monitoring\u0026#34; BEGIN SELECT mean(*) INTO \u0026#34;monitoring\u0026#34;.\u0026#34;one_year\u0026#34;.\u0026#34;cpu_usage_1d\u0026#34; FROM \u0026#34;monitoring\u0026#34;.\u0026#34;one_month\u0026#34;.\u0026#34;cpu_usage_1h\u0026#34; GROUP BY time(1d), * END 2. æ•°æ®å»ºæ¨¡æœ€ä½³å®è·µ #!/usr/bin/env python3 # scripts/influxdb_data_modeling.py import json import time import random from datetime import datetime, timedelta from influxdb import InfluxDBClient from typing import List, Dict, Any class InfluxDBDataModeler: def __init__(self, host=\u0026#39;localhost\u0026#39;, port=8086, username=\u0026#39;admin\u0026#39;, password=\u0026#39;admin\u0026#39;): self.client = InfluxDBClient(host=host, port=port, username=username, password=password) def create_database_schema(self, database_name: str): \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºæ•°æ®åº“å’Œä¿ç•™ç­–ç•¥\u0026#34;\u0026#34;\u0026#34; try: # åˆ›å»ºæ•°æ®åº“ self.client.create_database(database_name) self.client.switch_database(database_name) # åˆ›å»ºä¿ç•™ç­–ç•¥ retention_policies = [ { \u0026#39;name\u0026#39;: \u0026#39;realtime\u0026#39;, \u0026#39;duration\u0026#39;: \u0026#39;1h\u0026#39;, \u0026#39;replication\u0026#39;: 1, \u0026#39;default\u0026#39;: True }, { \u0026#39;name\u0026#39;: \u0026#39;short_term\u0026#39;, \u0026#39;duration\u0026#39;: \u0026#39;7d\u0026#39;, \u0026#39;replication\u0026#39;: 1, \u0026#39;default\u0026#39;: False }, { \u0026#39;name\u0026#39;: \u0026#39;medium_term\u0026#39;, \u0026#39;duration\u0026#39;: \u0026#39;30d\u0026#39;, \u0026#39;replication\u0026#39;: 1, \u0026#39;default\u0026#39;: False }, { \u0026#39;name\u0026#39;: \u0026#39;long_term\u0026#39;, \u0026#39;duration\u0026#39;: \u0026#39;365d\u0026#39;, \u0026#39;replication\u0026#39;: 1, \u0026#39;default\u0026#39;: False } ] for rp in retention_policies: self.client.create_retention_policy( name=rp[\u0026#39;name\u0026#39;], duration=rp[\u0026#39;duration\u0026#39;], replication=rp[\u0026#39;replication\u0026#39;], database=database_name, default=rp[\u0026#39;default\u0026#39;] ) print(f\u0026#34;æ•°æ®åº“ {database_name} å’Œä¿ç•™ç­–ç•¥åˆ›å»ºæˆåŠŸ\u0026#34;) except Exception as e: print(f\u0026#34;åˆ›å»ºæ•°æ®åº“æ¶æ„æ—¶å‘ç”Ÿé”™è¯¯: {e}\u0026#34;) def create_continuous_queries(self, database_name: str): \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºè¿ç»­æŸ¥è¯¢è¿›è¡Œæ•°æ®é™é‡‡æ ·\u0026#34;\u0026#34;\u0026#34; continuous_queries = [ # CPUä½¿ç”¨ç‡é™é‡‡æ · { \u0026#39;name\u0026#39;: \u0026#39;cq_cpu_5m\u0026#39;, \u0026#39;query\u0026#39;: \u0026#39;\u0026#39;\u0026#39; CREATE CONTINUOUS QUERY \u0026#34;cq_cpu_5m\u0026#34; ON \u0026#34;{database}\u0026#34; BEGIN SELECT mean(\u0026#34;usage_percent\u0026#34;) as \u0026#34;usage_percent\u0026#34; INTO \u0026#34;{database}\u0026#34;.\u0026#34;short_term\u0026#34;.\u0026#34;cpu_usage_5m\u0026#34; FROM \u0026#34;{database}\u0026#34;.\u0026#34;realtime\u0026#34;.\u0026#34;cpu_usage\u0026#34; GROUP BY time(5m), \u0026#34;host\u0026#34;, \u0026#34;cpu\u0026#34; END \u0026#39;\u0026#39;\u0026#39;.format(database=database_name) }, { \u0026#39;name\u0026#39;: \u0026#39;cq_cpu_1h\u0026#39;, \u0026#39;query\u0026#39;: \u0026#39;\u0026#39;\u0026#39; CREATE CONTINUOUS QUERY \u0026#34;cq_cpu_1h\u0026#34; ON \u0026#34;{database}\u0026#34; BEGIN SELECT mean(\u0026#34;usage_percent\u0026#34;) as \u0026#34;usage_percent\u0026#34; INTO \u0026#34;{database}\u0026#34;.\u0026#34;medium_term\u0026#34;.\u0026#34;cpu_usage_1h\u0026#34; FROM \u0026#34;{database}\u0026#34;.\u0026#34;short_term\u0026#34;.\u0026#34;cpu_usage_5m\u0026#34; GROUP BY time(1h), \u0026#34;host\u0026#34;, \u0026#34;cpu\u0026#34; END \u0026#39;\u0026#39;\u0026#39;.format(database=database_name) }, { \u0026#39;name\u0026#39;: \u0026#39;cq_cpu_1d\u0026#39;, \u0026#39;query\u0026#39;: \u0026#39;\u0026#39;\u0026#39; CREATE CONTINUOUS QUERY \u0026#34;cq_cpu_1d\u0026#34; ON \u0026#34;{database}\u0026#34; BEGIN SELECT mean(\u0026#34;usage_percent\u0026#34;) as \u0026#34;usage_percent\u0026#34; INTO \u0026#34;{database}\u0026#34;.\u0026#34;long_term\u0026#34;.\u0026#34;cpu_usage_1d\u0026#34; FROM \u0026#34;{database}\u0026#34;.\u0026#34;medium_term\u0026#34;.\u0026#34;cpu_usage_1h\u0026#34; GROUP BY time(1d), \u0026#34;host\u0026#34;, \u0026#34;cpu\u0026#34; END \u0026#39;\u0026#39;\u0026#39;.format(database=database_name) }, # å†…å­˜ä½¿ç”¨ç‡é™é‡‡æ · { \u0026#39;name\u0026#39;: \u0026#39;cq_memory_5m\u0026#39;, \u0026#39;query\u0026#39;: \u0026#39;\u0026#39;\u0026#39; CREATE CONTINUOUS QUERY \u0026#34;cq_memory_5m\u0026#34; ON \u0026#34;{database}\u0026#34; BEGIN SELECT mean(\u0026#34;usage_percent\u0026#34;) as \u0026#34;usage_percent\u0026#34;, mean(\u0026#34;available_bytes\u0026#34;) as \u0026#34;available_bytes\u0026#34;, mean(\u0026#34;used_bytes\u0026#34;) as \u0026#34;used_bytes\u0026#34; INTO \u0026#34;{database}\u0026#34;.\u0026#34;short_term\u0026#34;.\u0026#34;memory_usage_5m\u0026#34; FROM \u0026#34;{database}\u0026#34;.\u0026#34;realtime\u0026#34;.\u0026#34;memory_usage\u0026#34; GROUP BY time(5m), \u0026#34;host\u0026#34; END \u0026#39;\u0026#39;\u0026#39;.format(database=database_name) }, # ç½‘ç»œæµé‡é™é‡‡æ · { \u0026#39;name\u0026#39;: \u0026#39;cq_network_5m\u0026#39;, \u0026#39;query\u0026#39;: \u0026#39;\u0026#39;\u0026#39; CREATE CONTINUOUS QUERY \u0026#34;cq_network_5m\u0026#34; ON \u0026#34;{database}\u0026#34; BEGIN SELECT sum(\u0026#34;bytes_sent\u0026#34;) as \u0026#34;bytes_sent\u0026#34;, sum(\u0026#34;bytes_recv\u0026#34;) as \u0026#34;bytes_recv\u0026#34;, sum(\u0026#34;packets_sent\u0026#34;) as \u0026#34;packets_sent\u0026#34;, sum(\u0026#34;packets_recv\u0026#34;) as \u0026#34;packets_recv\u0026#34; INTO \u0026#34;{database}\u0026#34;.\u0026#34;short_term\u0026#34;.\u0026#34;network_usage_5m\u0026#34; FROM \u0026#34;{database}\u0026#34;.\u0026#34;realtime\u0026#34;.\u0026#34;network_usage\u0026#34; GROUP BY time(5m), \u0026#34;host\u0026#34;, \u0026#34;interface\u0026#34; END \u0026#39;\u0026#39;\u0026#39;.format(database=database_name) } ] for cq in continuous_queries: try: self.client.query(cq[\u0026#39;query\u0026#39;]) print(f\u0026#34;è¿ç»­æŸ¥è¯¢ {cq[\u0026#39;name\u0026#39;]} åˆ›å»ºæˆåŠŸ\u0026#34;) except Exception as e: print(f\u0026#34;åˆ›å»ºè¿ç»­æŸ¥è¯¢ {cq[\u0026#39;name\u0026#39;]} æ—¶å‘ç”Ÿé”™è¯¯: {e}\u0026#34;) def generate_sample_data(self, database_name: str, num_hosts: int = 10, duration_hours: int = 24): \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆç¤ºä¾‹ç›‘æ§æ•°æ®\u0026#34;\u0026#34;\u0026#34; self.client.switch_database(database_name) hosts = [f\u0026#34;server-{i:03d}\u0026#34; for i in range(1, num_hosts + 1)] interfaces = [\u0026#39;eth0\u0026#39;, \u0026#39;eth1\u0026#39;, \u0026#39;lo\u0026#39;] start_time = datetime.now() - timedelta(hours=duration_hours) current_time = start_time end_time = datetime.now() batch_size = 1000 points = [] while current_time \u0026lt; end_time: for host in hosts: # CPUä½¿ç”¨ç‡æ•°æ® for cpu_id in range(4): # 4æ ¸CPU cpu_usage = random.uniform(10, 90) point = { \u0026#34;measurement\u0026#34;: \u0026#34;cpu_usage\u0026#34;, \u0026#34;tags\u0026#34;: { \u0026#34;host\u0026#34;: host, \u0026#34;cpu\u0026#34;: f\u0026#34;cpu{cpu_id}\u0026#34; }, \u0026#34;fields\u0026#34;: { \u0026#34;usage_percent\u0026#34;: cpu_usage }, \u0026#34;time\u0026#34;: current_time.isoformat() } points.append(point) # å†…å­˜ä½¿ç”¨ç‡æ•°æ® total_memory = 16 * 1024 * 1024 * 1024 # 16GB used_memory = random.uniform(0.3, 0.8) * total_memory available_memory = total_memory - used_memory point = { \u0026#34;measurement\u0026#34;: \u0026#34;memory_usage\u0026#34;, \u0026#34;tags\u0026#34;: { \u0026#34;host\u0026#34;: host }, \u0026#34;fields\u0026#34;: { \u0026#34;usage_percent\u0026#34;: (used_memory / total_memory) * 100, \u0026#34;used_bytes\u0026#34;: int(used_memory), \u0026#34;available_bytes\u0026#34;: int(available_memory), \u0026#34;total_bytes\u0026#34;: int(total_memory) }, \u0026#34;time\u0026#34;: current_time.isoformat() } points.append(point) # ç½‘ç»œæµé‡æ•°æ® for interface in interfaces: bytes_sent = random.randint(1000, 100000) bytes_recv = random.randint(1000, 100000) packets_sent = random.randint(10, 1000) packets_recv = random.randint(10, 1000) point = { \u0026#34;measurement\u0026#34;: \u0026#34;network_usage\u0026#34;, \u0026#34;tags\u0026#34;: { \u0026#34;host\u0026#34;: host, \u0026#34;interface\u0026#34;: interface }, \u0026#34;fields\u0026#34;: { \u0026#34;bytes_sent\u0026#34;: bytes_sent, \u0026#34;bytes_recv\u0026#34;: bytes_recv, \u0026#34;packets_sent\u0026#34;: packets_sent, \u0026#34;packets_recv\u0026#34;: packets_recv }, \u0026#34;time\u0026#34;: current_time.isoformat() } points.append(point) # ç£ç›˜ä½¿ç”¨ç‡æ•°æ® for disk in [\u0026#39;/dev/sda1\u0026#39;, \u0026#39;/dev/sda2\u0026#39;]: total_space = random.randint(100, 1000) * 1024 * 1024 * 1024 # GB used_space = random.uniform(0.2, 0.9) * total_space available_space = total_space - used_space point = { \u0026#34;measurement\u0026#34;: \u0026#34;disk_usage\u0026#34;, \u0026#34;tags\u0026#34;: { \u0026#34;host\u0026#34;: host, \u0026#34;device\u0026#34;: disk, \u0026#34;fstype\u0026#34;: \u0026#34;ext4\u0026#34; }, \u0026#34;fields\u0026#34;: { \u0026#34;usage_percent\u0026#34;: (used_space / total_space) * 100, \u0026#34;used_bytes\u0026#34;: int(used_space), \u0026#34;available_bytes\u0026#34;: int(available_space), \u0026#34;total_bytes\u0026#34;: int(total_space) }, \u0026#34;time\u0026#34;: current_time.isoformat() } points.append(point) # æ‰¹é‡å†™å…¥æ•°æ® if len(points) \u0026gt;= batch_size: try: self.client.write_points(points) print(f\u0026#34;å·²å†™å…¥ {len(points)} ä¸ªæ•°æ®ç‚¹ï¼Œæ—¶é—´: {current_time}\u0026#34;) points = [] except Exception as e: print(f\u0026#34;å†™å…¥æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}\u0026#34;) current_time += timedelta(minutes=1) # å†™å…¥å‰©ä½™æ•°æ® if points: try: self.client.write_points(points) print(f\u0026#34;å·²å†™å…¥æœ€å {len(points)} ä¸ªæ•°æ®ç‚¹\u0026#34;) except Exception as e: print(f\u0026#34;å†™å…¥æœ€åæ‰¹æ¬¡æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}\u0026#34;) def optimize_database_performance(self, database_name: str): \u0026#34;\u0026#34;\u0026#34;ä¼˜åŒ–æ•°æ®åº“æ€§èƒ½\u0026#34;\u0026#34;\u0026#34; optimization_queries = [ # åˆ›å»ºç´¢å¼• \u0026#39;CREATE INDEX ON \u0026#34;cpu_usage\u0026#34; (\u0026#34;host\u0026#34;)\u0026#39;, \u0026#39;CREATE INDEX ON \u0026#34;memory_usage\u0026#34; (\u0026#34;host\u0026#34;)\u0026#39;, \u0026#39;CREATE INDEX ON \u0026#34;network_usage\u0026#34; (\u0026#34;host\u0026#34;, \u0026#34;interface\u0026#34;)\u0026#39;, \u0026#39;CREATE INDEX ON \u0026#34;disk_usage\u0026#34; (\u0026#34;host\u0026#34;, \u0026#34;device\u0026#34;)\u0026#39;, # è®¾ç½®åˆ†ç‰‡ç»„æŒç»­æ—¶é—´ \u0026#39;ALTER RETENTION POLICY \u0026#34;realtime\u0026#34; ON \u0026#34;{}\u0026#34; SHARD DURATION 1h\u0026#39;.format(database_name), \u0026#39;ALTER RETENTION POLICY \u0026#34;short_term\u0026#34; ON \u0026#34;{}\u0026#34; SHARD DURATION 1d\u0026#39;.format(database_name), \u0026#39;ALTER RETENTION POLICY \u0026#34;medium_term\u0026#34; ON \u0026#34;{}\u0026#34; SHARD DURATION 7d\u0026#39;.format(database_name), \u0026#39;ALTER RETENTION POLICY \u0026#34;long_term\u0026#34; ON \u0026#34;{}\u0026#34; SHARD DURATION 30d\u0026#39;.format(database_name), ] for query in optimization_queries: try: self.client.query(query) print(f\u0026#34;æ‰§è¡Œä¼˜åŒ–æŸ¥è¯¢: {query}\u0026#34;) except Exception as e: print(f\u0026#34;æ‰§è¡Œä¼˜åŒ–æŸ¥è¯¢æ—¶å‘ç”Ÿé”™è¯¯: {e}\u0026#34;) def main(): # åˆå§‹åŒ–æ•°æ®å»ºæ¨¡å™¨ modeler = InfluxDBDataModeler() database_name = \u0026#34;monitoring_demo\u0026#34; # åˆ›å»ºæ•°æ®åº“æ¶æ„ modeler.create_database_schema(database_name) # åˆ›å»ºè¿ç»­æŸ¥è¯¢ modeler.create_continuous_queries(database_name) # ç”Ÿæˆç¤ºä¾‹æ•°æ® print(\u0026#34;å¼€å§‹ç”Ÿæˆç¤ºä¾‹æ•°æ®...\u0026#34;) modeler.generate_sample_data(database_name, num_hosts=5, duration_hours=2) # ä¼˜åŒ–æ•°æ®åº“æ€§èƒ½ modeler.optimize_database_performance(database_name) print(\u0026#34;æ•°æ®å»ºæ¨¡å®Œæˆ!\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() æŸ¥è¯¢ä¼˜åŒ–ä¸æ€§èƒ½è°ƒä¼˜ 1. æŸ¥è¯¢æ€§èƒ½åˆ†æ #!/usr/bin/env python3 # scripts/influxdb_query_optimizer.py import time import json import statistics from datetime import datetime, timedelta from influxdb import InfluxDBClient from typing import List, Dict, Any, Tuple class InfluxDBQueryOptimizer: def __init__(self, host=\u0026#39;localhost\u0026#39;, port=8086, username=\u0026#39;admin\u0026#39;, password=\u0026#39;admin\u0026#39;): self.client = InfluxDBClient(host=host, port=port, username=username, password=password) def analyze_query_performance(self, database_name: str, queries: List[str], iterations: int = 5) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;åˆ†ææŸ¥è¯¢æ€§èƒ½\u0026#34;\u0026#34;\u0026#34; self.client.switch_database(database_name) results = {} for i, query in enumerate(queries): query_name = f\u0026#34;query_{i+1}\u0026#34; execution_times = [] print(f\u0026#34;åˆ†ææŸ¥è¯¢ {query_name}: {query[:100]}...\u0026#34;) for iteration in range(iterations): start_time = time.time() try: result = self.client.query(query) execution_time = time.time() - start_time execution_times.append(execution_time) # è·å–ç»“æœé›†å¤§å° result_count = sum(len(list(series.get(\u0026#39;values\u0026#39;, []))) for series in result.raw.get(\u0026#39;series\u0026#39;, [])) except Exception as e: print(f\u0026#34;æŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}\u0026#34;) execution_times.append(float(\u0026#39;inf\u0026#39;)) result_count = 0 time.sleep(0.1) # é¿å…è¿‡äºé¢‘ç¹çš„æŸ¥è¯¢ # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ valid_times = [t for t in execution_times if t != float(\u0026#39;inf\u0026#39;)] if valid_times: results[query_name] = { \u0026#39;query\u0026#39;: query, \u0026#39;avg_time\u0026#39;: statistics.mean(valid_times), \u0026#39;min_time\u0026#39;: min(valid_times), \u0026#39;max_time\u0026#39;: max(valid_times), \u0026#39;median_time\u0026#39;: statistics.median(valid_times), \u0026#39;std_dev\u0026#39;: statistics.stdev(valid_times) if len(valid_times) \u0026gt; 1 else 0, \u0026#39;result_count\u0026#39;: result_count, \u0026#39;success_rate\u0026#39;: len(valid_times) / iterations * 100 } else: results[query_name] = { \u0026#39;query\u0026#39;: query, \u0026#39;avg_time\u0026#39;: float(\u0026#39;inf\u0026#39;), \u0026#39;min_time\u0026#39;: float(\u0026#39;inf\u0026#39;), \u0026#39;max_time\u0026#39;: float(\u0026#39;inf\u0026#39;), \u0026#39;median_time\u0026#39;: float(\u0026#39;inf\u0026#39;), \u0026#39;std_dev\u0026#39;: 0, \u0026#39;result_count\u0026#39;: 0, \u0026#39;success_rate\u0026#39;: 0 } return results def generate_optimization_recommendations(self, analysis_results: Dict[str, Any]) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆä¼˜åŒ–å»ºè®®\u0026#34;\u0026#34;\u0026#34; recommendations = [] for query_name, stats in analysis_results.items(): avg_time = stats[\u0026#39;avg_time\u0026#39;] query = stats[\u0026#39;query\u0026#39;] # æ…¢æŸ¥è¯¢æ£€æµ‹ if avg_time \u0026gt; 1.0: # è¶…è¿‡1ç§’ recommendations.append(f\u0026#34;æŸ¥è¯¢ {query_name} æ‰§è¡Œæ—¶é—´è¿‡é•¿ ({avg_time:.2f}s)ï¼Œå»ºè®®ä¼˜åŒ–\u0026#34;) # åˆ†ææŸ¥è¯¢æ¨¡å¼å¹¶æä¾›å…·ä½“å»ºè®® if \u0026#39;GROUP BY time(\u0026#39; not in query: recommendations.append(f\u0026#34; - è€ƒè™‘æ·»åŠ æ—¶é—´åˆ†ç»„ä»¥å‡å°‘æ•°æ®ç‚¹æ•°é‡\u0026#34;) if \u0026#39;WHERE time \u0026gt;\u0026#39; not in query and \u0026#39;WHERE time \u0026gt;=\u0026#39; not in query: recommendations.append(f\u0026#34; - æ·»åŠ æ—¶é—´èŒƒå›´è¿‡æ»¤ä»¥é™åˆ¶æŸ¥è¯¢èŒƒå›´\u0026#34;) if \u0026#39;LIMIT\u0026#39; not in query: recommendations.append(f\u0026#34; - æ·»åŠ LIMITå­å¥é™åˆ¶è¿”å›ç»“æœæ•°é‡\u0026#34;) if \u0026#39;*\u0026#39; in query: recommendations.append(f\u0026#34; - é¿å…ä½¿ç”¨SELECT *ï¼Œæ˜ç¡®æŒ‡å®šéœ€è¦çš„å­—æ®µ\u0026#34;) # æˆåŠŸç‡æ£€æµ‹ if stats[\u0026#39;success_rate\u0026#39;] \u0026lt; 100: recommendations.append(f\u0026#34;æŸ¥è¯¢ {query_name} æˆåŠŸç‡è¾ƒä½ ({stats[\u0026#39;success_rate\u0026#39;]:.1f}%)ï¼Œéœ€è¦æ£€æŸ¥æŸ¥è¯¢è¯­æ³•\u0026#34;) # ç»“æœé›†å¤§å°æ£€æµ‹ if stats[\u0026#39;result_count\u0026#39;] \u0026gt; 10000: recommendations.append(f\u0026#34;æŸ¥è¯¢ {query_name} è¿”å›ç»“æœè¿‡å¤š ({stats[\u0026#39;result_count\u0026#39;]} æ¡)ï¼Œå»ºè®®æ·»åŠ è¿‡æ»¤æ¡ä»¶\u0026#34;) return recommendations def benchmark_different_query_patterns(self, database_name: str) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;åŸºå‡†æµ‹è¯•ä¸åŒçš„æŸ¥è¯¢æ¨¡å¼\u0026#34;\u0026#34;\u0026#34; self.client.switch_database(database_name) # å®šä¹‰ä¸åŒçš„æŸ¥è¯¢æ¨¡å¼ query_patterns = { \u0026#39;basic_select\u0026#39;: \u0026#39;SELECT * FROM \u0026#34;cpu_usage\u0026#34; LIMIT 1000\u0026#39;, \u0026#39;time_filtered\u0026#39;: \u0026#39;SELECT * FROM \u0026#34;cpu_usage\u0026#34; WHERE time \u0026gt;= now() - 1h LIMIT 1000\u0026#39;, \u0026#39;tag_filtered\u0026#39;: \u0026#39;SELECT * FROM \u0026#34;cpu_usage\u0026#34; WHERE \u0026#34;host\u0026#34; = \\\u0026#39;server-001\\\u0026#39; LIMIT 1000\u0026#39;, \u0026#39;field_specific\u0026#39;: \u0026#39;SELECT \u0026#34;usage_percent\u0026#34; FROM \u0026#34;cpu_usage\u0026#34; WHERE time \u0026gt;= now() - 1h LIMIT 1000\u0026#39;, \u0026#39;aggregated\u0026#39;: \u0026#39;SELECT mean(\u0026#34;usage_percent\u0026#34;) FROM \u0026#34;cpu_usage\u0026#34; WHERE time \u0026gt;= now() - 1h GROUP BY time(5m)\u0026#39;, \u0026#39;multi_tag_filter\u0026#39;: \u0026#39;SELECT mean(\u0026#34;usage_percent\u0026#34;) FROM \u0026#34;cpu_usage\u0026#34; WHERE \u0026#34;host\u0026#34; = \\\u0026#39;server-001\\\u0026#39; AND \u0026#34;cpu\u0026#34; = \\\u0026#39;cpu0\\\u0026#39; AND time \u0026gt;= now() - 1h GROUP BY time(1m)\u0026#39;, \u0026#39;complex_aggregation\u0026#39;: \u0026#39;\u0026#39;\u0026#39; SELECT mean(\u0026#34;usage_percent\u0026#34;) as \u0026#34;avg_cpu\u0026#34;, max(\u0026#34;usage_percent\u0026#34;) as \u0026#34;max_cpu\u0026#34;, min(\u0026#34;usage_percent\u0026#34;) as \u0026#34;min_cpu\u0026#34; FROM \u0026#34;cpu_usage\u0026#34; WHERE time \u0026gt;= now() - 6h GROUP BY time(10m), \u0026#34;host\u0026#34; \u0026#39;\u0026#39;\u0026#39;, \u0026#39;subquery\u0026#39;: \u0026#39;\u0026#39;\u0026#39; SELECT mean(\u0026#34;avg_cpu\u0026#34;) FROM ( SELECT mean(\u0026#34;usage_percent\u0026#34;) as \u0026#34;avg_cpu\u0026#34; FROM \u0026#34;cpu_usage\u0026#34; WHERE time \u0026gt;= now() - 1h GROUP BY time(1m), \u0026#34;host\u0026#34; ) GROUP BY time(10m) \u0026#39;\u0026#39;\u0026#39; } # æ‰§è¡ŒåŸºå‡†æµ‹è¯• results = self.analyze_query_performance(database_name, list(query_patterns.values()), iterations=3) # é‡æ–°æ˜ å°„ç»“æœ benchmark_results = {} for i, (pattern_name, query) in enumerate(query_patterns.items()): query_key = f\u0026#34;query_{i+1}\u0026#34; if query_key in results: benchmark_results[pattern_name] = results[query_key] return benchmark_results def test_index_effectiveness(self, database_name: str) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;æµ‹è¯•ç´¢å¼•æ•ˆæœ\u0026#34;\u0026#34;\u0026#34; self.client.switch_database(database_name) # æµ‹è¯•æŸ¥è¯¢ï¼ˆå¸¦å’Œä¸å¸¦ç´¢å¼•ï¼‰ test_queries = [ { \u0026#39;name\u0026#39;: \u0026#39;host_filter\u0026#39;, \u0026#39;query\u0026#39;: \u0026#39;SELECT * FROM \u0026#34;cpu_usage\u0026#34; WHERE \u0026#34;host\u0026#34; = \\\u0026#39;server-001\\\u0026#39; AND time \u0026gt;= now() - 1h\u0026#39;, \u0026#39;description\u0026#39;: \u0026#39;æŒ‰ä¸»æœºè¿‡æ»¤æŸ¥è¯¢\u0026#39; }, { \u0026#39;name\u0026#39;: \u0026#39;host_cpu_filter\u0026#39;, \u0026#39;query\u0026#39;: \u0026#39;SELECT * FROM \u0026#34;cpu_usage\u0026#34; WHERE \u0026#34;host\u0026#34; = \\\u0026#39;server-001\\\u0026#39; AND \u0026#34;cpu\u0026#34; = \\\u0026#39;cpu0\\\u0026#39; AND time \u0026gt;= now() - 1h\u0026#39;, \u0026#39;description\u0026#39;: \u0026#39;æŒ‰ä¸»æœºå’ŒCPUè¿‡æ»¤æŸ¥è¯¢\u0026#39; }, { \u0026#39;name\u0026#39;: \u0026#39;time_range_only\u0026#39;, \u0026#39;query\u0026#39;: \u0026#39;SELECT * FROM \u0026#34;cpu_usage\u0026#34; WHERE time \u0026gt;= now() - 1h\u0026#39;, \u0026#39;description\u0026#39;: \u0026#39;ä»…æ—¶é—´èŒƒå›´æŸ¥è¯¢\u0026#39; } ] results = {} for test in test_queries: print(f\u0026#34;æµ‹è¯•æŸ¥è¯¢: {test[\u0026#39;description\u0026#39;]}\u0026#34;) # æ‰§è¡ŒæŸ¥è¯¢å¹¶æµ‹é‡æ€§èƒ½ start_time = time.time() try: result = self.client.query(test[\u0026#39;query\u0026#39;]) execution_time = time.time() - start_time result_count = sum(len(list(series.get(\u0026#39;values\u0026#39;, []))) for series in result.raw.get(\u0026#39;series\u0026#39;, [])) results[test[\u0026#39;name\u0026#39;]] = { \u0026#39;query\u0026#39;: test[\u0026#39;query\u0026#39;], \u0026#39;description\u0026#39;: test[\u0026#39;description\u0026#39;], \u0026#39;execution_time\u0026#39;: execution_time, \u0026#39;result_count\u0026#39;: result_count, \u0026#39;success\u0026#39;: True } except Exception as e: results[test[\u0026#39;name\u0026#39;]] = { \u0026#39;query\u0026#39;: test[\u0026#39;query\u0026#39;], \u0026#39;description\u0026#39;: test[\u0026#39;description\u0026#39;], \u0026#39;execution_time\u0026#39;: float(\u0026#39;inf\u0026#39;), \u0026#39;result_count\u0026#39;: 0, \u0026#39;success\u0026#39;: False, \u0026#39;error\u0026#39;: str(e) } return results def generate_performance_report(self, database_name: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;å¼€å§‹ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š...\u0026#34;) # æ‰§è¡Œå„ç§æ€§èƒ½æµ‹è¯• benchmark_results = self.benchmark_different_query_patterns(database_name) index_results = self.test_index_effectiveness(database_name) recommendations = self.generate_optimization_recommendations(benchmark_results) # ç”ŸæˆHTMLæŠ¥å‘Š report_html = f\u0026#34;\u0026#34;\u0026#34; \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;InfluxDBæ€§èƒ½åˆ†ææŠ¥å‘Š\u0026lt;/title\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;style\u0026gt; body {{ font-family: Arial, sans-serif; margin: 20px; }} .header {{ background-color: #f0f0f0; padding: 20px; border-radius: 5px; }} .section {{ margin: 20px 0; }} .metric {{ display: inline-block; margin: 10px; padding: 15px; background-color: #e8f4f8; border-radius: 5px; }} table {{ border-collapse: collapse; width: 100%; margin: 10px 0; }} th, td {{ border: 1px solid #ddd; padding: 8px; text-align: left; }} th {{ background-color: #f2f2f2; }} .recommendation {{ background-color: #fff3cd; padding: 10px; margin: 5px 0; border-radius: 5px; }} .good {{ color: green; }} .warning {{ color: orange; }} .error {{ color: red; }} \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;header\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;InfluxDBæ€§èƒ½åˆ†ææŠ¥å‘Š\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;æ•°æ®åº“: {database_name}\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;ç”Ÿæˆæ—¶é—´: {datetime.now().strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;)}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;section\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;æŸ¥è¯¢æ¨¡å¼åŸºå‡†æµ‹è¯•\u0026lt;/h2\u0026gt; \u0026lt;table\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;æŸ¥è¯¢æ¨¡å¼\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;å¹³å‡æ‰§è¡Œæ—¶é—´(s)\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;ç»“æœæ•°é‡\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;æ€§èƒ½è¯„çº§\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026#34;\u0026#34;\u0026#34; for pattern_name, stats in benchmark_results.items(): avg_time = stats[\u0026#39;avg_time\u0026#39;] result_count = stats[\u0026#39;result_count\u0026#39;] # æ€§èƒ½è¯„çº§ if avg_time \u0026lt; 0.1: rating = \u0026#39;\u0026lt;span class=\u0026#34;good\u0026#34;\u0026gt;ä¼˜ç§€\u0026lt;/span\u0026gt;\u0026#39; elif avg_time \u0026lt; 0.5: rating = \u0026#39;\u0026lt;span class=\u0026#34;warning\u0026#34;\u0026gt;è‰¯å¥½\u0026lt;/span\u0026gt;\u0026#39; else: rating = \u0026#39;\u0026lt;span class=\u0026#34;error\u0026#34;\u0026gt;éœ€è¦ä¼˜åŒ–\u0026lt;/span\u0026gt;\u0026#39; report_html += f\u0026#34;\u0026#34;\u0026#34; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;{pattern_name}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;{avg_time:.3f}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;{result_count}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;{rating}\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026#34;\u0026#34;\u0026#34; report_html += \u0026#34;\u0026#34;\u0026#34; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;section\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;ç´¢å¼•æ•ˆæœæµ‹è¯•\u0026lt;/h2\u0026gt; \u0026lt;table\u0026gt; \u0026lt;tr\u0026gt; \u0026lt;th\u0026gt;æµ‹è¯•åç§°\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;æè¿°\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;æ‰§è¡Œæ—¶é—´(s)\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;ç»“æœæ•°é‡\u0026lt;/th\u0026gt; \u0026lt;th\u0026gt;çŠ¶æ€\u0026lt;/th\u0026gt; \u0026lt;/tr\u0026gt; \u0026#34;\u0026#34;\u0026#34; for test_name, stats in index_results.items(): status = \u0026#39;\u0026lt;span class=\u0026#34;good\u0026#34;\u0026gt;æˆåŠŸ\u0026lt;/span\u0026gt;\u0026#39; if stats[\u0026#39;success\u0026#39;] else \u0026#39;\u0026lt;span class=\u0026#34;error\u0026#34;\u0026gt;å¤±è´¥\u0026lt;/span\u0026gt;\u0026#39; report_html += f\u0026#34;\u0026#34;\u0026#34; \u0026lt;tr\u0026gt; \u0026lt;td\u0026gt;{test_name}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;{stats[\u0026#39;description\u0026#39;]}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;{stats[\u0026#39;execution_time\u0026#39;]:.3f}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;{stats[\u0026#39;result_count\u0026#39;]}\u0026lt;/td\u0026gt; \u0026lt;td\u0026gt;{status}\u0026lt;/td\u0026gt; \u0026lt;/tr\u0026gt; \u0026#34;\u0026#34;\u0026#34; report_html += \u0026#34;\u0026#34;\u0026#34; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;section\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;ä¼˜åŒ–å»ºè®®\u0026lt;/h2\u0026gt; \u0026#34;\u0026#34;\u0026#34; for recommendation in recommendations: report_html += f\u0026#39;\u0026lt;div class=\u0026#34;recommendation\u0026#34;\u0026gt;{recommendation}\u0026lt;/div\u0026gt;\u0026#39; report_html += \u0026#34;\u0026#34;\u0026#34; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;section\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ\u0026lt;/h2\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;strong\u0026gt;æ—¶é—´èŒƒå›´è¿‡æ»¤\u0026lt;/strong\u0026gt;: å§‹ç»ˆåœ¨æŸ¥è¯¢ä¸­åŒ…å«æ—¶é—´èŒƒå›´è¿‡æ»¤æ¡ä»¶\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;strong\u0026gt;æ ‡ç­¾ç´¢å¼•\u0026lt;/strong\u0026gt;: åˆç†ä½¿ç”¨æ ‡ç­¾ï¼Œé¿å…é«˜åŸºæ•°æ ‡ç­¾\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;strong\u0026gt;å­—æ®µé€‰æ‹©\u0026lt;/strong\u0026gt;: åªé€‰æ‹©éœ€è¦çš„å­—æ®µï¼Œé¿å…ä½¿ç”¨SELECT *\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;strong\u0026gt;èšåˆæŸ¥è¯¢\u0026lt;/strong\u0026gt;: ä½¿ç”¨GROUP BY time()è¿›è¡Œæ—¶é—´èšåˆ\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;strong\u0026gt;è¿ç»­æŸ¥è¯¢\u0026lt;/strong\u0026gt;: ä½¿ç”¨è¿ç»­æŸ¥è¯¢è¿›è¡Œæ•°æ®é¢„èšåˆ\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;strong\u0026gt;ä¿ç•™ç­–ç•¥\u0026lt;/strong\u0026gt;: è®¾ç½®åˆé€‚çš„æ•°æ®ä¿ç•™ç­–ç•¥\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;strong\u0026gt;åˆ†ç‰‡é…ç½®\u0026lt;/strong\u0026gt;: æ ¹æ®æ•°æ®é‡è°ƒæ•´åˆ†ç‰‡æŒç»­æ—¶é—´\u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; \u0026#34;\u0026#34;\u0026#34; # ä¿å­˜æŠ¥å‘Š report_filename = f\u0026#34;influxdb_performance_report_{datetime.now().strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;)}.html\u0026#34; with open(report_filename, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: f.write(report_html) print(f\u0026#34;æ€§èƒ½åˆ†ææŠ¥å‘Šå·²ç”Ÿæˆ: {report_filename}\u0026#34;) return report_filename def main(): optimizer = InfluxDBQueryOptimizer() database_name = \u0026#34;monitoring_demo\u0026#34; # ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š report_file = optimizer.generate_performance_report(database_name) print(f\u0026#34;æ€§èƒ½åˆ†æå®Œæˆï¼ŒæŠ¥å‘Šæ–‡ä»¶: {report_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() 2. é›†ç¾¤é…ç½®ä¼˜åŒ– # config/influxdb.conf # InfluxDBä¼ä¸šçº§é…ç½®æ–‡ä»¶ [meta] # å…ƒæ•°æ®ç›®å½• dir = \u0026#34;/var/lib/influxdb/meta\u0026#34; # ä¿ç•™ç­–ç•¥è‡ªåŠ¨åˆ›å»º retention-autocreate = true # æ—¥å¿—è®°å½• logging-enabled = true [data] # æ•°æ®ç›®å½• dir = \u0026#34;/var/lib/influxdb/data\u0026#34; # WALç›®å½• wal-dir = \u0026#34;/var/lib/influxdb/wal\u0026#34; # æŸ¥è¯¢æ—¥å¿— query-log-enabled = true # ç¼“å­˜é…ç½® cache-max-memory-size = \u0026#34;1g\u0026#34; cache-snapshot-memory-size = \u0026#34;25m\u0026#34; cache-snapshot-write-cold-duration = \u0026#34;10m\u0026#34; # å‹ç¼©é…ç½® compact-full-write-cold-duration = \u0026#34;4h\u0026#34; compact-throughput = \u0026#34;48m\u0026#34; compact-throughput-burst = \u0026#34;48m\u0026#34; # TSMæ–‡ä»¶é…ç½® tsm-use-madv-willneed = false # æœ€å¤§å¹¶å‘å‹ç¼©æ•° max-concurrent-compactions = 0 # æœ€å¤§ç´¢å¼•æ—¥å¿—æ–‡ä»¶å¤§å° max-index-log-file-size = \u0026#34;1m\u0026#34; # æ‰¹é‡å†™å…¥é…ç½® max-points-per-block = 1000 [coordinator] # å†™å…¥è¶…æ—¶ write-timeout = \u0026#34;10s\u0026#34; # æœ€å¤§å¹¶å‘æŸ¥è¯¢æ•° max-concurrent-queries = 0 # æŸ¥è¯¢è¶…æ—¶ query-timeout = \u0026#34;0s\u0026#34; # æ—¥å¿—æŸ¥è¯¢ log-queries-after = \u0026#34;0s\u0026#34; # æœ€å¤§é€‰æ‹©ç‚¹æ•° max-select-point = 0 # æœ€å¤§é€‰æ‹©ç³»åˆ—æ•° max-select-series = 0 # æœ€å¤§é€‰æ‹©æ¡¶æ•° max-select-buckets = 0 [retention] # å¯ç”¨ä¿ç•™ç­–ç•¥æœåŠ¡ enabled = true # æ£€æŸ¥é—´éš” check-interval = \u0026#34;30m\u0026#34; [shard-precreation] # å¯ç”¨åˆ†ç‰‡é¢„åˆ›å»º enabled = true # æ£€æŸ¥é—´éš” check-interval = \u0026#34;10m\u0026#34; # æå‰åˆ›å»ºæ—¶é—´ advance-period = \u0026#34;30m\u0026#34; [monitor] # å¯ç”¨ç›‘æ§ store-enabled = true # ç›‘æ§æ•°æ®åº“ store-database = \u0026#34;_internal\u0026#34; # ç›‘æ§é—´éš” store-interval = \u0026#34;10s\u0026#34; [subscriber] # å¯ç”¨è®¢é˜…è€…æœåŠ¡ enabled = true # HTTPè¶…æ—¶ http-timeout = \u0026#34;30s\u0026#34; # ä¸å®‰å…¨SSL insecure-skip-verify = false # CAè¯ä¹¦ ca-certs = \u0026#34;\u0026#34; # å†™å…¥å¹¶å‘æ•° write-concurrency = 40 # å†™å…¥ç¼“å†²åŒºå¤§å° write-buffer-size = 1000 [http] # å¯ç”¨HTTPæœåŠ¡ enabled = true # ç»‘å®šåœ°å€ bind-address = \u0026#34;:8086\u0026#34; # è®¤è¯å¯ç”¨ auth-enabled = false # æ—¥å¿—å¯ç”¨ log-enabled = true # å†™å…¥è·Ÿè¸ª write-tracing = false # åˆ†é¡µå¯ç”¨ pprof-enabled = true # è°ƒè¯•åˆ†é¡µå¯ç”¨ debug-pprof-enabled = false # HTTPSè¯ä¹¦ https-certificate = \u0026#34;/etc/ssl/influxdb.pem\u0026#34; # HTTPSç§é’¥ https-private-key = \u0026#34;/etc/ssl/influxdb-key.pem\u0026#34; # æœ€å¤§è¡Œé™åˆ¶ max-row-limit = 0 # æœ€å¤§è¿æ¥é™åˆ¶ max-connection-limit = 0 # å…±äº«å¯†é’¥ shared-secret = \u0026#34;\u0026#34; # é¢†åŸŸ realm = \u0026#34;InfluxDB\u0026#34; # Unixå¥—æ¥å­—å¯ç”¨ unix-socket-enabled = false # ç»‘å®šå¥—æ¥å­— bind-socket = \u0026#34;/var/run/influxdb.sock\u0026#34; # æœ€å¤§ä½“ç§¯å¤§å° max-body-size = 25000000 # è®¿é—®æ—¥å¿—è·¯å¾„ access-log-path = \u0026#34;\u0026#34; # æœ€å¤§å¹¶å‘å†™å…¥è¯·æ±‚ max-concurrent-write-limit = 0 # æœ€å¤§å…¥é˜Ÿå†™å…¥è¯·æ±‚ max-enqueued-write-limit = 0 # å…¥é˜Ÿå†™å…¥è¶…æ—¶ enqueued-write-timeout = 30000000000 [logging] # æ—¥å¿—æ ¼å¼ format = \u0026#34;auto\u0026#34; # æ—¥å¿—çº§åˆ« level = \u0026#34;info\u0026#34; # æŠ‘åˆ¶logo suppress-logo = false [[graphite]] # å¯ç”¨Graphiteè¾“å…¥ enabled = false # ç»‘å®šåœ°å€ bind-address = \u0026#34;:2003\u0026#34; # æ•°æ®åº“ database = \u0026#34;graphite\u0026#34; # ä¿ç•™ç­–ç•¥ retention-policy = \u0026#34;\u0026#34; # åè®® protocol = \u0026#34;tcp\u0026#34; # æ‰¹é‡å¤§å° batch-size = 5000 # æ‰¹é‡æŒ‚èµ· batch-pending = 10 # æ‰¹é‡è¶…æ—¶ batch-timeout = \u0026#34;1s\u0026#34; # UDPè¯»å–ç¼“å†²åŒº udp-read-buffer = 0 # åˆ†éš”ç¬¦ separator = \u0026#34;.\u0026#34; [[collectd]] # å¯ç”¨collectdè¾“å…¥ enabled = false # ç»‘å®šåœ°å€ bind-address = \u0026#34;:25826\u0026#34; # æ•°æ®åº“ database = \u0026#34;collectd\u0026#34; # ä¿ç•™ç­–ç•¥ retention-policy = \u0026#34;\u0026#34; # æ‰¹é‡å¤§å° batch-size = 5000 # æ‰¹é‡æŒ‚èµ· batch-pending = 10 # æ‰¹é‡è¶…æ—¶ batch-timeout = \u0026#34;10s\u0026#34; # è¯»å–ç¼“å†²åŒº read-buffer = 0 [[opentsdb]] # å¯ç”¨OpenTSDBè¾“å…¥ enabled = false # ç»‘å®šåœ°å€ bind-address = \u0026#34;:4242\u0026#34; # æ•°æ®åº“ database = \u0026#34;opentsdb\u0026#34; # ä¿ç•™ç­–ç•¥ retention-policy = \u0026#34;\u0026#34; # ä¸€è‡´æ€§çº§åˆ« consistency-level = \u0026#34;one\u0026#34; # TLSå¯ç”¨ tls-enabled = false # è¯ä¹¦ certificate = \u0026#34;/etc/ssl/influxdb.pem\u0026#34; # æ‰¹é‡å¤§å° batch-size = 1000 # æ‰¹é‡æŒ‚èµ· batch-pending = 5 # æ‰¹é‡è¶…æ—¶ batch-timeout = \u0026#34;1s\u0026#34; # æ—¥å¿—ç‚¹é”™è¯¯ log-point-errors = true [[udp]] # å¯ç”¨UDPè¾“å…¥ enabled = false # ç»‘å®šåœ°å€ bind-address = \u0026#34;:8089\u0026#34; # æ•°æ®åº“ database = \u0026#34;udp\u0026#34; # ä¿ç•™ç­–ç•¥ retention-policy = \u0026#34;\u0026#34; # æ‰¹é‡å¤§å° batch-size = 5000 # æ‰¹é‡æŒ‚èµ· batch-pending = 10 # è¯»å–ç¼“å†²åŒº read-buffer = 0 # æ‰¹é‡è¶…æ—¶ batch-timeout = \u0026#34;1s\u0026#34; # ç²¾åº¦ precision = \u0026#34;\u0026#34; [continuous_queries] # å¯ç”¨è¿ç»­æŸ¥è¯¢ enabled = true # æ—¥å¿—å¯ç”¨ log-enabled = true # æŸ¥è¯¢ç»Ÿè®¡å¯ç”¨ query-stats-enabled = false # è¿è¡Œé—´éš” run-interval = \u0026#34;1s\u0026#34; é«˜å¯ç”¨éƒ¨ç½²æ¶æ„ 1. é›†ç¾¤éƒ¨ç½²è„šæœ¬ #!/bin/bash # scripts/influxdb_cluster_deploy.sh set -euo pipefail # é…ç½®å‚æ•° CLUSTER_NAME=\u0026#34;${CLUSTER_NAME:-influxdb-cluster}\u0026#34; NODE_COUNT=\u0026#34;${NODE_COUNT:-3}\u0026#34; DATA_DIR=\u0026#34;${DATA_DIR:-/opt/influxdb}\u0026#34; CONFIG_DIR=\u0026#34;${CONFIG_DIR:-/etc/influxdb}\u0026#34; LOG_DIR=\u0026#34;${LOG_DIR:-/var/log/influxdb}\u0026#34; # èŠ‚ç‚¹é…ç½® declare -A NODES=( [\u0026#34;node1\u0026#34;]=\u0026#34;10.0.1.10\u0026#34; [\u0026#34;node2\u0026#34;]=\u0026#34;10.0.1.11\u0026#34; [\u0026#34;node3\u0026#34;]=\u0026#34;10.0.1.12\u0026#34; ) # æ—¥å¿—å‡½æ•° log() { echo \u0026#34;[$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)] $1\u0026#34; \u0026gt;\u0026amp;2 } # æ£€æŸ¥ä¾èµ– check_dependencies() { local missing_deps=() for cmd in curl wget systemctl; do if ! command -v \u0026#34;$cmd\u0026#34; \u0026amp;\u0026gt; /dev/null; then missing_deps+=(\u0026#34;$cmd\u0026#34;) fi done if [ ${#missing_deps[@]} -gt 0 ]; then log \u0026#34;ç¼ºå°‘ä¾èµ–: ${missing_deps[*]}\u0026#34; exit 1 fi } # å®‰è£…InfluxDB install_influxdb() { local node_ip=\u0026#34;$1\u0026#34; log \u0026#34;åœ¨èŠ‚ç‚¹ $node_ip ä¸Šå®‰è£…InfluxDB\u0026#34; ssh root@\u0026#34;$node_ip\u0026#34; \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; # æ·»åŠ InfluxDBä»“åº“ curl -sL https://repos.influxdata.com/influxdb.key | apt-key add - echo \u0026#34;deb https://repos.influxdata.com/ubuntu focal stable\u0026#34; | tee /etc/apt/sources.list.d/influxdb.list # æ›´æ–°åŒ…åˆ—è¡¨å¹¶å®‰è£… apt-get update apt-get install -y influxdb # åˆ›å»ºå¿…è¦çš„ç›®å½• mkdir -p /var/lib/influxdb/{meta,data,wal} mkdir -p /var/log/influxdb mkdir -p /etc/influxdb # è®¾ç½®æƒé™ chown -R influxdb:influxdb /var/lib/influxdb chown -R influxdb:influxdb /var/log/influxdb chown -R influxdb:influxdb /etc/influxdb # åœæ­¢æœåŠ¡ï¼ˆç¨åé…ç½®åå¯åŠ¨ï¼‰ systemctl stop influxdb systemctl disable influxdb EOF log \u0026#34;èŠ‚ç‚¹ $node_ip ä¸Šçš„InfluxDBå®‰è£…å®Œæˆ\u0026#34; } # ç”ŸæˆèŠ‚ç‚¹é…ç½® generate_node_config() { local node_name=\u0026#34;$1\u0026#34; local node_ip=\u0026#34;$2\u0026#34; local is_meta_node=\u0026#34;$3\u0026#34; local config_file=\u0026#34;/tmp/influxdb_${node_name}.conf\u0026#34; cat \u0026gt; \u0026#34;$config_file\u0026#34; \u0026lt;\u0026lt; EOF # InfluxDBé›†ç¾¤èŠ‚ç‚¹é…ç½® - $node_name [meta] dir = \u0026#34;/var/lib/influxdb/meta\u0026#34; hostname = \u0026#34;$node_ip\u0026#34; bind-address = \u0026#34;$node_ip:8088\u0026#34; http-bind-address = \u0026#34;$node_ip:8091\u0026#34; retention-autocreate = true election-timeout = \u0026#34;1s\u0026#34; heartbeat-timeout = \u0026#34;1s\u0026#34; leader-lease-timeout = \u0026#34;500ms\u0026#34; commit-timeout = \u0026#34;50ms\u0026#34; cluster-tracing = false raft-promotion-enabled = true logging-enabled = true pprof-enabled = false lease-duration = \u0026#34;1m0s\u0026#34; [data] dir = \u0026#34;/var/lib/influxdb/data\u0026#34; wal-dir = \u0026#34;/var/lib/influxdb/wal\u0026#34; query-log-enabled = false cache-max-memory-size = \u0026#34;1g\u0026#34; cache-snapshot-memory-size = \u0026#34;25m\u0026#34; cache-snapshot-write-cold-duration = \u0026#34;10m\u0026#34; compact-full-write-cold-duration = \u0026#34;4h\u0026#34; max-concurrent-compactions = 0 compact-throughput = \u0026#34;48m\u0026#34; compact-throughput-burst = \u0026#34;48m\u0026#34; tsm-use-madv-willneed = false max-points-per-block = 1000 max-series-per-database = 1000000 max-values-per-tag = 100000 [cluster] shard-writer-timeout = \u0026#34;5s\u0026#34; write-timeout = \u0026#34;10s\u0026#34; max-concurrent-queries = 0 query-timeout = \u0026#34;0s\u0026#34; max-select-point = 0 max-select-series = 0 max-select-buckets = 0 [retention] enabled = true check-interval = \u0026#34;30m\u0026#34; [shard-precreation] enabled = true check-interval = \u0026#34;10m\u0026#34; advance-period = \u0026#34;30m\u0026#34; [admin] enabled = true bind-address = \u0026#34;$node_ip:8083\u0026#34; https-enabled = false [monitor] store-enabled = true store-database = \u0026#34;_internal\u0026#34; store-interval = \u0026#34;10s\u0026#34; [subscriber] enabled = true http-timeout = \u0026#34;30s\u0026#34; insecure-skip-verify = false ca-certs = \u0026#34;\u0026#34; write-concurrency = 40 write-buffer-size = 1000 [http] enabled = true bind-address = \u0026#34;$node_ip:8086\u0026#34; auth-enabled = false log-enabled = true write-tracing = false pprof-enabled = false https-enabled = false max-row-limit = 0 max-connection-limit = 0 shared-secret = \u0026#34;\u0026#34; realm = \u0026#34;InfluxDB\u0026#34; unix-socket-enabled = false bind-socket = \u0026#34;/var/run/influxdb.sock\u0026#34; max-body-size = 25000000 access-log-path = \u0026#34;\u0026#34; max-concurrent-write-limit = 0 max-enqueued-write-limit = 0 enqueued-write-timeout = 30000000000 [logging] format = \u0026#34;auto\u0026#34; level = \u0026#34;info\u0026#34; suppress-logo = false [continuous_queries] enabled = true log-enabled = true run-interval = \u0026#34;1s\u0026#34; EOF echo \u0026#34;$config_file\u0026#34; } # éƒ¨ç½²é…ç½®æ–‡ä»¶ deploy_config() { local node_name=\u0026#34;$1\u0026#34; local node_ip=\u0026#34;$2\u0026#34; local config_file=\u0026#34;$3\u0026#34; log \u0026#34;éƒ¨ç½²é…ç½®æ–‡ä»¶åˆ°èŠ‚ç‚¹ $node_name ($node_ip)\u0026#34; scp \u0026#34;$config_file\u0026#34; root@\u0026#34;$node_ip\u0026#34;:/etc/influxdb/influxdb.conf # è®¾ç½®æƒé™ ssh root@\u0026#34;$node_ip\u0026#34; \u0026#34;chown influxdb:influxdb /etc/influxdb/influxdb.conf\u0026#34; } # åˆ›å»ºsystemdæœåŠ¡æ–‡ä»¶ create_systemd_service() { local node_ip=\u0026#34;$1\u0026#34; ssh root@\u0026#34;$node_ip\u0026#34; \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; cat \u0026gt; /etc/systemd/system/influxdb.service \u0026lt;\u0026lt; \u0026#39;SERVICE_EOF\u0026#39; [Unit] Description=InfluxDB is an open-source, distributed, time series database Documentation=https://docs.influxdata.com/influxdb/ After=network-online.target Wants=network-online.target [Service] User=influxdb Group=influxdb LimitNOFILE=65536 EnvironmentFile=-/etc/default/influxdb ExecStart=/usr/bin/influxd -config /etc/influxdb/influxdb.conf KillMode=control-group Restart=on-failure [Install] WantedBy=multi-user.target SERVICE_EOF systemctl daemon-reload EOF } # å¯åŠ¨é›†ç¾¤ start_cluster() { log \u0026#34;å¯åŠ¨InfluxDBé›†ç¾¤\u0026#34; # é¦–å…ˆå¯åŠ¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ local first_node_ip=\u0026#34;${NODES[node1]}\u0026#34; log \u0026#34;å¯åŠ¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹: $first_node_ip\u0026#34; ssh root@\u0026#34;$first_node_ip\u0026#34; \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; systemctl enable influxdb systemctl start influxdb sleep 10 EOF # ç­‰å¾…ç¬¬ä¸€ä¸ªèŠ‚ç‚¹å®Œå…¨å¯åŠ¨ sleep 30 # å¯åŠ¨å…¶ä»–èŠ‚ç‚¹ for node_name in \u0026#34;${!NODES[@]}\u0026#34;; do if [ \u0026#34;$node_name\u0026#34; != \u0026#34;node1\u0026#34; ]; then local node_ip=\u0026#34;${NODES[$node_name]}\u0026#34; log \u0026#34;å¯åŠ¨èŠ‚ç‚¹: $node_name ($node_ip)\u0026#34; ssh root@\u0026#34;$node_ip\u0026#34; \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; systemctl enable influxdb systemctl start influxdb sleep 5 EOF fi done log \u0026#34;ç­‰å¾…æ‰€æœ‰èŠ‚ç‚¹å¯åŠ¨å®Œæˆ...\u0026#34; sleep 60 } # é…ç½®é›†ç¾¤ configure_cluster() { local first_node_ip=\u0026#34;${NODES[node1]}\u0026#34; log \u0026#34;é…ç½®InfluxDBé›†ç¾¤\u0026#34; # åœ¨ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ä¸Šæ‰§è¡Œé›†ç¾¤é…ç½® ssh root@\u0026#34;$first_node_ip\u0026#34; \u0026lt;\u0026lt; EOF # ç­‰å¾…æœåŠ¡å®Œå…¨å¯åŠ¨ sleep 30 # æ·»åŠ å…¶ä»–èŠ‚ç‚¹åˆ°é›†ç¾¤ for node_ip in ${NODES[node2]} ${NODES[node3]}; do echo \u0026#34;æ·»åŠ èŠ‚ç‚¹ \\$node_ip åˆ°é›†ç¾¤\u0026#34; influx -execute \u0026#34;CREATE DATA NODE \\$node_ip:8088\u0026#34; sleep 5 done # æ˜¾ç¤ºé›†ç¾¤çŠ¶æ€ influx -execute \u0026#34;SHOW DATA NODES\u0026#34; influx -execute \u0026#34;SHOW META NODES\u0026#34; EOF } # éªŒè¯é›†ç¾¤çŠ¶æ€ verify_cluster() { log \u0026#34;éªŒè¯é›†ç¾¤çŠ¶æ€\u0026#34; for node_name in \u0026#34;${!NODES[@]}\u0026#34;; do local node_ip=\u0026#34;${NODES[$node_name]}\u0026#34; log \u0026#34;æ£€æŸ¥èŠ‚ç‚¹ $node_name ($node_ip) çŠ¶æ€\u0026#34; # æ£€æŸ¥æœåŠ¡çŠ¶æ€ if ssh root@\u0026#34;$node_ip\u0026#34; \u0026#34;systemctl is-active influxdb\u0026#34; \u0026amp;\u0026gt; /dev/null; then log \u0026#34; âœ“ æœåŠ¡è¿è¡Œæ­£å¸¸\u0026#34; else log \u0026#34; âœ— æœåŠ¡æœªè¿è¡Œ\u0026#34; continue fi # æ£€æŸ¥HTTPæ¥å£ if curl -f \u0026#34;http://$node_ip:8086/ping\u0026#34; \u0026amp;\u0026gt; /dev/null; then log \u0026#34; âœ“ HTTPæ¥å£å“åº”æ­£å¸¸\u0026#34; else log \u0026#34; âœ— HTTPæ¥å£æ— å“åº”\u0026#34; fi # æ£€æŸ¥é›†ç¾¤çŠ¶æ€ local cluster_status=$(curl -s \u0026#34;http://$node_ip:8086/query?q=SHOW%20DATA%20NODES\u0026#34; | jq -r \u0026#39;.results[0].series[0].values | length\u0026#39; 2\u0026gt;/dev/null || echo \u0026#34;0\u0026#34;) log \u0026#34; é›†ç¾¤èŠ‚ç‚¹æ•°: $cluster_status\u0026#34; done } # åˆ›å»ºç›‘æ§è„šæœ¬ create_monitoring_script() { cat \u0026gt; /tmp/influxdb_cluster_monitor.sh \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; #!/bin/bash # InfluxDBé›†ç¾¤ç›‘æ§è„šæœ¬ NODES=(\u0026#34;10.0.1.10\u0026#34; \u0026#34;10.0.1.11\u0026#34; \u0026#34;10.0.1.12\u0026#34;) LOG_FILE=\u0026#34;/var/log/influxdb_cluster_monitor.log\u0026#34; log() { echo \u0026#34;[$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)] $1\u0026#34; | tee -a \u0026#34;$LOG_FILE\u0026#34; } check_node() { local node_ip=\u0026#34;$1\u0026#34; local status=\u0026#34;OK\u0026#34; local details=\u0026#34;\u0026#34; # æ£€æŸ¥HTTPæ¥å£ if ! curl -f \u0026#34;http://$node_ip:8086/ping\u0026#34; \u0026amp;\u0026gt; /dev/null; then status=\u0026#34;ERROR\u0026#34; details=\u0026#34;HTTPæ¥å£æ— å“åº”\u0026#34; return 1 fi # æ£€æŸ¥ç³»ç»Ÿèµ„æº local cpu_usage=$(ssh root@\u0026#34;$node_ip\u0026#34; \u0026#34;top -bn1 | grep \u0026#39;Cpu(s)\u0026#39; | awk \u0026#39;{print \\$2}\u0026#39; | cut -d\u0026#39;%\u0026#39; -f1\u0026#34; 2\u0026gt;/dev/null || echo \u0026#34;N/A\u0026#34;) local memory_usage=$(ssh root@\u0026#34;$node_ip\u0026#34; \u0026#34;free | grep Mem | awk \u0026#39;{printf \\\u0026#34;%.1f\\\u0026#34;, \\$3/\\$2 * 100.0}\u0026#39;\u0026#34; 2\u0026gt;/dev/null || echo \u0026#34;N/A\u0026#34;) local disk_usage=$(ssh root@\u0026#34;$node_ip\u0026#34; \u0026#34;df /var/lib/influxdb | tail -1 | awk \u0026#39;{print \\$5}\u0026#39; | cut -d\u0026#39;%\u0026#39; -f1\u0026#34; 2\u0026gt;/dev/null || echo \u0026#34;N/A\u0026#34;) details=\u0026#34;CPU: ${cpu_usage}%, Memory: ${memory_usage}%, Disk: ${disk_usage}%\u0026#34; log \u0026#34;èŠ‚ç‚¹ $node_ip: $status - $details\u0026#34; return 0 } main() { log \u0026#34;å¼€å§‹é›†ç¾¤å¥åº·æ£€æŸ¥\u0026#34; local failed_nodes=0 for node_ip in \u0026#34;${NODES[@]}\u0026#34;; do if ! check_node \u0026#34;$node_ip\u0026#34;; then ((failed_nodes++)) fi done if [ $failed_nodes -eq 0 ]; then log \u0026#34;é›†ç¾¤çŠ¶æ€: å¥åº· (æ‰€æœ‰èŠ‚ç‚¹æ­£å¸¸)\u0026#34; else log \u0026#34;é›†ç¾¤çŠ¶æ€: è­¦å‘Š ($failed_nodes ä¸ªèŠ‚ç‚¹å¼‚å¸¸)\u0026#34; fi log \u0026#34;å¥åº·æ£€æŸ¥å®Œæˆ\u0026#34; } main \u0026#34;$@\u0026#34; EOF # éƒ¨ç½²ç›‘æ§è„šæœ¬åˆ°æ‰€æœ‰èŠ‚ç‚¹ for node_name in \u0026#34;${!NODES[@]}\u0026#34;; do local node_ip=\u0026#34;${NODES[$node_name]}\u0026#34; scp /tmp/influxdb_cluster_monitor.sh root@\u0026#34;$node_ip\u0026#34;:/usr/local/bin/ ssh root@\u0026#34;$node_ip\u0026#34; \u0026#34;chmod +x /usr/local/bin/influxdb_cluster_monitor.sh\u0026#34; done log \u0026#34;ç›‘æ§è„šæœ¬éƒ¨ç½²å®Œæˆ\u0026#34; } # ä¸»å‡½æ•° main() { local action=\u0026#34;${1:-deploy}\u0026#34; case \u0026#34;$action\u0026#34; in \u0026#34;install\u0026#34;) check_dependencies for node_name in \u0026#34;${!NODES[@]}\u0026#34;; do install_influxdb \u0026#34;${NODES[$node_name]}\u0026#34; done ;; \u0026#34;configure\u0026#34;) for node_name in \u0026#34;${!NODES[@]}\u0026#34;; do local node_ip=\u0026#34;${NODES[$node_name]}\u0026#34; local config_file=$(generate_node_config \u0026#34;$node_name\u0026#34; \u0026#34;$node_ip\u0026#34; \u0026#34;true\u0026#34;) deploy_config \u0026#34;$node_name\u0026#34; \u0026#34;$node_ip\u0026#34; \u0026#34;$config_file\u0026#34; create_systemd_service \u0026#34;$node_ip\u0026#34; done ;; \u0026#34;start\u0026#34;) start_cluster configure_cluster ;; \u0026#34;verify\u0026#34;) verify_cluster ;; \u0026#34;monitor\u0026#34;) create_monitoring_script ;; \u0026#34;deploy\u0026#34;) check_dependencies # å®Œæ•´éƒ¨ç½²æµç¨‹ for node_name in \u0026#34;${!NODES[@]}\u0026#34;; do install_influxdb \u0026#34;${NODES[$node_name]}\u0026#34; done for node_name in \u0026#34;${!NODES[@]}\u0026#34;; do local node_ip=\u0026#34;${NODES[$node_name]}\u0026#34; local config_file=$(generate_node_config \u0026#34;$node_name\u0026#34; \u0026#34;$node_ip\u0026#34; \u0026#34;true\u0026#34;) deploy_config \u0026#34;$node_name\u0026#34; \u0026#34;$node_ip\u0026#34; \u0026#34;$config_file\u0026#34; create_systemd_service \u0026#34;$node_ip\u0026#34; done start_cluster configure_cluster verify_cluster create_monitoring_script ;; *) echo \u0026#34;ç”¨æ³•: $0 {install|configure|start|verify|monitor|deploy}\u0026#34; echo \u0026#34; install - å®‰è£…InfluxDB\u0026#34; echo \u0026#34; configure - é…ç½®é›†ç¾¤\u0026#34; echo \u0026#34; start - å¯åŠ¨é›†ç¾¤\u0026#34; echo \u0026#34; verify - éªŒè¯é›†ç¾¤çŠ¶æ€\u0026#34; echo \u0026#34; monitor - éƒ¨ç½²ç›‘æ§è„šæœ¬\u0026#34; echo \u0026#34; deploy - å®Œæ•´éƒ¨ç½²æµç¨‹\u0026#34; exit 1 ;; esac } main \u0026#34;$@\u0026#34; æ€»ç»“ æœ¬æ–‡å…¨é¢ä»‹ç»äº†InfluxDBæ—¶åºæ•°æ®åº“çš„è®¾è®¡ä¸å®è·µï¼Œæ¶µç›–äº†ä»¥ä¸‹æ ¸å¿ƒå†…å®¹ï¼š\n","content":"InfluxDBæ—¶åºæ•°æ®åº“è®¾è®¡ä¸å®è·µï¼šä»æ•°æ®å»ºæ¨¡åˆ°é«˜æ€§èƒ½æŸ¥è¯¢ä¼˜åŒ– å¼•è¨€ æ—¶åºæ•°æ®åº“åœ¨ç°ä»£æ•°æ®æ¶æ„ä¸­æ‰®æ¼”ç€è¶Šæ¥è¶Šé‡è¦çš„è§’è‰²ï¼Œç‰¹åˆ«æ˜¯åœ¨IoTã€ç›‘æ§ã€é‡‘èäº¤æ˜“ç­‰åœºæ™¯ä¸­ã€‚InfluxDBä½œä¸ºé¢†å…ˆçš„æ—¶åºæ•°æ®åº“ï¼Œä»¥å…¶é«˜æ€§èƒ½ã€æ˜“ç”¨æ€§å’Œä¸°å¯Œçš„ç”Ÿæ€ç³»ç»Ÿè€Œå¹¿å—æ¬¢è¿ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨InfluxDBçš„æ¶æ„è®¾è®¡ã€æ•°æ®å»ºæ¨¡æœ€ä½³å®è·µã€æŸ¥è¯¢ä¼˜åŒ–ç­–ç•¥å’Œä¼ä¸šçº§éƒ¨ç½²æ–¹æ¡ˆã€‚\nInfluxDBæ¶æ„æ¦‚è¿° æ ¸å¿ƒæ¦‚å¿µ graph TB subgraph \u0026amp;#34;InfluxDBæ¶æ„\u0026amp;#34; A[Database] --\u0026amp;gt; B[Measurement] B --\u0026amp;gt; C[Field] B --\u0026amp;gt; D[Tag] B --\u0026amp;gt; E[Timestamp] F[Series] --\u0026amp;gt; G[Tag Set] F --\u0026amp;gt; H[Measurement] I[Point] --\u0026amp;gt; J[Measurement] I --\u0026amp;gt; K[Tag Set] I --\u0026amp;gt; L[Field Set] I --\u0026amp;gt; M[Timestamp] end subgraph \u0026amp;#34;å­˜å‚¨å¼•æ“\u0026amp;#34; N[TSM â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["InfluxDB","æ—¶åºæ•°æ®åº“","æ•°æ®å»ºæ¨¡","æ€§èƒ½ä¼˜åŒ–","ç›‘æ§ç³»ç»Ÿ"],"categories":["æ•°æ®åº“"],"author":"å¼ ä¼Ÿ","readingTime":16,"wordCount":3331,"section":"posts","type":"posts","draft":false,"featured":false,"series":null},{"title":"Kubernetes Container Orchestration: A Comprehensive Guide to Cloud-Native Deployment","url":"https://www.dishuihengxin.com/posts/kubernetes-container-orchestration-guide/","summary":"Kubernetes Container Orchestration: A Comprehensive Guide to Cloud-Native Deployment Introduction Kubernetes has revolutionized the way we deploy, manage, and scale containerized applications in cloud-native environments. As the de facto standard for container orchestration, Kubernetes provides a robust platform for automating deployment, scaling, and operations of application containers across clusters of hosts.\nThis comprehensive guide explores Kubernetes fundamentals, advanced features, and best practices for building resilient, scalable cloud-native applications.\n1. Kubernetes Architecture Overview 1.1 Control Plane Components The Kubernetes control plane manages the overall state of the cluster and makes global decisions about the cluster.\n","content":"Kubernetes Container Orchestration: A Comprehensive Guide to Cloud-Native Deployment Introduction Kubernetes has revolutionized the way we deploy, manage, and scale containerized applications in cloud-native environments. As the de facto standard for container orchestration, Kubernetes provides a robust platform for automating deployment, scaling, and operations of application containers across clusters of hosts.\nThis comprehensive guide explores Kubernetes fundamentals, advanced features, and â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["Kubernetes","Container Orchestration","Cloud Native","DevOps","Microservices"],"categories":["Cloud Native"],"author":"Cloud Architecture Team","readingTime":19,"wordCount":3915,"section":"posts","type":"posts","draft":false,"featured":false,"series":null},{"title":"Kuberneteså®‰å…¨åŠ å›ºå®æˆ˜ï¼šä»é›†ç¾¤åˆ°å·¥ä½œè´Ÿè½½çš„å…¨æ–¹ä½é˜²æŠ¤","url":"https://www.dishuihengxin.com/posts/devops-kubernetes-security/","summary":"Kuberneteså®‰å…¨åŠ å›ºå®æˆ˜ï¼šä»é›†ç¾¤åˆ°å·¥ä½œè´Ÿè½½çš„å…¨æ–¹ä½é˜²æŠ¤ Kubernetesä½œä¸ºå®¹å™¨ç¼–æ’çš„äº‹å®æ ‡å‡†ï¼Œå…¶å®‰å…¨æ€§ç›´æ¥å½±å“æ•´ä¸ªäº‘åŸç”Ÿåº”ç”¨çš„å®‰å…¨æ€åŠ¿ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨Kuberneteså®‰å…¨åŠ å›ºçš„æœ€ä½³å®è·µï¼Œä»é›†ç¾¤å±‚é¢åˆ°å·¥ä½œè´Ÿè½½å±‚é¢çš„å…¨æ–¹ä½é˜²æŠ¤ç­–ç•¥ã€‚\nKuberneteså®‰å…¨æ¶æ„ å®‰å…¨å±‚æ¬¡æ¨¡å‹ graph TB A[ç‰©ç†/äº‘åŸºç¡€è®¾æ–½å®‰å…¨] --\u0026gt; B[é›†ç¾¤å®‰å…¨] B --\u0026gt; C[èŠ‚ç‚¹å®‰å…¨] C --\u0026gt; D[ç½‘ç»œå®‰å…¨] D --\u0026gt; E[å·¥ä½œè´Ÿè½½å®‰å…¨] E --\u0026gt; F[åº”ç”¨å®‰å…¨] B1[API Serverå®‰å…¨] --\u0026gt; B B2[etcdåŠ å¯†] --\u0026gt; B B3[è¯ä¹¦ç®¡ç†] --\u0026gt; B C1[èŠ‚ç‚¹åŠ å›º] --\u0026gt; C C2[å®¹å™¨è¿è¡Œæ—¶å®‰å…¨] --\u0026gt; C C3[é•œåƒå®‰å…¨] --\u0026gt; C D1[ç½‘ç»œç­–ç•¥] --\u0026gt; D D2[æœåŠ¡ç½‘æ ¼] --\u0026gt; D D3[å…¥å£æ§åˆ¶] --\u0026gt; D E1[Podå®‰å…¨æ ‡å‡†] --\u0026gt; E E2[RBAC] --\u0026gt; E E3[å‡†å…¥æ§åˆ¶] --\u0026gt; E F1[åº”ç”¨ä»£ç å®‰å…¨] --\u0026gt; F F2[å¯†é’¥ç®¡ç†] --\u0026gt; F F3[è¿è¡Œæ—¶ä¿æŠ¤] --\u0026gt; F å¨èƒæ¨¡å‹åˆ†æ å¨èƒç±»åˆ« æ”»å‡»å‘é‡ å½±å“èŒƒå›´ é˜²æŠ¤æªæ–½ æœªæˆæƒè®¿é—® API Serveræš´éœ² æ•´ä¸ªé›†ç¾¤ è®¤è¯ã€æˆæƒã€ç½‘ç»œéš”ç¦» æƒé™æå‡ RBACé…ç½®é”™è¯¯ å‘½åç©ºé—´/é›†ç¾¤ æœ€å°æƒé™åŸåˆ™ã€å®šæœŸå®¡è®¡ å®¹å™¨é€ƒé€¸ å†…æ ¸æ¼æ´ã€ç‰¹æƒå®¹å™¨ èŠ‚ç‚¹ å®‰å…¨ä¸Šä¸‹æ–‡ã€è¿è¡Œæ—¶ä¿æŠ¤ æ•°æ®æ³„éœ² å¯†é’¥æš´éœ²ã€å­˜å‚¨æœªåŠ å¯† åº”ç”¨æ•°æ® å¯†é’¥ç®¡ç†ã€åŠ å¯†å­˜å‚¨ ä¾›åº”é“¾æ”»å‡» æ¶æ„é•œåƒ å·¥ä½œè´Ÿè½½ é•œåƒæ‰«æã€ç­¾åéªŒè¯ ç½‘ç»œæ”»å‡» æ¨ªå‘ç§»åŠ¨ å¤šä¸ªæœåŠ¡ ç½‘ç»œç­–ç•¥ã€é›¶ä¿¡ä»» é›†ç¾¤å®‰å…¨é…ç½® API Serverå®‰å…¨åŠ å›º # kube-apiserveré…ç½® apiVersion: v1 kind: Pod metadata: name: kube-apiserver namespace: kube-system spec: containers: - name: kube-apiserver image: k8s.gcr.io/kube-apiserver:v1.28.0 command: - kube-apiserver # åŸºç¡€å®‰å…¨é…ç½® - --secure-port=6443 - --insecure-port=0 - --bind-address=0.0.0.0 # è®¤è¯é…ç½® - --client-ca-file=/etc/kubernetes/pki/ca.crt - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key - --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 - --tls-min-version=VersionTLS12 # æˆæƒé…ç½® - --authorization-mode=Node,RBAC - --enable-admission-plugins=NodeRestriction,PodSecurityPolicy,ResourceQuota,LimitRanger # å®¡è®¡é…ç½® - --audit-log-path=/var/log/audit.log - --audit-log-maxage=30 - --audit-log-maxbackup=10 - --audit-log-maxsize=100 - --audit-policy-file=/etc/kubernetes/audit-policy.yaml # etcdå®‰å…¨ - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key - --etcd-servers=https://127.0.0.1:2379 # å…¶ä»–å®‰å…¨é€‰é¡¹ - --anonymous-auth=false - --enable-bootstrap-token-auth=false - --profiling=false - --repair-malformed-updates=false - --service-account-lookup=true - --service-account-key-file=/etc/kubernetes/pki/sa.pub - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key volumeMounts: - name: ca-certs mountPath: /etc/ssl/certs readOnly: true - name: etc-ca-certificates mountPath: /etc/ca-certificates readOnly: true - name: k8s-certs mountPath: /etc/kubernetes/pki readOnly: true - name: usr-local-share-ca-certificates mountPath: /usr/local/share/ca-certificates readOnly: true - name: usr-share-ca-certificates mountPath: /usr/share/ca-certificates readOnly: true å®¡è®¡ç­–ç•¥é…ç½® # /etc/kubernetes/audit-policy.yaml apiVersion: audit.k8s.io/v1 kind: Policy rules: # è®°å½•æ‰€æœ‰è®¤è¯å¤±è´¥ - level: Metadata namespaces: [\u0026#34;\u0026#34;] verbs: [\u0026#34;\u0026#34;] resources: - group: \u0026#34;\u0026#34; resources: [\u0026#34;\u0026#34;] omitStages: - RequestReceived # è®°å½•å¯†é’¥å’Œé…ç½®æ˜ å°„çš„è®¿é—® - level: RequestResponse resources: - group: \u0026#34;\u0026#34; resources: [\u0026#34;secrets\u0026#34;, \u0026#34;configmaps\u0026#34;] # è®°å½•æ‰€æœ‰RBACå˜æ›´ - level: RequestResponse resources: - group: \u0026#34;rbac.authorization.k8s.io\u0026#34; resources: [\u0026#34;roles\u0026#34;, \u0026#34;rolebindings\u0026#34;, \u0026#34;clusterroles\u0026#34;, \u0026#34;clusterrolebindings\u0026#34;] # è®°å½•Podçš„åˆ›å»ºã€æ›´æ–°ã€åˆ é™¤ - level: Request resources: - group: \u0026#34;\u0026#34; resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;] # è®°å½•æœåŠ¡è´¦æˆ·ä»¤ç‰Œçš„åˆ›å»º - level: Metadata resources: - group: \u0026#34;\u0026#34; resources: [\u0026#34;serviceaccounts/token\u0026#34;] # è®°å½•æ‰€æœ‰å‡†å…¥æ§åˆ¶å™¨çš„å†³ç­– - level: Request users: [\u0026#34;system:serviceaccount:kube-system:generic-garbage-collector\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] resources: - group: \u0026#34;\u0026#34; resources: [\u0026#34;*\u0026#34;] # é»˜è®¤è§„åˆ™ï¼šè®°å½•å…ƒæ•°æ® - level: Metadata omitStages: - RequestReceived etcdå®‰å…¨é…ç½® # etcdå®‰å…¨é…ç½® apiVersion: v1 kind: Pod metadata: name: etcd namespace: kube-system spec: containers: - name: etcd image: k8s.gcr.io/etcd:3.5.9-0 command: - etcd # åŸºç¡€é…ç½® - --name=master - --data-dir=/var/lib/etcd - --listen-client-urls=https://127.0.0.1:2379 - --advertise-client-urls=https://127.0.0.1:2379 - --listen-peer-urls=https://127.0.0.1:2380 - --initial-advertise-peer-urls=https://127.0.0.1:2380 - --initial-cluster=master=https://127.0.0.1:2380 - --initial-cluster-token=etcd-cluster-1 - --initial-cluster-state=new # å®‰å…¨é…ç½® - --client-cert-auth=true - --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt - --cert-file=/etc/kubernetes/pki/etcd/server.crt - --key-file=/etc/kubernetes/pki/etcd/server.key - --peer-client-cert-auth=true - --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.crt - --peer-cert-file=/etc/kubernetes/pki/etcd/peer.crt - --peer-key-file=/etc/kubernetes/pki/etcd/peer.key # æ•°æ®åŠ å¯† - --experimental-encryption-provider-config=/etc/kubernetes/encryption-config.yaml volumeMounts: - name: etcd-data mountPath: /var/lib/etcd - name: etcd-certs mountPath: /etc/kubernetes/pki/etcd readOnly: true æ•°æ®åŠ å¯†é…ç½® # /etc/kubernetes/encryption-config.yaml apiVersion: apiserver.config.k8s.io/v1 kind: EncryptionConfiguration resources: - resources: - secrets - configmaps - events providers: - aescbc: keys: - name: key1 secret: \u0026lt;base64-encoded-32-byte-key\u0026gt; - identity: {} - resources: - persistentvolumes - persistentvolumeclaims providers: - aescbc: keys: - name: key1 secret: \u0026lt;base64-encoded-32-byte-key\u0026gt; - identity: {} RBACæƒé™ç®¡ç† æœ€å°æƒé™åŸåˆ™å®ç° # æœåŠ¡è´¦æˆ·åˆ›å»º apiVersion: v1 kind: ServiceAccount metadata: name: app-service-account namespace: production automountServiceAccountToken: false --- # è§’è‰²å®šä¹‰ apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: production name: app-role rules: # åªå…è®¸è¯»å–ConfigMapå’ŒSecret - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;configmaps\u0026#34;, \u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] resourceNames: [\u0026#34;app-config\u0026#34;, \u0026#34;app-secrets\u0026#34;] # åªå…è®¸æ›´æ–°è‡ªå·±çš„PodçŠ¶æ€ - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;patch\u0026#34;] resourceNames: [] # å…è®¸åˆ›å»ºäº‹ä»¶ - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;events\u0026#34;] verbs: [\u0026#34;create\u0026#34;] --- # è§’è‰²ç»‘å®š apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: app-role-binding namespace: production subjects: - kind: ServiceAccount name: app-service-account namespace: production roleRef: kind: Role name: app-role apiGroup: rbac.authorization.k8s.io --- # é›†ç¾¤çº§åˆ«çš„åªè¯»è§’è‰² apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: monitoring-reader rules: - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;nodes\u0026#34;, \u0026#34;nodes/metrics\u0026#34;, \u0026#34;services\u0026#34;, \u0026#34;endpoints\u0026#34;, \u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;extensions\u0026#34;, \u0026#34;apps\u0026#34;] resources: [\u0026#34;deployments\u0026#34;, \u0026#34;replicasets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] - apiGroups: [\u0026#34;metrics.k8s.io\u0026#34;] resources: [\u0026#34;nodes\u0026#34;, \u0026#34;pods\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] --- # å¼€å‘è€…è§’è‰²ï¼ˆå‘½åç©ºé—´çº§åˆ«ï¼‰ apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: namespace: development name: developer-role rules: # å®Œå…¨æ§åˆ¶å¤§éƒ¨åˆ†èµ„æº - apiGroups: [\u0026#34;\u0026#34;, \u0026#34;apps\u0026#34;, \u0026#34;extensions\u0026#34;] resources: [\u0026#34;*\u0026#34;] verbs: [\u0026#34;*\u0026#34;] # ä½†ä¸èƒ½è®¿é—®å¯†é’¥ - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;secrets\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] resourceNames: [\u0026#34;allowed-secret-1\u0026#34;, \u0026#34;allowed-secret-2\u0026#34;] # ä¸èƒ½åˆ é™¤å‘½åç©ºé—´ - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;namespaces\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] --- # è¿ç»´è§’è‰²ï¼ˆé›†ç¾¤çº§åˆ«ï¼‰ apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: ops-role rules: # èŠ‚ç‚¹ç®¡ç† - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;nodes\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;update\u0026#34;] # ç³»ç»Ÿå‘½åç©ºé—´è®¿é—® - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;*\u0026#34;] verbs: [\u0026#34;*\u0026#34;] resourceNames: [] # ä½†é™åˆ¶åœ¨ç‰¹å®šå‘½åç©ºé—´ - apiGroups: [\u0026#34;\u0026#34;] resources: [\u0026#34;namespaces\u0026#34;] verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;] resourceNames: [\u0026#34;kube-system\u0026#34;, \u0026#34;kube-public\u0026#34;, \u0026#34;monitoring\u0026#34;] åŠ¨æ€RBACç®¡ç† // rbac-manager.go package main import ( \u0026#34;context\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;time\u0026#34; rbacv1 \u0026#34;k8s.io/api/rbac/v1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/client-go/kubernetes\u0026#34; \u0026#34;k8s.io/client-go/rest\u0026#34; ) type RBACManager struct { clientset *kubernetes.Clientset } func NewRBACManager() (*RBACManager, error) { config, err := rest.InClusterConfig() if err != nil { return nil, err } clientset, err := kubernetes.NewForConfig(config) if err != nil { return nil, err } return \u0026amp;RBACManager{clientset: clientset}, nil } // åˆ›å»ºä¸´æ—¶è®¿é—®æƒé™ func (r *RBACManager) CreateTemporaryAccess(username, namespace string, duration time.Duration) error { // åˆ›å»ºè§’è‰² role := \u0026amp;rbacv1.Role{ ObjectMeta: metav1.ObjectMeta{ Name: fmt.Sprintf(\u0026#34;temp-access-%s\u0026#34;, username), Namespace: namespace, Annotations: map[string]string{ \u0026#34;rbac.security.io/expires-at\u0026#34;: time.Now().Add(duration).Format(time.RFC3339), \u0026#34;rbac.security.io/created-by\u0026#34;: \u0026#34;rbac-manager\u0026#34;, }, }, Rules: []rbacv1.PolicyRule{ { APIGroups: []string{\u0026#34;\u0026#34;}, Resources: []string{\u0026#34;pods\u0026#34;, \u0026#34;services\u0026#34;, \u0026#34;configmaps\u0026#34;}, Verbs: []string{\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;}, }, }, } _, err := r.clientset.RbacV1().Roles(namespace).Create(context.TODO(), role, metav1.CreateOptions{}) if err != nil { return err } // åˆ›å»ºè§’è‰²ç»‘å®š roleBinding := \u0026amp;rbacv1.RoleBinding{ ObjectMeta: metav1.ObjectMeta{ Name: fmt.Sprintf(\u0026#34;temp-binding-%s\u0026#34;, username), Namespace: namespace, Annotations: map[string]string{ \u0026#34;rbac.security.io/expires-at\u0026#34;: time.Now().Add(duration).Format(time.RFC3339), \u0026#34;rbac.security.io/created-by\u0026#34;: \u0026#34;rbac-manager\u0026#34;, }, }, Subjects: []rbacv1.Subject{ { Kind: \u0026#34;User\u0026#34;, Name: username, }, }, RoleRef: rbacv1.RoleRef{ Kind: \u0026#34;Role\u0026#34;, Name: role.Name, APIGroup: \u0026#34;rbac.authorization.k8s.io\u0026#34;, }, } _, err = r.clientset.RbacV1().RoleBindings(namespace).Create(context.TODO(), roleBinding, metav1.CreateOptions{}) return err } // æ¸…ç†è¿‡æœŸæƒé™ func (r *RBACManager) CleanupExpiredPermissions() error { namespaces, err := r.clientset.CoreV1().Namespaces().List(context.TODO(), metav1.ListOptions{}) if err != nil { return err } now := time.Now() for _, ns := range namespaces.Items { // æ¸…ç†è¿‡æœŸè§’è‰² roles, err := r.clientset.RbacV1().Roles(ns.Name).List(context.TODO(), metav1.ListOptions{}) if err != nil { continue } for _, role := range roles.Items { if expiresAt, exists := role.Annotations[\u0026#34;rbac.security.io/expires-at\u0026#34;]; exists { expireTime, err := time.Parse(time.RFC3339, expiresAt) if err == nil \u0026amp;\u0026amp; now.After(expireTime) { r.clientset.RbacV1().Roles(ns.Name).Delete(context.TODO(), role.Name, metav1.DeleteOptions{}) } } } // æ¸…ç†è¿‡æœŸè§’è‰²ç»‘å®š roleBindings, err := r.clientset.RbacV1().RoleBindings(ns.Name).List(context.TODO(), metav1.ListOptions{}) if err != nil { continue } for _, binding := range roleBindings.Items { if expiresAt, exists := binding.Annotations[\u0026#34;rbac.security.io/expires-at\u0026#34;]; exists { expireTime, err := time.Parse(time.RFC3339, expiresAt) if err == nil \u0026amp;\u0026amp; now.After(expireTime) { r.clientset.RbacV1().RoleBindings(ns.Name).Delete(context.TODO(), binding.Name, metav1.DeleteOptions{}) } } } } return nil } // æƒé™å®¡è®¡ func (r *RBACManager) AuditPermissions() (map[string][]string, error) { audit := make(map[string][]string) // è·å–æ‰€æœ‰ClusterRoleBindings clusterBindings, err := r.clientset.RbacV1().ClusterRoleBindings().List(context.TODO(), metav1.ListOptions{}) if err != nil { return nil, err } for _, binding := range clusterBindings.Items { for _, subject := range binding.Subjects { key := fmt.Sprintf(\u0026#34;%s/%s\u0026#34;, subject.Kind, subject.Name) audit[key] = append(audit[key], fmt.Sprintf(\u0026#34;ClusterRole: %s\u0026#34;, binding.RoleRef.Name)) } } // è·å–æ‰€æœ‰å‘½åç©ºé—´çš„RoleBindings namespaces, err := r.clientset.CoreV1().Namespaces().List(context.TODO(), metav1.ListOptions{}) if err != nil { return audit, nil } for _, ns := range namespaces.Items { bindings, err := r.clientset.RbacV1().RoleBindings(ns.Name).List(context.TODO(), metav1.ListOptions{}) if err != nil { continue } for _, binding := range bindings.Items { for _, subject := range binding.Subjects { key := fmt.Sprintf(\u0026#34;%s/%s\u0026#34;, subject.Kind, subject.Name) audit[key] = append(audit[key], fmt.Sprintf(\u0026#34;Role: %s (namespace: %s)\u0026#34;, binding.RoleRef.Name, ns.Name)) } } } return audit, nil } ç½‘ç»œå®‰å…¨ç­–ç•¥ ç½‘ç»œç­–ç•¥é…ç½® # é»˜è®¤æ‹’ç»æ‰€æœ‰å…¥ç«™æµé‡ apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-ingress namespace: production spec: podSelector: {} policyTypes: - Ingress --- # é»˜è®¤æ‹’ç»æ‰€æœ‰å‡ºç«™æµé‡ apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: default-deny-egress namespace: production spec: podSelector: {} policyTypes: - Egress --- # å…è®¸å‰ç«¯è®¿é—®åç«¯ apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: frontend-to-backend namespace: production spec: podSelector: matchLabels: app: backend policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: app: frontend ports: - protocol: TCP port: 8080 --- # å…è®¸åç«¯è®¿é—®æ•°æ®åº“ apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: backend-to-database namespace: production spec: podSelector: matchLabels: app: backend policyTypes: - Egress egress: - to: - podSelector: matchLabels: app: database ports: - protocol: TCP port: 5432 # å…è®¸DNSè§£æ - to: [] ports: - protocol: UDP port: 53 --- # å…è®¸ç›‘æ§ç³»ç»Ÿè®¿é—®æ‰€æœ‰Pod apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: monitoring-access namespace: production spec: podSelector: {} policyTypes: - Ingress ingress: - from: - namespaceSelector: matchLabels: name: monitoring ports: - protocol: TCP port: 8080 - protocol: TCP port: 9090 --- # è·¨å‘½åç©ºé—´è®¿é—®æ§åˆ¶ apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: cross-namespace-api namespace: production spec: podSelector: matchLabels: app: api-gateway policyTypes: - Ingress ingress: - from: - namespaceSelector: matchLabels: environment: staging podSelector: matchLabels: app: test-client ports: - protocol: TCP port: 443 Calicoé«˜çº§ç½‘ç»œç­–ç•¥ # Calico GlobalNetworkPolicy apiVersion: projectcalico.org/v3 kind: GlobalNetworkPolicy metadata: name: security-controls spec: # åº”ç”¨åˆ°æ‰€æœ‰å‘½åç©ºé—´ namespaceSelector: has(projectcalico.org/name) # æ‹’ç»æ‰€æœ‰æœªæ˜ç¡®å…è®¸çš„æµé‡ types: - Ingress - Egress # å…¥ç«™è§„åˆ™ ingress: # å…è®¸åŒä¸€å‘½åç©ºé—´å†…çš„é€šä¿¡ - action: Allow source: namespaceSelector: projectcalico.org/name == global() # å…è®¸æ¥è‡ªingressæ§åˆ¶å™¨çš„æµé‡ - action: Allow source: namespaceSelector: projectcalico.org/name == \u0026#34;ingress-nginx\u0026#34; destination: ports: - 80 - 443 # å‡ºç«™è§„åˆ™ egress: # å…è®¸DNSæŸ¥è¯¢ - action: Allow protocol: UDP destination: ports: - 53 # å…è®¸HTTPSå‡ºç«™ï¼ˆç”¨äºæ‹‰å–é•œåƒç­‰ï¼‰ - action: Allow protocol: TCP destination: ports: - 443 # å…è®¸è®¿é—®Kubernetes API - action: Allow protocol: TCP destination: services: name: kubernetes namespace: default --- # åŸºäºæ—¶é—´çš„è®¿é—®æ§åˆ¶ apiVersion: projectcalico.org/v3 kind: NetworkPolicy metadata: name: business-hours-only namespace: production spec: selector: app == \u0026#34;admin-panel\u0026#34; types: - Ingress ingress: - action: Allow source: selector: role == \u0026#34;admin\u0026#34; # ä»…åœ¨å·¥ä½œæ—¶é—´å…è®¸è®¿é—®ï¼ˆéœ€è¦é…åˆå¤–éƒ¨æ§åˆ¶å™¨ï¼‰ metadata: annotations: schedule: \u0026#34;0 9-17 * * 1-5\u0026#34; # å‘¨ä¸€åˆ°å‘¨äº” 9-17ç‚¹ --- # åœ°ç†ä½ç½®é™åˆ¶ apiVersion: projectcalico.org/v3 kind: GlobalNetworkPolicy metadata: name: geo-restriction spec: selector: app == \u0026#34;sensitive-app\u0026#34; types: - Ingress ingress: - action: Deny source: nets: # æ‹’ç»æ¥è‡ªç‰¹å®šå›½å®¶/åœ°åŒºçš„IPæ®µ - 192.0.2.0/24 - 203.0.113.0/24 - action: Allow source: nets: # åªå…è®¸æ¥è‡ªå…¬å¸IPæ®µ - 10.0.0.0/8 - 172.16.0.0/12 Podå®‰å…¨æ ‡å‡† Pod Security Standardsé…ç½® # å‘½åç©ºé—´çº§åˆ«çš„Podå®‰å…¨ç­–ç•¥ apiVersion: v1 kind: Namespace metadata: name: production labels: # å¼ºåˆ¶æ‰§è¡Œrestrictedç­–ç•¥ pod-security.kubernetes.io/enforce: restricted pod-security.kubernetes.io/enforce-version: v1.28 # å¯¹baselineç­–ç•¥å‘å‡ºè­¦å‘Š pod-security.kubernetes.io/warn: baseline pod-security.kubernetes.io/warn-version: v1.28 # å®¡è®¡æ‰€æœ‰è¿åprivilegedç­–ç•¥çš„è¡Œä¸º pod-security.kubernetes.io/audit: privileged pod-security.kubernetes.io/audit-version: v1.28 --- # å®‰å…¨çš„Podé…ç½®ç¤ºä¾‹ apiVersion: v1 kind: Pod metadata: name: secure-app namespace: production spec: # ä½¿ç”¨éç‰¹æƒæœåŠ¡è´¦æˆ· serviceAccountName: app-service-account automountServiceAccountToken: false # å®‰å…¨ä¸Šä¸‹æ–‡ securityContext: # è¿è¡Œä¸ºérootç”¨æˆ· runAsNonRoot: true runAsUser: 1000 runAsGroup: 1000 fsGroup: 1000 # è®¾ç½®seccompé…ç½® seccompProfile: type: RuntimeDefault # è®¾ç½®SELinuxé€‰é¡¹ seLinuxOptions: level: \u0026#34;s0:c123,c456\u0026#34; containers: - name: app image: myapp:v1.2.3 # å®¹å™¨å®‰å…¨ä¸Šä¸‹æ–‡ securityContext: # ç¦æ­¢ç‰¹æƒæ¨¡å¼ privileged: false # ç¦æ­¢æƒé™æå‡ allowPrivilegeEscalation: false # åªè¯»æ ¹æ–‡ä»¶ç³»ç»Ÿ readOnlyRootFilesystem: true # åˆ é™¤æ‰€æœ‰capabilities capabilities: drop: - ALL # åªæ·»åŠ å¿…éœ€çš„capabilities add: - NET_BIND_SERVICE # èµ„æºé™åˆ¶ resources: requests: memory: \u0026#34;64Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;128Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; # å­˜æ´»æ€§å’Œå°±ç»ªæ€§æ¢é’ˆ livenessProbe: httpGet: path: /health port: 8080 scheme: HTTP initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: httpGet: path: /ready port: 8080 scheme: HTTP initialDelaySeconds: 5 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 # æŒ‚è½½ä¸´æ—¶å·ç”¨äºå†™å…¥ volumeMounts: - name: tmp-volume mountPath: /tmp - name: cache-volume mountPath: /app/cache volumes: - name: tmp-volume emptyDir: {} - name: cache-volume emptyDir: {} å‡†å…¥æ§åˆ¶å™¨é…ç½® # OPA Gatekeeperçº¦æŸæ¨¡æ¿ apiVersion: templates.gatekeeper.sh/v1beta1 kind: ConstraintTemplate metadata: name: k8srequiredsecuritycontext spec: crd: spec: names: kind: K8sRequiredSecurityContext validation: type: object properties: runAsNonRoot: type: boolean runAsUser: type: integer fsGroup: type: integer targets: - target: admission.k8s.gatekeeper.sh rego: | package k8srequiredsecuritycontext violation[{\u0026#34;msg\u0026#34;: msg}] { container := input.review.object.spec.containers[_] not container.securityContext.runAsNonRoot msg := \u0026#34;Container must run as non-root user\u0026#34; } violation[{\u0026#34;msg\u0026#34;: msg}] { not input.review.object.spec.securityContext.runAsUser msg := \u0026#34;Must specify runAsUser in securityContext\u0026#34; } violation[{\u0026#34;msg\u0026#34;: msg}] { input.review.object.spec.securityContext.runAsUser == 0 msg := \u0026#34;Container must not run as root (runAsUser: 0)\u0026#34; } violation[{\u0026#34;msg\u0026#34;: msg}] { container := input.review.object.spec.containers[_] container.securityContext.privileged msg := \u0026#34;Privileged containers are not allowed\u0026#34; } --- # åº”ç”¨çº¦æŸ apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sRequiredSecurityContext metadata: name: must-have-security-context spec: match: kinds: - apiGroups: [\u0026#34;\u0026#34;] kinds: [\u0026#34;Pod\u0026#34;] namespaces: [\u0026#34;production\u0026#34;, \u0026#34;staging\u0026#34;] parameters: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 --- # é•œåƒå®‰å…¨çº¦æŸ apiVersion: templates.gatekeeper.sh/v1beta1 kind: ConstraintTemplate metadata: name: k8sallowedrepos spec: crd: spec: names: kind: K8sAllowedRepos validation: type: object properties: repos: type: array items: type: string targets: - target: admission.k8s.gatekeeper.sh rego: | package k8sallowedrepos violation[{\u0026#34;msg\u0026#34;: msg}] { container := input.review.object.spec.containers[_] satisfied := [good | repo = input.parameters.repos[_] ; good = startswith(container.image, repo)] not any(satisfied) msg := sprintf(\u0026#34;Container image \u0026lt;%v\u0026gt; comes from untrusted registry\u0026#34;, [container.image]) } --- apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sAllowedRepos metadata: name: must-come-from-trusted-registry spec: match: kinds: - apiGroups: [\u0026#34;\u0026#34;] kinds: [\u0026#34;Pod\u0026#34;] parameters: repos: - \u0026#34;gcr.io/my-company/\u0026#34; - \u0026#34;registry.company.com/\u0026#34; - \u0026#34;docker.io/library/\u0026#34; å®¹å™¨é•œåƒå®‰å…¨ é•œåƒæ‰«æå’Œç­¾å # Trivyé•œåƒæ‰«æé…ç½® apiVersion: v1 kind: ConfigMap metadata: name: trivy-config namespace: security data: trivy.yaml: | # æ‰«æé…ç½® scan: security-checks: vuln,config,secret severity: CRITICAL,HIGH,MEDIUM ignore-unfixed: false # è¾“å‡ºé…ç½® format: json output: /tmp/trivy-report.json # æ•°æ®åº“é…ç½® cache-dir: /tmp/trivy-cache # å¿½ç•¥æ–‡ä»¶ ignorefile: .trivyignore --- # é•œåƒæ‰«æJob apiVersion: batch/v1 kind: Job metadata: name: image-scan namespace: security spec: template: spec: restartPolicy: Never containers: - name: trivy image: aquasec/trivy:latest command: - trivy - image - --config - /config/trivy.yaml - --exit-code - \u0026#34;1\u0026#34; # å‘ç°æ¼æ´æ—¶é€€å‡ºç ä¸º1 - $(IMAGE_NAME) env: - name: IMAGE_NAME value: \u0026#34;myapp:latest\u0026#34; volumeMounts: - name: config mountPath: /config - name: cache mountPath: /tmp/trivy-cache volumes: - name: config configMap: name: trivy-config - name: cache emptyDir: {} --- # Cosigné•œåƒç­¾åéªŒè¯ apiVersion: v1 kind: ConfigMap metadata: name: cosign-policy namespace: security data: policy.yaml: | apiVersion: v1alpha1 kind: ClusterImagePolicy metadata: name: signed-images-policy spec: images: - glob: \u0026#34;gcr.io/my-company/*\u0026#34; authorities: - keyless: url: https://fulcio.sigstore.dev identities: - issuer: https://accounts.google.com subject: build@company.com - key: data: | -----BEGIN PUBLIC KEY----- MFkwEwYHKoZIzj0CAQYIKoZIzj0DAQcDQgAE... -----END PUBLIC KEY----- --- # é•œåƒç­–ç•¥æ‰§è¡Œå™¨ apiVersion: apps/v1 kind: Deployment metadata: name: image-policy-webhook namespace: security spec: replicas: 2 selector: matchLabels: app: image-policy-webhook template: metadata: labels: app: image-policy-webhook spec: containers: - name: webhook image: sigstore/cosign:latest command: - cosign - webhook - --port=8443 - --cert=/certs/tls.crt - --key=/certs/tls.key ports: - containerPort: 8443 volumeMounts: - name: certs mountPath: /certs readOnly: true - name: policy mountPath: /policy readOnly: true volumes: - name: certs secret: secretName: webhook-certs - name: policy configMap: name: cosign-policy è¿è¡Œæ—¶å®‰å…¨ç›‘æ§ # Falcoå®‰å…¨ç›‘æ§é…ç½® apiVersion: v1 kind: ConfigMap metadata: name: falco-config namespace: security data: falco.yaml: | # è§„åˆ™æ–‡ä»¶ rules_file: - /etc/falco/falco_rules.yaml - /etc/falco/falco_rules.local.yaml - /etc/falco/k8s_audit_rules.yaml # è¾“å‡ºé…ç½® json_output: true json_include_output_property: true # æ—¥å¿—çº§åˆ« log_level: info # è¾“å‡ºé€šé“ stdout_output: enabled: true syslog_output: enabled: false file_output: enabled: true keep_alive: false filename: /var/log/falco.log http_output: enabled: true url: \u0026#34;http://falco-exporter:9376/events\u0026#34; # ç³»ç»Ÿè°ƒç”¨ç›‘æ§ syscall_event_drops: actions: - log - alert rate: 0.03333 max_burst: 1000 # è§„åˆ™åŒ¹é… priority: debug falco_rules.local.yaml: | # è‡ªå®šä¹‰è§„åˆ™ - rule: Detect crypto mining desc: Detect cryptocurrency mining condition: \u0026gt; spawned_process and (proc.name in (cryptonight, xmrig, minergate) or proc.cmdline contains \u0026#34;stratum+tcp\u0026#34; or proc.cmdline contains \u0026#34;pool.minergate.com\u0026#34;) output: \u0026gt; Cryptocurrency mining detected (user=%user.name command=%proc.cmdline container=%container.name image=%container.image.repository) priority: CRITICAL tags: [cryptocurrency, mining] - rule: Detect reverse shell desc: Detect reverse shell attempts condition: \u0026gt; spawned_process and (proc.name in (nc, ncat, netcat, socat) and (proc.cmdline contains \u0026#34;-e\u0026#34; or proc.cmdline contains \u0026#34;-c\u0026#34;)) output: \u0026gt; Reverse shell detected (user=%user.name command=%proc.cmdline container=%container.name image=%container.image.repository) priority: CRITICAL tags: [shell, reverse_shell] - rule: Detect privilege escalation desc: Detect attempts to escalate privileges condition: \u0026gt; spawned_process and (proc.name in (sudo, su, doas) or proc.cmdline contains \u0026#34;chmod +s\u0026#34; or proc.cmdline contains \u0026#34;setuid\u0026#34;) output: \u0026gt; Privilege escalation attempt (user=%user.name command=%proc.cmdline container=%container.name image=%container.image.repository) priority: HIGH tags: [privilege_escalation] --- # Falco DaemonSet apiVersion: apps/v1 kind: DaemonSet metadata: name: falco namespace: security spec: selector: matchLabels: app: falco template: metadata: labels: app: falco spec: serviceAccountName: falco hostNetwork: true hostPID: true containers: - name: falco image: falcosecurity/falco:latest securityContext: privileged: true args: - /usr/bin/falco - --cri=/run/containerd/containerd.sock - --k8s-api=https://kubernetes.default.svc.cluster.local - --k8s-api-cert=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt - --k8s-api-token=/var/run/secrets/kubernetes.io/serviceaccount/token volumeMounts: - name: dev-fs mountPath: /host/dev readOnly: true - name: proc-fs mountPath: /host/proc readOnly: true - name: boot-fs mountPath: /host/boot readOnly: true - name: lib-modules mountPath: /host/lib/modules readOnly: true - name: usr-fs mountPath: /host/usr readOnly: true - name: etc-fs mountPath: /host/etc readOnly: true - name: config mountPath: /etc/falco - name: containerd-sock mountPath: /run/containerd/containerd.sock volumes: - name: dev-fs hostPath: path: /dev - name: proc-fs hostPath: path: /proc - name: boot-fs hostPath: path: /boot - name: lib-modules hostPath: path: /lib/modules - name: usr-fs hostPath: path: /usr - name: etc-fs hostPath: path: /etc - name: config configMap: name: falco-config - name: containerd-sock hostPath: path: /run/containerd/containerd.sock å¯†é’¥ç®¡ç† External Secrets Operator # External Secrets Operatoré…ç½® apiVersion: external-secrets.io/v1beta1 kind: SecretStore metadata: name: vault-backend namespace: production spec: provider: vault: server: \u0026#34;https://vault.company.com\u0026#34; path: \u0026#34;secret\u0026#34; version: \u0026#34;v2\u0026#34; auth: kubernetes: mountPath: \u0026#34;kubernetes\u0026#34; role: \u0026#34;external-secrets\u0026#34; serviceAccountRef: name: external-secrets-sa --- # å¤–éƒ¨å¯†é’¥åŒæ­¥ apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: app-secrets namespace: production spec: refreshInterval: 15s secretStoreRef: name: vault-backend kind: SecretStore target: name: app-secrets creationPolicy: Owner template: type: Opaque data: database-url: \u0026#34;postgresql://{{ .username }}:{{ .password }}@{{ .host }}:5432/{{ .database }}\u0026#34; data: - secretKey: username remoteRef: key: database/production property: username - secretKey: password remoteRef: key: database/production property: password - secretKey: host remoteRef: key: database/production property: host - secretKey: database remoteRef: key: database/production property: database --- # å¯†é’¥è½®æ¢é…ç½® apiVersion: external-secrets.io/v1beta1 kind: ExternalSecret metadata: name: rotating-api-key namespace: production spec: refreshInterval: 1h # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡ secretStoreRef: name: vault-backend kind: SecretStore target: name: api-key creationPolicy: Owner data: - secretKey: api-key remoteRef: key: api/production property: key version: latest # æ€»æ˜¯è·å–æœ€æ–°ç‰ˆæœ¬ å¯†é’¥åŠ å¯†å’Œè®¿é—®æ§åˆ¶ // secret-manager.go package main import ( \u0026#34;context\u0026#34; \u0026#34;crypto/aes\u0026#34; \u0026#34;crypto/cipher\u0026#34; \u0026#34;crypto/rand\u0026#34; \u0026#34;encoding/base64\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;io\u0026#34; corev1 \u0026#34;k8s.io/api/core/v1\u0026#34; metav1 \u0026#34;k8s.io/apimachinery/pkg/apis/meta/v1\u0026#34; \u0026#34;k8s.io/client-go/kubernetes\u0026#34; ) type SecretManager struct { clientset *kubernetes.Clientset gcm cipher.AEAD } func NewSecretManager(clientset *kubernetes.Clientset, key []byte) (*SecretManager, error) { block, err := aes.NewCipher(key) if err != nil { return nil, err } gcm, err := cipher.NewGCM(block) if err != nil { return nil, err } return \u0026amp;SecretManager{ clientset: clientset, gcm: gcm, }, nil } // åŠ å¯†å¯†é’¥æ•°æ® func (sm *SecretManager) encrypt(data []byte) (string, error) { nonce := make([]byte, sm.gcm.NonceSize()) if _, err := io.ReadFull(rand.Reader, nonce); err != nil { return \u0026#34;\u0026#34;, err } ciphertext := sm.gcm.Seal(nonce, nonce, data, nil) return base64.StdEncoding.EncodeToString(ciphertext), nil } // è§£å¯†å¯†é’¥æ•°æ® func (sm *SecretManager) decrypt(encryptedData string) ([]byte, error) { data, err := base64.StdEncoding.DecodeString(encryptedData) if err != nil { return nil, err } nonceSize := sm.gcm.NonceSize() if len(data) \u0026lt; nonceSize { return nil, fmt.Errorf(\u0026#34;ciphertext too short\u0026#34;) } nonce, ciphertext := data[:nonceSize], data[nonceSize:] return sm.gcm.Open(nil, nonce, ciphertext, nil) } // åˆ›å»ºåŠ å¯†çš„Secret func (sm *SecretManager) CreateEncryptedSecret(namespace, name string, data map[string][]byte) error { encryptedData := make(map[string][]byte) for key, value := range data { encrypted, err := sm.encrypt(value) if err != nil { return err } encryptedData[key] = []byte(encrypted) } secret := \u0026amp;corev1.Secret{ ObjectMeta: metav1.ObjectMeta{ Name: name, Namespace: namespace, Annotations: map[string]string{ \u0026#34;security.company.com/encrypted\u0026#34;: \u0026#34;true\u0026#34;, \u0026#34;security.company.com/algorithm\u0026#34;: \u0026#34;AES-256-GCM\u0026#34;, }, }, Type: corev1.SecretTypeOpaque, Data: encryptedData, } _, err := sm.clientset.CoreV1().Secrets(namespace).Create(context.TODO(), secret, metav1.CreateOptions{}) return err } // è¯»å–å¹¶è§£å¯†Secret func (sm *SecretManager) GetDecryptedSecret(namespace, name string) (map[string][]byte, error) { secret, err := sm.clientset.CoreV1().Secrets(namespace).Get(context.TODO(), name, metav1.GetOptions{}) if err != nil { return nil, err } // æ£€æŸ¥æ˜¯å¦ä¸ºåŠ å¯†çš„Secret if secret.Annotations[\u0026#34;security.company.com/encrypted\u0026#34;] != \u0026#34;true\u0026#34; { return secret.Data, nil } decryptedData := make(map[string][]byte) for key, encryptedValue := range secret.Data { decrypted, err := sm.decrypt(string(encryptedValue)) if err != nil { return nil, err } decryptedData[key] = decrypted } return decryptedData, nil } // å¯†é’¥è½®æ¢ func (sm *SecretManager) RotateSecret(namespace, name string, newData map[string][]byte) error { // åˆ›å»ºæ–°ç‰ˆæœ¬çš„å¯†é’¥ newName := fmt.Sprintf(\u0026#34;%s-v%d\u0026#34;, name, time.Now().Unix()) err := sm.CreateEncryptedSecret(namespace, newName, newData) if err != nil { return err } // æ›´æ–°åŸå¯†é’¥çš„æ ‡ç­¾ï¼ŒæŒ‡å‘æ–°ç‰ˆæœ¬ secret, err := sm.clientset.CoreV1().Secrets(namespace).Get(context.TODO(), name, metav1.GetOptions{}) if err != nil { return err } if secret.Labels == nil { secret.Labels = make(map[string]string) } secret.Labels[\u0026#34;security.company.com/current-version\u0026#34;] = newName _, err = sm.clientset.CoreV1().Secrets(namespace).Update(context.TODO(), secret, metav1.UpdateOptions{}) return err } é€šè¿‡æœ¬æ–‡çš„å…¨é¢å®‰å…¨åŠ å›ºç­–ç•¥ï¼Œæ‚¨å¯ä»¥æ„å»ºä¸€ä¸ªå¤šå±‚æ¬¡ã€æ·±åº¦é˜²å¾¡çš„Kuberneteså®‰å…¨ä½“ç³»ï¼Œä»é›†ç¾¤åŸºç¡€è®¾æ–½åˆ°åº”ç”¨å·¥ä½œè´Ÿè½½çš„å…¨æ–¹ä½ä¿æŠ¤ï¼Œç¡®ä¿äº‘åŸç”Ÿç¯å¢ƒçš„å®‰å…¨æ€§å’Œåˆè§„æ€§ã€‚\n","content":"Kuberneteså®‰å…¨åŠ å›ºå®æˆ˜ï¼šä»é›†ç¾¤åˆ°å·¥ä½œè´Ÿè½½çš„å…¨æ–¹ä½é˜²æŠ¤ Kubernetesä½œä¸ºå®¹å™¨ç¼–æ’çš„äº‹å®æ ‡å‡†ï¼Œå…¶å®‰å…¨æ€§ç›´æ¥å½±å“æ•´ä¸ªäº‘åŸç”Ÿåº”ç”¨çš„å®‰å…¨æ€åŠ¿ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨Kuberneteså®‰å…¨åŠ å›ºçš„æœ€ä½³å®è·µï¼Œä»é›†ç¾¤å±‚é¢åˆ°å·¥ä½œè´Ÿè½½å±‚é¢çš„å…¨æ–¹ä½é˜²æŠ¤ç­–ç•¥ã€‚\nKuberneteså®‰å…¨æ¶æ„ å®‰å…¨å±‚æ¬¡æ¨¡å‹ graph TB A[ç‰©ç†/äº‘åŸºç¡€è®¾æ–½å®‰å…¨] --\u0026amp;gt; B[é›†ç¾¤å®‰å…¨] B --\u0026amp;gt; C[èŠ‚ç‚¹å®‰å…¨] C --\u0026amp;gt; D[ç½‘ç»œå®‰å…¨] D --\u0026amp;gt; E[å·¥ä½œè´Ÿè½½å®‰å…¨] E --\u0026amp;gt; F[åº”ç”¨å®‰å…¨] B1[API Serverå®‰å…¨] --\u0026amp;gt; B B2[etcdåŠ å¯†] --\u0026amp;gt; B B3[è¯ä¹¦ç®¡ç†] --\u0026amp;gt; B C1[èŠ‚ç‚¹åŠ å›º] --\u0026amp;gt; C C2[å®¹å™¨è¿è¡Œæ—¶å®‰å…¨] --\u0026amp;gt; C C3[é•œåƒå®‰å…¨] --\u0026amp;gt; C D1[ç½‘ç»œç­–ç•¥] --\u0026amp;gt; D D2[æœåŠ¡ç½‘æ ¼] --\u0026amp;gt; D D3[å…¥å£æ§åˆ¶] --\u0026amp;gt; D E1[Podå®‰å…¨æ ‡å‡†] --\u0026amp;gt; E E2[RBAC] --\u0026amp;gt; E E3[å‡†å…¥æ§åˆ¶] --\u0026amp;gt; E F1[åº”ç”¨ä»£ç å®‰å…¨] â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["Kubernetes","å®‰å…¨","RBAC","ç½‘ç»œç­–ç•¥","å®¹å™¨å®‰å…¨","DevSecOps"],"categories":["è¿ç»´"],"author":"å®‰å…¨æ¶æ„å¸ˆ","readingTime":14,"wordCount":2955,"section":"posts","type":"posts","draft":false,"featured":false,"series":["Kuberneteså®æˆ˜"]},{"title":"MongoDBåˆ†å¸ƒå¼æ¶æ„è®¾è®¡ä¸å®è·µï¼šä»å‰¯æœ¬é›†åˆ°åˆ†ç‰‡é›†ç¾¤çš„å®Œæ•´æ–¹æ¡ˆ","url":"https://www.dishuihengxin.com/posts/database-mongodb-distributed/","summary":"MongoDBåˆ†å¸ƒå¼æ¶æ„è®¾è®¡ä¸å®è·µï¼šä»å‰¯æœ¬é›†åˆ°åˆ†ç‰‡é›†ç¾¤çš„å®Œæ•´æ–¹æ¡ˆ MongoDBä½œä¸ºé¢†å…ˆçš„NoSQLæ–‡æ¡£æ•°æ®åº“ï¼Œåœ¨ç°ä»£åº”ç”¨æ¶æ„ä¸­å¹¿æ³›åº”ç”¨ã€‚éšç€æ•°æ®é‡çš„å¢é•¿å’Œä¸šåŠ¡å¤æ‚åº¦çš„æå‡ï¼Œå•æœºMongoDBå·²æ— æ³•æ»¡è¶³é«˜å¯ç”¨ã€é«˜æ€§èƒ½çš„éœ€æ±‚ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨MongoDBåˆ†å¸ƒå¼æ¶æ„çš„è®¾è®¡åŸç†å’Œå®è·µæ–¹æ¡ˆï¼Œä»åŸºç¡€çš„å‰¯æœ¬é›†åˆ°å¤æ‚çš„åˆ†ç‰‡é›†ç¾¤éƒ¨ç½²ã€‚\nMongoDBåˆ†å¸ƒå¼æ¶æ„æ¦‚è¿° MongoDBé›†ç¾¤æ¶æ„å¯¹æ¯” æ¶æ„æ¨¡å¼ ç‰¹ç‚¹ é€‚ç”¨åœºæ™¯ ä¼˜ç‚¹ ç¼ºç‚¹ å•æœºæ¨¡å¼ å•ä¸ªMongoDBå®ä¾‹ å¼€å‘æµ‹è¯•ã€å°å‹åº”ç”¨ ç®€å•æ˜“ç”¨ã€æ€§èƒ½é«˜ æ— é«˜å¯ç”¨ã€å®¹é‡å—é™ å‰¯æœ¬é›† ä¸€ä¸»å¤šä»ï¼Œè‡ªåŠ¨æ•…éšœè½¬ç§» ä¸­ç­‰è§„æ¨¡åº”ç”¨ é«˜å¯ç”¨ã€è¯»æ‰©å±• å†™æ€§èƒ½æ— æ³•æ‰©å±• åˆ†ç‰‡é›†ç¾¤ æ°´å¹³åˆ†ç‰‡ï¼Œåˆ†å¸ƒå¼å­˜å‚¨ å¤§è§„æ¨¡åº”ç”¨ æ°´å¹³æ‰©å±•ã€é«˜æ€§èƒ½ å¤æ‚åº¦é«˜ã€è¿ç»´æˆæœ¬é«˜ æ··åˆæ¶æ„ åˆ†ç‰‡+å‰¯æœ¬é›† ä¼ä¸šçº§åº”ç”¨ é«˜å¯ç”¨+é«˜æ€§èƒ½ æ¶æ„å¤æ‚ã€èµ„æºæ¶ˆè€—å¤§ MongoDBåˆ†å¸ƒå¼æ¶æ„æ¼”è¿› graph TB subgraph \u0026#34;å•æœºæ¨¡å¼\u0026#34; A[MongoDB Server] A1[Application] A1 --\u0026gt; A end subgraph \u0026#34;å‰¯æœ¬é›†æ¨¡å¼\u0026#34; B[Primary] B1[Secondary 1] B2[Secondary 2] B3[Application] B --\u0026gt; B1 B --\u0026gt; B2 B3 --\u0026gt; B B3 -.-\u0026gt; B1 B3 -.-\u0026gt; B2 end subgraph \u0026#34;åˆ†ç‰‡é›†ç¾¤æ¨¡å¼\u0026#34; C[mongos] C1[Config Server RS] C2[Shard 1 RS] C3[Shard 2 RS] C4[Shard 3 RS] C5[Application] C --\u0026gt; C1 C --\u0026gt; C2 C --\u0026gt; C3 C --\u0026gt; C4 C5 --\u0026gt; C end MongoDBå‰¯æœ¬é›†é…ç½®ä¸ç®¡ç† å‰¯æœ¬é›†æ¶æ„è®¾è®¡ MongoDBå‰¯æœ¬é›†æ˜¯ä¸€ç»„ç»´æŠ¤ç›¸åŒæ•°æ®é›†çš„MongoDBæœåŠ¡å™¨ï¼Œæä¾›å†—ä½™å’Œé«˜å¯ç”¨æ€§ã€‚\n","content":"MongoDBåˆ†å¸ƒå¼æ¶æ„è®¾è®¡ä¸å®è·µï¼šä»å‰¯æœ¬é›†åˆ°åˆ†ç‰‡é›†ç¾¤çš„å®Œæ•´æ–¹æ¡ˆ MongoDBä½œä¸ºé¢†å…ˆçš„NoSQLæ–‡æ¡£æ•°æ®åº“ï¼Œåœ¨ç°ä»£åº”ç”¨æ¶æ„ä¸­å¹¿æ³›åº”ç”¨ã€‚éšç€æ•°æ®é‡çš„å¢é•¿å’Œä¸šåŠ¡å¤æ‚åº¦çš„æå‡ï¼Œå•æœºMongoDBå·²æ— æ³•æ»¡è¶³é«˜å¯ç”¨ã€é«˜æ€§èƒ½çš„éœ€æ±‚ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨MongoDBåˆ†å¸ƒå¼æ¶æ„çš„è®¾è®¡åŸç†å’Œå®è·µæ–¹æ¡ˆï¼Œä»åŸºç¡€çš„å‰¯æœ¬é›†åˆ°å¤æ‚çš„åˆ†ç‰‡é›†ç¾¤éƒ¨ç½²ã€‚\nMongoDBåˆ†å¸ƒå¼æ¶æ„æ¦‚è¿° MongoDBé›†ç¾¤æ¶æ„å¯¹æ¯” æ¶æ„æ¨¡å¼ ç‰¹ç‚¹ é€‚ç”¨åœºæ™¯ ä¼˜ç‚¹ ç¼ºç‚¹ å•æœºæ¨¡å¼ å•ä¸ªMongoDBå®ä¾‹ å¼€å‘æµ‹è¯•ã€å°å‹åº”ç”¨ ç®€å•æ˜“ç”¨ã€æ€§èƒ½é«˜ æ— é«˜å¯ç”¨ã€å®¹é‡å—é™ å‰¯æœ¬é›† ä¸€ä¸»å¤šä»ï¼Œè‡ªåŠ¨æ•…éšœè½¬ç§» ä¸­ç­‰è§„æ¨¡åº”ç”¨ é«˜å¯ç”¨ã€è¯»æ‰©å±• å†™æ€§èƒ½æ— æ³•æ‰©å±• åˆ†ç‰‡é›†ç¾¤ æ°´å¹³åˆ†ç‰‡ï¼Œåˆ†å¸ƒå¼å­˜å‚¨ å¤§è§„æ¨¡åº”ç”¨ æ°´å¹³æ‰©å±•ã€é«˜æ€§èƒ½ å¤æ‚åº¦é«˜ã€è¿ç»´æˆæœ¬é«˜ æ··åˆæ¶æ„ åˆ†ç‰‡+å‰¯æœ¬é›† ä¼ä¸šçº§åº”ç”¨ é«˜å¯ç”¨+é«˜æ€§èƒ½ æ¶æ„å¤æ‚ã€èµ„æºæ¶ˆè€—å¤§ MongoDBåˆ†å¸ƒå¼æ¶æ„æ¼”è¿› graph TB subgraph \u0026amp;#34;å•æœºæ¨¡å¼\u0026amp;#34; A[MongoDB Server] A1[Application] A1 --\u0026amp;gt; A end subgraph \u0026amp;#34;å‰¯æœ¬é›†æ¨¡å¼\u0026amp;#34; â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["MongoDB","åˆ†å¸ƒå¼æ¶æ„","å‰¯æœ¬é›†","åˆ†ç‰‡é›†ç¾¤","é«˜å¯ç”¨","æ•°æ®åˆ†å¸ƒ","æ•…éšœè½¬ç§»","æ€§èƒ½ä¼˜åŒ–"],"categories":["æ•°æ®åº“"],"author":"æ•°æ®åº“æ¶æ„å›¢é˜Ÿ","readingTime":21,"wordCount":4327,"section":"posts","type":"posts","draft":false,"featured":false,"series":["MongoDBå®æˆ˜"]},{"title":"MySQLåˆ†åº“åˆ†è¡¨å®è·µï¼šä»ç†è®ºåˆ°è½åœ°çš„å…¨é¢æŒ‡å—","url":"https://www.dishuihengxin.com/posts/database-mysql-sharding/","summary":"MySQLåˆ†åº“åˆ†è¡¨å®è·µï¼šä»ç†è®ºåˆ°è½åœ°çš„å…¨é¢æŒ‡å— éšç€ä¸šåŠ¡æ•°æ®é‡çš„çˆ†å‘å¼å¢é•¿ï¼Œå•åº“å•è¡¨çš„MySQLæ¶æ„å¾€å¾€æ— æ³•æ»¡è¶³é«˜å¹¶å‘ã€å¤§æ•°æ®é‡çš„ä¸šåŠ¡éœ€æ±‚ã€‚åˆ†åº“åˆ†è¡¨ä½œä¸ºè§£å†³æ•°æ®åº“å®¹é‡å’Œæ€§èƒ½ç“¶é¢ˆçš„å…³é”®æŠ€æœ¯ï¼Œå·²æˆä¸ºå¤§å‹äº’è”ç½‘åº”ç”¨çš„æ ‡é…ã€‚æœ¬æ–‡å°†ä»ç†è®ºåˆ°å®è·µï¼Œå…¨é¢ä»‹ç»MySQLåˆ†åº“åˆ†è¡¨çš„å®æ–½æ–¹æ¡ˆå’Œæœ€ä½³å®è·µã€‚\nåˆ†åº“åˆ†è¡¨åŸºç¡€ç†è®º ä¸ºä»€ä¹ˆéœ€è¦åˆ†åº“åˆ†è¡¨ åœ¨è®¨è®ºå…·ä½“å®ç°æ–¹æ¡ˆå‰ï¼Œæˆ‘ä»¬éœ€è¦æ˜ç¡®åˆ†åº“åˆ†è¡¨çš„é©±åŠ¨å› ç´ ï¼š\né©±åŠ¨å› ç´  è¡¨ç° å½±å“ è§£å†³æ–¹æ¡ˆ æ•°æ®é‡è¿‡å¤§ å•è¡¨æ•°æ®è¶…è¿‡åƒä¸‡çº§ æŸ¥è¯¢æ€§èƒ½ä¸‹é™ï¼Œç´¢å¼•æ•ˆç‡é™ä½ æ°´å¹³åˆ†è¡¨ å†™å…¥å‹åŠ›å¤§ é«˜å¹¶å‘å†™å…¥å¯¼è‡´é”ç«äº‰ å†™å…¥å»¶è¿Ÿå¢åŠ ï¼Œå½±å“ç”¨æˆ·ä½“éªŒ æ°´å¹³åˆ†åº“ è¯»å–å‹åŠ›å¤§ æŸ¥è¯¢å“åº”æ—¶é—´å¢åŠ  ç³»ç»Ÿæ•´ä½“æ€§èƒ½ä¸‹é™ è¯»å†™åˆ†ç¦»+åˆ†åº“ å•æœºèµ„æºç“¶é¢ˆ CPU/å†…å­˜/IOèµ„æºä¸è¶³ æ•°æ®åº“æœåŠ¡ä¸ç¨³å®š åˆ†åº“+ç¡¬ä»¶æ‰©å®¹ åˆ†åº“åˆ†è¡¨çš„åŸºæœ¬æ¦‚å¿µ graph TB subgraph \u0026#34;åˆ†åº“åˆ†è¡¨æ¨¡å¼\u0026#34; A[åŸå§‹æ•°æ®åº“] subgraph \u0026#34;å‚ç›´åˆ†åº“\u0026#34; B1[ç”¨æˆ·åº“] B2[è®¢å•åº“] B3[å•†å“åº“] end subgraph \u0026#34;å‚ç›´åˆ†è¡¨\u0026#34; C1[ç”¨æˆ·åŸºæœ¬ä¿¡æ¯è¡¨] C2[ç”¨æˆ·è¯¦ç»†ä¿¡æ¯è¡¨] C3[ç”¨æˆ·è¡Œä¸ºè¡¨] end subgraph \u0026#34;æ°´å¹³åˆ†åº“\u0026#34; D1[åº“0] D2[åº“1] D3[åº“n] end subgraph \u0026#34;æ°´å¹³åˆ†è¡¨\u0026#34; E1[è¡¨0] E2[è¡¨1] E3[è¡¨n] end A --\u0026gt; B1 A --\u0026gt; B2 A --\u0026gt; B3 A --\u0026gt; C1 A --\u0026gt; C2 A --\u0026gt; C3 A --\u0026gt; D1 A --\u0026gt; D2 A --\u0026gt; D3 A --\u0026gt; E1 A --\u0026gt; E2 A --\u0026gt; E3 end å‚ç›´æ‹†åˆ† å‚ç›´åˆ†åº“ï¼šæŒ‰ä¸šåŠ¡é¢†åŸŸå°†ä¸åŒè¡¨æ‹†åˆ†åˆ°ä¸åŒçš„æ•°æ®åº“ä¸­ å‚ç›´åˆ†è¡¨ï¼šå°†ä¸€ä¸ªè¡¨æŒ‰å­—æ®µæ‹†åˆ†æˆå¤šä¸ªè¡¨ï¼Œæ¯ä¸ªè¡¨å­˜å‚¨éƒ¨åˆ†å­—æ®µ æ°´å¹³æ‹†åˆ† æ°´å¹³åˆ†åº“ï¼šå°†åŒä¸€ä¸ªè¡¨çš„æ•°æ®æŒ‰ç…§æŸä¸ªç»´åº¦åˆ†æ•£åˆ°ä¸åŒçš„æ•°æ®åº“ä¸­ æ°´å¹³åˆ†è¡¨ï¼šå°†åŒä¸€ä¸ªè¡¨çš„æ•°æ®æŒ‰ç…§æŸä¸ªç»´åº¦åˆ†æ•£åˆ°åŒä¸€ä¸ªæ•°æ®åº“çš„å¤šä¸ªè¡¨ä¸­ åˆ†ç‰‡ç­–ç•¥è®¾è®¡ åˆ†ç‰‡é”®é€‰æ‹© åˆ†ç‰‡é”®çš„é€‰æ‹©ç›´æ¥å½±å“åˆ†åº“åˆ†è¡¨çš„æ•ˆç‡å’Œå‡è¡¡æ€§ï¼Œå¸¸è§çš„åˆ†ç‰‡é”®é€‰æ‹©ç­–ç•¥ï¼š\n","content":"MySQLåˆ†åº“åˆ†è¡¨å®è·µï¼šä»ç†è®ºåˆ°è½åœ°çš„å…¨é¢æŒ‡å— éšç€ä¸šåŠ¡æ•°æ®é‡çš„çˆ†å‘å¼å¢é•¿ï¼Œå•åº“å•è¡¨çš„MySQLæ¶æ„å¾€å¾€æ— æ³•æ»¡è¶³é«˜å¹¶å‘ã€å¤§æ•°æ®é‡çš„ä¸šåŠ¡éœ€æ±‚ã€‚åˆ†åº“åˆ†è¡¨ä½œä¸ºè§£å†³æ•°æ®åº“å®¹é‡å’Œæ€§èƒ½ç“¶é¢ˆçš„å…³é”®æŠ€æœ¯ï¼Œå·²æˆä¸ºå¤§å‹äº’è”ç½‘åº”ç”¨çš„æ ‡é…ã€‚æœ¬æ–‡å°†ä»ç†è®ºåˆ°å®è·µï¼Œå…¨é¢ä»‹ç»MySQLåˆ†åº“åˆ†è¡¨çš„å®æ–½æ–¹æ¡ˆå’Œæœ€ä½³å®è·µã€‚\nåˆ†åº“åˆ†è¡¨åŸºç¡€ç†è®º ä¸ºä»€ä¹ˆéœ€è¦åˆ†åº“åˆ†è¡¨ åœ¨è®¨è®ºå…·ä½“å®ç°æ–¹æ¡ˆå‰ï¼Œæˆ‘ä»¬éœ€è¦æ˜ç¡®åˆ†åº“åˆ†è¡¨çš„é©±åŠ¨å› ç´ ï¼š\né©±åŠ¨å› ç´  è¡¨ç° å½±å“ è§£å†³æ–¹æ¡ˆ æ•°æ®é‡è¿‡å¤§ å•è¡¨æ•°æ®è¶…è¿‡åƒä¸‡çº§ æŸ¥è¯¢æ€§èƒ½ä¸‹é™ï¼Œç´¢å¼•æ•ˆç‡é™ä½ æ°´å¹³åˆ†è¡¨ å†™å…¥å‹åŠ›å¤§ é«˜å¹¶å‘å†™å…¥å¯¼è‡´é”ç«äº‰ å†™å…¥å»¶è¿Ÿå¢åŠ ï¼Œå½±å“ç”¨æˆ·ä½“éªŒ æ°´å¹³åˆ†åº“ è¯»å–å‹åŠ›å¤§ æŸ¥è¯¢å“åº”æ—¶é—´å¢åŠ  ç³»ç»Ÿæ•´ä½“æ€§èƒ½ä¸‹é™ è¯»å†™åˆ†ç¦»+åˆ†åº“ å•æœºèµ„æºç“¶é¢ˆ CPU/å†…å­˜/IOèµ„æºä¸è¶³ æ•°æ®åº“æœåŠ¡ä¸ç¨³å®š åˆ†åº“+ç¡¬ä»¶æ‰©å®¹ åˆ†åº“åˆ†è¡¨çš„åŸºæœ¬æ¦‚å¿µ graph TB subgraph \u0026amp;#34;åˆ†åº“åˆ†è¡¨æ¨¡å¼\u0026amp;#34; A[åŸå§‹æ•°æ®åº“] subgraph \u0026amp;#34;å‚ç›´åˆ†åº“\u0026amp;#34; B1[ç”¨æˆ·åº“] B2[è®¢å•åº“] B3[å•†å“åº“] end subgraph \u0026amp;#34;å‚ç›´åˆ†è¡¨\u0026amp;#34; C1[ç”¨æˆ·åŸºæœ¬ä¿¡æ¯è¡¨] C2[ç”¨æˆ·è¯¦ç»†ä¿¡æ¯è¡¨] â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["MySQL","åˆ†åº“åˆ†è¡¨","æ•°æ®åº“æ¶æ„","ShardingSphere","MyCat","æ•°æ®è¿ç§»","åˆ†ç‰‡ç­–ç•¥"],"categories":["æ•°æ®åº“"],"author":"æ•°æ®åº“æ¶æ„å›¢é˜Ÿ","readingTime":17,"wordCount":3419,"section":"posts","type":"posts","draft":false,"featured":false,"series":["MySQLå®æˆ˜"]},{"title":"PostgreSQLé«˜å¯ç”¨æ¶æ„è®¾è®¡ä¸å®è·µï¼šä»ä¸»ä»å¤åˆ¶åˆ°åˆ†å¸ƒå¼é›†ç¾¤","url":"https://www.dishuihengxin.com/posts/database-postgresql-ha/","summary":"PostgreSQLé«˜å¯ç”¨æ¶æ„è®¾è®¡ä¸å®è·µï¼šä»ä¸»ä»å¤åˆ¶åˆ°åˆ†å¸ƒå¼é›†ç¾¤ åœ¨ç°ä»£ä¼ä¸šçº§åº”ç”¨ä¸­ï¼Œæ•°æ®åº“çš„é«˜å¯ç”¨æ€§æ˜¯ç³»ç»Ÿç¨³å®šè¿è¡Œçš„åŸºçŸ³ã€‚PostgreSQLä½œä¸ºåŠŸèƒ½å¼ºå¤§çš„å¼€æºå…³ç³»å‹æ•°æ®åº“ï¼Œæä¾›äº†å¤šç§é«˜å¯ç”¨è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨PostgreSQLé«˜å¯ç”¨æ¶æ„çš„è®¾è®¡åŸç†å’Œå®è·µæ–¹æ¡ˆï¼Œä»åŸºç¡€çš„ä¸»ä»å¤åˆ¶åˆ°å¤æ‚çš„åˆ†å¸ƒå¼é›†ç¾¤éƒ¨ç½²ã€‚\nPostgreSQLé«˜å¯ç”¨æ¶æ„æ¦‚è¿° é«˜å¯ç”¨æ€§æŒ‡æ ‡å®šä¹‰ åœ¨è®¾è®¡PostgreSQLé«˜å¯ç”¨æ¶æ„æ—¶ï¼Œæˆ‘ä»¬éœ€è¦æ˜ç¡®ä»¥ä¸‹å…³é”®æŒ‡æ ‡ï¼š\næŒ‡æ ‡ å®šä¹‰ ç›®æ ‡å€¼ å½±å“å› ç´  å¯ç”¨æ€§(Availability) ç³»ç»Ÿæ­£å¸¸è¿è¡Œæ—¶é—´æ¯”ä¾‹ 99.9% - 99.99% æ•…éšœæ£€æµ‹ã€åˆ‡æ¢æ—¶é—´ æ¢å¤æ—¶é—´ç›®æ ‡(RTO) ç³»ç»Ÿæ•…éšœåæ¢å¤æœåŠ¡çš„æœ€å¤§æ—¶é—´ \u0026lt; 5åˆ†é’Ÿ è‡ªåŠ¨åŒ–ç¨‹åº¦ã€ç½‘ç»œå»¶è¿Ÿ æ¢å¤ç‚¹ç›®æ ‡(RPO) å¯æ¥å—çš„æœ€å¤§æ•°æ®ä¸¢å¤±é‡ \u0026lt; 1åˆ†é’Ÿ å¤åˆ¶å»¶è¿Ÿã€åŒæ­¥ç­–ç•¥ æ•…éšœæ£€æµ‹æ—¶é—´(MTTR) ä»æ•…éšœå‘ç”Ÿåˆ°æ£€æµ‹åˆ°çš„æ—¶é—´ \u0026lt; 30ç§’ ç›‘æ§é¢‘ç‡ã€å¥åº·æ£€æŸ¥ é«˜å¯ç”¨æ¶æ„æ¨¡å¼ graph TB subgraph \u0026#34;å•æœºæ¨¡å¼\u0026#34; A[Primary Server] A1[å®šæœŸå¤‡ä»½] A --\u0026gt; A1 end subgraph \u0026#34;ä¸»ä»å¤åˆ¶æ¨¡å¼\u0026#34; B[Primary Server] B1[Standby Server 1] B2[Standby Server 2] B --\u0026gt; B1 B --\u0026gt; B2 end subgraph \u0026#34;ä¸»ä»+è´Ÿè½½å‡è¡¡æ¨¡å¼\u0026#34; C[Primary Server] C1[Standby Server 1] C2[Standby Server 2] C3[Load Balancer] C3 --\u0026gt; C C3 --\u0026gt; C1 C3 --\u0026gt; C2 C --\u0026gt; C1 C --\u0026gt; C2 end subgraph \u0026#34;åˆ†å¸ƒå¼é›†ç¾¤æ¨¡å¼\u0026#34; D[Node 1] D1[Node 2] D2[Node 3] D3[Coordinator] D3 --\u0026gt; D D3 --\u0026gt; D1 D3 --\u0026gt; D2 D \u0026lt;--\u0026gt; D1 D1 \u0026lt;--\u0026gt; D2 D2 \u0026lt;--\u0026gt; D end æµå¤åˆ¶(Streaming Replication)é…ç½® ä¸»æœåŠ¡å™¨é…ç½® é¦–å…ˆé…ç½®ä¸»æœåŠ¡å™¨çš„PostgreSQLå‚æ•°ï¼š\n","content":"PostgreSQLé«˜å¯ç”¨æ¶æ„è®¾è®¡ä¸å®è·µï¼šä»ä¸»ä»å¤åˆ¶åˆ°åˆ†å¸ƒå¼é›†ç¾¤ åœ¨ç°ä»£ä¼ä¸šçº§åº”ç”¨ä¸­ï¼Œæ•°æ®åº“çš„é«˜å¯ç”¨æ€§æ˜¯ç³»ç»Ÿç¨³å®šè¿è¡Œçš„åŸºçŸ³ã€‚PostgreSQLä½œä¸ºåŠŸèƒ½å¼ºå¤§çš„å¼€æºå…³ç³»å‹æ•°æ®åº“ï¼Œæä¾›äº†å¤šç§é«˜å¯ç”¨è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨PostgreSQLé«˜å¯ç”¨æ¶æ„çš„è®¾è®¡åŸç†å’Œå®è·µæ–¹æ¡ˆï¼Œä»åŸºç¡€çš„ä¸»ä»å¤åˆ¶åˆ°å¤æ‚çš„åˆ†å¸ƒå¼é›†ç¾¤éƒ¨ç½²ã€‚\nPostgreSQLé«˜å¯ç”¨æ¶æ„æ¦‚è¿° é«˜å¯ç”¨æ€§æŒ‡æ ‡å®šä¹‰ åœ¨è®¾è®¡PostgreSQLé«˜å¯ç”¨æ¶æ„æ—¶ï¼Œæˆ‘ä»¬éœ€è¦æ˜ç¡®ä»¥ä¸‹å…³é”®æŒ‡æ ‡ï¼š\næŒ‡æ ‡ å®šä¹‰ ç›®æ ‡å€¼ å½±å“å› ç´  å¯ç”¨æ€§(Availability) ç³»ç»Ÿæ­£å¸¸è¿è¡Œæ—¶é—´æ¯”ä¾‹ 99.9% - 99.99% æ•…éšœæ£€æµ‹ã€åˆ‡æ¢æ—¶é—´ æ¢å¤æ—¶é—´ç›®æ ‡(RTO) ç³»ç»Ÿæ•…éšœåæ¢å¤æœåŠ¡çš„æœ€å¤§æ—¶é—´ \u0026amp;lt; 5åˆ†é’Ÿ è‡ªåŠ¨åŒ–ç¨‹åº¦ã€ç½‘ç»œå»¶è¿Ÿ æ¢å¤ç‚¹ç›®æ ‡(RPO) å¯æ¥å—çš„æœ€å¤§æ•°æ®ä¸¢å¤±é‡ \u0026amp;lt; 1åˆ†é’Ÿ å¤åˆ¶å»¶è¿Ÿã€åŒæ­¥ç­–ç•¥ æ•…éšœæ£€æµ‹æ—¶é—´(MTTR) ä»æ•…éšœå‘ç”Ÿåˆ°æ£€æµ‹åˆ°çš„æ—¶é—´ \u0026amp;lt; 30ç§’ ç›‘æ§é¢‘ç‡ã€å¥åº·æ£€æŸ¥ é«˜å¯ç”¨æ¶æ„æ¨¡å¼ graph TB subgraph \u0026amp;#34;å•æœºæ¨¡å¼\u0026amp;#34; A[Primary Server] A1[å®šæœŸå¤‡ä»½] A --\u0026amp;gt; A1 â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["PostgreSQL","é«˜å¯ç”¨","ä¸»ä»å¤åˆ¶","æ•…éšœè½¬ç§»","è´Ÿè½½å‡è¡¡","åˆ†å¸ƒå¼","æ•°æ®åº“æ¶æ„"],"categories":["æ•°æ®åº“"],"author":"æ•°æ®åº“æ¶æ„å›¢é˜Ÿ","readingTime":10,"wordCount":2044,"section":"posts","type":"posts","draft":false,"featured":false,"series":["PostgreSQLå®æˆ˜"]},{"title":"Prometheusç›‘æ§ç³»ç»Ÿå®æˆ˜ï¼šä»é›¶åˆ°ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²","url":"https://www.dishuihengxin.com/posts/devops-monitoring-prometheus/","summary":"Prometheusç›‘æ§ç³»ç»Ÿå®æˆ˜ï¼šä»é›¶åˆ°ç”Ÿäº§ç¯å¢ƒéƒ¨ç½² åœ¨ç°ä»£å¾®æœåŠ¡æ¶æ„ä¸­ï¼Œç›‘æ§ç³»ç»Ÿæ˜¯ä¿éšœæœåŠ¡ç¨³å®šæ€§çš„å…³é”®åŸºç¡€è®¾æ–½ã€‚Prometheusä½œä¸ºCNCFæ¯•ä¸šé¡¹ç›®ï¼Œå·²æˆä¸ºäº‘åŸç”Ÿç›‘æ§çš„äº‹å®æ ‡å‡†ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨Prometheusçš„æ¶æ„è®¾è®¡ã€éƒ¨ç½²å®è·µå’Œæ€§èƒ½ä¼˜åŒ–ã€‚\nPrometheusæ¶æ„æ¦‚è§ˆ æ ¸å¿ƒç»„ä»¶ Prometheusç›‘æ§ç³»ç»Ÿç”±ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶æ„æˆï¼š\n# prometheus.yml æ ¸å¿ƒé…ç½® global: scrape_interval: 15s evaluation_interval: 15s external_labels: cluster: \u0026#39;production\u0026#39; region: \u0026#39;us-west-2\u0026#39; rule_files: - \u0026#34;rules/*.yml\u0026#34; scrape_configs: - job_name: \u0026#39;prometheus\u0026#39; static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] - job_name: \u0026#39;node-exporter\u0026#39; static_configs: - targets: [\u0026#39;node1:9100\u0026#39;, \u0026#39;node2:9100\u0026#39;, \u0026#39;node3:9100\u0026#39;] scrape_interval: 10s metrics_path: /metrics - job_name: \u0026#39;kubernetes-pods\u0026#39; kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true æ•°æ®æ¨¡å‹ Prometheusé‡‡ç”¨å¤šç»´æ—¶é—´åºåˆ—æ•°æ®æ¨¡å‹ï¼š\n","content":"Prometheusç›‘æ§ç³»ç»Ÿå®æˆ˜ï¼šä»é›¶åˆ°ç”Ÿäº§ç¯å¢ƒéƒ¨ç½² åœ¨ç°ä»£å¾®æœåŠ¡æ¶æ„ä¸­ï¼Œç›‘æ§ç³»ç»Ÿæ˜¯ä¿éšœæœåŠ¡ç¨³å®šæ€§çš„å…³é”®åŸºç¡€è®¾æ–½ã€‚Prometheusä½œä¸ºCNCFæ¯•ä¸šé¡¹ç›®ï¼Œå·²æˆä¸ºäº‘åŸç”Ÿç›‘æ§çš„äº‹å®æ ‡å‡†ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨Prometheusçš„æ¶æ„è®¾è®¡ã€éƒ¨ç½²å®è·µå’Œæ€§èƒ½ä¼˜åŒ–ã€‚\nPrometheusæ¶æ„æ¦‚è§ˆ æ ¸å¿ƒç»„ä»¶ Prometheusç›‘æ§ç³»ç»Ÿç”±ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶æ„æˆï¼š\n# prometheus.yml æ ¸å¿ƒé…ç½® global: scrape_interval: 15s evaluation_interval: 15s external_labels: cluster: \u0026amp;#39;production\u0026amp;#39; region: \u0026amp;#39;us-west-2\u0026amp;#39; rule_files: - \u0026amp;#34;rules/*.yml\u0026amp;#34; scrape_configs: - job_name: \u0026amp;#39;prometheus\u0026amp;#39; static_configs: - targets: [\u0026amp;#39;localhost:9090\u0026amp;#39;] - job_name: \u0026amp;#39;node-exporter\u0026amp;#39; â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["Prometheus","ç›‘æ§","è¿ç»´","DevOps","æ€§èƒ½ä¼˜åŒ–"],"categories":["è¿ç»´"],"author":"è¿ç»´ä¸“å®¶","readingTime":4,"wordCount":818,"section":"posts","type":"posts","draft":false,"featured":false,"series":["ç›‘æ§ç³»ç»Ÿå®æˆ˜"]},{"title":"React åŸºç¡€å…¥é—¨ - å¼€å§‹å­¦ä¹ ","url":"https://www.dishuihengxin.com/tutorials/react-basics/getting-started/","summary":"æœ¬æ•™ç¨‹å°†å¸¦ä½ ä»é›¶å¼€å§‹å­¦ä¹  Reactï¼Œäº†è§£ç»„ä»¶ã€JSXã€çŠ¶æ€ç®¡ç†ç­‰æ ¸å¿ƒæ¦‚å¿µï¼Œä¸ºåç»­æ·±å…¥å­¦ä¹ æ‰“ä¸‹åšå®åŸºç¡€ã€‚","content":"æ¬¢è¿æ¥åˆ° React ä¸–ç•Œ React æ˜¯ç”± Facebook å¼€å‘çš„ä¸€ä¸ªç”¨äºæ„å»ºç”¨æˆ·ç•Œé¢çš„ JavaScript åº“ã€‚å®ƒé‡‡ç”¨ç»„ä»¶åŒ–çš„å¼€å‘æ–¹å¼ï¼Œè®©æˆ‘ä»¬èƒ½å¤Ÿæ„å»ºå¯å¤ç”¨ã€å¯ç»´æŠ¤çš„å‰ç«¯åº”ç”¨ã€‚\nä¸ºä»€ä¹ˆé€‰æ‹© Reactï¼Ÿ React æœ‰ä»¥ä¸‹å‡ ä¸ªæ˜¾è‘—ä¼˜åŠ¿ï¼š\nç»„ä»¶åŒ–å¼€å‘ - å°† UI æ‹†åˆ†ä¸ºç‹¬ç«‹ã€å¯å¤ç”¨çš„ç»„ä»¶ è™šæ‹Ÿ DOM - æä¾›é«˜æ•ˆçš„é¡µé¢æ›´æ–°æœºåˆ¶ å•å‘æ•°æ®æµ - è®©æ•°æ®æµå‘æ›´åŠ æ¸…æ™°å’Œå¯é¢„æµ‹ ä¸°å¯Œçš„ç”Ÿæ€ç³»ç»Ÿ - æ‹¥æœ‰åºå¤§çš„ç¤¾åŒºå’Œä¸°å¯Œçš„ç¬¬ä¸‰æ–¹åº“ å­¦ä¹ ç›®æ ‡ é€šè¿‡æœ¬ç³»åˆ—æ•™ç¨‹ï¼Œä½ å°†å­¦ä¼šï¼š\nâœ… React çš„åŸºæœ¬æ¦‚å¿µå’Œæ ¸å¿ƒæ€æƒ³ âœ… å¦‚ä½•åˆ›å»ºå’Œä½¿ç”¨ React ç»„ä»¶ âœ… JSX è¯­æ³•çš„ä½¿ç”¨æ–¹æ³• âœ… çŠ¶æ€ç®¡ç†å’Œäº‹ä»¶å¤„ç† âœ… ç»„ä»¶é—´çš„æ•°æ®ä¼ é€’ âœ… React Hooks çš„ä½¿ç”¨ ç¯å¢ƒå‡†å¤‡ åœ¨å¼€å§‹å­¦ä¹ ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡å¼€å‘ç¯å¢ƒã€‚\nå‰ç½®è¦æ±‚ ç¡®ä¿ä½ çš„ç³»ç»Ÿå·²å®‰è£…ï¼š\nNode.js (ç‰ˆæœ¬ 14 æˆ–æ›´é«˜) npm æˆ– yarn åŒ…ç®¡ç†å™¨ ä»£ç ç¼–è¾‘å™¨ï¼ˆæ¨è VS Codeï¼‰ åˆ›å»ºç¬¬ä¸€ä¸ª React åº”ç”¨ ä½¿ç”¨ Create React App å¿«é€Ÿåˆ›å»ºé¡¹ç›®ï¼š\n# ä½¿ç”¨ npx åˆ›å»ºæ–°é¡¹ç›® npx â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["React","JavaScript","å‰ç«¯å¼€å‘","å…¥é—¨æ•™ç¨‹"],"categories":["æ•™ç¨‹"],"author":"åšä¸»","readingTime":2,"wordCount":327,"section":"tutorials","type":"tutorials","draft":false,"featured":false,"series":null},{"title":"React åŸºç¡€å…¥é—¨ - ç»„ä»¶ä¸ Props","url":"https://www.dishuihengxin.com/tutorials/react-basics/components/","summary":"å­¦ä¹ å¦‚ä½•åˆ›å»ºå¯å¤ç”¨çš„ React ç»„ä»¶ï¼Œç†è§£ Props çš„æ¦‚å¿µå’Œä½¿ç”¨æ–¹æ³•ï¼Œæ„å»ºæ›´åŠ æ¨¡å—åŒ–çš„åº”ç”¨ã€‚","content":"React ç»„ä»¶æ·±å…¥ åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ç¬¬ä¸€ä¸ª React åº”ç”¨å¹¶äº†è§£äº†åŸºæœ¬çš„ç»„ä»¶ç»“æ„ã€‚ç°åœ¨è®©æˆ‘ä»¬æ·±å…¥å­¦ä¹ å¦‚ä½•åˆ›å»ºå’Œä½¿ç”¨è‡ªå®šä¹‰ç»„ä»¶ã€‚\nä»€ä¹ˆæ˜¯ç»„ä»¶ï¼Ÿ ç»„ä»¶æ˜¯ React åº”ç”¨çš„åŸºæœ¬æ„å»ºå—ã€‚ä½ å¯ä»¥æŠŠç»„ä»¶æƒ³è±¡æˆï¼š\nğŸ§© ä¹é«˜ç§¯æœ¨ - å¯ä»¥ç»„åˆæˆå¤æ‚çš„ç»“æ„ ğŸ—ï¸ å»ºç­‘æ¨¡å— - æ¯ä¸ªç»„ä»¶è´Ÿè´£ç‰¹å®šçš„åŠŸèƒ½ ğŸ”§ å·¥å…·å‡½æ•° - æ¥æ”¶è¾“å…¥ï¼Œè¿”å› UI è¾“å‡º åˆ›å»ºä½ çš„ç¬¬ä¸€ä¸ªè‡ªå®šä¹‰ç»„ä»¶ è®©æˆ‘ä»¬åˆ›å»ºä¸€ä¸ªç®€å•çš„æ¬¢è¿ç»„ä»¶ï¼š\n// åˆ›å»º Welcome.js æ–‡ä»¶ import React from \u0026amp;#39;react\u0026amp;#39;; function Welcome(props) { return ( \u0026amp;lt;div className=\u0026amp;#34;welcome-card\u0026amp;#34;\u0026amp;gt; \u0026amp;lt;h2\u0026amp;gt;æ¬¢è¿ï¼Œ{props.name}!\u0026amp;lt;/h2\u0026amp;gt; \u0026amp;lt;p\u0026amp;gt;ä½ çš„è§’è‰²æ˜¯ï¼š{props.role}\u0026amp;lt;/p\u0026amp;gt; \u0026amp;lt;/div\u0026amp;gt; ); } export default Welcome; åœ¨ App ç»„ä»¶ä¸­ä½¿ç”¨ // App.js import React from â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["React","ç»„ä»¶","Props","å‰ç«¯å¼€å‘"],"categories":["æ•™ç¨‹"],"author":"åšä¸»","readingTime":3,"wordCount":582,"section":"tutorials","type":"tutorials","draft":false,"featured":false,"series":null},{"title":"Redisé›†ç¾¤æ¶æ„è®¾è®¡ä¸å®è·µï¼šä»å•æœºåˆ°åˆ†å¸ƒå¼çš„å®Œæ•´æ–¹æ¡ˆ","url":"https://www.dishuihengxin.com/posts/database-redis-cluster/","summary":"Redisé›†ç¾¤æ¶æ„è®¾è®¡ä¸å®è·µï¼šä»å•æœºåˆ°åˆ†å¸ƒå¼çš„å®Œæ•´æ–¹æ¡ˆ Redisä½œä¸ºé«˜æ€§èƒ½çš„å†…å­˜æ•°æ®åº“ï¼Œåœ¨ç°ä»£äº’è”ç½‘æ¶æ„ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚éšç€ä¸šåŠ¡è§„æ¨¡çš„å¢é•¿ï¼Œå•æœºRediså¾€å¾€æ— æ³•æ»¡è¶³é«˜å¹¶å‘ã€å¤§å®¹é‡çš„éœ€æ±‚ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨Redisé›†ç¾¤æ¶æ„çš„è®¾è®¡åŸç†å’Œå®è·µæ–¹æ¡ˆï¼Œä»åŸºç¡€çš„ä¸»ä»å¤åˆ¶åˆ°å¤æ‚çš„åˆ†å¸ƒå¼é›†ç¾¤éƒ¨ç½²ã€‚\nRedisé›†ç¾¤æ¶æ„æ¦‚è¿° Redisé›†ç¾¤æ¨¡å¼å¯¹æ¯” æ¨¡å¼ ç‰¹ç‚¹ é€‚ç”¨åœºæ™¯ ä¼˜ç‚¹ ç¼ºç‚¹ å•æœºæ¨¡å¼ å•ä¸ªRediså®ä¾‹ å¼€å‘æµ‹è¯•ã€å°å‹åº”ç”¨ ç®€å•æ˜“ç”¨ã€æ€§èƒ½é«˜ æ— é«˜å¯ç”¨ã€å®¹é‡å—é™ ä¸»ä»å¤åˆ¶ ä¸€ä¸»å¤šä»ï¼Œè¯»å†™åˆ†ç¦» è¯»å¤šå†™å°‘åœºæ™¯ è¯»æ€§èƒ½æ‰©å±•ã€æ•°æ®å¤‡ä»½ ä¸»èŠ‚ç‚¹å•ç‚¹æ•…éšœ å“¨å…µæ¨¡å¼ ä¸»ä»+è‡ªåŠ¨æ•…éšœè½¬ç§» ä¸­ç­‰è§„æ¨¡åº”ç”¨ è‡ªåŠ¨æ•…éšœè½¬ç§»ã€é«˜å¯ç”¨ å†™æ€§èƒ½æ— æ³•æ‰©å±• Clusteré›†ç¾¤ åˆ†å¸ƒå¼é›†ç¾¤ å¤§è§„æ¨¡åº”ç”¨ æ°´å¹³æ‰©å±•ã€é«˜å¯ç”¨ å¤æ‚åº¦é«˜ã€éƒ¨åˆ†å‘½ä»¤é™åˆ¶ ä»£ç†æ¨¡å¼ é€šè¿‡ä»£ç†åˆ†ç‰‡ å¹³æ»‘è¿ç§»åœºæ™¯ å¯¹å®¢æˆ·ç«¯é€æ˜ ä»£ç†æˆä¸ºç“¶é¢ˆ Redisé›†ç¾¤æ¶æ„æ¼”è¿› graph TB subgraph \u0026#34;å•æœºæ¨¡å¼\u0026#34; A[Redis Server] A1[Client] A1 --\u0026gt; A end subgraph \u0026#34;ä¸»ä»å¤åˆ¶æ¨¡å¼\u0026#34; B[Master] B1[Slave 1] B2[Slave 2] B3[Client] B --\u0026gt; B1 B --\u0026gt; B2 B3 --\u0026gt; B B3 --\u0026gt; B1 B3 --\u0026gt; B2 end subgraph \u0026#34;å“¨å…µæ¨¡å¼\u0026#34; C[Master] C1[Slave 1] C2[Slave 2] C3[Sentinel 1] C4[Sentinel 2] C5[Sentinel 3] C6[Client] C --\u0026gt; C1 C --\u0026gt; C2 C3 --\u0026gt; C C4 --\u0026gt; C C5 --\u0026gt; C C6 --\u0026gt; C3 end subgraph \u0026#34;Clusteré›†ç¾¤æ¨¡å¼\u0026#34; D[Node 1\u0026lt;br/\u0026gt;Master] D1[Node 2\u0026lt;br/\u0026gt;Master] D2[Node 3\u0026lt;br/\u0026gt;Master] D3[Node 4\u0026lt;br/\u0026gt;Slave] D4[Node 5\u0026lt;br/\u0026gt;Slave] D5[Node 6\u0026lt;br/\u0026gt;Slave] D6[Client] D --\u0026gt; D3 D1 --\u0026gt; D4 D2 --\u0026gt; D5 D6 --\u0026gt; D D6 --\u0026gt; D1 D6 --\u0026gt; D2 end ä¸»ä»å¤åˆ¶é…ç½®ä¸ä¼˜åŒ– ä¸»ä»å¤åˆ¶åŸç† Redisä¸»ä»å¤åˆ¶é‡‡ç”¨å¼‚æ­¥å¤åˆ¶æœºåˆ¶ï¼ŒåŒ…å«ä»¥ä¸‹å…³é”®æ­¥éª¤ï¼š\n","content":"Redisé›†ç¾¤æ¶æ„è®¾è®¡ä¸å®è·µï¼šä»å•æœºåˆ°åˆ†å¸ƒå¼çš„å®Œæ•´æ–¹æ¡ˆ Redisä½œä¸ºé«˜æ€§èƒ½çš„å†…å­˜æ•°æ®åº“ï¼Œåœ¨ç°ä»£äº’è”ç½‘æ¶æ„ä¸­æ‰®æ¼”ç€é‡è¦è§’è‰²ã€‚éšç€ä¸šåŠ¡è§„æ¨¡çš„å¢é•¿ï¼Œå•æœºRediså¾€å¾€æ— æ³•æ»¡è¶³é«˜å¹¶å‘ã€å¤§å®¹é‡çš„éœ€æ±‚ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨Redisé›†ç¾¤æ¶æ„çš„è®¾è®¡åŸç†å’Œå®è·µæ–¹æ¡ˆï¼Œä»åŸºç¡€çš„ä¸»ä»å¤åˆ¶åˆ°å¤æ‚çš„åˆ†å¸ƒå¼é›†ç¾¤éƒ¨ç½²ã€‚\nRedisé›†ç¾¤æ¶æ„æ¦‚è¿° Redisé›†ç¾¤æ¨¡å¼å¯¹æ¯” æ¨¡å¼ ç‰¹ç‚¹ é€‚ç”¨åœºæ™¯ ä¼˜ç‚¹ ç¼ºç‚¹ å•æœºæ¨¡å¼ å•ä¸ªRediså®ä¾‹ å¼€å‘æµ‹è¯•ã€å°å‹åº”ç”¨ ç®€å•æ˜“ç”¨ã€æ€§èƒ½é«˜ æ— é«˜å¯ç”¨ã€å®¹é‡å—é™ ä¸»ä»å¤åˆ¶ ä¸€ä¸»å¤šä»ï¼Œè¯»å†™åˆ†ç¦» è¯»å¤šå†™å°‘åœºæ™¯ è¯»æ€§èƒ½æ‰©å±•ã€æ•°æ®å¤‡ä»½ ä¸»èŠ‚ç‚¹å•ç‚¹æ•…éšœ å“¨å…µæ¨¡å¼ ä¸»ä»+è‡ªåŠ¨æ•…éšœè½¬ç§» ä¸­ç­‰è§„æ¨¡åº”ç”¨ è‡ªåŠ¨æ•…éšœè½¬ç§»ã€é«˜å¯ç”¨ å†™æ€§èƒ½æ— æ³•æ‰©å±• Clusteré›†ç¾¤ åˆ†å¸ƒå¼é›†ç¾¤ å¤§è§„æ¨¡åº”ç”¨ æ°´å¹³æ‰©å±•ã€é«˜å¯ç”¨ å¤æ‚åº¦é«˜ã€éƒ¨åˆ†å‘½ä»¤é™åˆ¶ ä»£ç†æ¨¡å¼ é€šè¿‡ä»£ç†åˆ†ç‰‡ å¹³æ»‘è¿ç§»åœºæ™¯ å¯¹å®¢æˆ·ç«¯é€æ˜ ä»£ç†æˆä¸ºç“¶é¢ˆ Redisé›†ç¾¤æ¶æ„æ¼”è¿› graph TB subgraph \u0026amp;#34;å•æœºæ¨¡å¼\u0026amp;#34; A[Redis Server] A1[Client] A1 --\u0026amp;gt; A end subgraph \u0026amp;#34;ä¸»ä»å¤åˆ¶æ¨¡å¼\u0026amp;#34; â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["Redis","é›†ç¾¤æ¶æ„","é«˜å¯ç”¨","ä¸»ä»å¤åˆ¶","å“¨å…µæ¨¡å¼","Redis Cluster","åˆ†ç‰‡","æ•…éšœè½¬ç§»"],"categories":["æ•°æ®åº“"],"author":"æ•°æ®åº“æ¶æ„å›¢é˜Ÿ","readingTime":23,"wordCount":4693,"section":"posts","type":"posts","draft":false,"featured":false,"series":["Rediså®æˆ˜"]},{"title":"Serverlessæ¶æ„æ¨¡å¼ä¸æœ€ä½³å®è·µï¼šæ„å»ºé«˜æ•ˆã€å¯æ‰©å±•çš„æ— æœåŠ¡å™¨åº”ç”¨","url":"https://www.dishuihengxin.com/posts/serverless-architecture-patterns-best-practices/","summary":"å¼•è¨€ Serverlessæ¶æ„ä½œä¸ºäº‘è®¡ç®—çš„é‡è¦å‘å±•æ–¹å‘ï¼Œé€šè¿‡æŠ½è±¡åŒ–æœåŠ¡å™¨ç®¡ç†ï¼Œè®©å¼€å‘è€…ä¸“æ³¨äºä¸šåŠ¡é€»è¾‘å®ç°ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨Serverlessæ¶æ„çš„æ ¸å¿ƒæ¦‚å¿µã€è®¾è®¡æ¨¡å¼ã€æœ€ä½³å®è·µå’Œå®æ–½ç­–ç•¥ï¼Œå¸®åŠ©è¯»è€…æ„å»ºé«˜æ•ˆã€å¯æ‰©å±•çš„æ— æœåŠ¡å™¨åº”ç”¨ã€‚\nç›®å½• Serverlessæ¶æ„æ¦‚è¿° æ ¸å¿ƒè®¾è®¡æ¨¡å¼ å‡½æ•°è®¾è®¡ä¸ä¼˜åŒ– äº‹ä»¶é©±åŠ¨æ¶æ„ çŠ¶æ€ç®¡ç†ç­–ç•¥ æ•°æ®å¤„ç†æ¨¡å¼ å®‰å…¨ä¸æƒé™ç®¡ç† ç›‘æ§ä¸è¿ç»´ æˆæœ¬ä¼˜åŒ– æœ€ä½³å®è·µä¸å»ºè®® Serverlessæ¶æ„æ¦‚è¿° æ¶æ„ç‰¹ç‚¹ graph TB subgraph \u0026#34;ä¼ ç»Ÿæ¶æ„\u0026#34; A[åº”ç”¨ä»£ç ] --\u0026gt; B[åº”ç”¨æœåŠ¡å™¨] B --\u0026gt; C[æ“ä½œç³»ç»Ÿ] C --\u0026gt; D[ç‰©ç†/è™šæ‹ŸæœåŠ¡å™¨] end subgraph \u0026#34;Serverlessæ¶æ„\u0026#34; E[ä¸šåŠ¡é€»è¾‘] --\u0026gt; F[å‡½æ•°è¿è¡Œæ—¶] F --\u0026gt; G[äº‘å¹³å°ç®¡ç†] G --\u0026gt; H[è‡ªåŠ¨æ‰©ç¼©å®¹] end subgraph \u0026#34;Serverlessç”Ÿæ€\u0026#34; I[å‡½æ•°è®¡ç®—\u0026lt;br/\u0026gt;Lambda/Functions] J[APIç½‘å…³\u0026lt;br/\u0026gt;API Gateway] K[äº‹ä»¶æº\u0026lt;br/\u0026gt;EventBridge/Event Grid] L[æ•°æ®å­˜å‚¨\u0026lt;br/\u0026gt;DynamoDB/CosmosDB] M[æ¶ˆæ¯é˜Ÿåˆ—\u0026lt;br/\u0026gt;SQS/Service Bus] N[å·¥ä½œæµ\u0026lt;br/\u0026gt;Step Functions] end E --\u0026gt; I I --\u0026gt; J I --\u0026gt; K I --\u0026gt; L I --\u0026gt; M I --\u0026gt; N Serverlessæ¶æ„åˆ†æå™¨ import json import boto3 import azure.functions as func from google.cloud import functions_v1 from dataclasses import dataclass, asdict from typing import Dict, List, Optional, Any from datetime import datetime, timedelta import logging @dataclass class FunctionMetrics: \u0026#34;\u0026#34;\u0026#34;å‡½æ•°æŒ‡æ ‡æ•°æ®\u0026#34;\u0026#34;\u0026#34; function_name: str invocations: int duration_avg: float duration_p99: float error_rate: float cold_start_rate: float memory_utilization: float cost: float last_updated: datetime @dataclass class ArchitecturePattern: \u0026#34;\u0026#34;\u0026#34;æ¶æ„æ¨¡å¼å®šä¹‰\u0026#34;\u0026#34;\u0026#34; pattern_name: str description: str use_cases: List[str] components: List[str] benefits: List[str] challenges: List[str] implementation_guide: str class ServerlessArchitectureAnalyzer: \u0026#34;\u0026#34;\u0026#34;Serverlessæ¶æ„åˆ†æå™¨\u0026#34;\u0026#34;\u0026#34; def __init__(self, cloud_provider: str = \u0026#34;aws\u0026#34;): self.cloud_provider = cloud_provider self.logger = logging.getLogger(__name__) self._init_clients() def _init_clients(self): \u0026#34;\u0026#34;\u0026#34;åˆå§‹åŒ–äº‘æœåŠ¡å®¢æˆ·ç«¯\u0026#34;\u0026#34;\u0026#34; if self.cloud_provider == \u0026#34;aws\u0026#34;: self.lambda_client = boto3.client(\u0026#39;lambda\u0026#39;) self.cloudwatch_client = boto3.client(\u0026#39;cloudwatch\u0026#39;) self.apigateway_client = boto3.client(\u0026#39;apigateway\u0026#39;) elif self.cloud_provider == \u0026#34;azure\u0026#34;: # Azure Functions å®¢æˆ·ç«¯åˆå§‹åŒ– pass elif self.cloud_provider == \u0026#34;gcp\u0026#34;: # Google Cloud Functions å®¢æˆ·ç«¯åˆå§‹åŒ– pass def analyze_function_performance(self, function_name: str, days: int = 7) -\u0026gt; FunctionMetrics: \u0026#34;\u0026#34;\u0026#34;åˆ†æå‡½æ•°æ€§èƒ½æŒ‡æ ‡\u0026#34;\u0026#34;\u0026#34; try: end_time = datetime.utcnow() start_time = end_time - timedelta(days=days) if self.cloud_provider == \u0026#34;aws\u0026#34;: return self._analyze_aws_lambda(function_name, start_time, end_time) elif self.cloud_provider == \u0026#34;azure\u0026#34;: return self._analyze_azure_function(function_name, start_time, end_time) elif self.cloud_provider == \u0026#34;gcp\u0026#34;: return self._analyze_gcp_function(function_name, start_time, end_time) except Exception as e: self.logger.error(f\u0026#34;åˆ†æå‡½æ•°æ€§èƒ½å¤±è´¥: {e}\u0026#34;) raise def _analyze_aws_lambda(self, function_name: str, start_time: datetime, end_time: datetime) -\u0026gt; FunctionMetrics: \u0026#34;\u0026#34;\u0026#34;åˆ†æAWS Lambdaå‡½æ•°\u0026#34;\u0026#34;\u0026#34; # è·å–è°ƒç”¨æ¬¡æ•° invocations = self._get_cloudwatch_metric( \u0026#39;AWS/Lambda\u0026#39;, \u0026#39;Invocations\u0026#39;, [{\u0026#39;Name\u0026#39;: \u0026#39;FunctionName\u0026#39;, \u0026#39;Value\u0026#39;: function_name}], start_time, end_time, \u0026#39;Sum\u0026#39; ) # è·å–å¹³å‡æ‰§è¡Œæ—¶é—´ duration_avg = self._get_cloudwatch_metric( \u0026#39;AWS/Lambda\u0026#39;, \u0026#39;Duration\u0026#39;, [{\u0026#39;Name\u0026#39;: \u0026#39;FunctionName\u0026#39;, \u0026#39;Value\u0026#39;: function_name}], start_time, end_time, \u0026#39;Average\u0026#39; ) # è·å–P99æ‰§è¡Œæ—¶é—´ duration_p99 = self._get_cloudwatch_metric( \u0026#39;AWS/Lambda\u0026#39;, \u0026#39;Duration\u0026#39;, [{\u0026#39;Name\u0026#39;: \u0026#39;FunctionName\u0026#39;, \u0026#39;Value\u0026#39;: function_name}], start_time, end_time, \u0026#39;ExtendedStatistics\u0026#39;, [\u0026#39;p99\u0026#39;] ) # è·å–é”™è¯¯ç‡ errors = self._get_cloudwatch_metric( \u0026#39;AWS/Lambda\u0026#39;, \u0026#39;Errors\u0026#39;, [{\u0026#39;Name\u0026#39;: \u0026#39;FunctionName\u0026#39;, \u0026#39;Value\u0026#39;: function_name}], start_time, end_time, \u0026#39;Sum\u0026#39; ) error_rate = (errors / invocations * 100) if invocations \u0026gt; 0 else 0 # è·å–å†·å¯åŠ¨ç‡ï¼ˆé€šè¿‡InitDurationæŒ‡æ ‡ä¼°ç®—ï¼‰ cold_starts = self._get_cloudwatch_metric( \u0026#39;AWS/Lambda\u0026#39;, \u0026#39;InitDuration\u0026#39;, [{\u0026#39;Name\u0026#39;: \u0026#39;FunctionName\u0026#39;, \u0026#39;Value\u0026#39;: function_name}], start_time, end_time, \u0026#39;SampleCount\u0026#39; ) cold_start_rate = (cold_starts / invocations * 100) if invocations \u0026gt; 0 else 0 # è®¡ç®—æˆæœ¬ï¼ˆç®€åŒ–è®¡ç®—ï¼‰ function_config = self.lambda_client.get_function_configuration( FunctionName=function_name ) memory_mb = function_config[\u0026#39;MemorySize\u0026#39;] gb_seconds = (duration_avg / 1000) * (memory_mb / 1024) * invocations cost = gb_seconds * 0.0000166667 # AWS Lambdaå®šä»· return FunctionMetrics( function_name=function_name, invocations=int(invocations), duration_avg=duration_avg, duration_p99=duration_p99, error_rate=error_rate, cold_start_rate=cold_start_rate, memory_utilization=0.0, # éœ€è¦é¢å¤–æŒ‡æ ‡è®¡ç®— cost=cost, last_updated=datetime.utcnow() ) def _get_cloudwatch_metric(self, namespace: str, metric_name: str, dimensions: List[Dict], start_time: datetime, end_time: datetime, statistic: str, extended_statistics: List[str] = None) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;è·å–CloudWatchæŒ‡æ ‡\u0026#34;\u0026#34;\u0026#34; try: params = { \u0026#39;Namespace\u0026#39;: namespace, \u0026#39;MetricName\u0026#39;: metric_name, \u0026#39;Dimensions\u0026#39;: dimensions, \u0026#39;StartTime\u0026#39;: start_time, \u0026#39;EndTime\u0026#39;: end_time, \u0026#39;Period\u0026#39;: 3600, # 1å°æ—¶ } if extended_statistics: params[\u0026#39;ExtendedStatistics\u0026#39;] = extended_statistics else: params[\u0026#39;Statistics\u0026#39;] = [statistic] response = self.cloudwatch_client.get_metric_statistics(**params) if response[\u0026#39;Datapoints\u0026#39;]: if extended_statistics: return sum(dp[\u0026#39;ExtendedStatistics\u0026#39;][extended_statistics[0]] for dp in response[\u0026#39;Datapoints\u0026#39;]) / len(response[\u0026#39;Datapoints\u0026#39;]) else: return sum(dp[statistic] for dp in response[\u0026#39;Datapoints\u0026#39;]) return 0.0 except Exception as e: self.logger.error(f\u0026#34;è·å–CloudWatchæŒ‡æ ‡å¤±è´¥: {e}\u0026#34;) return 0.0 def get_architecture_patterns(self) -\u0026gt; List[ArchitecturePattern]: \u0026#34;\u0026#34;\u0026#34;è·å–Serverlessæ¶æ„æ¨¡å¼\u0026#34;\u0026#34;\u0026#34; patterns = [ ArchitecturePattern( pattern_name=\u0026#34;APIåç«¯æ¨¡å¼\u0026#34;, description=\u0026#34;ä½¿ç”¨å‡½æ•°è®¡ç®—æ„å»ºRESTful APIåç«¯æœåŠ¡\u0026#34;, use_cases=[\u0026#34;Webåº”ç”¨åç«¯\u0026#34;, \u0026#34;ç§»åŠ¨åº”ç”¨API\u0026#34;, \u0026#34;ç¬¬ä¸‰æ–¹é›†æˆ\u0026#34;], components=[\u0026#34;API Gateway\u0026#34;, \u0026#34;Lambda Functions\u0026#34;, \u0026#34;æ•°æ®åº“\u0026#34;], benefits=[\u0026#34;è‡ªåŠ¨æ‰©ç¼©å®¹\u0026#34;, \u0026#34;æŒ‰éœ€ä»˜è´¹\u0026#34;, \u0026#34;é«˜å¯ç”¨æ€§\u0026#34;], challenges=[\u0026#34;å†·å¯åŠ¨å»¶è¿Ÿ\u0026#34;, \u0026#34;çŠ¶æ€ç®¡ç†\u0026#34;, \u0026#34;è°ƒè¯•å¤æ‚\u0026#34;], implementation_guide=\u0026#34;é€šè¿‡API Gatewayè§¦å‘Lambdaå‡½æ•°å¤„ç†HTTPè¯·æ±‚\u0026#34; ), ArchitecturePattern( pattern_name=\u0026#34;äº‹ä»¶é©±åŠ¨å¤„ç†\u0026#34;, description=\u0026#34;åŸºäºäº‹ä»¶è§¦å‘çš„å¼‚æ­¥æ•°æ®å¤„ç†\u0026#34;, use_cases=[\u0026#34;æ–‡ä»¶å¤„ç†\u0026#34;, \u0026#34;æ•°æ®ETL\u0026#34;, \u0026#34;æ¶ˆæ¯å¤„ç†\u0026#34;], components=[\u0026#34;äº‹ä»¶æº\u0026#34;, \u0026#34;Lambda Functions\u0026#34;, \u0026#34;å­˜å‚¨æœåŠ¡\u0026#34;], benefits=[\u0026#34;è§£è€¦æ¶æ„\u0026#34;, \u0026#34;å¼¹æ€§æ‰©å±•\u0026#34;, \u0026#34;å®¹é”™èƒ½åŠ›\u0026#34;], challenges=[\u0026#34;äº‹ä»¶é¡ºåº\u0026#34;, \u0026#34;é‡å¤å¤„ç†\u0026#34;, \u0026#34;é”™è¯¯å¤„ç†\u0026#34;], implementation_guide=\u0026#34;é…ç½®äº‹ä»¶æºè§¦å‘å™¨ï¼Œå®ç°å¼‚æ­¥å¤„ç†é€»è¾‘\u0026#34; ), ArchitecturePattern( pattern_name=\u0026#34;CQRSæ¨¡å¼\u0026#34;, description=\u0026#34;å‘½ä»¤æŸ¥è¯¢è´£ä»»åˆ†ç¦»çš„Serverlesså®ç°\u0026#34;, use_cases=[\u0026#34;å¤æ‚ä¸šåŠ¡é€»è¾‘\u0026#34;, \u0026#34;è¯»å†™åˆ†ç¦»\u0026#34;, \u0026#34;äº‹ä»¶æº¯æº\u0026#34;], components=[\u0026#34;å‘½ä»¤å‡½æ•°\u0026#34;, \u0026#34;æŸ¥è¯¢å‡½æ•°\u0026#34;, \u0026#34;äº‹ä»¶å­˜å‚¨\u0026#34;], benefits=[\u0026#34;èŒè´£åˆ†ç¦»\u0026#34;, \u0026#34;æ€§èƒ½ä¼˜åŒ–\u0026#34;, \u0026#34;å¯æ‰©å±•æ€§\u0026#34;], challenges=[\u0026#34;æ•°æ®ä¸€è‡´æ€§\u0026#34;, \u0026#34;å¤æ‚æ€§å¢åŠ \u0026#34;, \u0026#34;äº‹ä»¶ç®¡ç†\u0026#34;], implementation_guide=\u0026#34;åˆ†åˆ«å®ç°å‘½ä»¤å’ŒæŸ¥è¯¢å¤„ç†å‡½æ•°\u0026#34; ), ArchitecturePattern( pattern_name=\u0026#34;å¾®æœåŠ¡ç¼–æ’\u0026#34;, description=\u0026#34;ä½¿ç”¨å·¥ä½œæµç¼–æ’å¤šä¸ªå¾®æœåŠ¡\u0026#34;, use_cases=[\u0026#34;ä¸šåŠ¡æµç¨‹\u0026#34;, \u0026#34;æ•°æ®ç®¡é“\u0026#34;, \u0026#34;æ‰¹å¤„ç†ä»»åŠ¡\u0026#34;], components=[\u0026#34;Step Functions\u0026#34;, \u0026#34;Lambda Functions\u0026#34;, \u0026#34;çŠ¶æ€æœº\u0026#34;], benefits=[\u0026#34;å¯è§†åŒ–æµç¨‹\u0026#34;, \u0026#34;é”™è¯¯å¤„ç†\u0026#34;, \u0026#34;çŠ¶æ€ç®¡ç†\u0026#34;], challenges=[\u0026#34;å¤æ‚åº¦ç®¡ç†\u0026#34;, \u0026#34;è°ƒè¯•å›°éš¾\u0026#34;, \u0026#34;æˆæœ¬æ§åˆ¶\u0026#34;], implementation_guide=\u0026#34;å®šä¹‰çŠ¶æ€æœºç¼–æ’å¤šä¸ªå‡½æ•°æ‰§è¡Œ\u0026#34; ) ] return patterns def optimize_function_configuration(self, function_name: str, metrics: FunctionMetrics) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ä¼˜åŒ–å‡½æ•°é…ç½®å»ºè®®\u0026#34;\u0026#34;\u0026#34; recommendations = { \u0026#34;memory_optimization\u0026#34;: {}, \u0026#34;timeout_optimization\u0026#34;: {}, \u0026#34;concurrency_optimization\u0026#34;: {}, \u0026#34;cost_optimization\u0026#34;: {} } # å†…å­˜ä¼˜åŒ–å»ºè®® if metrics.memory_utilization \u0026lt; 50: recommendations[\u0026#34;memory_optimization\u0026#34;] = { \u0026#34;action\u0026#34;: \u0026#34;å‡å°‘å†…å­˜åˆ†é…\u0026#34;, \u0026#34;current_memory\u0026#34;: \u0026#34;è·å–å½“å‰é…ç½®\u0026#34;, \u0026#34;recommended_memory\u0026#34;: \u0026#34;å»ºè®®å‡å°‘20-30%\u0026#34;, \u0026#34;expected_savings\u0026#34;: \u0026#34;é¢„è®¡èŠ‚çœæˆæœ¬\u0026#34; } elif metrics.memory_utilization \u0026gt; 80: recommendations[\u0026#34;memory_optimization\u0026#34;] = { \u0026#34;action\u0026#34;: \u0026#34;å¢åŠ å†…å­˜åˆ†é…\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå¯èƒ½å½±å“æ€§èƒ½\u0026#34;, \u0026#34;recommended_increase\u0026#34;: \u0026#34;å»ºè®®å¢åŠ 20-50%\u0026#34; } # è¶…æ—¶ä¼˜åŒ–å»ºè®® if metrics.duration_p99 \u0026gt; 25000: # 25ç§’ recommendations[\u0026#34;timeout_optimization\u0026#34;] = { \u0026#34;action\u0026#34;: \u0026#34;æ£€æŸ¥å‡½æ•°é€»è¾‘\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;æ‰§è¡Œæ—¶é—´è¿‡é•¿ï¼Œå¯èƒ½éœ€è¦ä¼˜åŒ–\u0026#34;, \u0026#34;suggestions\u0026#34;: [\u0026#34;ä»£ç ä¼˜åŒ–\u0026#34;, \u0026#34;å¼‚æ­¥å¤„ç†\u0026#34;, \u0026#34;æ‹†åˆ†å‡½æ•°\u0026#34;] } # å¹¶å‘ä¼˜åŒ–å»ºè®® if metrics.error_rate \u0026gt; 5: recommendations[\u0026#34;concurrency_optimization\u0026#34;] = { \u0026#34;action\u0026#34;: \u0026#34;è®¾ç½®é¢„ç•™å¹¶å‘\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;é”™è¯¯ç‡è¾ƒé«˜ï¼Œå¯èƒ½å­˜åœ¨å¹¶å‘é™åˆ¶\u0026#34;, \u0026#34;recommended_concurrency\u0026#34;: \u0026#34;åŸºäºå³°å€¼æµé‡è®¾ç½®\u0026#34; } # æˆæœ¬ä¼˜åŒ–å»ºè®® if metrics.cost \u0026gt; 100: # æœˆæˆæœ¬è¶…è¿‡100ç¾å…ƒ recommendations[\u0026#34;cost_optimization\u0026#34;] = { \u0026#34;action\u0026#34;: \u0026#34;æˆæœ¬ä¼˜åŒ–åˆ†æ\u0026#34;, \u0026#34;suggestions\u0026#34;: [ \u0026#34;ä¼˜åŒ–å‡½æ•°æ‰§è¡Œæ—¶é—´\u0026#34;, \u0026#34;è°ƒæ•´å†…å­˜é…ç½®\u0026#34;, \u0026#34;ä½¿ç”¨é¢„ç•™å®¹é‡\u0026#34;, \u0026#34;è€ƒè™‘æ›¿ä»£æ–¹æ¡ˆ\u0026#34; ] } return recommendations def generate_architecture_report(self, functions: List[str]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆæ¶æ„åˆ†ææŠ¥å‘Š\u0026#34;\u0026#34;\u0026#34; report = { \u0026#34;summary\u0026#34;: { \u0026#34;total_functions\u0026#34;: len(functions), \u0026#34;analysis_date\u0026#34;: datetime.utcnow().isoformat(), \u0026#34;cloud_provider\u0026#34;: self.cloud_provider }, \u0026#34;function_metrics\u0026#34;: [], \u0026#34;optimization_recommendations\u0026#34;: [], \u0026#34;architecture_patterns\u0026#34;: [], \u0026#34;best_practices\u0026#34;: [] } # åˆ†ææ¯ä¸ªå‡½æ•° for function_name in functions: try: metrics = self.analyze_function_performance(function_name) report[\u0026#34;function_metrics\u0026#34;].append(asdict(metrics)) # ç”Ÿæˆä¼˜åŒ–å»ºè®® recommendations = self.optimize_function_configuration( function_name, metrics ) report[\u0026#34;optimization_recommendations\u0026#34;].append({ \u0026#34;function_name\u0026#34;: function_name, \u0026#34;recommendations\u0026#34;: recommendations }) except Exception as e: self.logger.error(f\u0026#34;åˆ†æå‡½æ•° {function_name} å¤±è´¥: {e}\u0026#34;) # æ·»åŠ æ¶æ„æ¨¡å¼ patterns = self.get_architecture_patterns() report[\u0026#34;architecture_patterns\u0026#34;] = [asdict(pattern) for pattern in patterns] # æ·»åŠ æœ€ä½³å®è·µ report[\u0026#34;best_practices\u0026#34;] = self._get_best_practices() return report def _get_best_practices(self) -\u0026gt; List[Dict[str, str]]: \u0026#34;\u0026#34;\u0026#34;è·å–æœ€ä½³å®è·µå»ºè®®\u0026#34;\u0026#34;\u0026#34; return [ { \u0026#34;category\u0026#34;: \u0026#34;å‡½æ•°è®¾è®¡\u0026#34;, \u0026#34;practice\u0026#34;: \u0026#34;ä¿æŒå‡½æ•°å•ä¸€èŒè´£\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;æ¯ä¸ªå‡½æ•°åªå¤„ç†ä¸€ä¸ªç‰¹å®šçš„ä¸šåŠ¡é€»è¾‘\u0026#34; }, { \u0026#34;category\u0026#34;: \u0026#34;æ€§èƒ½ä¼˜åŒ–\u0026#34;, \u0026#34;practice\u0026#34;: \u0026#34;ä¼˜åŒ–å†·å¯åŠ¨æ—¶é—´\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;å‡å°‘ä¾èµ–åŒ…å¤§å°ï¼Œä½¿ç”¨è¿æ¥æ± ï¼Œé¢„çƒ­å‡½æ•°\u0026#34; }, { \u0026#34;category\u0026#34;: \u0026#34;é”™è¯¯å¤„ç†\u0026#34;, \u0026#34;practice\u0026#34;: \u0026#34;å®ç°é‡è¯•æœºåˆ¶\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;é…ç½®æ­»ä¿¡é˜Ÿåˆ—ï¼Œå®ç°æŒ‡æ•°é€€é¿é‡è¯•\u0026#34; }, { \u0026#34;category\u0026#34;: \u0026#34;ç›‘æ§è¿ç»´\u0026#34;, \u0026#34;practice\u0026#34;: \u0026#34;å®Œå–„ç›‘æ§å‘Šè­¦\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;ç›‘æ§å…³é”®æŒ‡æ ‡ï¼Œè®¾ç½®åˆç†çš„å‘Šè­¦é˜ˆå€¼\u0026#34; }, { \u0026#34;category\u0026#34;: \u0026#34;å®‰å…¨ç®¡ç†\u0026#34;, \u0026#34;practice\u0026#34;: \u0026#34;æœ€å°æƒé™åŸåˆ™\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;ä¸ºå‡½æ•°åˆ†é…æœ€å°å¿…è¦çš„æƒé™\u0026#34; } ] def main(): \u0026#34;\u0026#34;\u0026#34;ä¸»å‡½æ•°ç¤ºä¾‹\u0026#34;\u0026#34;\u0026#34; # åˆå§‹åŒ–åˆ†æå™¨ analyzer = ServerlessArchitectureAnalyzer(\u0026#34;aws\u0026#34;) # åˆ†æå‡½æ•°åˆ—è¡¨ functions = [\u0026#34;user-service\u0026#34;, \u0026#34;order-service\u0026#34;, \u0026#34;payment-service\u0026#34;] # ç”Ÿæˆæ¶æ„æŠ¥å‘Š report = analyzer.generate_architecture_report(functions) # è¾“å‡ºæŠ¥å‘Š print(\u0026#34;=== Serverlessæ¶æ„åˆ†ææŠ¥å‘Š ===\u0026#34;) print(f\u0026#34;åˆ†æå‡½æ•°æ•°é‡: {report[\u0026#39;summary\u0026#39;][\u0026#39;total_functions\u0026#39;]}\u0026#34;) print(f\u0026#34;åˆ†ææ—¶é—´: {report[\u0026#39;summary\u0026#39;][\u0026#39;analysis_date\u0026#39;]}\u0026#34;) # è¾“å‡ºå‡½æ•°æŒ‡æ ‡ for metrics in report[\u0026#34;function_metrics\u0026#34;]: print(f\u0026#34;\\nå‡½æ•°: {metrics[\u0026#39;function_name\u0026#39;]}\u0026#34;) print(f\u0026#34; è°ƒç”¨æ¬¡æ•°: {metrics[\u0026#39;invocations\u0026#39;]}\u0026#34;) print(f\u0026#34; å¹³å‡æ‰§è¡Œæ—¶é—´: {metrics[\u0026#39;duration_avg\u0026#39;]:.2f}ms\u0026#34;) print(f\u0026#34; é”™è¯¯ç‡: {metrics[\u0026#39;error_rate\u0026#39;]:.2f}%\u0026#34;) print(f\u0026#34; å†·å¯åŠ¨ç‡: {metrics[\u0026#39;cold_start_rate\u0026#39;]:.2f}%\u0026#34;) print(f\u0026#34; æˆæœ¬: ${metrics[\u0026#39;cost\u0026#39;]:.2f}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() æ ¸å¿ƒè®¾è®¡æ¨¡å¼ 1. å‡½æ•°ç»„åˆæ¨¡å¼ from typing import Callable, Any, Dict, List import asyncio import json from functools import wraps class FunctionComposer: \u0026#34;\u0026#34;\u0026#34;å‡½æ•°ç»„åˆå™¨\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.functions = {} self.middleware = [] def register(self, name: str): \u0026#34;\u0026#34;\u0026#34;æ³¨å†Œå‡½æ•°è£…é¥°å™¨\u0026#34;\u0026#34;\u0026#34; def decorator(func: Callable): self.functions[name] = func return func return decorator def middleware(self, middleware_func: Callable): \u0026#34;\u0026#34;\u0026#34;æ·»åŠ ä¸­é—´ä»¶\u0026#34;\u0026#34;\u0026#34; self.middleware.append(middleware_func) return middleware_func def compose(self, *function_names: str) -\u0026gt; Callable: \u0026#34;\u0026#34;\u0026#34;ç»„åˆå¤šä¸ªå‡½æ•°\u0026#34;\u0026#34;\u0026#34; def composed_function(event: Dict[str, Any]) -\u0026gt; Dict[str, Any]: result = event # åº”ç”¨ä¸­é—´ä»¶ for middleware in self.middleware: result = middleware(result) # ä¾æ¬¡æ‰§è¡Œå‡½æ•° for name in function_names: if name in self.functions: result = self.functions[name](result) else: raise ValueError(f\u0026#34;å‡½æ•° {name} æœªæ³¨å†Œ\u0026#34;) return result return composed_function async def compose_async(self, *function_names: str) -\u0026gt; Callable: \u0026#34;\u0026#34;\u0026#34;å¼‚æ­¥å‡½æ•°ç»„åˆ\u0026#34;\u0026#34;\u0026#34; async def composed_function(event: Dict[str, Any]) -\u0026gt; Dict[str, Any]: result = event # åº”ç”¨ä¸­é—´ä»¶ for middleware in self.middleware: if asyncio.iscoroutinefunction(middleware): result = await middleware(result) else: result = middleware(result) # ä¾æ¬¡æ‰§è¡Œå‡½æ•° for name in function_names: if name in self.functions: func = self.functions[name] if asyncio.iscoroutinefunction(func): result = await func(result) else: result = func(result) else: raise ValueError(f\u0026#34;å‡½æ•° {name} æœªæ³¨å†Œ\u0026#34;) return result return composed_function # ä½¿ç”¨ç¤ºä¾‹ composer = FunctionComposer() @composer.register(\u0026#34;validate\u0026#34;) def validate_input(event: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è¾“å…¥éªŒè¯\u0026#34;\u0026#34;\u0026#34; if \u0026#34;user_id\u0026#34; not in event: raise ValueError(\u0026#34;ç¼ºå°‘user_idå‚æ•°\u0026#34;) return event @composer.register(\u0026#34;process\u0026#34;) def process_data(event: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;æ•°æ®å¤„ç†\u0026#34;\u0026#34;\u0026#34; event[\u0026#34;processed\u0026#34;] = True event[\u0026#34;timestamp\u0026#34;] = \u0026#34;2024-06-15T10:00:00Z\u0026#34; return event @composer.register(\u0026#34;save\u0026#34;) def save_result(event: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ä¿å­˜ç»“æœ\u0026#34;\u0026#34;\u0026#34; event[\u0026#34;saved\u0026#34;] = True return event # ç»„åˆå‡½æ•° pipeline = composer.compose(\u0026#34;validate\u0026#34;, \u0026#34;process\u0026#34;, \u0026#34;save\u0026#34;) 2. äº‹ä»¶è·¯ç”±æ¨¡å¼ import re from typing import Dict, Any, Callable, List, Optional from dataclasses import dataclass from enum import Enum class EventType(Enum): \u0026#34;\u0026#34;\u0026#34;äº‹ä»¶ç±»å‹\u0026#34;\u0026#34;\u0026#34; HTTP_REQUEST = \u0026#34;http_request\u0026#34; S3_OBJECT_CREATED = \u0026#34;s3_object_created\u0026#34; DYNAMODB_STREAM = \u0026#34;dynamodb_stream\u0026#34; SQS_MESSAGE = \u0026#34;sqs_message\u0026#34; SCHEDULED = \u0026#34;scheduled\u0026#34; CUSTOM = \u0026#34;custom\u0026#34; @dataclass class EventRoute: \u0026#34;\u0026#34;\u0026#34;äº‹ä»¶è·¯ç”±é…ç½®\u0026#34;\u0026#34;\u0026#34; pattern: str handler: Callable event_type: EventType priority: int = 0 middleware: List[Callable] = None class EventRouter: \u0026#34;\u0026#34;\u0026#34;äº‹ä»¶è·¯ç”±å™¨\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.routes: List[EventRoute] = [] self.global_middleware: List[Callable] = [] def add_route(self, pattern: str, event_type: EventType, priority: int = 0, middleware: List[Callable] = None): \u0026#34;\u0026#34;\u0026#34;æ·»åŠ è·¯ç”±è£…é¥°å™¨\u0026#34;\u0026#34;\u0026#34; def decorator(handler: Callable): route = EventRoute( pattern=pattern, handler=handler, event_type=event_type, priority=priority, middleware=middleware or [] ) self.routes.append(route) # æŒ‰ä¼˜å…ˆçº§æ’åº self.routes.sort(key=lambda r: r.priority, reverse=True) return handler return decorator def add_middleware(self, middleware: Callable): \u0026#34;\u0026#34;\u0026#34;æ·»åŠ å…¨å±€ä¸­é—´ä»¶\u0026#34;\u0026#34;\u0026#34; self.global_middleware.append(middleware) return middleware def route_event(self, event: Dict[str, Any], context: Any = None) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è·¯ç”±äº‹ä»¶åˆ°å¯¹åº”å¤„ç†å™¨\u0026#34;\u0026#34;\u0026#34; event_type = self._detect_event_type(event) event_path = self._extract_event_path(event, event_type) # æŸ¥æ‰¾åŒ¹é…çš„è·¯ç”± for route in self.routes: if route.event_type == event_type and self._match_pattern(route.pattern, event_path): return self._execute_handler(route, event, context) # æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„è·¯ç”± return { \u0026#34;statusCode\u0026#34;: 404, \u0026#34;body\u0026#34;: json.dumps({\u0026#34;error\u0026#34;: \u0026#34;No matching route found\u0026#34;}) } def _detect_event_type(self, event: Dict[str, Any]) -\u0026gt; EventType: \u0026#34;\u0026#34;\u0026#34;æ£€æµ‹äº‹ä»¶ç±»å‹\u0026#34;\u0026#34;\u0026#34; if \u0026#34;httpMethod\u0026#34; in event or \u0026#34;requestContext\u0026#34; in event: return EventType.HTTP_REQUEST elif \u0026#34;Records\u0026#34; in event: records = event[\u0026#34;Records\u0026#34;] if records and \u0026#34;s3\u0026#34; in records[0]: return EventType.S3_OBJECT_CREATED elif records and \u0026#34;dynamodb\u0026#34; in records[0]: return EventType.DYNAMODB_STREAM elif records and \u0026#34;eventSource\u0026#34; in records[0] and records[0][\u0026#34;eventSource\u0026#34;] == \u0026#34;aws:sqs\u0026#34;: return EventType.SQS_MESSAGE elif \u0026#34;source\u0026#34; in event and event[\u0026#34;source\u0026#34;] == \u0026#34;aws.events\u0026#34;: return EventType.SCHEDULED else: return EventType.CUSTOM def _extract_event_path(self, event: Dict[str, Any], event_type: EventType) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;æå–äº‹ä»¶è·¯å¾„\u0026#34;\u0026#34;\u0026#34; if event_type == EventType.HTTP_REQUEST: return event.get(\u0026#34;path\u0026#34;, \u0026#34;/\u0026#34;) elif event_type == EventType.S3_OBJECT_CREATED: return event[\u0026#34;Records\u0026#34;][0][\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] elif event_type == EventType.SQS_MESSAGE: return event[\u0026#34;Records\u0026#34;][0].get(\u0026#34;messageAttributes\u0026#34;, {}).get(\u0026#34;route\u0026#34;, {}).get(\u0026#34;stringValue\u0026#34;, \u0026#34;default\u0026#34;) else: return event.get(\u0026#34;detail-type\u0026#34;, \u0026#34;default\u0026#34;) def _match_pattern(self, pattern: str, path: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;åŒ¹é…è·¯ç”±æ¨¡å¼\u0026#34;\u0026#34;\u0026#34; # æ”¯æŒé€šé…ç¬¦å’Œæ­£åˆ™è¡¨è¾¾å¼ if \u0026#34;*\u0026#34; in pattern: regex_pattern = pattern.replace(\u0026#34;*\u0026#34;, \u0026#34;.*\u0026#34;) return re.match(f\u0026#34;^{regex_pattern}$\u0026#34;, path) is not None else: return re.match(f\u0026#34;^{pattern}$\u0026#34;, path) is not None def _execute_handler(self, route: EventRoute, event: Dict[str, Any], context: Any) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;æ‰§è¡Œå¤„ç†å™¨\u0026#34;\u0026#34;\u0026#34; try: # åº”ç”¨å…¨å±€ä¸­é—´ä»¶ for middleware in self.global_middleware: event = middleware(event, context) # åº”ç”¨è·¯ç”±ä¸­é—´ä»¶ for middleware in route.middleware: event = middleware(event, context) # æ‰§è¡Œå¤„ç†å™¨ result = route.handler(event, context) # ç¡®ä¿è¿”å›æ ¼å¼æ­£ç¡® if isinstance(result, dict) and \u0026#34;statusCode\u0026#34; in result: return result else: return { \u0026#34;statusCode\u0026#34;: 200, \u0026#34;body\u0026#34;: json.dumps(result) if not isinstance(result, str) else result } except Exception as e: return { \u0026#34;statusCode\u0026#34;: 500, \u0026#34;body\u0026#34;: json.dumps({\u0026#34;error\u0026#34;: str(e)}) } # ä½¿ç”¨ç¤ºä¾‹ router = EventRouter() @router.add_middleware def cors_middleware(event: Dict[str, Any], context: Any) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;CORSä¸­é—´ä»¶\u0026#34;\u0026#34;\u0026#34; event[\u0026#34;_cors_enabled\u0026#34;] = True return event @router.add_middleware def auth_middleware(event: Dict[str, Any], context: Any) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è®¤è¯ä¸­é—´ä»¶\u0026#34;\u0026#34;\u0026#34; # ç®€åŒ–çš„è®¤è¯é€»è¾‘ headers = event.get(\u0026#34;headers\u0026#34;, {}) if \u0026#34;authorization\u0026#34; not in headers: raise Exception(\u0026#34;Missing authorization header\u0026#34;) return event @router.add_route(\u0026#34;/api/users/*\u0026#34;, EventType.HTTP_REQUEST, priority=10) def handle_user_api(event: Dict[str, Any], context: Any) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;å¤„ç†ç”¨æˆ·APIè¯·æ±‚\u0026#34;\u0026#34;\u0026#34; method = event.get(\u0026#34;httpMethod\u0026#34;, \u0026#34;GET\u0026#34;) path = event.get(\u0026#34;path\u0026#34;, \u0026#34;\u0026#34;) if method == \u0026#34;GET\u0026#34;: return {\u0026#34;message\u0026#34;: f\u0026#34;Getting user from {path}\u0026#34;} elif method == \u0026#34;POST\u0026#34;: return {\u0026#34;message\u0026#34;: f\u0026#34;Creating user at {path}\u0026#34;} else: return {\u0026#34;error\u0026#34;: \u0026#34;Method not allowed\u0026#34;} @router.add_route(\u0026#34;*.jpg\u0026#34;, EventType.S3_OBJECT_CREATED) def handle_image_upload(event: Dict[str, Any], context: Any) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;å¤„ç†å›¾ç‰‡ä¸Šä¼ \u0026#34;\u0026#34;\u0026#34; bucket = event[\u0026#34;Records\u0026#34;][0][\u0026#34;s3\u0026#34;][\u0026#34;bucket\u0026#34;][\u0026#34;name\u0026#34;] key = event[\u0026#34;Records\u0026#34;][0][\u0026#34;s3\u0026#34;][\u0026#34;object\u0026#34;][\u0026#34;key\u0026#34;] return { \u0026#34;message\u0026#34;: f\u0026#34;Processing image {key} from bucket {bucket}\u0026#34;, \u0026#34;action\u0026#34;: \u0026#34;thumbnail_generation\u0026#34; } def lambda_handler(event: Dict[str, Any], context: Any) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;Lambdaå…¥å£å‡½æ•°\u0026#34;\u0026#34;\u0026#34; return router.route_event(event, context) å‡½æ•°è®¾è®¡ä¸ä¼˜åŒ– å‡½æ•°æ€§èƒ½ä¼˜åŒ–å™¨ import time import psutil import threading from typing import Dict, Any, Callable, Optional from functools import wraps from dataclasses import dataclass import json @dataclass class PerformanceMetrics: \u0026#34;\u0026#34;\u0026#34;æ€§èƒ½æŒ‡æ ‡\u0026#34;\u0026#34;\u0026#34; execution_time: float memory_usage: float cpu_usage: float cold_start: bool error_occurred: bool error_message: Optional[str] = None class FunctionOptimizer: \u0026#34;\u0026#34;\u0026#34;å‡½æ•°æ€§èƒ½ä¼˜åŒ–å™¨\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.metrics_history: Dict[str, List[PerformanceMetrics]] = {} self.connection_pool = {} self.cache = {} self.is_cold_start = True def performance_monitor(self, function_name: str): \u0026#34;\u0026#34;\u0026#34;æ€§èƒ½ç›‘æ§è£…é¥°å™¨\u0026#34;\u0026#34;\u0026#34; def decorator(func: Callable): @wraps(func) def wrapper(*args, **kwargs): start_time = time.time() start_memory = psutil.Process().memory_info().rss / 1024 / 1024 # MB start_cpu = psutil.cpu_percent() error_occurred = False error_message = None result = None try: result = func(*args, **kwargs) except Exception as e: error_occurred = True error_message = str(e) raise finally: end_time = time.time() end_memory = psutil.Process().memory_info().rss / 1024 / 1024 end_cpu = psutil.cpu_percent() metrics = PerformanceMetrics( execution_time=(end_time - start_time) * 1000, # ms memory_usage=end_memory - start_memory, cpu_usage=end_cpu - start_cpu, cold_start=self.is_cold_start, error_occurred=error_occurred, error_message=error_message ) self._record_metrics(function_name, metrics) self.is_cold_start = False return result return wrapper return decorator def _record_metrics(self, function_name: str, metrics: PerformanceMetrics): \u0026#34;\u0026#34;\u0026#34;è®°å½•æ€§èƒ½æŒ‡æ ‡\u0026#34;\u0026#34;\u0026#34; if function_name not in self.metrics_history: self.metrics_history[function_name] = [] self.metrics_history[function_name].append(metrics) # ä¿æŒæœ€è¿‘100æ¬¡è®°å½• if len(self.metrics_history[function_name]) \u0026gt; 100: self.metrics_history[function_name] = self.metrics_history[function_name][-100:] def connection_pool_manager(self, pool_name: str, create_connection: Callable): \u0026#34;\u0026#34;\u0026#34;è¿æ¥æ± ç®¡ç†è£…é¥°å™¨\u0026#34;\u0026#34;\u0026#34; def decorator(func: Callable): @wraps(func) def wrapper(*args, **kwargs): if pool_name not in self.connection_pool: self.connection_pool[pool_name] = create_connection() # å°†è¿æ¥æ³¨å…¥åˆ°å‡½æ•°å‚æ•°ä¸­ kwargs[\u0026#39;connection\u0026#39;] = self.connection_pool[pool_name] return func(*args, **kwargs) return wrapper return decorator def cache_result(self, ttl: int = 300, key_func: Callable = None): \u0026#34;\u0026#34;\u0026#34;ç»“æœç¼“å­˜è£…é¥°å™¨\u0026#34;\u0026#34;\u0026#34; def decorator(func: Callable): @wraps(func) def wrapper(*args, **kwargs): # ç”Ÿæˆç¼“å­˜é”® if key_func: cache_key = key_func(*args, **kwargs) else: cache_key = f\u0026#34;{func.__name__}:{hash(str(args) + str(kwargs))}\u0026#34; # æ£€æŸ¥ç¼“å­˜ if cache_key in self.cache: cached_result, timestamp = self.cache[cache_key] if time.time() - timestamp \u0026lt; ttl: return cached_result # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ result = func(*args, **kwargs) self.cache[cache_key] = (result, time.time()) return result return wrapper return decorator def async_processor(self, max_workers: int = 5): \u0026#34;\u0026#34;\u0026#34;å¼‚æ­¥å¤„ç†è£…é¥°å™¨\u0026#34;\u0026#34;\u0026#34; def decorator(func: Callable): @wraps(func) def wrapper(*args, **kwargs): import concurrent.futures # æ£€æŸ¥æ˜¯å¦æœ‰æ‰¹é‡æ•°æ®éœ€è¦å¤„ç† if \u0026#39;batch_data\u0026#39; in kwargs: batch_data = kwargs.pop(\u0026#39;batch_data\u0026#39;) with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor: futures = [] for item in batch_data: future = executor.submit(func, item, *args, **kwargs) futures.append(future) results = [] for future in concurrent.futures.as_completed(futures): try: result = future.result() results.append(result) except Exception as e: results.append({\u0026#34;error\u0026#34;: str(e)}) return results else: return func(*args, **kwargs) return wrapper return decorator def get_optimization_recommendations(self, function_name: str) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è·å–ä¼˜åŒ–å»ºè®®\u0026#34;\u0026#34;\u0026#34; if function_name not in self.metrics_history: return {\u0026#34;error\u0026#34;: \u0026#34;No metrics available for this function\u0026#34;} metrics = self.metrics_history[function_name] if not metrics: return {\u0026#34;error\u0026#34;: \u0026#34;No metrics data\u0026#34;} # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ avg_execution_time = sum(m.execution_time for m in metrics) / len(metrics) avg_memory_usage = sum(m.memory_usage for m in metrics) / len(metrics) cold_start_rate = sum(1 for m in metrics if m.cold_start) / len(metrics) * 100 error_rate = sum(1 for m in metrics if m.error_occurred) / len(metrics) * 100 recommendations = { \u0026#34;performance_summary\u0026#34;: { \u0026#34;avg_execution_time_ms\u0026#34;: round(avg_execution_time, 2), \u0026#34;avg_memory_usage_mb\u0026#34;: round(avg_memory_usage, 2), \u0026#34;cold_start_rate_percent\u0026#34;: round(cold_start_rate, 2), \u0026#34;error_rate_percent\u0026#34;: round(error_rate, 2) }, \u0026#34;recommendations\u0026#34;: [] } # ç”Ÿæˆä¼˜åŒ–å»ºè®® if avg_execution_time \u0026gt; 5000: # 5ç§’ recommendations[\u0026#34;recommendations\u0026#34;].append({ \u0026#34;type\u0026#34;: \u0026#34;performance\u0026#34;, \u0026#34;issue\u0026#34;: \u0026#34;æ‰§è¡Œæ—¶é—´è¿‡é•¿\u0026#34;, \u0026#34;suggestion\u0026#34;: \u0026#34;è€ƒè™‘ä¼˜åŒ–ç®—æ³•ã€ä½¿ç”¨ç¼“å­˜æˆ–å¼‚æ­¥å¤„ç†\u0026#34;, \u0026#34;priority\u0026#34;: \u0026#34;high\u0026#34; }) if avg_memory_usage \u0026gt; 100: # 100MB recommendations[\u0026#34;recommendations\u0026#34;].append({ \u0026#34;type\u0026#34;: \u0026#34;memory\u0026#34;, \u0026#34;issue\u0026#34;: \u0026#34;å†…å­˜ä½¿ç”¨é‡è¾ƒé«˜\u0026#34;, \u0026#34;suggestion\u0026#34;: \u0026#34;ä¼˜åŒ–æ•°æ®ç»“æ„ã€åŠæ—¶é‡Šæ”¾èµ„æº\u0026#34;, \u0026#34;priority\u0026#34;: \u0026#34;medium\u0026#34; }) if cold_start_rate \u0026gt; 20: # 20% recommendations[\u0026#34;recommendations\u0026#34;].append({ \u0026#34;type\u0026#34;: \u0026#34;cold_start\u0026#34;, \u0026#34;issue\u0026#34;: \u0026#34;å†·å¯åŠ¨ç‡è¾ƒé«˜\u0026#34;, \u0026#34;suggestion\u0026#34;: \u0026#34;ä½¿ç”¨é¢„ç•™å¹¶å‘ã€å‡å°‘ä¾èµ–åŒ…å¤§å°ã€å®ç°é¢„çƒ­æœºåˆ¶\u0026#34;, \u0026#34;priority\u0026#34;: \u0026#34;high\u0026#34; }) if error_rate \u0026gt; 5: # 5% recommendations[\u0026#34;recommendations\u0026#34;].append({ \u0026#34;type\u0026#34;: \u0026#34;reliability\u0026#34;, \u0026#34;issue\u0026#34;: \u0026#34;é”™è¯¯ç‡è¾ƒé«˜\u0026#34;, \u0026#34;suggestion\u0026#34;: \u0026#34;åŠ å¼ºé”™è¯¯å¤„ç†ã€å®ç°é‡è¯•æœºåˆ¶ã€å®Œå–„ç›‘æ§\u0026#34;, \u0026#34;priority\u0026#34;: \u0026#34;critical\u0026#34; }) return recommendations # ä½¿ç”¨ç¤ºä¾‹ optimizer = FunctionOptimizer() # æ•°æ®åº“è¿æ¥åˆ›å»ºå‡½æ•° def create_db_connection(): \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºæ•°æ®åº“è¿æ¥ï¼ˆç¤ºä¾‹ï¼‰\u0026#34;\u0026#34;\u0026#34; return {\u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 5432, \u0026#34;connected\u0026#34;: True} @optimizer.performance_monitor(\u0026#34;user_service\u0026#34;) @optimizer.connection_pool_manager(\u0026#34;db_pool\u0026#34;, create_db_connection) @optimizer.cache_result(ttl=300) def get_user_profile(user_id: str, connection=None) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è·å–ç”¨æˆ·èµ„æ–™\u0026#34;\u0026#34;\u0026#34; # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢ time.sleep(0.1) # æ¨¡æ‹ŸæŸ¥è¯¢å»¶è¿Ÿ return { \u0026#34;user_id\u0026#34;: user_id, \u0026#34;name\u0026#34;: f\u0026#34;User {user_id}\u0026#34;, \u0026#34;email\u0026#34;: f\u0026#34;user{user_id}@example.com\u0026#34;, \u0026#34;connection_info\u0026#34;: connection } @optimizer.performance_monitor(\u0026#34;batch_processor\u0026#34;) @optimizer.async_processor(max_workers=10) def process_user_data(user_data: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;å¤„ç†ç”¨æˆ·æ•°æ®\u0026#34;\u0026#34;\u0026#34; # æ¨¡æ‹Ÿæ•°æ®å¤„ç† time.sleep(0.05) return { \u0026#34;processed\u0026#34;: True, \u0026#34;user_id\u0026#34;: user_data.get(\u0026#34;user_id\u0026#34;), \u0026#34;timestamp\u0026#34;: time.time() } äº‹ä»¶é©±åŠ¨æ¶æ„ äº‹ä»¶å¤„ç†æ¡†æ¶ import json import uuid import asyncio from typing import Dict, Any, List, Callable, Optional, Union from dataclasses import dataclass, asdict from datetime import datetime, timezone from enum import Enum import boto3 from abc import ABC, abstractmethod class EventStatus(Enum): \u0026#34;\u0026#34;\u0026#34;äº‹ä»¶çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; PENDING = \u0026#34;pending\u0026#34; PROCESSING = \u0026#34;processing\u0026#34; COMPLETED = \u0026#34;completed\u0026#34; FAILED = \u0026#34;failed\u0026#34; RETRYING = \u0026#34;retrying\u0026#34; @dataclass class Event: \u0026#34;\u0026#34;\u0026#34;äº‹ä»¶æ•°æ®ç»“æ„\u0026#34;\u0026#34;\u0026#34; id: str type: str source: str data: Dict[str, Any] timestamp: datetime correlation_id: Optional[str] = None status: EventStatus = EventStatus.PENDING retry_count: int = 0 max_retries: int = 3 def to_dict(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è½¬æ¢ä¸ºå­—å…¸\u0026#34;\u0026#34;\u0026#34; return { **asdict(self), \u0026#39;timestamp\u0026#39;: self.timestamp.isoformat(), \u0026#39;status\u0026#39;: self.status.value } class EventHandler(ABC): \u0026#34;\u0026#34;\u0026#34;äº‹ä»¶å¤„ç†å™¨æŠ½è±¡åŸºç±»\u0026#34;\u0026#34;\u0026#34; @abstractmethod async def handle(self, event: Event) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;å¤„ç†äº‹ä»¶\u0026#34;\u0026#34;\u0026#34; pass @abstractmethod def can_handle(self, event: Event) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;åˆ¤æ–­æ˜¯å¦èƒ½å¤„ç†è¯¥äº‹ä»¶\u0026#34;\u0026#34;\u0026#34; pass class EventBus: \u0026#34;\u0026#34;\u0026#34;äº‹ä»¶æ€»çº¿\u0026#34;\u0026#34;\u0026#34; def __init__(self, dead_letter_queue: Optional[str] = None): self.handlers: List[EventHandler] = [] self.middleware: List[Callable] = [] self.dead_letter_queue = dead_letter_queue self.event_store: List[Event] = [] # AWSæœåŠ¡å®¢æˆ·ç«¯ self.sqs = boto3.client(\u0026#39;sqs\u0026#39;) self.sns = boto3.client(\u0026#39;sns\u0026#39;) def register_handler(self, handler: EventHandler): \u0026#34;\u0026#34;\u0026#34;æ³¨å†Œäº‹ä»¶å¤„ç†å™¨\u0026#34;\u0026#34;\u0026#34; self.handlers.append(handler) def add_middleware(self, middleware: Callable): \u0026#34;\u0026#34;\u0026#34;æ·»åŠ ä¸­é—´ä»¶\u0026#34;\u0026#34;\u0026#34; self.middleware.append(middleware) async def publish(self, event: Event): \u0026#34;\u0026#34;\u0026#34;å‘å¸ƒäº‹ä»¶\u0026#34;\u0026#34;\u0026#34; # åº”ç”¨ä¸­é—´ä»¶ for middleware in self.middleware: event = await middleware(event) if asyncio.iscoroutinefunction(middleware) else middleware(event) # å­˜å‚¨äº‹ä»¶ self.event_store.append(event) # å¤„ç†äº‹ä»¶ await self._process_event(event) async def _process_event(self, event: Event): \u0026#34;\u0026#34;\u0026#34;å¤„ç†äº‹ä»¶\u0026#34;\u0026#34;\u0026#34; event.status = EventStatus.PROCESSING # æŸ¥æ‰¾åˆé€‚çš„å¤„ç†å™¨ suitable_handlers = [h for h in self.handlers if h.can_handle(event)] if not suitable_handlers: await self._send_to_dead_letter_queue(event, \u0026#34;No suitable handler found\u0026#34;) return # å¹¶è¡Œå¤„ç†ï¼ˆå¦‚æœæœ‰å¤šä¸ªå¤„ç†å™¨ï¼‰ tasks = [] for handler in suitable_handlers: task = asyncio.create_task(self._handle_with_retry(handler, event)) tasks.append(task) try: results = await asyncio.gather(*tasks, return_exceptions=True) # æ£€æŸ¥å¤„ç†ç»“æœ success_count = sum(1 for r in results if not isinstance(r, Exception)) if success_count \u0026gt; 0: event.status = EventStatus.COMPLETED else: event.status = EventStatus.FAILED await self._send_to_dead_letter_queue(event, f\u0026#34;All handlers failed: {results}\u0026#34;) except Exception as e: event.status = EventStatus.FAILED await self._send_to_dead_letter_queue(event, str(e)) async def _handle_with_retry(self, handler: EventHandler, event: Event) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;å¸¦é‡è¯•çš„äº‹ä»¶å¤„ç†\u0026#34;\u0026#34;\u0026#34; last_exception = None for attempt in range(event.max_retries + 1): try: event.retry_count = attempt if attempt \u0026gt; 0: event.status = EventStatus.RETRYING # æŒ‡æ•°é€€é¿ await asyncio.sleep(2 ** attempt) result = await handler.handle(event) return result except Exception as e: last_exception = e if attempt == event.max_retries: raise e raise last_exception async def _send_to_dead_letter_queue(self, event: Event, error_message: str): \u0026#34;\u0026#34;\u0026#34;å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—\u0026#34;\u0026#34;\u0026#34; if self.dead_letter_queue: try: message = { \u0026#34;event\u0026#34;: event.to_dict(), \u0026#34;error\u0026#34;: error_message, \u0026#34;failed_at\u0026#34;: datetime.now(timezone.utc).isoformat() } self.sqs.send_message( QueueUrl=self.dead_letter_queue, MessageBody=json.dumps(message) ) except Exception as e: print(f\u0026#34;Failed to send to dead letter queue: {e}\u0026#34;) # å…·ä½“äº‹ä»¶å¤„ç†å™¨å®ç° class UserRegistrationHandler(EventHandler): \u0026#34;\u0026#34;\u0026#34;ç”¨æˆ·æ³¨å†Œäº‹ä»¶å¤„ç†å™¨\u0026#34;\u0026#34;\u0026#34; async def handle(self, event: Event) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;å¤„ç†ç”¨æˆ·æ³¨å†Œäº‹ä»¶\u0026#34;\u0026#34;\u0026#34; user_data = event.data # å‘é€æ¬¢è¿é‚®ä»¶ await self._send_welcome_email(user_data) # åˆ›å»ºç”¨æˆ·é…ç½®æ–‡ä»¶ await self._create_user_profile(user_data) # åˆ†é…é»˜è®¤æƒé™ await self._assign_default_permissions(user_data) return {\u0026#34;status\u0026#34;: \u0026#34;success\u0026#34;, \u0026#34;user_id\u0026#34;: user_data[\u0026#34;user_id\u0026#34;]} def can_handle(self, event: Event) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;åˆ¤æ–­æ˜¯å¦èƒ½å¤„ç†è¯¥äº‹ä»¶\u0026#34;\u0026#34;\u0026#34; return event.type == \u0026#34;user.registered\u0026#34; async def _send_welcome_email(self, user_data: Dict[str, Any]): \u0026#34;\u0026#34;\u0026#34;å‘é€æ¬¢è¿é‚®ä»¶\u0026#34;\u0026#34;\u0026#34; # æ¨¡æ‹Ÿé‚®ä»¶å‘é€ await asyncio.sleep(0.1) print(f\u0026#34;Welcome email sent to {user_data[\u0026#39;email\u0026#39;]}\u0026#34;) async def _create_user_profile(self, user_data: Dict[str, Any]): \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºç”¨æˆ·é…ç½®æ–‡ä»¶\u0026#34;\u0026#34;\u0026#34; # æ¨¡æ‹Ÿæ•°æ®åº“æ“ä½œ await asyncio.sleep(0.05) print(f\u0026#34;User profile created for {user_data[\u0026#39;user_id\u0026#39;]}\u0026#34;) async def _assign_default_permissions(self, user_data: Dict[str, Any]): \u0026#34;\u0026#34;\u0026#34;åˆ†é…é»˜è®¤æƒé™\u0026#34;\u0026#34;\u0026#34; # æ¨¡æ‹Ÿæƒé™åˆ†é… await asyncio.sleep(0.02) print(f\u0026#34;Default permissions assigned to {user_data[\u0026#39;user_id\u0026#39;]}\u0026#34;) class OrderProcessingHandler(EventHandler): \u0026#34;\u0026#34;\u0026#34;è®¢å•å¤„ç†äº‹ä»¶å¤„ç†å™¨\u0026#34;\u0026#34;\u0026#34; async def handle(self, event: Event) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;å¤„ç†è®¢å•äº‹ä»¶\u0026#34;\u0026#34;\u0026#34; order_data = event.data if event.type == \u0026#34;order.created\u0026#34;: return await self._process_new_order(order_data) elif event.type == \u0026#34;order.paid\u0026#34;: return await self._process_payment(order_data) elif event.type == \u0026#34;order.shipped\u0026#34;: return await self._process_shipment(order_data) return {\u0026#34;status\u0026#34;: \u0026#34;unknown_event_type\u0026#34;} def can_handle(self, event: Event) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;åˆ¤æ–­æ˜¯å¦èƒ½å¤„ç†è¯¥äº‹ä»¶\u0026#34;\u0026#34;\u0026#34; return event.type.startswith(\u0026#34;order.\u0026#34;) async def _process_new_order(self, order_data: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;å¤„ç†æ–°è®¢å•\u0026#34;\u0026#34;\u0026#34; # åº“å­˜æ£€æŸ¥ await self._check_inventory(order_data) # è®¡ç®—ä»·æ ¼ total_price = await self._calculate_price(order_data) # åˆ›å»ºæ”¯ä»˜è¯·æ±‚ payment_id = await self._create_payment_request(order_data, total_price) return { \u0026#34;status\u0026#34;: \u0026#34;order_processed\u0026#34;, \u0026#34;order_id\u0026#34;: order_data[\u0026#34;order_id\u0026#34;], \u0026#34;payment_id\u0026#34;: payment_id, \u0026#34;total_price\u0026#34;: total_price } async def _check_inventory(self, order_data: Dict[str, Any]): \u0026#34;\u0026#34;\u0026#34;æ£€æŸ¥åº“å­˜\u0026#34;\u0026#34;\u0026#34; await asyncio.sleep(0.1) print(f\u0026#34;Inventory checked for order {order_data[\u0026#39;order_id\u0026#39;]}\u0026#34;) async def _calculate_price(self, order_data: Dict[str, Any]) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;è®¡ç®—ä»·æ ¼\u0026#34;\u0026#34;\u0026#34; await asyncio.sleep(0.05) return sum(item[\u0026#34;price\u0026#34;] * item[\u0026#34;quantity\u0026#34;] for item in order_data[\u0026#34;items\u0026#34;]) async def _create_payment_request(self, order_data: Dict[str, Any], total_price: float) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºæ”¯ä»˜è¯·æ±‚\u0026#34;\u0026#34;\u0026#34; await asyncio.sleep(0.1) payment_id = str(uuid.uuid4()) print(f\u0026#34;Payment request created: {payment_id} for ${total_price}\u0026#34;) return payment_id async def _process_payment(self, order_data: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;å¤„ç†æ”¯ä»˜\u0026#34;\u0026#34;\u0026#34; await asyncio.sleep(0.2) print(f\u0026#34;Payment processed for order {order_data[\u0026#39;order_id\u0026#39;]}\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;payment_processed\u0026#34;} async def _process_shipment(self, order_data: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;å¤„ç†å‘è´§\u0026#34;\u0026#34;\u0026#34; await asyncio.sleep(0.15) print(f\u0026#34;Shipment processed for order {order_data[\u0026#39;order_id\u0026#39;]}\u0026#34;) return {\u0026#34;status\u0026#34;: \u0026#34;shipment_processed\u0026#34;} # ä¸­é—´ä»¶ç¤ºä¾‹ async def logging_middleware(event: Event) -\u0026gt; Event: \u0026#34;\u0026#34;\u0026#34;æ—¥å¿—ä¸­é—´ä»¶\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Processing event: {event.type} - {event.id}\u0026#34;) return event async def validation_middleware(event: Event) -\u0026gt; Event: \u0026#34;\u0026#34;\u0026#34;éªŒè¯ä¸­é—´ä»¶\u0026#34;\u0026#34;\u0026#34; if not event.data: raise ValueError(\u0026#34;Event data cannot be empty\u0026#34;) return event # ä½¿ç”¨ç¤ºä¾‹ async def main(): \u0026#34;\u0026#34;\u0026#34;ä¸»å‡½æ•°ç¤ºä¾‹\u0026#34;\u0026#34;\u0026#34; # åˆ›å»ºäº‹ä»¶æ€»çº¿ event_bus = EventBus(dead_letter_queue=\u0026#34;https://sqs.region.amazonaws.com/account/dlq\u0026#34;) # æ³¨å†Œå¤„ç†å™¨ event_bus.register_handler(UserRegistrationHandler()) event_bus.register_handler(OrderProcessingHandler()) # æ·»åŠ ä¸­é—´ä»¶ event_bus.add_middleware(logging_middleware) event_bus.add_middleware(validation_middleware) # åˆ›å»ºå¹¶å‘å¸ƒäº‹ä»¶ user_event = Event( id=str(uuid.uuid4()), type=\u0026#34;user.registered\u0026#34;, source=\u0026#34;user-service\u0026#34;, data={ \u0026#34;user_id\u0026#34;: \u0026#34;user123\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;user@example.com\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34; }, timestamp=datetime.now(timezone.utc), correlation_id=str(uuid.uuid4()) ) order_event = Event( id=str(uuid.uuid4()), type=\u0026#34;order.created\u0026#34;, source=\u0026#34;order-service\u0026#34;, data={ \u0026#34;order_id\u0026#34;: \u0026#34;order456\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user123\u0026#34;, \u0026#34;items\u0026#34;: [ {\u0026#34;product_id\u0026#34;: \u0026#34;prod1\u0026#34;, \u0026#34;quantity\u0026#34;: 2, \u0026#34;price\u0026#34;: 29.99}, {\u0026#34;product_id\u0026#34;: \u0026#34;prod2\u0026#34;, \u0026#34;quantity\u0026#34;: 1, \u0026#34;price\u0026#34;: 49.99} ] }, timestamp=datetime.now(timezone.utc), correlation_id=str(uuid.uuid4()) ) # å‘å¸ƒäº‹ä»¶ await event_bus.publish(user_event) await event_bus.publish(order_event) print(\u0026#34;Events published and processed\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: asyncio.run(main()) çŠ¶æ€ç®¡ç†ç­–ç•¥ åˆ†å¸ƒå¼çŠ¶æ€ç®¡ç†å™¨ import json import redis import boto3 from typing import Dict, Any, Optional, List, Union from dataclasses import dataclass, asdict from datetime import datetime, timedelta import hashlib import pickle from abc import ABC, abstractmethod class StateStore(ABC): \u0026#34;\u0026#34;\u0026#34;çŠ¶æ€å­˜å‚¨æŠ½è±¡åŸºç±»\u0026#34;\u0026#34;\u0026#34; @abstractmethod async def get(self, key: str) -\u0026gt; Optional[Any]: \u0026#34;\u0026#34;\u0026#34;è·å–çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; pass @abstractmethod async def set(self, key: str, value: Any, ttl: Optional[int] = None) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;è®¾ç½®çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; pass @abstractmethod async def delete(self, key: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;åˆ é™¤çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; pass @abstractmethod async def exists(self, key: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ£€æŸ¥çŠ¶æ€æ˜¯å¦å­˜åœ¨\u0026#34;\u0026#34;\u0026#34; pass class RedisStateStore(StateStore): \u0026#34;\u0026#34;\u0026#34;RedisçŠ¶æ€å­˜å‚¨\u0026#34;\u0026#34;\u0026#34; def __init__(self, host: str = \u0026#34;localhost\u0026#34;, port: int = 6379, password: Optional[str] = None, db: int = 0): self.redis_client = redis.Redis( host=host, port=port, password=password, db=db, decode_responses=True ) async def get(self, key: str) -\u0026gt; Optional[Any]: \u0026#34;\u0026#34;\u0026#34;è·å–çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; try: value = self.redis_client.get(key) if value: return json.loads(value) return None except Exception as e: print(f\u0026#34;Redis get error: {e}\u0026#34;) return None async def set(self, key: str, value: Any, ttl: Optional[int] = None) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;è®¾ç½®çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; try: serialized_value = json.dumps(value, default=str) if ttl: return self.redis_client.setex(key, ttl, serialized_value) else: return self.redis_client.set(key, serialized_value) except Exception as e: print(f\u0026#34;Redis set error: {e}\u0026#34;) return False async def delete(self, key: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;åˆ é™¤çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; try: return bool(self.redis_client.delete(key)) except Exception as e: print(f\u0026#34;Redis delete error: {e}\u0026#34;) return False async def exists(self, key: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ£€æŸ¥çŠ¶æ€æ˜¯å¦å­˜åœ¨\u0026#34;\u0026#34;\u0026#34; try: return bool(self.redis_client.exists(key)) except Exception as e: print(f\u0026#34;Redis exists error: {e}\u0026#34;) return False class DynamoDBStateStore(StateStore): \u0026#34;\u0026#34;\u0026#34;DynamoDBçŠ¶æ€å­˜å‚¨\u0026#34;\u0026#34;\u0026#34; def __init__(self, table_name: str, region: str = \u0026#34;us-east-1\u0026#34;): self.table_name = table_name self.dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;, region_name=region) self.table = self.dynamodb.Table(table_name) async def get(self, key: str) -\u0026gt; Optional[Any]: \u0026#34;\u0026#34;\u0026#34;è·å–çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; try: response = self.table.get_item(Key={\u0026#39;id\u0026#39;: key}) if \u0026#39;Item\u0026#39; in response: item = response[\u0026#39;Item\u0026#39;] # æ£€æŸ¥TTL if \u0026#39;ttl\u0026#39; in item and item[\u0026#39;ttl\u0026#39;] \u0026lt; datetime.utcnow().timestamp(): await self.delete(key) return None return json.loads(item[\u0026#39;data\u0026#39;]) return None except Exception as e: print(f\u0026#34;DynamoDB get error: {e}\u0026#34;) return None async def set(self, key: str, value: Any, ttl: Optional[int] = None) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;è®¾ç½®çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; try: item = { \u0026#39;id\u0026#39;: key, \u0026#39;data\u0026#39;: json.dumps(value, default=str), \u0026#39;updated_at\u0026#39;: datetime.utcnow().isoformat() } if ttl: item[\u0026#39;ttl\u0026#39;] = int((datetime.utcnow() + timedelta(seconds=ttl)).timestamp()) self.table.put_item(Item=item) return True except Exception as e: print(f\u0026#34;DynamoDB set error: {e}\u0026#34;) return False async def delete(self, key: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;åˆ é™¤çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; try: self.table.delete_item(Key={\u0026#39;id\u0026#39;: key}) return True except Exception as e: print(f\u0026#34;DynamoDB delete error: {e}\u0026#34;) return False async def exists(self, key: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ£€æŸ¥çŠ¶æ€æ˜¯å¦å­˜åœ¨\u0026#34;\u0026#34;\u0026#34; try: response = self.table.get_item(Key={\u0026#39;id\u0026#39;: key}) return \u0026#39;Item\u0026#39; in response except Exception as e: print(f\u0026#34;DynamoDB exists error: {e}\u0026#34;) return False @dataclass class WorkflowState: \u0026#34;\u0026#34;\u0026#34;å·¥ä½œæµçŠ¶æ€\u0026#34;\u0026#34;\u0026#34; workflow_id: str current_step: str steps_completed: List[str] context: Dict[str, Any] status: str created_at: datetime updated_at: datetime error_message: Optional[str] = None class StatefulWorkflowManager: \u0026#34;\u0026#34;\u0026#34;æœ‰çŠ¶æ€å·¥ä½œæµç®¡ç†å™¨\u0026#34;\u0026#34;\u0026#34; def __init__(self, state_store: StateStore): self.state_store = state_store self.workflow_definitions: Dict[str, Dict] = {} def register_workflow(self, workflow_name: str, steps: List[str], step_handlers: Dict[str, callable]): \u0026#34;\u0026#34;\u0026#34;æ³¨å†Œå·¥ä½œæµå®šä¹‰\u0026#34;\u0026#34;\u0026#34; self.workflow_definitions[workflow_name] = { \u0026#34;steps\u0026#34;: steps, \u0026#34;handlers\u0026#34;: step_handlers } async def start_workflow(self, workflow_name: str, workflow_id: str, initial_context: Dict[str, Any]) -\u0026gt; WorkflowState: \u0026#34;\u0026#34;\u0026#34;å¯åŠ¨å·¥ä½œæµ\u0026#34;\u0026#34;\u0026#34; if workflow_name not in self.workflow_definitions: raise ValueError(f\u0026#34;Workflow {workflow_name} not found\u0026#34;) workflow_def = self.workflow_definitions[workflow_name] first_step = workflow_def[\u0026#34;steps\u0026#34;][0] state = WorkflowState( workflow_id=workflow_id, current_step=first_step, steps_completed=[], context=initial_context, status=\u0026#34;running\u0026#34;, created_at=datetime.utcnow(), updated_at=datetime.utcnow() ) await self._save_state(state) return state async def execute_step(self, workflow_id: str) -\u0026gt; WorkflowState: \u0026#34;\u0026#34;\u0026#34;æ‰§è¡Œå½“å‰æ­¥éª¤\u0026#34;\u0026#34;\u0026#34; state = await self._load_state(workflow_id) if not state: raise ValueError(f\u0026#34;Workflow {workflow_id} not found\u0026#34;) if state.status != \u0026#34;running\u0026#34;: return state # è·å–å·¥ä½œæµå®šä¹‰ workflow_name = state.context.get(\u0026#34;workflow_name\u0026#34;) if not workflow_name or workflow_name not in self.workflow_definitions: state.status = \u0026#34;error\u0026#34; state.error_message = \u0026#34;Workflow definition not found\u0026#34; await self._save_state(state) return state workflow_def = self.workflow_definitions[workflow_name] current_step = state.current_step try: # æ‰§è¡Œå½“å‰æ­¥éª¤ if current_step in workflow_def[\u0026#34;handlers\u0026#34;]: handler = workflow_def[\u0026#34;handlers\u0026#34;][current_step] result = await handler(state.context) # æ›´æ–°ä¸Šä¸‹æ–‡ if isinstance(result, dict): state.context.update(result) # æ ‡è®°æ­¥éª¤å®Œæˆ state.steps_completed.append(current_step) # ç§»åŠ¨åˆ°ä¸‹ä¸€æ­¥ current_step_index = workflow_def[\u0026#34;steps\u0026#34;].index(current_step) if current_step_index + 1 \u0026lt; len(workflow_def[\u0026#34;steps\u0026#34;]): state.current_step = workflow_def[\u0026#34;steps\u0026#34;][current_step_index + 1] else: state.status = \u0026#34;completed\u0026#34; state.current_step = \u0026#34;\u0026#34; state.updated_at = datetime.utcnow() await self._save_state(state) else: state.status = \u0026#34;error\u0026#34; state.error_message = f\u0026#34;Handler for step {current_step} not found\u0026#34; await self._save_state(state) except Exception as e: state.status = \u0026#34;error\u0026#34; state.error_message = str(e) state.updated_at = datetime.utcnow() await self._save_state(state) return state async def get_workflow_status(self, workflow_id: str) -\u0026gt; Optional[WorkflowState]: \u0026#34;\u0026#34;\u0026#34;è·å–å·¥ä½œæµçŠ¶æ€\u0026#34;\u0026#34;\u0026#34; return await self._load_state(workflow_id) async def _save_state(self, state: WorkflowState): \u0026#34;\u0026#34;\u0026#34;ä¿å­˜çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; key = f\u0026#34;workflow:{state.workflow_id}\u0026#34; state_dict = asdict(state) state_dict[\u0026#39;created_at\u0026#39;] = state.created_at.isoformat() state_dict[\u0026#39;updated_at\u0026#39;] = state.updated_at.isoformat() await self.state_store.set(key, state_dict, ttl=86400) # 24å°æ—¶TTL async def _load_state(self, workflow_id: str) -\u0026gt; Optional[WorkflowState]: \u0026#34;\u0026#34;\u0026#34;åŠ è½½çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; key = f\u0026#34;workflow:{workflow_id}\u0026#34; state_dict = await self.state_store.get(key) if state_dict: state_dict[\u0026#39;created_at\u0026#39;] = datetime.fromisoformat(state_dict[\u0026#39;created_at\u0026#39;]) state_dict[\u0026#39;updated_at\u0026#39;] = datetime.fromisoformat(state_dict[\u0026#39;updated_at\u0026#39;]) return WorkflowState(**state_dict) return None class SessionManager: \u0026#34;\u0026#34;\u0026#34;ä¼šè¯ç®¡ç†å™¨\u0026#34;\u0026#34;\u0026#34; def __init__(self, state_store: StateStore, default_ttl: int = 3600): self.state_store = state_store self.default_ttl = default_ttl async def create_session(self, user_id: str, session_data: Dict[str, Any]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºä¼šè¯\u0026#34;\u0026#34;\u0026#34; session_id = self._generate_session_id(user_id) session = { \u0026#34;user_id\u0026#34;: user_id, \u0026#34;created_at\u0026#34;: datetime.utcnow().isoformat(), \u0026#34;last_accessed\u0026#34;: datetime.utcnow().isoformat(), \u0026#34;data\u0026#34;: session_data } key = f\u0026#34;session:{session_id}\u0026#34; await self.state_store.set(key, session, ttl=self.default_ttl) return session_id async def get_session(self, session_id: str) -\u0026gt; Optional[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;è·å–ä¼šè¯\u0026#34;\u0026#34;\u0026#34; key = f\u0026#34;session:{session_id}\u0026#34; session = await self.state_store.get(key) if session: # æ›´æ–°æœ€åè®¿é—®æ—¶é—´ session[\u0026#34;last_accessed\u0026#34;] = datetime.utcnow().isoformat() await self.state_store.set(key, session, ttl=self.default_ttl) return session async def update_session(self, session_id: str, data: Dict[str, Any]) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ›´æ–°ä¼šè¯æ•°æ®\u0026#34;\u0026#34;\u0026#34; session = await self.get_session(session_id) if session: session[\u0026#34;data\u0026#34;].update(data) session[\u0026#34;last_accessed\u0026#34;] = datetime.utcnow().isoformat() key = f\u0026#34;session:{session_id}\u0026#34; return await self.state_store.set(key, session, ttl=self.default_ttl) return False async def delete_session(self, session_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;åˆ é™¤ä¼šè¯\u0026#34;\u0026#34;\u0026#34; key = f\u0026#34;session:{session_id}\u0026#34; return await self.state_store.delete(key) def _generate_session_id(self, user_id: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆä¼šè¯ID\u0026#34;\u0026#34;\u0026#34; timestamp = str(datetime.utcnow().timestamp()) data = f\u0026#34;{user_id}:{timestamp}\u0026#34; return hashlib.sha256(data.encode()).hexdigest() # å·¥ä½œæµæ­¥éª¤å¤„ç†å™¨ç¤ºä¾‹ async def validate_order_step(context: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;éªŒè¯è®¢å•æ­¥éª¤\u0026#34;\u0026#34;\u0026#34; order_id = context.get(\u0026#34;order_id\u0026#34;) print(f\u0026#34;Validating order: {order_id}\u0026#34;) # æ¨¡æ‹ŸéªŒè¯é€»è¾‘ await asyncio.sleep(0.1) return { \u0026#34;order_validated\u0026#34;: True, \u0026#34;validation_timestamp\u0026#34;: datetime.utcnow().isoformat() } async def process_payment_step(context: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;å¤„ç†æ”¯ä»˜æ­¥éª¤\u0026#34;\u0026#34;\u0026#34; order_id = context.get(\u0026#34;order_id\u0026#34;) amount = context.get(\u0026#34;amount\u0026#34;, 0) print(f\u0026#34;Processing payment for order: {order_id}, amount: ${amount}\u0026#34;) # æ¨¡æ‹Ÿæ”¯ä»˜å¤„ç† await asyncio.sleep(0.2) return { \u0026#34;payment_processed\u0026#34;: True, \u0026#34;transaction_id\u0026#34;: f\u0026#34;txn_{order_id}_{int(datetime.utcnow().timestamp())}\u0026#34;, \u0026#34;payment_timestamp\u0026#34;: datetime.utcnow().isoformat() } async def fulfill_order_step(context: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;å±¥è¡Œè®¢å•æ­¥éª¤\u0026#34;\u0026#34;\u0026#34; order_id = context.get(\u0026#34;order_id\u0026#34;) print(f\u0026#34;Fulfilling order: {order_id}\u0026#34;) # æ¨¡æ‹Ÿè®¢å•å±¥è¡Œ await asyncio.sleep(0.15) return { \u0026#34;order_fulfilled\u0026#34;: True, \u0026#34;tracking_number\u0026#34;: f\u0026#34;track_{order_id}\u0026#34;, \u0026#34;fulfillment_timestamp\u0026#34;: datetime.utcnow().isoformat() } # ä½¿ç”¨ç¤ºä¾‹ async def state_management_example(): \u0026#34;\u0026#34;\u0026#34;çŠ¶æ€ç®¡ç†ç¤ºä¾‹\u0026#34;\u0026#34;\u0026#34; # åˆå§‹åŒ–çŠ¶æ€å­˜å‚¨ state_store = RedisStateStore() # æˆ– DynamoDBStateStore(\u0026#34;workflow-states\u0026#34;) # åˆå§‹åŒ–å·¥ä½œæµç®¡ç†å™¨ workflow_manager = StatefulWorkflowManager(state_store) # æ³¨å†Œè®¢å•å¤„ç†å·¥ä½œæµ workflow_manager.register_workflow( \u0026#34;order_processing\u0026#34;, steps=[\u0026#34;validate_order\u0026#34;, \u0026#34;process_payment\u0026#34;, \u0026#34;fulfill_order\u0026#34;], step_handlers={ \u0026#34;validate_order\u0026#34;: validate_order_step, \u0026#34;process_payment\u0026#34;: process_payment_step, \u0026#34;fulfill_order\u0026#34;: fulfill_order_step } ) # å¯åŠ¨å·¥ä½œæµ workflow_id = \u0026#34;order_12345\u0026#34; initial_context = { \u0026#34;workflow_name\u0026#34;: \u0026#34;order_processing\u0026#34;, \u0026#34;order_id\u0026#34;: \u0026#34;order_12345\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user_789\u0026#34;, \u0026#34;amount\u0026#34;: 99.99 } state = await workflow_manager.start_workflow( \u0026#34;order_processing\u0026#34;, workflow_id, initial_context ) print(f\u0026#34;Workflow started: {state.workflow_id}, current step: {state.current_step}\u0026#34;) # æ‰§è¡Œå·¥ä½œæµæ­¥éª¤ while state.status == \u0026#34;running\u0026#34;: state = await workflow_manager.execute_step(workflow_id) print(f\u0026#34;Step completed: {state.current_step}, status: {state.status}\u0026#34;) if state.status == \u0026#34;error\u0026#34;: print(f\u0026#34;Workflow error: {state.error_message}\u0026#34;) break if state.status == \u0026#34;completed\u0026#34;: print(\u0026#34;Workflow completed successfully!\u0026#34;) print(f\u0026#34;Final context: {state.context}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: import asyncio asyncio.run(state_management_example()) å®‰å…¨æ€§ä¸åˆè§„æ€§ å®‰å…¨æ¡†æ¶ import json import hashlib import hmac import base64 import time import uuid import jwt import boto3 from typing import Dict, Any, List, Optional, Union, Callable from dataclasses import dataclass, asdict from datetime import datetime, timedelta from enum import Enum import re import logging # é…ç½®æ—¥å¿— logging.basicConfig(level=logging.INFO) logger = logging.getLogger(\u0026#34;serverless-security\u0026#34;) class SecurityLevel(Enum): \u0026#34;\u0026#34;\u0026#34;å®‰å…¨çº§åˆ«\u0026#34;\u0026#34;\u0026#34; LOW = \u0026#34;low\u0026#34; MEDIUM = \u0026#34;medium\u0026#34; HIGH = \u0026#34;high\u0026#34; CRITICAL = \u0026#34;critical\u0026#34; @dataclass class SecurityPolicy: \u0026#34;\u0026#34;\u0026#34;å®‰å…¨ç­–ç•¥\u0026#34;\u0026#34;\u0026#34; id: str name: str description: str level: SecurityLevel rules: Dict[str, Any] created_at: datetime updated_at: datetime enabled: bool = True class ServerlessSecurityManager: \u0026#34;\u0026#34;\u0026#34;Serverlesså®‰å…¨ç®¡ç†å™¨\u0026#34;\u0026#34;\u0026#34; def __init__(self, aws_region: str = \u0026#34;us-east-1\u0026#34;): self.policies: Dict[str, SecurityPolicy] = {} self.aws_region = aws_region # AWSæœåŠ¡å®¢æˆ·ç«¯ self.lambda_client = boto3.client(\u0026#39;lambda\u0026#39;, region_name=aws_region) self.iam = boto3.client(\u0026#39;iam\u0026#39;, region_name=aws_region) self.secretsmanager = boto3.client(\u0026#39;secretsmanager\u0026#39;, region_name=aws_region) self.kms = boto3.client(\u0026#39;kms\u0026#39;, region_name=aws_region) def add_policy(self, policy: SecurityPolicy): \u0026#34;\u0026#34;\u0026#34;æ·»åŠ å®‰å…¨ç­–ç•¥\u0026#34;\u0026#34;\u0026#34; self.policies[policy.id] = policy def get_policy(self, policy_id: str) -\u0026gt; Optional[SecurityPolicy]: \u0026#34;\u0026#34;\u0026#34;è·å–å®‰å…¨ç­–ç•¥\u0026#34;\u0026#34;\u0026#34; return self.policies.get(policy_id) def list_policies(self, level: Optional[SecurityLevel] = None) -\u0026gt; List[SecurityPolicy]: \u0026#34;\u0026#34;\u0026#34;åˆ—å‡ºå®‰å…¨ç­–ç•¥\u0026#34;\u0026#34;\u0026#34; if level: return [p for p in self.policies.values() if p.level == level and p.enabled] return [p for p in self.policies.values() if p.enabled] def apply_policies(self, resource_type: str, resource_config: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;åº”ç”¨å®‰å…¨ç­–ç•¥åˆ°èµ„æºé…ç½®\u0026#34;\u0026#34;\u0026#34; applicable_policies = [p for p in self.policies.values() if p.enabled and resource_type in p.rules] for policy in applicable_policies: rules = policy.rules.get(resource_type, {}) resource_config = self._apply_rules(resource_config, rules) logger.info(f\u0026#34;Applied policy \u0026#39;{policy.name}\u0026#39; to {resource_type}\u0026#34;) return resource_config def _apply_rules(self, config: Dict[str, Any], rules: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;åº”ç”¨è§„åˆ™åˆ°é…ç½®\u0026#34;\u0026#34;\u0026#34; # æ·±æ‹·è´é…ç½®ä»¥é¿å…ä¿®æ”¹åŸå§‹é…ç½® result = json.loads(json.dumps(config)) # åº”ç”¨æ·»åŠ è§„åˆ™ if \u0026#34;add\u0026#34; in rules: for path, value in rules[\u0026#34;add\u0026#34;].items(): self._set_nested_value(result, path, value) # åº”ç”¨ä¿®æ”¹è§„åˆ™ if \u0026#34;modify\u0026#34; in rules: for path, value in rules[\u0026#34;modify\u0026#34;].items(): if self._has_nested_path(result, path): self._set_nested_value(result, path, value) # åº”ç”¨åˆ é™¤è§„åˆ™ if \u0026#34;remove\u0026#34; in rules: for path in rules[\u0026#34;remove\u0026#34;]: self._remove_nested_value(result, path) return result def _set_nested_value(self, obj: Dict[str, Any], path: str, value: Any): \u0026#34;\u0026#34;\u0026#34;è®¾ç½®åµŒå¥—å€¼\u0026#34;\u0026#34;\u0026#34; parts = path.split(\u0026#39;.\u0026#39;) current = obj for i, part in enumerate(parts[:-1]): if part not in current: current[part] = {} current = current[part] current[parts[-1]] = value def _has_nested_path(self, obj: Dict[str, Any], path: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ£€æŸ¥æ˜¯å¦æœ‰åµŒå¥—è·¯å¾„\u0026#34;\u0026#34;\u0026#34; parts = path.split(\u0026#39;.\u0026#39;) current = obj for part in parts[:-1]: if part not in current: return False current = current[part] return parts[-1] in current def _remove_nested_value(self, obj: Dict[str, Any], path: str): \u0026#34;\u0026#34;\u0026#34;åˆ é™¤åµŒå¥—å€¼\u0026#34;\u0026#34;\u0026#34; parts = path.split(\u0026#39;.\u0026#39;) current = obj for part in parts[:-1]: if part not in current: return current = current[part] if parts[-1] in current: del current[parts[-1]] def create_iam_role(self, role_name: str, trust_policy: Dict[str, Any], policy_arns: List[str]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºIAMè§’è‰²\u0026#34;\u0026#34;\u0026#34; try: # åˆ›å»ºè§’è‰² response = self.iam.create_role( RoleName=role_name, AssumeRolePolicyDocument=json.dumps(trust_policy) ) role_arn = response[\u0026#39;Role\u0026#39;][\u0026#39;Arn\u0026#39;] # é™„åŠ ç­–ç•¥ for policy_arn in policy_arns: self.iam.attach_role_policy( RoleName=role_name, PolicyArn=policy_arn ) logger.info(f\u0026#34;Created IAM role: {role_name}\u0026#34;) return role_arn except Exception as e: logger.error(f\u0026#34;Failed to create IAM role: {e}\u0026#34;) raise def create_least_privilege_role(self, function_name: str, required_services: List[str]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºæœ€å°æƒé™è§’è‰²\u0026#34;\u0026#34;\u0026#34; role_name = f\u0026#34;{function_name}-role\u0026#34; # åˆ›å»ºä¿¡ä»»ç­–ç•¥ trust_policy = { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: { \u0026#34;Service\u0026#34;: \u0026#34;lambda.amazonaws.com\u0026#34; }, \u0026#34;Action\u0026#34;: \u0026#34;sts:AssumeRole\u0026#34; } ] } # åŸºæœ¬ç­–ç•¥ARNåˆ—è¡¨ policy_arns = [\u0026#34;arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole\u0026#34;] # æ ¹æ®éœ€è¦çš„æœåŠ¡æ·»åŠ ç­–ç•¥ service_policy_map = { \u0026#34;dynamodb\u0026#34;: \u0026#34;arn:aws:iam::aws:policy/AmazonDynamoDBReadOnlyAccess\u0026#34;, \u0026#34;s3\u0026#34;: \u0026#34;arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\u0026#34;, \u0026#34;sqs\u0026#34;: \u0026#34;arn:aws:iam::aws:policy/AmazonSQSFullAccess\u0026#34;, \u0026#34;sns\u0026#34;: \u0026#34;arn:aws:iam::aws:policy/AmazonSNSFullAccess\u0026#34; } for service in required_services: if service in service_policy_map: policy_arns.append(service_policy_map[service]) return self.create_iam_role(role_name, trust_policy, policy_arns) def generate_secure_api_config(self, api_name: str, auth_type: str = \u0026#34;JWT\u0026#34;) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆå®‰å…¨çš„APIé…ç½®\u0026#34;\u0026#34;\u0026#34; api_config = { \u0026#34;name\u0026#34;: api_name, \u0026#34;description\u0026#34;: f\u0026#34;Secure API for {api_name}\u0026#34;, \u0026#34;cors\u0026#34;: { \u0026#34;allowOrigins\u0026#34;: [\u0026#34;*\u0026#34;], \u0026#34;allowMethods\u0026#34;: [\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;DELETE\u0026#34;, \u0026#34;OPTIONS\u0026#34;], \u0026#34;allowHeaders\u0026#34;: [\u0026#34;Content-Type\u0026#34;, \u0026#34;Authorization\u0026#34;, \u0026#34;X-Api-Key\u0026#34;], \u0026#34;maxAge\u0026#34;: 3600 }, \u0026#34;auth\u0026#34;: {} } if auth_type == \u0026#34;JWT\u0026#34;: # JWTè®¤è¯é…ç½® api_config[\u0026#34;auth\u0026#34;] = { \u0026#34;type\u0026#34;: \u0026#34;JWT\u0026#34;, \u0026#34;jwtConfiguration\u0026#34;: { \u0026#34;issuer\u0026#34;: f\u0026#34;https://auth.example.com/{api_name}\u0026#34;, \u0026#34;audience\u0026#34;: [api_name], \u0026#34;tokenValidityInMinutes\u0026#34;: 60 } } elif auth_type == \u0026#34;IAM\u0026#34;: # IAMè®¤è¯é…ç½® api_config[\u0026#34;auth\u0026#34;] = { \u0026#34;type\u0026#34;: \u0026#34;AWS_IAM\u0026#34; } elif auth_type == \u0026#34;API_KEY\u0026#34;: # APIå¯†é’¥è®¤è¯é…ç½® api_config[\u0026#34;auth\u0026#34;] = { \u0026#34;type\u0026#34;: \u0026#34;API_KEY\u0026#34;, \u0026#34;apiKeyRequired\u0026#34;: True } # åº”ç”¨å®‰å…¨ç­–ç•¥ return self.apply_policies(\u0026#34;api\u0026#34;, api_config) def create_encryption_key(self, key_description: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºåŠ å¯†å¯†é’¥\u0026#34;\u0026#34;\u0026#34; try: response = self.kms.create_key( Description=key_description, KeyUsage=\u0026#39;ENCRYPT_DECRYPT\u0026#39;, Origin=\u0026#39;AWS_KMS\u0026#39; ) key_id = response[\u0026#39;KeyMetadata\u0026#39;][\u0026#39;KeyId\u0026#39;] logger.info(f\u0026#34;Created KMS key: {key_id}\u0026#34;) return key_id except Exception as e: logger.error(f\u0026#34;Failed to create KMS key: {e}\u0026#34;) raise def store_secret(self, secret_name: str, secret_value: Dict[str, Any], description: str = \u0026#34;\u0026#34;) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;å­˜å‚¨å¯†é’¥\u0026#34;\u0026#34;\u0026#34; try: response = self.secretsmanager.create_secret( Name=secret_name, Description=description, SecretString=json.dumps(secret_value) ) secret_arn = response[\u0026#39;ARN\u0026#39;] logger.info(f\u0026#34;Stored secret: {secret_name}\u0026#34;) return secret_arn except Exception as e: logger.error(f\u0026#34;Failed to store secret: {e}\u0026#34;) raise def generate_jwt_token(self, payload: Dict[str, Any], secret: str, expires_in: int = 3600) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆJWTä»¤ç‰Œ\u0026#34;\u0026#34;\u0026#34; # æ·»åŠ æ ‡å‡†å£°æ˜ payload.update({ \u0026#34;iat\u0026#34;: int(time.time()), \u0026#34;exp\u0026#34;: int(time.time()) + expires_in, \u0026#34;jti\u0026#34;: str(uuid.uuid4()) }) # ç”Ÿæˆä»¤ç‰Œ token = jwt.encode(payload, secret, algorithm=\u0026#34;HS256\u0026#34;) return token def verify_jwt_token(self, token: str, secret: str) -\u0026gt; Optional[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;éªŒè¯JWTä»¤ç‰Œ\u0026#34;\u0026#34;\u0026#34; try: payload = jwt.decode(token, secret, algorithms=[\u0026#34;HS256\u0026#34;]) return payload except jwt.ExpiredSignatureError: logger.warning(\u0026#34;Token has expired\u0026#34;) return None except jwt.InvalidTokenError as e: logger.warning(f\u0026#34;Invalid token: {e}\u0026#34;) return None def generate_secure_function_config(self, function_name: str, runtime: str, handler: str, memory_size: int = 128, timeout: int = 30) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆå®‰å…¨çš„å‡½æ•°é…ç½®\u0026#34;\u0026#34;\u0026#34; function_config = { \u0026#34;name\u0026#34;: function_name, \u0026#34;runtime\u0026#34;: runtime, \u0026#34;handler\u0026#34;: handler, \u0026#34;memorySize\u0026#34;: memory_size, \u0026#34;timeout\u0026#34;: timeout, \u0026#34;environment\u0026#34;: { \u0026#34;variables\u0026#34;: { \u0026#34;NODE_OPTIONS\u0026#34;: \u0026#34;--enable-source-maps\u0026#34; } }, \u0026#34;tracing\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;Active\u0026#34; }, \u0026#34;logRetention\u0026#34;: 14 } # åº”ç”¨å®‰å…¨ç­–ç•¥ return self.apply_policies(\u0026#34;function\u0026#34;, function_config) # å®‰å…¨ç­–ç•¥ç¤ºä¾‹ def create_security_policies() -\u0026gt; List[SecurityPolicy]: \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºå®‰å…¨ç­–ç•¥ç¤ºä¾‹\u0026#34;\u0026#34;\u0026#34; policies = [] # åŸºæœ¬å®‰å…¨ç­–ç•¥ basic_policy = SecurityPolicy( id=\u0026#34;basic-security\u0026#34;, name=\u0026#34;Basic Security Policy\u0026#34;, description=\u0026#34;åŸºæœ¬å®‰å…¨æªæ–½ï¼Œé€‚ç”¨äºæ‰€æœ‰Serverlessèµ„æº\u0026#34;, level=SecurityLevel.LOW, rules={ \u0026#34;function\u0026#34;: { \u0026#34;add\u0026#34;: { \u0026#34;environment.variables.SECURITY_LEVEL\u0026#34;: \u0026#34;basic\u0026#34;, \u0026#34;tracing.mode\u0026#34;: \u0026#34;Active\u0026#34; } }, \u0026#34;api\u0026#34;: { \u0026#34;add\u0026#34;: { \u0026#34;minimumCompressionSize\u0026#34;: 1024, \u0026#34;logging\u0026#34;: { \u0026#34;accessLogging\u0026#34;: True, \u0026#34;executionLogging\u0026#34;: True } } } }, created_at=datetime.utcnow(), updated_at=datetime.utcnow() ) policies.append(basic_policy) # ä¸­ç­‰å®‰å…¨ç­–ç•¥ medium_policy = SecurityPolicy( id=\u0026#34;medium-security\u0026#34;, name=\u0026#34;Medium Security Policy\u0026#34;, description=\u0026#34;ä¸­ç­‰å®‰å…¨æªæ–½ï¼Œå¢åŠ äº†æ›´å¤šä¿æŠ¤\u0026#34;, level=SecurityLevel.MEDIUM, rules={ \u0026#34;function\u0026#34;: { \u0026#34;add\u0026#34;: { \u0026#34;environment.variables.SECURITY_LEVEL\u0026#34;: \u0026#34;medium\u0026#34;, \u0026#34;reservedConcurrency\u0026#34;: 10, \u0026#34;vpc\u0026#34;: { \u0026#34;enabled\u0026#34;: True } } }, \u0026#34;api\u0026#34;: { \u0026#34;add\u0026#34;: { \u0026#34;throttling\u0026#34;: { \u0026#34;burstLimit\u0026#34;: 100, \u0026#34;rateLimit\u0026#34;: 50 }, \u0026#34;cors\u0026#34;: { \u0026#34;allowOrigins\u0026#34;: [\u0026#34;https://*.example.com\u0026#34;], \u0026#34;allowMethods\u0026#34;: [\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;DELETE\u0026#34;], \u0026#34;allowHeaders\u0026#34;: [\u0026#34;Content-Type\u0026#34;, \u0026#34;Authorization\u0026#34;] } } } }, created_at=datetime.utcnow(), updated_at=datetime.utcnow() ) policies.append(medium_policy) # é«˜å®‰å…¨ç­–ç•¥ high_policy = SecurityPolicy( id=\u0026#34;high-security\u0026#34;, name=\u0026#34;High Security Policy\u0026#34;, description=\u0026#34;é«˜å®‰å…¨æªæ–½ï¼Œé€‚ç”¨äºå¤„ç†æ•æ„Ÿæ•°æ®çš„èµ„æº\u0026#34;, level=SecurityLevel.HIGH, rules={ \u0026#34;function\u0026#34;: { \u0026#34;add\u0026#34;: { \u0026#34;environment.variables.SECURITY_LEVEL\u0026#34;: \u0026#34;high\u0026#34;, \u0026#34;reservedConcurrency\u0026#34;: 5, \u0026#34;vpc\u0026#34;: { \u0026#34;enabled\u0026#34;: True, \u0026#34;securityGroupIds\u0026#34;: [\u0026#34;sg-123456\u0026#34;], \u0026#34;subnetIds\u0026#34;: [\u0026#34;subnet-123456\u0026#34;, \u0026#34;subnet-789012\u0026#34;] }, \u0026#34;kmsKeyArn\u0026#34;: \u0026#34;arn:aws:kms:region:account:key/key-id\u0026#34; } }, \u0026#34;api\u0026#34;: { \u0026#34;add\u0026#34;: { \u0026#34;throttling\u0026#34;: { \u0026#34;burstLimit\u0026#34;: 50, \u0026#34;rateLimit\u0026#34;: 20 }, \u0026#34;cors\u0026#34;: { \u0026#34;allowOrigins\u0026#34;: [\u0026#34;https://app.example.com\u0026#34;], \u0026#34;allowMethods\u0026#34;: [\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;], \u0026#34;allowHeaders\u0026#34;: [\u0026#34;Content-Type\u0026#34;, \u0026#34;Authorization\u0026#34;, \u0026#34;X-Api-Key\u0026#34;] }, \u0026#34;waf\u0026#34;: { \u0026#34;enabled\u0026#34;: True, \u0026#34;webAclId\u0026#34;: \u0026#34;web-acl-id\u0026#34; } } } }, created_at=datetime.utcnow(), updated_at=datetime.utcnow() ) policies.append(high_policy) return policies # ä½¿ç”¨ç¤ºä¾‹ def security_example(): \u0026#34;\u0026#34;\u0026#34;å®‰å…¨ç¤ºä¾‹\u0026#34;\u0026#34;\u0026#34; # åˆ›å»ºå®‰å…¨ç®¡ç†å™¨ security_manager = ServerlessSecurityManager() # æ·»åŠ å®‰å…¨ç­–ç•¥ for policy in create_security_policies(): security_manager.add_policy(policy) # ç”Ÿæˆå®‰å…¨çš„APIé…ç½® api_config = security_manager.generate_secure_api_config( \u0026#34;user-service-api\u0026#34;, auth_type=\u0026#34;JWT\u0026#34; ) print(\u0026#34;Secure API Configuration:\u0026#34;) print(json.dumps(api_config, indent=2)) # ç”Ÿæˆå®‰å…¨çš„å‡½æ•°é…ç½® function_config = security_manager.generate_secure_function_config( \u0026#34;user-service-function\u0026#34;, runtime=\u0026#34;nodejs14.x\u0026#34;, handler=\u0026#34;index.handler\u0026#34;, memory_size=256, timeout=60 ) print(\u0026#34;\\nSecure Function Configuration:\u0026#34;) print(json.dumps(function_config, indent=2)) # åˆ›å»ºæœ€å°æƒé™è§’è‰² role_arn = security_manager.create_least_privilege_role( \u0026#34;user-service-function\u0026#34;, required_services=[\u0026#34;dynamodb\u0026#34;, \u0026#34;sqs\u0026#34;] ) print(f\u0026#34;\\nCreated IAM Role: {role_arn}\u0026#34;) # ç”ŸæˆJWTä»¤ç‰Œ secret = \u0026#34;your-secret-key\u0026#34; payload = { \u0026#34;sub\u0026#34;: \u0026#34;user123\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;role\u0026#34;: \u0026#34;admin\u0026#34; } token = security_manager.generate_jwt_token(payload, secret) print(f\u0026#34;\\nGenerated JWT Token: {token}\u0026#34;) # éªŒè¯JWTä»¤ç‰Œ verified_payload = security_manager.verify_jwt_token(token, secret) print(\u0026#34;\\nVerified JWT Payload:\u0026#34;) print(json.dumps(verified_payload, indent=2)) if __name__ == \u0026#34;__main__\u0026#34;: security_example() æ€§èƒ½ä¼˜åŒ– å†·å¯åŠ¨ä¼˜åŒ– import time import json import random import statistics from typing import Dict, Any, List, Callable, Optional, Tuple import matplotlib.pyplot as plt import numpy as np from dataclasses import dataclass from enum import Enum import boto3 import threading import concurrent.futures class RuntimeType(Enum): \u0026#34;\u0026#34;\u0026#34;è¿è¡Œæ—¶ç±»å‹\u0026#34;\u0026#34;\u0026#34; PYTHON = \u0026#34;python\u0026#34; NODE_JS = \u0026#34;nodejs\u0026#34; JAVA = \u0026#34;java\u0026#34; GO = \u0026#34;go\u0026#34; RUBY = \u0026#34;ruby\u0026#34; DOTNET = \u0026#34;dotnet\u0026#34; @dataclass class FunctionConfig: \u0026#34;\u0026#34;\u0026#34;å‡½æ•°é…ç½®\u0026#34;\u0026#34;\u0026#34; name: str runtime: RuntimeType memory_size: int code_size: int dependencies_count: int has_vpc: bool has_layers: bool initialization_code: bool @dataclass class PerformanceResult: \u0026#34;\u0026#34;\u0026#34;æ€§èƒ½ç»“æœ\u0026#34;\u0026#34;\u0026#34; cold_start_time: float warm_start_time: float execution_time: float memory_usage: float initialization_time: float class ColdStartOptimizer: \u0026#34;\u0026#34;\u0026#34;å†·å¯åŠ¨ä¼˜åŒ–å™¨\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.lambda_client = boto3.client(\u0026#39;lambda\u0026#39;) self.cloudwatch = boto3.client(\u0026#39;cloudwatch\u0026#39;) def analyze_cold_start(self, function_config: FunctionConfig) -\u0026gt; PerformanceResult: \u0026#34;\u0026#34;\u0026#34;åˆ†æå†·å¯åŠ¨æ€§èƒ½\u0026#34;\u0026#34;\u0026#34; # æ¨¡æ‹Ÿæ€§èƒ½åˆ†æ # åœ¨å®é™…ç¯å¢ƒä¸­ï¼Œè¿™å°†ä»CloudWatchç­‰æœåŠ¡è·å–çœŸå®æ•°æ® # åŸºç¡€å†·å¯åŠ¨æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ base_cold_start = { RuntimeType.PYTHON: 250, RuntimeType.NODE_JS: 300, RuntimeType.JAVA: 1500, RuntimeType.GO: 400, RuntimeType.RUBY: 500, RuntimeType.DOTNET: 1200 } # åŸºç¡€æ‰§è¡Œæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ base_execution = { RuntimeType.PYTHON: 50, RuntimeType.NODE_JS: 60, RuntimeType.JAVA: 100, RuntimeType.GO: 40, RuntimeType.RUBY: 80, RuntimeType.DOTNET: 90 } # è®¡ç®—å†·å¯åŠ¨æ—¶é—´ cold_start_time = base_cold_start[function_config.runtime] # å†…å­˜å¤§å°å½±å“ memory_factor = 1.0 - (function_config.memory_size - 128) / 2048 cold_start_time *= max(0.5, memory_factor) # ä»£ç å¤§å°å½±å“ code_size_factor = 1.0 + (function_config.code_size / 50000) cold_start_time *= code_size_factor # ä¾èµ–æ•°é‡å½±å“ deps_factor = 1.0 + (function_config.dependencies_count * 0.05) cold_start_time *= deps_factor # VPCå½±å“ if function_config.has_vpc: cold_start_time += 500 # VPCè¿æ¥å¢åŠ çº¦500ms # å±‚å½±å“ if function_config.has_layers: cold_start_time += 100 * random.randint(1, 3) # æ¯å±‚å¢åŠ 100-300ms # åˆå§‹åŒ–ä»£ç å½±å“ initialization_time = 0 if function_config.initialization_code: initialization_time = cold_start_time * 0.3 # åˆå§‹åŒ–ä»£ç å å†·å¯åŠ¨æ—¶é—´çš„30% cold_start_time += initialization_time # è®¡ç®—çƒ­å¯åŠ¨æ—¶é—´ï¼ˆé€šå¸¸æ˜¯å†·å¯åŠ¨çš„5-10%ï¼‰ warm_start_time = base_execution[function_config.runtime] * (1.0 + random.random() * 0.5) # æ‰§è¡Œæ—¶é—´ execution_time = base_execution[function_config.runtime] * (1.0 + random.random() * 0.5) # å†…å­˜ä½¿ç”¨ memory_usage = function_config.memory_size * (0.2 + random.random() * 0.3) return PerformanceResult( cold_start_time=cold_start_time, warm_start_time=warm_start_time, execution_time=execution_time, memory_usage=memory_usage, initialization_time=initialization_time ) def optimize_function(self, function_config: FunctionConfig) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ä¼˜åŒ–å‡½æ•°é…ç½®\u0026#34;\u0026#34;\u0026#34; result = self.analyze_cold_start(function_config) optimizations = { \u0026#34;original_config\u0026#34;: { \u0026#34;memory_size\u0026#34;: function_config.memory_size, \u0026#34;has_vpc\u0026#34;: function_config.has_vpc, \u0026#34;has_layers\u0026#34;: function_config.has_layers, \u0026#34;initialization_code\u0026#34;: function_config.initialization_code }, \u0026#34;performance\u0026#34;: { \u0026#34;cold_start_time_ms\u0026#34;: result.cold_start_time, \u0026#34;warm_start_time_ms\u0026#34;: result.warm_start_time, \u0026#34;execution_time_ms\u0026#34;: result.execution_time, \u0026#34;memory_usage_mb\u0026#34;: result.memory_usage, \u0026#34;initialization_time_ms\u0026#34;: result.initialization_time }, \u0026#34;recommendations\u0026#34;: [] } # å†…å­˜ä¼˜åŒ– if result.cold_start_time \u0026gt; 1000 and function_config.memory_size \u0026lt; 1024: optimized_memory = min(3008, function_config.memory_size * 2) optimizations[\u0026#34;recommendations\u0026#34;].append({ \u0026#34;type\u0026#34;: \u0026#34;memory_increase\u0026#34;, \u0026#34;description\u0026#34;: f\u0026#34;å¢åŠ å†…å­˜ä» {function_config.memory_size}MB åˆ° {optimized_memory}MB å¯ä»¥å‡å°‘å†·å¯åŠ¨æ—¶é—´\u0026#34;, \u0026#34;estimated_improvement\u0026#34;: \u0026#34;30-50%\u0026#34; }) # VPCä¼˜åŒ– if function_config.has_vpc: optimizations[\u0026#34;recommendations\u0026#34;].append({ \u0026#34;type\u0026#34;: \u0026#34;vpc_optimization\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;å¦‚æœä¸éœ€è¦VPCè®¿é—®ï¼Œç§»é™¤VPCé…ç½®å¯ä»¥å‡å°‘å†·å¯åŠ¨æ—¶é—´\u0026#34;, \u0026#34;estimated_improvement\u0026#34;: \u0026#34;300-700ms\u0026#34; }) # å±‚ä¼˜åŒ– if function_config.has_layers: optimizations[\u0026#34;recommendations\u0026#34;].append({ \u0026#34;type\u0026#34;: \u0026#34;layer_optimization\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;åˆå¹¶å±‚æˆ–å‡å°‘å±‚çš„æ•°é‡å¯ä»¥å‡å°‘å†·å¯åŠ¨æ—¶é—´\u0026#34;, \u0026#34;estimated_improvement\u0026#34;: \u0026#34;100-300ms per layer\u0026#34; }) # ä»£ç ä¼˜åŒ– if function_config.code_size \u0026gt; 5000: optimizations[\u0026#34;recommendations\u0026#34;].append({ \u0026#34;type\u0026#34;: \u0026#34;code_optimization\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;å‡å°ä»£ç åŒ…å¤§å°ï¼Œç§»é™¤ä¸å¿…è¦çš„ä¾èµ–\u0026#34;, \u0026#34;estimated_improvement\u0026#34;: \u0026#34;10-30%\u0026#34; }) # åˆå§‹åŒ–ä»£ç ä¼˜åŒ– if function_config.initialization_code and result.initialization_time \u0026gt; 200: optimizations[\u0026#34;recommendations\u0026#34;].append({ \u0026#34;type\u0026#34;: \u0026#34;initialization_optimization\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;ä¼˜åŒ–åˆå§‹åŒ–ä»£ç ï¼Œå°†éå¿…è¦çš„åˆå§‹åŒ–ç§»åˆ°å¤„ç†ç¨‹åºå†…\u0026#34;, \u0026#34;estimated_improvement\u0026#34;: f\u0026#34;çº¦ {int(result.initialization_time)}ms\u0026#34; }) # é¢„çƒ­ç­–ç•¥ if result.cold_start_time \u0026gt; 1000: optimizations[\u0026#34;recommendations\u0026#34;].append({ \u0026#34;type\u0026#34;: \u0026#34;warming_strategy\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;å®æ–½é¢„çƒ­ç­–ç•¥ï¼Œå®šæœŸè°ƒç”¨å‡½æ•°ä»¥ä¿æŒçƒ­å®ä¾‹\u0026#34;, \u0026#34;estimated_improvement\u0026#34;: f\u0026#34;æ¶ˆé™¤å¤§éƒ¨åˆ†å†·å¯åŠ¨ ({int(result.cold_start_time - result.warm_start_time)}ms)\u0026#34; }) return optimizations def simulate_warming_strategies(self, function_name: str, invocation_pattern: List[int], strategies: List[Dict[str, Any]]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;æ¨¡æ‹Ÿä¸åŒé¢„çƒ­ç­–ç•¥çš„æ•ˆæœ\u0026#34;\u0026#34;\u0026#34; results = {} for strategy in strategies: strategy_name = strategy[\u0026#34;name\u0026#34;] warming_interval = strategy[\u0026#34;interval_minutes\u0026#34;] concurrency = strategy.get(\u0026#34;concurrency\u0026#34;, 1) # æ¨¡æ‹Ÿ24å°æ—¶å†…çš„è°ƒç”¨ hours = 24 minutes_per_hour = 60 total_minutes = hours * minutes_per_hour # åˆå§‹åŒ–ç»“æœæ•°ç»„ cold_starts = [0] * total_minutes warm_starts = [0] * total_minutes warming_calls = [0] * total_minutes # è®¾ç½®é¢„çƒ­è°ƒç”¨æ—¶é—´ç‚¹ for minute in range(0, total_minutes, warming_interval): warming_calls[minute] = concurrency # æ¨¡æ‹Ÿå®é™…è°ƒç”¨ for minute, calls in enumerate(invocation_pattern * (total_minutes // len(invocation_pattern) + 1)): if minute \u0026gt;= total_minutes: break # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„çƒ­å®ä¾‹ warm_instances = sum(warming_calls[max(0, minute - 15):minute]) if calls \u0026lt;= warm_instances: # æ‰€æœ‰è°ƒç”¨éƒ½æ˜¯çƒ­å¯åŠ¨ warm_starts[minute] = calls else: # éƒ¨åˆ†è°ƒç”¨æ˜¯å†·å¯åŠ¨ warm_starts[minute] = warm_instances cold_starts[minute] = calls - warm_instances # è®¡ç®—ç»“æœ total_cold_starts = sum(cold_starts) total_warm_starts = sum(warm_starts) total_warming_calls = sum(warming_calls) total_calls = total_cold_starts + total_warm_starts results[strategy_name] = { \u0026#34;total_calls\u0026#34;: total_calls, \u0026#34;cold_starts\u0026#34;: total_cold_starts, \u0026#34;cold_start_percentage\u0026#34;: (total_cold_starts / total_calls) * 100 if total_calls \u0026gt; 0 else 0, \u0026#34;warming_calls\u0026#34;: total_warming_calls, \u0026#34;cost_overhead_percentage\u0026#34;: (total_warming_calls / (total_calls + total_warming_calls)) * 100 if (total_calls + total_warming_calls) \u0026gt; 0 else 0 } return results def visualize_cold_start_comparison(self, configs: List[FunctionConfig]) -\u0026gt; None: \u0026#34;\u0026#34;\u0026#34;å¯è§†åŒ–ä¸åŒé…ç½®çš„å†·å¯åŠ¨æ¯”è¾ƒ\u0026#34;\u0026#34;\u0026#34; results = [self.analyze_cold_start(config) for config in configs] # å‡†å¤‡æ•°æ® names = [config.name for config in configs] cold_starts = [result.cold_start_time for result in results] warm_starts = [result.warm_start_time for result in results] # åˆ›å»ºå›¾è¡¨ x = np.arange(len(names)) width = 0.35 fig, ax = plt.subplots(figsize=(12, 6)) rects1 = ax.bar(x - width/2, cold_starts, width, label=\u0026#39;Cold Start (ms)\u0026#39;) rects2 = ax.bar(x + width/2, warm_starts, width, label=\u0026#39;Warm Start (ms)\u0026#39;) # æ·»åŠ æ ‡ç­¾å’Œæ ‡é¢˜ ax.set_ylabel(\u0026#39;Time (ms)\u0026#39;) ax.set_title(\u0026#39;Cold Start vs Warm Start Comparison\u0026#39;) ax.set_xticks(x) ax.set_xticklabels(names) ax.legend() # æ·»åŠ æ•°å€¼æ ‡ç­¾ def autolabel(rects): for rect in rects: height = rect.get_height() ax.annotate(f\u0026#39;{int(height)}\u0026#39;, xy=(rect.get_x() + rect.get_width() / 2, height), xytext=(0, 3), textcoords=\u0026#34;offset points\u0026#34;, ha=\u0026#39;center\u0026#39;, va=\u0026#39;bottom\u0026#39;) autolabel(rects1) autolabel(rects2) fig.tight_layout() plt.show() # ä½¿ç”¨ç¤ºä¾‹ def cold_start_optimization_example(): \u0026#34;\u0026#34;\u0026#34;å†·å¯åŠ¨ä¼˜åŒ–ç¤ºä¾‹\u0026#34;\u0026#34;\u0026#34; optimizer = ColdStartOptimizer() # åˆ›å»ºä¸åŒé…ç½®çš„å‡½æ•° configs = [ FunctionConfig( name=\u0026#34;Python-Small\u0026#34;, runtime=RuntimeType.PYTHON, memory_size=128, code_size=1000, dependencies_count=5, has_vpc=False, has_layers=False, initialization_code=False ), FunctionConfig( name=\u0026#34;Python-Medium\u0026#34;, runtime=RuntimeType.PYTHON, memory_size=512, code_size=5000, dependencies_count=15, has_vpc=True, has_layers=True, initialization_code=True ), FunctionConfig( name=\u0026#34;Node-Small\u0026#34;, runtime=RuntimeType.NODE_JS, memory_size=128, code_size=1000, dependencies_count=10, has_vpc=False, has_layers=False, initialization_code=False ), FunctionConfig( name=\u0026#34;Java-Large\u0026#34;, runtime=RuntimeType.JAVA, memory_size=1024, code_size=10000, dependencies_count=20, has_vpc=True, has_layers=True, initialization_code=True ) ] # åˆ†æå¹¶ä¼˜åŒ–æ¯ä¸ªé…ç½® for config in configs: print(f\u0026#34;\\nåˆ†æå‡½æ•°: {config.name}\u0026#34;) optimizations = optimizer.optimize_function(config) print(f\u0026#34;æ€§èƒ½æŒ‡æ ‡:\u0026#34;) for metric, value in optimizations[\u0026#34;performance\u0026#34;].items(): print(f\u0026#34; {metric}: {value:.2f}\u0026#34;) print(\u0026#34;ä¼˜åŒ–å»ºè®®:\u0026#34;) for i, recommendation in enumerate(optimizations[\u0026#34;recommendations\u0026#34;], 1): print(f\u0026#34; {i}. {recommendation[\u0026#39;type\u0026#39;]}: {recommendation[\u0026#39;description\u0026#39;]}\u0026#34;) print(f\u0026#34; é¢„è®¡æ”¹è¿›: {recommendation[\u0026#39;estimated_improvement\u0026#39;]}\u0026#34;) # å¯è§†åŒ–æ¯”è¾ƒ # optimizer.visualize_cold_start_comparison(configs) # æ¨¡æ‹Ÿé¢„çƒ­ç­–ç•¥ invocation_pattern = [0, 0, 0, 1, 2, 5, 10, 15, 20, 15, 10, 5, 2, 1, 0, 0] # æ¯å°æ—¶è°ƒç”¨æ¨¡å¼ warming_strategies = [ {\u0026#34;name\u0026#34;: \u0026#34;No Warming\u0026#34;, \u0026#34;interval_minutes\u0026#34;: 10000}, # å®é™…ä¸Šæ²¡æœ‰é¢„çƒ­ {\u0026#34;name\u0026#34;: \u0026#34;Every 5 min\u0026#34;, \u0026#34;interval_minutes\u0026#34;: 5, \u0026#34;concurrency\u0026#34;: 1}, {\u0026#34;name\u0026#34;: \u0026#34;Every 10 min\u0026#34;, \u0026#34;interval_minutes\u0026#34;: 10, \u0026#34;concurrency\u0026#34;: 2}, {\u0026#34;name\u0026#34;: \u0026#34;Every 15 min\u0026#34;, \u0026#34;interval_minutes\u0026#34;: 15, \u0026#34;concurrency\u0026#34;: 3} ] warming_results = optimizer.simulate_warming_strategies( \u0026#34;example-function\u0026#34;, invocation_pattern, warming_strategies ) print(\u0026#34;\\né¢„çƒ­ç­–ç•¥æ¯”è¾ƒ:\u0026#34;) for strategy, result in warming_results.items(): print(f\u0026#34;\\n {strategy}:\u0026#34;) print(f\u0026#34; æ€»è°ƒç”¨: {result[\u0026#39;total_calls\u0026#39;]}\u0026#34;) print(f\u0026#34; å†·å¯åŠ¨: {result[\u0026#39;cold_starts\u0026#39;]} ({result[\u0026#39;cold_start_percentage\u0026#39;]:.2f}%)\u0026#34;) print(f\u0026#34; é¢„çƒ­è°ƒç”¨: {result[\u0026#39;warming_calls\u0026#39;]}\u0026#34;) print(f\u0026#34; æˆæœ¬å¼€é”€: {result[\u0026#39;cost_overhead_percentage\u0026#39;]:.2f}%\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: cold_start_optimization_example() æœ€ä½³å®è·µä¸å»ºè®® æ¶æ„è®¾è®¡åŸåˆ™ åœ¨è®¾è®¡Serverlessæ¶æ„æ—¶ï¼Œåº”éµå¾ªä»¥ä¸‹æ ¸å¿ƒåŸåˆ™ï¼š\n","content":"å¼•è¨€ Serverlessæ¶æ„ä½œä¸ºäº‘è®¡ç®—çš„é‡è¦å‘å±•æ–¹å‘ï¼Œé€šè¿‡æŠ½è±¡åŒ–æœåŠ¡å™¨ç®¡ç†ï¼Œè®©å¼€å‘è€…ä¸“æ³¨äºä¸šåŠ¡é€»è¾‘å®ç°ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨Serverlessæ¶æ„çš„æ ¸å¿ƒæ¦‚å¿µã€è®¾è®¡æ¨¡å¼ã€æœ€ä½³å®è·µå’Œå®æ–½ç­–ç•¥ï¼Œå¸®åŠ©è¯»è€…æ„å»ºé«˜æ•ˆã€å¯æ‰©å±•çš„æ— æœåŠ¡å™¨åº”ç”¨ã€‚\nç›®å½• Serverlessæ¶æ„æ¦‚è¿° æ ¸å¿ƒè®¾è®¡æ¨¡å¼ å‡½æ•°è®¾è®¡ä¸ä¼˜åŒ– äº‹ä»¶é©±åŠ¨æ¶æ„ çŠ¶æ€ç®¡ç†ç­–ç•¥ æ•°æ®å¤„ç†æ¨¡å¼ å®‰å…¨ä¸æƒé™ç®¡ç† ç›‘æ§ä¸è¿ç»´ æˆæœ¬ä¼˜åŒ– æœ€ä½³å®è·µä¸å»ºè®® Serverlessæ¶æ„æ¦‚è¿° æ¶æ„ç‰¹ç‚¹ graph TB subgraph \u0026amp;#34;ä¼ ç»Ÿæ¶æ„\u0026amp;#34; A[åº”ç”¨ä»£ç ] --\u0026amp;gt; B[åº”ç”¨æœåŠ¡å™¨] B --\u0026amp;gt; C[æ“ä½œç³»ç»Ÿ] C --\u0026amp;gt; D[ç‰©ç†/è™šæ‹ŸæœåŠ¡å™¨] end subgraph \u0026amp;#34;Serverlessæ¶æ„\u0026amp;#34; E[ä¸šåŠ¡é€»è¾‘] --\u0026amp;gt; F[å‡½æ•°è¿è¡Œæ—¶] F --\u0026amp;gt; G[äº‘å¹³å°ç®¡ç†] G --\u0026amp;gt; H[è‡ªåŠ¨æ‰©ç¼©å®¹] end subgraph \u0026amp;#34;Serverlessç”Ÿæ€\u0026amp;#34; I[å‡½æ•°è®¡ç®—\u0026amp;lt;br/\u0026amp;gt;Lambda/Functions] J[APIç½‘å…³\u0026amp;lt;br/\u0026amp;gt;API â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["Serverless","äº‘æ¶æ„","AWS Lambda","Azure Functions","å¾®æœåŠ¡","äº‹ä»¶é©±åŠ¨"],"categories":["äº‘æ¶æ„"],"author":"äº‘æ¶æ„ä¸“å®¶","readingTime":29,"wordCount":6052,"section":"posts","type":"posts","draft":false,"featured":false,"series":null},{"title":"å¤šäº‘ç¯å¢ƒä¸‹çš„æ¶æ„æ²»ç†ä¸æˆæœ¬ä¼˜åŒ–ï¼šæ„å»ºç»Ÿä¸€ã€é«˜æ•ˆçš„äº‘ç®¡ç†å¹³å°","url":"https://www.dishuihengxin.com/posts/multi-cloud-governance-cost-optimization/","summary":"å¤šäº‘ç¯å¢ƒä¸‹çš„æ¶æ„æ²»ç†ä¸æˆæœ¬ä¼˜åŒ–ï¼šæ„å»ºç»Ÿä¸€ã€é«˜æ•ˆçš„äº‘ç®¡ç†å¹³å° ç›®å½• å¤šäº‘æ¶æ„æ¦‚è¿° å¤šäº‘æ²»ç†æ¡†æ¶ ç»Ÿä¸€èº«ä»½ä¸è®¿é—®ç®¡ç† èµ„æºç®¡ç†ä¸ç¼–æ’ æˆæœ¬ç›‘æ§ä¸ä¼˜åŒ– åˆè§„æ€§ä¸é£é™©ç®¡ç† è¿ç»´è‡ªåŠ¨åŒ– æ€»ç»“ å¤šäº‘æ¶æ„æ¦‚è¿° å¤šäº‘ç¯å¢ƒå·²æˆä¸ºç°ä»£ä¼ä¸šITæ¶æ„çš„ä¸»æµé€‰æ‹©ï¼Œå®ƒæä¾›äº†æ›´å¥½çš„çµæ´»æ€§ã€å¯é æ€§å’Œæˆæœ¬æ•ˆç›Šã€‚ç„¶è€Œï¼Œå¤šäº‘ç¯å¢ƒä¹Ÿå¸¦æ¥äº†å¤æ‚çš„æ²»ç†å’Œç®¡ç†æŒ‘æˆ˜ã€‚\ngraph TB subgraph \u0026#34;å¤šäº‘æ¶æ„å…¨æ™¯å›¾\u0026#34; subgraph \u0026#34;æ²»ç†å±‚\u0026#34; A[ç»Ÿä¸€æ²»ç†å¹³å°] B[ç­–ç•¥ç®¡ç†] C[åˆè§„ç›‘æ§] end subgraph \u0026#34;ç®¡ç†å±‚\u0026#34; D[èº«ä»½ç®¡ç†] E[èµ„æºç¼–æ’] F[æˆæœ¬ç®¡ç†] G[ç›‘æ§å‘Šè­¦] end subgraph \u0026#34;äº‘æœåŠ¡æä¾›å•†\u0026#34; H[AWS] I[Azure] J[GCP] K[é˜¿é‡Œäº‘] end subgraph \u0026#34;åº”ç”¨å±‚\u0026#34; L[Webåº”ç”¨] M[æ•°æ®åº“] N[å­˜å‚¨æœåŠ¡] O[AI/MLæœåŠ¡] end end A --\u0026gt; D A --\u0026gt; E A --\u0026gt; F A --\u0026gt; G D --\u0026gt; H D --\u0026gt; I D --\u0026gt; J D --\u0026gt; K E --\u0026gt; H E --\u0026gt; I E --\u0026gt; J E --\u0026gt; K H --\u0026gt; L H --\u0026gt; M I --\u0026gt; N J --\u0026gt; O å¤šäº‘æ¶æ„åˆ†æå™¨ #!/usr/bin/env python3 \u0026#34;\u0026#34;\u0026#34; å¤šäº‘æ¶æ„åˆ†æå™¨ åˆ†æå’Œè¯„ä¼°å¤šäº‘ç¯å¢ƒçš„æ¶æ„ç‰¹å¾å’Œä¼˜åŒ–å»ºè®® \u0026#34;\u0026#34;\u0026#34; import json import boto3 import azure.identity import azure.mgmt.resource from google.cloud import resource_manager from typing import Dict, List, Any, Optional from dataclasses import dataclass, asdict from datetime import datetime, timedelta import logging # é…ç½®æ—¥å¿— logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) @dataclass class CloudProvider: \u0026#34;\u0026#34;\u0026#34;äº‘æœåŠ¡æä¾›å•†é…ç½®\u0026#34;\u0026#34;\u0026#34; name: str region: str credentials: Dict[str, str] services: List[str] cost_center: str compliance_requirements: List[str] @dataclass class ResourceInventory: \u0026#34;\u0026#34;\u0026#34;èµ„æºæ¸…å•\u0026#34;\u0026#34;\u0026#34; provider: str region: str resource_type: str resource_id: str name: str status: str cost_monthly: float tags: Dict[str, str] compliance_status: str class MultiCloudAnalyzer: \u0026#34;\u0026#34;\u0026#34;å¤šäº‘æ¶æ„åˆ†æå™¨\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.providers = {} self.resource_inventory = [] self.cost_data = {} self.compliance_rules = [] def add_cloud_provider(self, provider: CloudProvider): \u0026#34;\u0026#34;\u0026#34;æ·»åŠ äº‘æœåŠ¡æä¾›å•†\u0026#34;\u0026#34;\u0026#34; self.providers[provider.name] = provider logger.info(f\u0026#34;æ·»åŠ äº‘æœåŠ¡æä¾›å•†: {provider.name}\u0026#34;) def analyze_aws_resources(self, aws_config: Dict[str, str]) -\u0026gt; List[ResourceInventory]: \u0026#34;\u0026#34;\u0026#34;åˆ†æAWSèµ„æº\u0026#34;\u0026#34;\u0026#34; try: session = boto3.Session( aws_access_key_id=aws_config.get(\u0026#39;access_key\u0026#39;), aws_secret_access_key=aws_config.get(\u0026#39;secret_key\u0026#39;), region_name=aws_config.get(\u0026#39;region\u0026#39;, \u0026#39;us-east-1\u0026#39;) ) resources = [] # EC2å®ä¾‹ ec2 = session.client(\u0026#39;ec2\u0026#39;) instances = ec2.describe_instances() for reservation in instances[\u0026#39;Reservations\u0026#39;]: for instance in reservation[\u0026#39;Instances\u0026#39;]: tags = {tag[\u0026#39;Key\u0026#39;]: tag[\u0026#39;Value\u0026#39;] for tag in instance.get(\u0026#39;Tags\u0026#39;, [])} resources.append(ResourceInventory( provider=\u0026#34;AWS\u0026#34;, region=instance[\u0026#39;Placement\u0026#39;][\u0026#39;AvailabilityZone\u0026#39;][:-1], resource_type=\u0026#34;EC2\u0026#34;, resource_id=instance[\u0026#39;InstanceId\u0026#39;], name=tags.get(\u0026#39;Name\u0026#39;, instance[\u0026#39;InstanceId\u0026#39;]), status=instance[\u0026#39;State\u0026#39;][\u0026#39;Name\u0026#39;], cost_monthly=self._estimate_ec2_cost(instance[\u0026#39;InstanceType\u0026#39;]), tags=tags, compliance_status=self._check_compliance(tags) )) # RDSå®ä¾‹ rds = session.client(\u0026#39;rds\u0026#39;) db_instances = rds.describe_db_instances() for db in db_instances[\u0026#39;DBInstances\u0026#39;]: tags = {tag[\u0026#39;Key\u0026#39;]: tag[\u0026#39;Value\u0026#39;] for tag in db.get(\u0026#39;TagList\u0026#39;, [])} resources.append(ResourceInventory( provider=\u0026#34;AWS\u0026#34;, region=db[\u0026#39;AvailabilityZone\u0026#39;][:-1] if db.get(\u0026#39;AvailabilityZone\u0026#39;) else aws_config.get(\u0026#39;region\u0026#39;), resource_type=\u0026#34;RDS\u0026#34;, resource_id=db[\u0026#39;DBInstanceIdentifier\u0026#39;], name=db[\u0026#39;DBInstanceIdentifier\u0026#39;], status=db[\u0026#39;DBInstanceStatus\u0026#39;], cost_monthly=self._estimate_rds_cost(db[\u0026#39;DBInstanceClass\u0026#39;]), tags=tags, compliance_status=self._check_compliance(tags) )) # S3å­˜å‚¨æ¡¶ s3 = session.client(\u0026#39;s3\u0026#39;) buckets = s3.list_buckets() for bucket in buckets[\u0026#39;Buckets\u0026#39;]: try: tags_response = s3.get_bucket_tagging(Bucket=bucket[\u0026#39;Name\u0026#39;]) tags = {tag[\u0026#39;Key\u0026#39;]: tag[\u0026#39;Value\u0026#39;] for tag in tags_response.get(\u0026#39;TagSet\u0026#39;, [])} except: tags = {} resources.append(ResourceInventory( provider=\u0026#34;AWS\u0026#34;, region=aws_config.get(\u0026#39;region\u0026#39;), resource_type=\u0026#34;S3\u0026#34;, resource_id=bucket[\u0026#39;Name\u0026#39;], name=bucket[\u0026#39;Name\u0026#39;], status=\u0026#34;Active\u0026#34;, cost_monthly=self._estimate_s3_cost(bucket[\u0026#39;Name\u0026#39;], session), tags=tags, compliance_status=self._check_compliance(tags) )) return resources except Exception as e: logger.error(f\u0026#34;åˆ†æAWSèµ„æºå¤±è´¥: {e}\u0026#34;) return [] def analyze_azure_resources(self, azure_config: Dict[str, str]) -\u0026gt; List[ResourceInventory]: \u0026#34;\u0026#34;\u0026#34;åˆ†æAzureèµ„æº\u0026#34;\u0026#34;\u0026#34; try: credential = azure.identity.ClientSecretCredential( tenant_id=azure_config.get(\u0026#39;tenant_id\u0026#39;), client_id=azure_config.get(\u0026#39;client_id\u0026#39;), client_secret=azure_config.get(\u0026#39;client_secret\u0026#39;) ) resource_client = azure.mgmt.resource.ResourceManagementClient( credential, azure_config.get(\u0026#39;subscription_id\u0026#39;) ) resources = [] # è·å–æ‰€æœ‰èµ„æºç»„ resource_groups = resource_client.resource_groups.list() for rg in resource_groups: # è·å–èµ„æºç»„ä¸­çš„èµ„æº rg_resources = resource_client.resources.list_by_resource_group(rg.name) for resource in rg_resources: tags = resource.tags or {} resources.append(ResourceInventory( provider=\u0026#34;Azure\u0026#34;, region=resource.location, resource_type=resource.type.split(\u0026#39;/\u0026#39;)[-1], resource_id=resource.id, name=resource.name, status=\u0026#34;Active\u0026#34;, cost_monthly=self._estimate_azure_cost(resource.type), tags=tags, compliance_status=self._check_compliance(tags) )) return resources except Exception as e: logger.error(f\u0026#34;åˆ†æAzureèµ„æºå¤±è´¥: {e}\u0026#34;) return [] def analyze_gcp_resources(self, gcp_config: Dict[str, str]) -\u0026gt; List[ResourceInventory]: \u0026#34;\u0026#34;\u0026#34;åˆ†æGCPèµ„æº\u0026#34;\u0026#34;\u0026#34; try: # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦é…ç½®GCPè®¤è¯ resources = [] # æ¨¡æ‹ŸGCPèµ„æºåˆ†æ sample_resources = [ { \u0026#34;name\u0026#34;: \u0026#34;web-server-instance\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;compute.instances\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;us-central1-a\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;RUNNING\u0026#34;, \u0026#34;labels\u0026#34;: {\u0026#34;env\u0026#34;: \u0026#34;production\u0026#34;, \u0026#34;team\u0026#34;: \u0026#34;backend\u0026#34;} }, { \u0026#34;name\u0026#34;: \u0026#34;database-instance\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;sql.instances\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;us-central1\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;RUNNABLE\u0026#34;, \u0026#34;labels\u0026#34;: {\u0026#34;env\u0026#34;: \u0026#34;production\u0026#34;, \u0026#34;team\u0026#34;: \u0026#34;data\u0026#34;} } ] for resource in sample_resources: resources.append(ResourceInventory( provider=\u0026#34;GCP\u0026#34;, region=resource.get(\u0026#39;zone\u0026#39;, resource.get(\u0026#39;region\u0026#39;, \u0026#39;us-central1\u0026#39;)), resource_type=resource[\u0026#39;type\u0026#39;].split(\u0026#39;.\u0026#39;)[-1], resource_id=resource[\u0026#39;name\u0026#39;], name=resource[\u0026#39;name\u0026#39;], status=resource[\u0026#39;status\u0026#39;], cost_monthly=self._estimate_gcp_cost(resource[\u0026#39;type\u0026#39;]), tags=resource.get(\u0026#39;labels\u0026#39;, {}), compliance_status=self._check_compliance(resource.get(\u0026#39;labels\u0026#39;, {})) )) return resources except Exception as e: logger.error(f\u0026#34;åˆ†æGCPèµ„æºå¤±è´¥: {e}\u0026#34;) return [] def _estimate_ec2_cost(self, instance_type: str) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;ä¼°ç®—EC2æˆæœ¬\u0026#34;\u0026#34;\u0026#34; cost_map = { \u0026#39;t3.micro\u0026#39;: 8.5, \u0026#39;t3.small\u0026#39;: 17.0, \u0026#39;t3.medium\u0026#39;: 34.0, \u0026#39;t3.large\u0026#39;: 68.0, \u0026#39;m5.large\u0026#39;: 88.0, \u0026#39;m5.xlarge\u0026#39;: 176.0, \u0026#39;c5.large\u0026#39;: 78.0, \u0026#39;r5.large\u0026#39;: 115.0 } return cost_map.get(instance_type, 50.0) def _estimate_rds_cost(self, instance_class: str) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;ä¼°ç®—RDSæˆæœ¬\u0026#34;\u0026#34;\u0026#34; cost_map = { \u0026#39;db.t3.micro\u0026#39;: 15.0, \u0026#39;db.t3.small\u0026#39;: 30.0, \u0026#39;db.t3.medium\u0026#39;: 60.0, \u0026#39;db.m5.large\u0026#39;: 150.0, \u0026#39;db.r5.large\u0026#39;: 200.0 } return cost_map.get(instance_class, 75.0) def _estimate_s3_cost(self, bucket_name: str, session) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;ä¼°ç®—S3æˆæœ¬\u0026#34;\u0026#34;\u0026#34; try: cloudwatch = session.client(\u0026#39;cloudwatch\u0026#39;) # è·å–å­˜å‚¨ä½¿ç”¨é‡æŒ‡æ ‡ response = cloudwatch.get_metric_statistics( Namespace=\u0026#39;AWS/S3\u0026#39;, MetricName=\u0026#39;BucketSizeBytes\u0026#39;, Dimensions=[ {\u0026#39;Name\u0026#39;: \u0026#39;BucketName\u0026#39;, \u0026#39;Value\u0026#39;: bucket_name}, {\u0026#39;Name\u0026#39;: \u0026#39;StorageType\u0026#39;, \u0026#39;Value\u0026#39;: \u0026#39;StandardStorage\u0026#39;} ], StartTime=datetime.now() - timedelta(days=1), EndTime=datetime.now(), Period=86400, Statistics=[\u0026#39;Average\u0026#39;] ) if response[\u0026#39;Datapoints\u0026#39;]: size_bytes = response[\u0026#39;Datapoints\u0026#39;][0][\u0026#39;Average\u0026#39;] size_gb = size_bytes / (1024 ** 3) return size_gb * 0.023 # $0.023 per GB except: pass return 10.0 # é»˜è®¤ä¼°ç®— def _estimate_azure_cost(self, resource_type: str) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;ä¼°ç®—Azureæˆæœ¬\u0026#34;\u0026#34;\u0026#34; cost_map = { \u0026#39;virtualMachines\u0026#39;: 80.0, \u0026#39;sqlDatabases\u0026#39;: 120.0, \u0026#39;storageAccounts\u0026#39;: 15.0, \u0026#39;networkSecurityGroups\u0026#39;: 0.0, \u0026#39;publicIPAddresses\u0026#39;: 3.0 } return cost_map.get(resource_type.split(\u0026#39;/\u0026#39;)[-1], 25.0) def _estimate_gcp_cost(self, resource_type: str) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;ä¼°ç®—GCPæˆæœ¬\u0026#34;\u0026#34;\u0026#34; cost_map = { \u0026#39;instances\u0026#39;: 75.0, \u0026#39;sql.instances\u0026#39;: 110.0, \u0026#39;storage.buckets\u0026#39;: 12.0 } return cost_map.get(resource_type, 30.0) def _check_compliance(self, tags: Dict[str, str]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;æ£€æŸ¥åˆè§„æ€§\u0026#34;\u0026#34;\u0026#34; required_tags = [\u0026#39;Environment\u0026#39;, \u0026#39;Owner\u0026#39;, \u0026#39;Project\u0026#39;] missing_tags = [tag for tag in required_tags if tag not in tags] if not missing_tags: return \u0026#34;Compliant\u0026#34; elif len(missing_tags) \u0026lt;= 1: return \u0026#34;Warning\u0026#34; else: return \u0026#34;Non-Compliant\u0026#34; def generate_multi_cloud_report(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆå¤šäº‘åˆ†ææŠ¥å‘Š\u0026#34;\u0026#34;\u0026#34; try: # æŒ‰äº‘æœåŠ¡æä¾›å•†åˆ†ç»„ provider_summary = {} total_cost = 0 compliance_summary = {\u0026#34;Compliant\u0026#34;: 0, \u0026#34;Warning\u0026#34;: 0, \u0026#34;Non-Compliant\u0026#34;: 0} for resource in self.resource_inventory: provider = resource.provider if provider not in provider_summary: provider_summary[provider] = { \u0026#34;resource_count\u0026#34;: 0, \u0026#34;total_cost\u0026#34;: 0, \u0026#34;resource_types\u0026#34;: set(), \u0026#34;regions\u0026#34;: set() } provider_summary[provider][\u0026#34;resource_count\u0026#34;] += 1 provider_summary[provider][\u0026#34;total_cost\u0026#34;] += resource.cost_monthly provider_summary[provider][\u0026#34;resource_types\u0026#34;].add(resource.resource_type) provider_summary[provider][\u0026#34;regions\u0026#34;].add(resource.region) total_cost += resource.cost_monthly compliance_summary[resource.compliance_status] += 1 # è½¬æ¢setä¸ºlistä»¥ä¾¿JSONåºåˆ—åŒ– for provider in provider_summary: provider_summary[provider][\u0026#34;resource_types\u0026#34;] = list(provider_summary[provider][\u0026#34;resource_types\u0026#34;]) provider_summary[provider][\u0026#34;regions\u0026#34;] = list(provider_summary[provider][\u0026#34;regions\u0026#34;]) # æˆæœ¬åˆ†æ cost_analysis = { \u0026#34;total_monthly_cost\u0026#34;: total_cost, \u0026#34;cost_by_provider\u0026#34;: { provider: data[\u0026#34;total_cost\u0026#34;] for provider, data in provider_summary.items() }, \u0026#34;cost_by_resource_type\u0026#34;: {}, \u0026#34;cost_optimization_opportunities\u0026#34;: [] } # æŒ‰èµ„æºç±»å‹ç»Ÿè®¡æˆæœ¬ for resource in self.resource_inventory: resource_type = resource.resource_type if resource_type not in cost_analysis[\u0026#34;cost_by_resource_type\u0026#34;]: cost_analysis[\u0026#34;cost_by_resource_type\u0026#34;][resource_type] = 0 cost_analysis[\u0026#34;cost_by_resource_type\u0026#34;][resource_type] += resource.cost_monthly # è¯†åˆ«æˆæœ¬ä¼˜åŒ–æœºä¼š for resource in self.resource_inventory: if resource.status in [\u0026#39;stopped\u0026#39;, \u0026#39;deallocated\u0026#39;] and resource.cost_monthly \u0026gt; 0: cost_analysis[\u0026#34;cost_optimization_opportunities\u0026#34;].append({ \u0026#34;type\u0026#34;: \u0026#34;unused_resource\u0026#34;, \u0026#34;resource\u0026#34;: f\u0026#34;{resource.provider}:{resource.resource_id}\u0026#34;, \u0026#34;potential_savings\u0026#34;: resource.cost_monthly, \u0026#34;recommendation\u0026#34;: \u0026#34;Consider terminating unused resource\u0026#34; }) return { \u0026#34;timestamp\u0026#34;: datetime.now().isoformat(), \u0026#34;summary\u0026#34;: { \u0026#34;total_resources\u0026#34;: len(self.resource_inventory), \u0026#34;total_providers\u0026#34;: len(provider_summary), \u0026#34;total_monthly_cost\u0026#34;: total_cost, \u0026#34;compliance_score\u0026#34;: compliance_summary[\u0026#34;Compliant\u0026#34;] / len(self.resource_inventory) * 100 if self.resource_inventory else 0 }, \u0026#34;provider_summary\u0026#34;: provider_summary, \u0026#34;cost_analysis\u0026#34;: cost_analysis, \u0026#34;compliance_summary\u0026#34;: compliance_summary, \u0026#34;recommendations\u0026#34;: self._generate_recommendations() } except Exception as e: logger.error(f\u0026#34;ç”Ÿæˆå¤šäº‘åˆ†ææŠ¥å‘Šå¤±è´¥: {e}\u0026#34;) return {} def _generate_recommendations(self) -\u0026gt; List[Dict[str, str]]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆä¼˜åŒ–å»ºè®®\u0026#34;\u0026#34;\u0026#34; recommendations = [] # æˆæœ¬ä¼˜åŒ–å»ºè®® if len(self.providers) \u0026gt; 1: recommendations.append({ \u0026#34;category\u0026#34;: \u0026#34;Cost Optimization\u0026#34;, \u0026#34;priority\u0026#34;: \u0026#34;High\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;å®æ–½å¤šäº‘æˆæœ¬æ¯”è¾ƒ\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;å®šæœŸæ¯”è¾ƒä¸åŒäº‘æœåŠ¡æä¾›å•†çš„ä»·æ ¼ï¼Œé€‰æ‹©æœ€å…·æˆæœ¬æ•ˆç›Šçš„æœåŠ¡\u0026#34; }) # åˆè§„æ€§å»ºè®® non_compliant_count = sum(1 for r in self.resource_inventory if r.compliance_status == \u0026#34;Non-Compliant\u0026#34;) if non_compliant_count \u0026gt; 0: recommendations.append({ \u0026#34;category\u0026#34;: \u0026#34;Compliance\u0026#34;, \u0026#34;priority\u0026#34;: \u0026#34;Critical\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;ä¿®å¤åˆè§„æ€§é—®é¢˜\u0026#34;, \u0026#34;description\u0026#34;: f\u0026#34;æœ‰{non_compliant_count}ä¸ªèµ„æºä¸ç¬¦åˆæ ‡ç­¾è¦æ±‚ï¼Œéœ€è¦ç«‹å³ä¿®å¤\u0026#34; }) # æ¶æ„ä¼˜åŒ–å»ºè®® recommendations.append({ \u0026#34;category\u0026#34;: \u0026#34;Architecture\u0026#34;, \u0026#34;priority\u0026#34;: \u0026#34;Medium\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;å®æ–½ç»Ÿä¸€ç›‘æ§\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;å»ºç«‹è·¨äº‘çš„ç»Ÿä¸€ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ\u0026#34; }) recommendations.append({ \u0026#34;category\u0026#34;: \u0026#34;Security\u0026#34;, \u0026#34;priority\u0026#34;: \u0026#34;High\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;ç»Ÿä¸€èº«ä»½ç®¡ç†\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;å®æ–½è·¨äº‘çš„ç»Ÿä¸€èº«ä»½å’Œè®¿é—®ç®¡ç†(IAM)ç³»ç»Ÿ\u0026#34; }) return recommendations # ä½¿ç”¨ç¤ºä¾‹ def main(): \u0026#34;\u0026#34;\u0026#34;ä¸»å‡½æ•° - å¤šäº‘æ¶æ„åˆ†æç¤ºä¾‹\u0026#34;\u0026#34;\u0026#34; analyzer = MultiCloudAnalyzer() print(\u0026#34;=== å¤šäº‘æ¶æ„åˆ†æç¤ºä¾‹ ===\u0026#34;) # æ·»åŠ äº‘æœåŠ¡æä¾›å•† aws_provider = CloudProvider( name=\u0026#34;AWS\u0026#34;, region=\u0026#34;us-east-1\u0026#34;, credentials={\u0026#34;access_key\u0026#34;: \u0026#34;AKIA...\u0026#34;, \u0026#34;secret_key\u0026#34;: \u0026#34;xxx\u0026#34;}, services=[\u0026#34;EC2\u0026#34;, \u0026#34;RDS\u0026#34;, \u0026#34;S3\u0026#34;, \u0026#34;Lambda\u0026#34;], cost_center=\u0026#34;IT-Infrastructure\u0026#34;, compliance_requirements=[\u0026#34;SOC2\u0026#34;, \u0026#34;ISO27001\u0026#34;] ) azure_provider = CloudProvider( name=\u0026#34;Azure\u0026#34;, region=\u0026#34;East US\u0026#34;, credentials={\u0026#34;tenant_id\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;client_secret\u0026#34;: \u0026#34;xxx\u0026#34;}, services=[\u0026#34;VirtualMachines\u0026#34;, \u0026#34;SQLDatabase\u0026#34;, \u0026#34;Storage\u0026#34;], cost_center=\u0026#34;IT-Infrastructure\u0026#34;, compliance_requirements=[\u0026#34;SOC2\u0026#34;, \u0026#34;GDPR\u0026#34;] ) analyzer.add_cloud_provider(aws_provider) analyzer.add_cloud_provider(azure_provider) # æ¨¡æ‹Ÿèµ„æºæ¸…å• analyzer.resource_inventory = [ ResourceInventory( provider=\u0026#34;AWS\u0026#34;, region=\u0026#34;us-east-1\u0026#34;, resource_type=\u0026#34;EC2\u0026#34;, resource_id=\u0026#34;i-1234567890abcdef0\u0026#34;, name=\u0026#34;web-server-1\u0026#34;, status=\u0026#34;running\u0026#34;, cost_monthly=88.0, tags={\u0026#34;Environment\u0026#34;: \u0026#34;production\u0026#34;, \u0026#34;Owner\u0026#34;: \u0026#34;team-a\u0026#34;, \u0026#34;Project\u0026#34;: \u0026#34;web-app\u0026#34;}, compliance_status=\u0026#34;Compliant\u0026#34; ), ResourceInventory( provider=\u0026#34;Azure\u0026#34;, region=\u0026#34;East US\u0026#34;, resource_type=\u0026#34;VirtualMachine\u0026#34;, resource_id=\u0026#34;/subscriptions/.../vm-web-2\u0026#34;, name=\u0026#34;web-server-2\u0026#34;, status=\u0026#34;running\u0026#34;, cost_monthly=95.0, tags={\u0026#34;Environment\u0026#34;: \u0026#34;production\u0026#34;, \u0026#34;Project\u0026#34;: \u0026#34;web-app\u0026#34;}, compliance_status=\u0026#34;Warning\u0026#34; ), ResourceInventory( provider=\u0026#34;AWS\u0026#34;, region=\u0026#34;us-west-2\u0026#34;, resource_type=\u0026#34;RDS\u0026#34;, resource_id=\u0026#34;database-prod\u0026#34;, name=\u0026#34;main-database\u0026#34;, status=\u0026#34;available\u0026#34;, cost_monthly=150.0, tags={\u0026#34;Environment\u0026#34;: \u0026#34;production\u0026#34;, \u0026#34;Owner\u0026#34;: \u0026#34;team-b\u0026#34;}, compliance_status=\u0026#34;Warning\u0026#34; ) ] # ç”Ÿæˆåˆ†ææŠ¥å‘Š report = analyzer.generate_multi_cloud_report() print(\u0026#34;\\nå¤šäº‘æ¶æ„åˆ†ææŠ¥å‘Š:\u0026#34;) print(json.dumps(report, indent=2, ensure_ascii=False)) if __name__ == \u0026#34;__main__\u0026#34;: main() å¤šäº‘æ²»ç†æ¡†æ¶ å»ºç«‹ç»Ÿä¸€çš„å¤šäº‘æ²»ç†æ¡†æ¶æ˜¯ç®¡ç†å¤æ‚å¤šäº‘ç¯å¢ƒçš„å…³é”®ã€‚\n","content":"å¤šäº‘ç¯å¢ƒä¸‹çš„æ¶æ„æ²»ç†ä¸æˆæœ¬ä¼˜åŒ–ï¼šæ„å»ºç»Ÿä¸€ã€é«˜æ•ˆçš„äº‘ç®¡ç†å¹³å° ç›®å½• å¤šäº‘æ¶æ„æ¦‚è¿° å¤šäº‘æ²»ç†æ¡†æ¶ ç»Ÿä¸€èº«ä»½ä¸è®¿é—®ç®¡ç† èµ„æºç®¡ç†ä¸ç¼–æ’ æˆæœ¬ç›‘æ§ä¸ä¼˜åŒ– åˆè§„æ€§ä¸é£é™©ç®¡ç† è¿ç»´è‡ªåŠ¨åŒ– æ€»ç»“ å¤šäº‘æ¶æ„æ¦‚è¿° å¤šäº‘ç¯å¢ƒå·²æˆä¸ºç°ä»£ä¼ä¸šITæ¶æ„çš„ä¸»æµé€‰æ‹©ï¼Œå®ƒæä¾›äº†æ›´å¥½çš„çµæ´»æ€§ã€å¯é æ€§å’Œæˆæœ¬æ•ˆç›Šã€‚ç„¶è€Œï¼Œå¤šäº‘ç¯å¢ƒä¹Ÿå¸¦æ¥äº†å¤æ‚çš„æ²»ç†å’Œç®¡ç†æŒ‘æˆ˜ã€‚\ngraph TB subgraph \u0026amp;#34;å¤šäº‘æ¶æ„å…¨æ™¯å›¾\u0026amp;#34; subgraph \u0026amp;#34;æ²»ç†å±‚\u0026amp;#34; A[ç»Ÿä¸€æ²»ç†å¹³å°] B[ç­–ç•¥ç®¡ç†] C[åˆè§„ç›‘æ§] end subgraph \u0026amp;#34;ç®¡ç†å±‚\u0026amp;#34; D[èº«ä»½ç®¡ç†] E[èµ„æºç¼–æ’] F[æˆæœ¬ç®¡ç†] G[ç›‘æ§å‘Šè­¦] end subgraph \u0026amp;#34;äº‘æœåŠ¡æä¾›å•†\u0026amp;#34; H[AWS] I[Azure] J[GCP] K[é˜¿é‡Œäº‘] end subgraph \u0026amp;#34;åº”ç”¨å±‚\u0026amp;#34; L[Webåº”ç”¨] M[æ•°æ®åº“] N[å­˜å‚¨æœåŠ¡] O[AI/MLæœåŠ¡] end end A --\u0026amp;gt; D A --\u0026amp;gt; E A --\u0026amp;gt; F A --\u0026amp;gt; G D --\u0026amp;gt; H D --\u0026amp;gt; â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["å¤šäº‘æ¶æ„","äº‘æ²»ç†","æˆæœ¬ä¼˜åŒ–","äº‘ç®¡ç†","ä¼ä¸šæ¶æ„"],"categories":["äº‘æ¶æ„"],"author":"äº‘æ¶æ„ä¸“å®¶","readingTime":45,"wordCount":9488,"section":"posts","type":"posts","draft":false,"featured":false,"series":null},{"title":"å¤šäº‘æ¶æ„ç­–ç•¥ä¸æ··åˆäº‘ç®¡ç†ï¼šæ„å»ºçµæ´»å¯é çš„ä¼ä¸šäº‘åŸºç¡€è®¾æ–½","url":"https://www.dishuihengxin.com/posts/multi-cloud-hybrid-architecture/","summary":"å¤šäº‘æ¶æ„ç­–ç•¥ä¸æ··åˆäº‘ç®¡ç†ï¼šæ„å»ºçµæ´»å¯é çš„ä¼ä¸šäº‘åŸºç¡€è®¾æ–½ å¼•è¨€ éšç€äº‘è®¡ç®—æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å’Œä¼ä¸šæ•°å­—åŒ–è½¬å‹çš„æ·±å…¥æ¨è¿›ï¼Œå•ä¸€äº‘æœåŠ¡å•†å·²ç»æ— æ³•æ»¡è¶³ä¼ä¸šæ—¥ç›Šå¤æ‚çš„ä¸šåŠ¡éœ€æ±‚ã€‚å¤šäº‘æ¶æ„å’Œæ··åˆäº‘ç®¡ç†æˆä¸ºç°ä»£ä¼ä¸šITæˆ˜ç•¥çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä¸ºä¼ä¸šæä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€å¯é æ€§å’Œæˆæœ¬ä¼˜åŒ–ç©ºé—´ã€‚\næœ¬æ–‡å°†æ·±å…¥æ¢è®¨å¤šäº‘æ¶æ„ç­–ç•¥çš„è®¾è®¡åŸåˆ™ã€å®æ–½æ–¹æ³•å’Œç®¡ç†å®è·µï¼Œå¸®åŠ©ä¼ä¸šæ„å»ºé€‚åˆè‡ªèº«ä¸šåŠ¡ç‰¹ç‚¹çš„äº‘åŸºç¡€è®¾æ–½ã€‚\nç›®å½• å¤šäº‘æ¶æ„æ¦‚è¿° å¤šäº‘ç­–ç•¥è®¾è®¡ æ··åˆäº‘æ¶æ„æ¨¡å¼ äº‘èµ„æºç®¡ç†ä¸ç¼–æ’ æ•°æ®ç®¡ç†ä¸åŒæ­¥ ç½‘ç»œè¿æ¥ä¸å®‰å…¨ æˆæœ¬ä¼˜åŒ–ä¸æ²»ç† ç›‘æ§ä¸è¿ç»´ç®¡ç† æœ€ä½³å®è·µä¸æ¡ˆä¾‹ æ€»ç»“ 1. å¤šäº‘æ¶æ„æ¦‚è¿° 1.1 å¤šäº‘æ¶æ„çš„å®šä¹‰ä¸ä»·å€¼ å¤šäº‘æ¶æ„æ˜¯æŒ‡ä¼ä¸šåŒæ—¶ä½¿ç”¨å¤šä¸ªäº‘æœåŠ¡æä¾›å•†çš„æœåŠ¡æ¥æ„å»ºå…¶ITåŸºç¡€è®¾æ–½çš„ç­–ç•¥ã€‚è¿™ç§æ¶æ„æ¨¡å¼ä¸ºä¼ä¸šå¸¦æ¥äº†æ˜¾è‘—çš„ä»·å€¼ï¼š\ngraph TB A[å¤šäº‘æ¶æ„ä»·å€¼] --\u0026gt; B[é¿å…ä¾›åº”å•†é”å®š] A --\u0026gt; C[æé«˜å¯ç”¨æ€§å’Œå®¹ç¾èƒ½åŠ›] A --\u0026gt; D[ä¼˜åŒ–æˆæœ¬å’Œæ€§èƒ½] A --\u0026gt; E[æ»¡è¶³åˆè§„å’Œæ•°æ®ä¸»æƒè¦æ±‚] A --\u0026gt; F[åˆ©ç”¨æœ€ä½³æœåŠ¡ç»„åˆ] B --\u0026gt; B1[é™ä½ä¾èµ–é£é™©] B --\u0026gt; B2[å¢å¼ºè°ˆåˆ¤èƒ½åŠ›] C --\u0026gt; C1[è·¨åŒºåŸŸå†—ä½™] C --\u0026gt; C2[æ•…éšœéš”ç¦»] D --\u0026gt; D1[ä»·æ ¼ç«äº‰ä¼˜åŠ¿] D --\u0026gt; D2[æ€§èƒ½ä¼˜åŒ–é€‰æ‹©] E --\u0026gt; E1[æ•°æ®æœ¬åœ°åŒ–] E --\u0026gt; E2[ç›‘ç®¡åˆè§„] F --\u0026gt; F1[AI/MLæœåŠ¡] F --\u0026gt; F2[ä¸“ä¸šåŒ–æœåŠ¡] 1.2 å¤šäº‘æ¶æ„åˆ†æå™¨ è®©æˆ‘ä»¬å®ç°ä¸€ä¸ªå¤šäº‘æ¶æ„åˆ†æå™¨ï¼Œå¸®åŠ©ä¼ä¸šè¯„ä¼°å’Œè®¾è®¡å¤šäº‘ç­–ç•¥ï¼š\n","content":"å¤šäº‘æ¶æ„ç­–ç•¥ä¸æ··åˆäº‘ç®¡ç†ï¼šæ„å»ºçµæ´»å¯é çš„ä¼ä¸šäº‘åŸºç¡€è®¾æ–½ å¼•è¨€ éšç€äº‘è®¡ç®—æŠ€æœ¯çš„å¿«é€Ÿå‘å±•å’Œä¼ä¸šæ•°å­—åŒ–è½¬å‹çš„æ·±å…¥æ¨è¿›ï¼Œå•ä¸€äº‘æœåŠ¡å•†å·²ç»æ— æ³•æ»¡è¶³ä¼ä¸šæ—¥ç›Šå¤æ‚çš„ä¸šåŠ¡éœ€æ±‚ã€‚å¤šäº‘æ¶æ„å’Œæ··åˆäº‘ç®¡ç†æˆä¸ºç°ä»£ä¼ä¸šITæˆ˜ç•¥çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œä¸ºä¼ä¸šæä¾›äº†æ›´å¤§çš„çµæ´»æ€§ã€å¯é æ€§å’Œæˆæœ¬ä¼˜åŒ–ç©ºé—´ã€‚\næœ¬æ–‡å°†æ·±å…¥æ¢è®¨å¤šäº‘æ¶æ„ç­–ç•¥çš„è®¾è®¡åŸåˆ™ã€å®æ–½æ–¹æ³•å’Œç®¡ç†å®è·µï¼Œå¸®åŠ©ä¼ä¸šæ„å»ºé€‚åˆè‡ªèº«ä¸šåŠ¡ç‰¹ç‚¹çš„äº‘åŸºç¡€è®¾æ–½ã€‚\nç›®å½• å¤šäº‘æ¶æ„æ¦‚è¿° å¤šäº‘ç­–ç•¥è®¾è®¡ æ··åˆäº‘æ¶æ„æ¨¡å¼ äº‘èµ„æºç®¡ç†ä¸ç¼–æ’ æ•°æ®ç®¡ç†ä¸åŒæ­¥ ç½‘ç»œè¿æ¥ä¸å®‰å…¨ æˆæœ¬ä¼˜åŒ–ä¸æ²»ç† ç›‘æ§ä¸è¿ç»´ç®¡ç† æœ€ä½³å®è·µä¸æ¡ˆä¾‹ æ€»ç»“ 1. å¤šäº‘æ¶æ„æ¦‚è¿° 1.1 å¤šäº‘æ¶æ„çš„å®šä¹‰ä¸ä»·å€¼ å¤šäº‘æ¶æ„æ˜¯æŒ‡ä¼ä¸šåŒæ—¶ä½¿ç”¨å¤šä¸ªäº‘æœåŠ¡æä¾›å•†çš„æœåŠ¡æ¥æ„å»ºå…¶ITåŸºç¡€è®¾æ–½çš„ç­–ç•¥ã€‚è¿™ç§æ¶æ„æ¨¡å¼ä¸ºä¼ä¸šå¸¦æ¥äº†æ˜¾è‘—çš„ä»·å€¼ï¼š\ngraph TB A[å¤šäº‘æ¶æ„ä»·å€¼] --\u0026amp;gt; B[é¿å…ä¾›åº”å•†é”å®š] A --\u0026amp;gt; C[æé«˜å¯ç”¨æ€§å’Œå®¹ç¾èƒ½åŠ›] A --\u0026amp;gt; D[ä¼˜åŒ–æˆæœ¬å’Œæ€§èƒ½] A --\u0026amp;gt; E[æ»¡è¶³åˆè§„å’Œæ•°æ®ä¸»æƒè¦æ±‚] A --\u0026amp;gt; F[åˆ©ç”¨æœ€ä½³æœåŠ¡ç»„åˆ] B --\u0026amp;gt; B1[é™ä½ä¾èµ–é£é™©] B --\u0026amp;gt; B2[ â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["å¤šäº‘æ¶æ„","æ··åˆäº‘","äº‘ç®¡ç†","ä¼ä¸šæ¶æ„","äº‘ç­–ç•¥"],"categories":["äº‘æ¶æ„"],"author":"äº‘æ¶æ„å›¢é˜Ÿ","readingTime":30,"wordCount":6370,"section":"posts","type":"posts","draft":false,"featured":false,"series":null},{"title":"åˆ†å¸ƒå¼æ•°æ®åº“æ¶æ„è®¾è®¡ä¸å®è·µï¼šä»åˆ†ç‰‡ç­–ç•¥åˆ°ä¸€è‡´æ€§ä¿è¯çš„å®Œæ•´æ–¹æ¡ˆ","url":"https://www.dishuihengxin.com/posts/database-distributed-architecture/","summary":"åˆ†å¸ƒå¼æ•°æ®åº“æ¶æ„è®¾è®¡ä¸å®è·µï¼šä»åˆ†ç‰‡ç­–ç•¥åˆ°ä¸€è‡´æ€§ä¿è¯çš„å®Œæ•´æ–¹æ¡ˆ å¼•è¨€ éšç€æ•°æ®é‡çš„çˆ†ç‚¸å¼å¢é•¿å’Œä¸šåŠ¡å¤æ‚åº¦çš„ä¸æ–­æå‡ï¼Œä¼ ç»Ÿçš„å•æœºæ•°æ®åº“å·²ç»æ— æ³•æ»¡è¶³ç°ä»£åº”ç”¨çš„éœ€æ±‚ã€‚åˆ†å¸ƒå¼æ•°æ®åº“ä½œä¸ºè§£å†³å¤§è§„æ¨¡æ•°æ®å­˜å‚¨å’Œå¤„ç†çš„å…³é”®æŠ€æœ¯ï¼Œåœ¨ä¿è¯é«˜å¯ç”¨æ€§ã€å¯æ‰©å±•æ€§å’Œä¸€è‡´æ€§æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨åˆ†å¸ƒå¼æ•°æ®åº“çš„æ¶æ„è®¾è®¡åŸç†å’Œå®è·µæ–¹æ¡ˆã€‚\nåˆ†å¸ƒå¼æ•°æ®åº“æ¶æ„æ¦‚è¿° 1. æ ¸å¿ƒæ¶æ„ç»„ä»¶ graph TB subgraph \u0026#34;å®¢æˆ·ç«¯å±‚\u0026#34; A[åº”ç”¨ç¨‹åº] --\u0026gt; B[æ•°æ®åº“ä»£ç†] B --\u0026gt; C[è¿æ¥æ± ç®¡ç†å™¨] end subgraph \u0026#34;è·¯ç”±å±‚\u0026#34; C --\u0026gt; D[åˆ†ç‰‡è·¯ç”±å™¨] D --\u0026gt; E[è´Ÿè½½å‡è¡¡å™¨] E --\u0026gt; F[æŸ¥è¯¢ä¼˜åŒ–å™¨] end subgraph \u0026#34;æ•°æ®å±‚\u0026#34; F --\u0026gt; G[åˆ†ç‰‡1] F --\u0026gt; H[åˆ†ç‰‡2] F --\u0026gt; I[åˆ†ç‰‡3] F --\u0026gt; J[åˆ†ç‰‡N] end subgraph \u0026#34;å…ƒæ•°æ®å±‚\u0026#34; K[é…ç½®ä¸­å¿ƒ] --\u0026gt; D L[åˆ†ç‰‡æ˜ å°„è¡¨] --\u0026gt; D M[èŠ‚ç‚¹çŠ¶æ€ç›‘æ§] --\u0026gt; E end subgraph \u0026#34;ä¸€è‡´æ€§å±‚\u0026#34; N[åˆ†å¸ƒå¼é”] --\u0026gt; O[äº‹åŠ¡åè°ƒå™¨] O --\u0026gt; P[ä¸¤é˜¶æ®µæäº¤] P --\u0026gt; Q[Raftå…±è¯†ç®—æ³•] end 2. åˆ†å¸ƒå¼æ•°æ®åº“é…ç½® # config/distributed_database.yaml distributed_database: cluster_name: \u0026#34;production_cluster\u0026#34; # èŠ‚ç‚¹é…ç½® nodes: - id: \u0026#34;node1\u0026#34; host: \u0026#34;192.168.1.10\u0026#34; port: 3306 role: \u0026#34;primary\u0026#34; datacenter: \u0026#34;dc1\u0026#34; rack: \u0026#34;rack1\u0026#34; weight: 100 - id: \u0026#34;node2\u0026#34; host: \u0026#34;192.168.1.11\u0026#34; port: 3306 role: \u0026#34;secondary\u0026#34; datacenter: \u0026#34;dc1\u0026#34; rack: \u0026#34;rack2\u0026#34; weight: 100 - id: \u0026#34;node3\u0026#34; host: \u0026#34;192.168.1.12\u0026#34; port: 3306 role: \u0026#34;secondary\u0026#34; datacenter: \u0026#34;dc2\u0026#34; rack: \u0026#34;rack1\u0026#34; weight: 100 # åˆ†ç‰‡é…ç½® sharding: strategy: \u0026#34;hash\u0026#34; # hash, range, directory shard_count: 16 replication_factor: 3 # åˆ†ç‰‡é”®é…ç½® shard_keys: users: [\u0026#34;user_id\u0026#34;] orders: [\u0026#34;user_id\u0026#34;, \u0026#34;order_date\u0026#34;] products: [\u0026#34;category_id\u0026#34;] # åˆ†ç‰‡æ˜ å°„ shard_mapping: shard_0: [\u0026#34;node1\u0026#34;, \u0026#34;node2\u0026#34;] shard_1: [\u0026#34;node2\u0026#34;, \u0026#34;node3\u0026#34;] shard_2: [\u0026#34;node3\u0026#34;, \u0026#34;node1\u0026#34;] # ... å…¶ä»–åˆ†ç‰‡æ˜ å°„ # ä¸€è‡´æ€§é…ç½® consistency: level: \u0026#34;eventual\u0026#34; # strong, eventual, weak read_preference: \u0026#34;primary_preferred\u0026#34; write_concern: \u0026#34;majority\u0026#34; # äº‹åŠ¡é…ç½® transaction: isolation_level: \u0026#34;read_committed\u0026#34; timeout: 30000 # æ¯«ç§’ retry_count: 3 # æ•…éšœæ¢å¤é…ç½® failover: detection_timeout: 5000 # æ¯«ç§’ election_timeout: 10000 # æ¯«ç§’ heartbeat_interval: 1000 # æ¯«ç§’ # æ€§èƒ½é…ç½® performance: connection_pool_size: 100 query_timeout: 30000 batch_size: 1000 cache_size: \u0026#34;1GB\u0026#34; æ•°æ®åˆ†ç‰‡ç­–ç•¥å®ç° 1. åˆ†ç‰‡è·¯ç”±å™¨ #!/usr/bin/env python3 # src/sharding/shard_router.py import hashlib import bisect import logging from typing import Dict, List, Any, Optional, Tuple from enum import Enum from dataclasses import dataclass import yaml import threading import time class ShardingStrategy(Enum): HASH = \u0026#34;hash\u0026#34; RANGE = \u0026#34;range\u0026#34; DIRECTORY = \u0026#34;directory\u0026#34; CONSISTENT_HASH = \u0026#34;consistent_hash\u0026#34; @dataclass class ShardInfo: shard_id: str nodes: List[str] range_start: Optional[Any] = None range_end: Optional[Any] = None weight: int = 100 status: str = \u0026#34;active\u0026#34; @dataclass class NodeInfo: node_id: str host: str port: int role: str datacenter: str rack: str weight: int status: str = \u0026#34;active\u0026#34; class ShardRouter: def __init__(self, config_file: str): self.config = self._load_config(config_file) self.logger = self._setup_logging() # åˆå§‹åŒ–åˆ†ç‰‡ä¿¡æ¯ self.shards: Dict[str, ShardInfo] = {} self.nodes: Dict[str, NodeInfo] = {} self.shard_keys: Dict[str, List[str]] = {} # ä¸€è‡´æ€§å“ˆå¸Œç¯ï¼ˆç”¨äºconsistent_hashç­–ç•¥ï¼‰ self.hash_ring: List[Tuple[int, str]] = [] self.virtual_nodes = 150 # æ¯ä¸ªç‰©ç†èŠ‚ç‚¹çš„è™šæ‹ŸèŠ‚ç‚¹æ•° # çº¿ç¨‹é” self.lock = threading.RLock() # åˆå§‹åŒ–è·¯ç”±å™¨ self._initialize_router() def _load_config(self, config_file: str) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;åŠ è½½é…ç½®æ–‡ä»¶\u0026#34;\u0026#34;\u0026#34; with open(config_file, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: return yaml.safe_load(f) def _setup_logging(self) -\u0026gt; logging.Logger: \u0026#34;\u0026#34;\u0026#34;è®¾ç½®æ—¥å¿—è®°å½•\u0026#34;\u0026#34;\u0026#34; logger = logging.getLogger(\u0026#39;ShardRouter\u0026#39;) logger.setLevel(logging.INFO) handler = logging.StreamHandler() formatter = logging.Formatter( \u0026#39;%(asctime)s - %(name)s - %(levelname)s - %(message)s\u0026#39; ) handler.setFormatter(formatter) logger.addHandler(handler) return logger def _initialize_router(self): \u0026#34;\u0026#34;\u0026#34;åˆå§‹åŒ–è·¯ç”±å™¨\u0026#34;\u0026#34;\u0026#34; # åŠ è½½èŠ‚ç‚¹ä¿¡æ¯ for node_config in self.config[\u0026#39;distributed_database\u0026#39;][\u0026#39;nodes\u0026#39;]: node = NodeInfo( node_id=node_config[\u0026#39;id\u0026#39;], host=node_config[\u0026#39;host\u0026#39;], port=node_config[\u0026#39;port\u0026#39;], role=node_config[\u0026#39;role\u0026#39;], datacenter=node_config[\u0026#39;datacenter\u0026#39;], rack=node_config[\u0026#39;rack\u0026#39;], weight=node_config[\u0026#39;weight\u0026#39;] ) self.nodes[node.node_id] = node # åŠ è½½åˆ†ç‰‡é”®é…ç½® self.shard_keys = self.config[\u0026#39;distributed_database\u0026#39;][\u0026#39;sharding\u0026#39;][\u0026#39;shard_keys\u0026#39;] # æ ¹æ®åˆ†ç‰‡ç­–ç•¥åˆå§‹åŒ–åˆ†ç‰‡ strategy = ShardingStrategy(self.config[\u0026#39;distributed_database\u0026#39;][\u0026#39;sharding\u0026#39;][\u0026#39;strategy\u0026#39;]) if strategy == ShardingStrategy.HASH: self._initialize_hash_sharding() elif strategy == ShardingStrategy.RANGE: self._initialize_range_sharding() elif strategy == ShardingStrategy.CONSISTENT_HASH: self._initialize_consistent_hash_sharding() elif strategy == ShardingStrategy.DIRECTORY: self._initialize_directory_sharding() self.logger.info(f\u0026#34;åˆ†ç‰‡è·¯ç”±å™¨åˆå§‹åŒ–å®Œæˆï¼Œç­–ç•¥: {strategy.value}\u0026#34;) def _initialize_hash_sharding(self): \u0026#34;\u0026#34;\u0026#34;åˆå§‹åŒ–å“ˆå¸Œåˆ†ç‰‡\u0026#34;\u0026#34;\u0026#34; shard_count = self.config[\u0026#39;distributed_database\u0026#39;][\u0026#39;sharding\u0026#39;][\u0026#39;shard_count\u0026#39;] shard_mapping = self.config[\u0026#39;distributed_database\u0026#39;][\u0026#39;sharding\u0026#39;][\u0026#39;shard_mapping\u0026#39;] for i in range(shard_count): shard_id = f\u0026#34;shard_{i}\u0026#34; nodes = shard_mapping.get(shard_id, []) self.shards[shard_id] = ShardInfo( shard_id=shard_id, nodes=nodes ) def _initialize_range_sharding(self): \u0026#34;\u0026#34;\u0026#34;åˆå§‹åŒ–èŒƒå›´åˆ†ç‰‡\u0026#34;\u0026#34;\u0026#34; # è¿™é‡Œå¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚é…ç½®èŒƒå›´åˆ†ç‰‡ # ç¤ºä¾‹ï¼šæŒ‰ç”¨æˆ·IDèŒƒå›´åˆ†ç‰‡ ranges = [ (0, 10000), (10000, 20000), (20000, 30000), (30000, float(\u0026#39;inf\u0026#39;)) ] for i, (start, end) in enumerate(ranges): shard_id = f\u0026#34;shard_{i}\u0026#34; self.shards[shard_id] = ShardInfo( shard_id=shard_id, nodes=[f\u0026#34;node{(i % len(self.nodes)) + 1}\u0026#34;], range_start=start, range_end=end ) def _initialize_consistent_hash_sharding(self): \u0026#34;\u0026#34;\u0026#34;åˆå§‹åŒ–ä¸€è‡´æ€§å“ˆå¸Œåˆ†ç‰‡\u0026#34;\u0026#34;\u0026#34; self.hash_ring = [] for node_id, node in self.nodes.items(): # ä¸ºæ¯ä¸ªç‰©ç†èŠ‚ç‚¹åˆ›å»ºå¤šä¸ªè™šæ‹ŸèŠ‚ç‚¹ for i in range(self.virtual_nodes): virtual_node_key = f\u0026#34;{node_id}:{i}\u0026#34; hash_value = self._hash_function(virtual_node_key) self.hash_ring.append((hash_value, node_id)) # æŒ‰å“ˆå¸Œå€¼æ’åº self.hash_ring.sort() self.logger.info(f\u0026#34;ä¸€è‡´æ€§å“ˆå¸Œç¯åˆå§‹åŒ–å®Œæˆï¼Œè™šæ‹ŸèŠ‚ç‚¹æ•°: {len(self.hash_ring)}\u0026#34;) def _initialize_directory_sharding(self): \u0026#34;\u0026#34;\u0026#34;åˆå§‹åŒ–ç›®å½•åˆ†ç‰‡\u0026#34;\u0026#34;\u0026#34; # ç›®å½•åˆ†ç‰‡éœ€è¦ç»´æŠ¤ä¸€ä¸ªåˆ†ç‰‡ç›®å½•è¡¨ # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…åº”è¯¥ä»é…ç½®æˆ–æ•°æ®åº“åŠ è½½ directory_mapping = { \u0026#39;users_1\u0026#39;: \u0026#39;shard_0\u0026#39;, \u0026#39;users_2\u0026#39;: \u0026#39;shard_1\u0026#39;, \u0026#39;orders_2023\u0026#39;: \u0026#39;shard_2\u0026#39;, \u0026#39;orders_2024\u0026#39;: \u0026#39;shard_3\u0026#39; } for table_partition, shard_id in directory_mapping.items(): if shard_id not in self.shards: self.shards[shard_id] = ShardInfo( shard_id=shard_id, nodes=[f\u0026#34;node{(len(self.shards) % len(self.nodes)) + 1}\u0026#34;] ) def _hash_function(self, key: str) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;å“ˆå¸Œå‡½æ•°\u0026#34;\u0026#34;\u0026#34; return int(hashlib.md5(key.encode()).hexdigest(), 16) def route_query(self, table: str, query_params: Dict[str, Any]) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;è·¯ç”±æŸ¥è¯¢åˆ°ç›¸åº”çš„åˆ†ç‰‡\u0026#34;\u0026#34;\u0026#34; with self.lock: strategy = ShardingStrategy(self.config[\u0026#39;distributed_database\u0026#39;][\u0026#39;sharding\u0026#39;][\u0026#39;strategy\u0026#39;]) if strategy == ShardingStrategy.HASH: return self._route_hash_query(table, query_params) elif strategy == ShardingStrategy.RANGE: return self._route_range_query(table, query_params) elif strategy == ShardingStrategy.CONSISTENT_HASH: return self._route_consistent_hash_query(table, query_params) elif strategy == ShardingStrategy.DIRECTORY: return self._route_directory_query(table, query_params) return [] def _route_hash_query(self, table: str, query_params: Dict[str, Any]) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;å“ˆå¸Œåˆ†ç‰‡è·¯ç”±\u0026#34;\u0026#34;\u0026#34; if table not in self.shard_keys: # å¦‚æœæ²¡æœ‰é…ç½®åˆ†ç‰‡é”®ï¼Œå¹¿æ’­åˆ°æ‰€æœ‰åˆ†ç‰‡ return list(self.shards.keys()) shard_key_columns = self.shard_keys[table] # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰åˆ†ç‰‡é”® if not all(col in query_params for col in shard_key_columns): # å¦‚æœç¼ºå°‘åˆ†ç‰‡é”®ï¼Œå¹¿æ’­åˆ°æ‰€æœ‰åˆ†ç‰‡ return list(self.shards.keys()) # æ„å»ºåˆ†ç‰‡é”®å€¼ shard_key_value = \u0026#34;|\u0026#34;.join(str(query_params[col]) for col in shard_key_columns) # è®¡ç®—å“ˆå¸Œå€¼ hash_value = self._hash_function(shard_key_value) shard_count = self.config[\u0026#39;distributed_database\u0026#39;][\u0026#39;sharding\u0026#39;][\u0026#39;shard_count\u0026#39;] shard_index = hash_value % shard_count shard_id = f\u0026#34;shard_{shard_index}\u0026#34; return [shard_id] if shard_id in self.shards else [] def _route_range_query(self, table: str, query_params: Dict[str, Any]) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;èŒƒå›´åˆ†ç‰‡è·¯ç”±\u0026#34;\u0026#34;\u0026#34; if table not in self.shard_keys: return list(self.shards.keys()) shard_key_columns = self.shard_keys[table] primary_key = shard_key_columns[0] # ä½¿ç”¨ç¬¬ä¸€ä¸ªåˆ†ç‰‡é”®ä½œä¸ºèŒƒå›´é”® if primary_key not in query_params: return list(self.shards.keys()) key_value = query_params[primary_key] target_shards = [] for shard_id, shard in self.shards.items(): if (shard.range_start is None or key_value \u0026gt;= shard.range_start) and \\ (shard.range_end is None or key_value \u0026lt; shard.range_end): target_shards.append(shard_id) return target_shards def _route_consistent_hash_query(self, table: str, query_params: Dict[str, Any]) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;ä¸€è‡´æ€§å“ˆå¸Œåˆ†ç‰‡è·¯ç”±\u0026#34;\u0026#34;\u0026#34; if table not in self.shard_keys: return list(set(node_id for _, node_id in self.hash_ring)) shard_key_columns = self.shard_keys[table] if not all(col in query_params for col in shard_key_columns): return list(set(node_id for _, node_id in self.hash_ring)) # æ„å»ºåˆ†ç‰‡é”®å€¼ shard_key_value = \u0026#34;|\u0026#34;.join(str(query_params[col]) for col in shard_key_columns) hash_value = self._hash_function(shard_key_value) # åœ¨å“ˆå¸Œç¯ä¸­æŸ¥æ‰¾ç›®æ ‡èŠ‚ç‚¹ index = bisect.bisect_right([h for h, _ in self.hash_ring], hash_value) if index == len(self.hash_ring): index = 0 target_node = self.hash_ring[index][1] return [target_node] def _route_directory_query(self, table: str, query_params: Dict[str, Any]) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;ç›®å½•åˆ†ç‰‡è·¯ç”±\u0026#34;\u0026#34;\u0026#34; # ç®€åŒ–å®ç°ï¼šæ ¹æ®è¡¨åå’Œåˆ†åŒºä¿¡æ¯æŸ¥æ‰¾åˆ†ç‰‡ # å®é™…å®ç°éœ€è¦ç»´æŠ¤å®Œæ•´çš„ç›®å½•æ˜ å°„è¡¨ # ç¤ºä¾‹ï¼šæ ¹æ®æ—¶é—´åˆ†åŒº if \u0026#39;partition_key\u0026#39; in query_params: partition_key = query_params[\u0026#39;partition_key\u0026#39;] # æŸ¥æ‰¾å¯¹åº”çš„åˆ†ç‰‡ for shard_id in self.shards: if partition_key in shard_id: return [shard_id] return list(self.shards.keys()) def get_shard_nodes(self, shard_id: str) -\u0026gt; List[NodeInfo]: \u0026#34;\u0026#34;\u0026#34;è·å–åˆ†ç‰‡å¯¹åº”çš„èŠ‚ç‚¹ä¿¡æ¯\u0026#34;\u0026#34;\u0026#34; if shard_id not in self.shards: return [] shard = self.shards[shard_id] return [self.nodes[node_id] for node_id in shard.nodes if node_id in self.nodes] def add_shard(self, shard_id: str, nodes: List[str]): \u0026#34;\u0026#34;\u0026#34;æ·»åŠ æ–°åˆ†ç‰‡\u0026#34;\u0026#34;\u0026#34; with self.lock: self.shards[shard_id] = ShardInfo( shard_id=shard_id, nodes=nodes ) self.logger.info(f\u0026#34;æ·»åŠ æ–°åˆ†ç‰‡: {shard_id}\u0026#34;) def remove_shard(self, shard_id: str): \u0026#34;\u0026#34;\u0026#34;ç§»é™¤åˆ†ç‰‡\u0026#34;\u0026#34;\u0026#34; with self.lock: if shard_id in self.shards: del self.shards[shard_id] self.logger.info(f\u0026#34;ç§»é™¤åˆ†ç‰‡: {shard_id}\u0026#34;) def update_node_status(self, node_id: str, status: str): \u0026#34;\u0026#34;\u0026#34;æ›´æ–°èŠ‚ç‚¹çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; with self.lock: if node_id in self.nodes: self.nodes[node_id].status = status self.logger.info(f\u0026#34;æ›´æ–°èŠ‚ç‚¹çŠ¶æ€: {node_id} -\u0026gt; {status}\u0026#34;) def get_cluster_topology(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è·å–é›†ç¾¤æ‹“æ‰‘ä¿¡æ¯\u0026#34;\u0026#34;\u0026#34; return { \u0026#39;nodes\u0026#39;: {node_id: { \u0026#39;host\u0026#39;: node.host, \u0026#39;port\u0026#39;: node.port, \u0026#39;role\u0026#39;: node.role, \u0026#39;datacenter\u0026#39;: node.datacenter, \u0026#39;rack\u0026#39;: node.rack, \u0026#39;status\u0026#39;: node.status } for node_id, node in self.nodes.items()}, \u0026#39;shards\u0026#39;: {shard_id: { \u0026#39;nodes\u0026#39;: shard.nodes, \u0026#39;status\u0026#39;: shard.status, \u0026#39;range_start\u0026#39;: shard.range_start, \u0026#39;range_end\u0026#39;: shard.range_end } for shard_id, shard in self.shards.items()} } def main(): # ç¤ºä¾‹ç”¨æ³• router = ShardRouter(\u0026#39;config/distributed_database.yaml\u0026#39;) # æµ‹è¯•æŸ¥è¯¢è·¯ç”± test_queries = [ { \u0026#39;table\u0026#39;: \u0026#39;users\u0026#39;, \u0026#39;params\u0026#39;: {\u0026#39;user_id\u0026#39;: 12345} }, { \u0026#39;table\u0026#39;: \u0026#39;orders\u0026#39;, \u0026#39;params\u0026#39;: {\u0026#39;user_id\u0026#39;: 12345, \u0026#39;order_date\u0026#39;: \u0026#39;2024-01-15\u0026#39;} }, { \u0026#39;table\u0026#39;: \u0026#39;products\u0026#39;, \u0026#39;params\u0026#39;: {\u0026#39;category_id\u0026#39;: \u0026#39;electronics\u0026#39;} } ] for query in test_queries: shards = router.route_query(query[\u0026#39;table\u0026#39;], query[\u0026#39;params\u0026#39;]) print(f\u0026#34;æŸ¥è¯¢ {query[\u0026#39;table\u0026#39;]} è·¯ç”±åˆ°åˆ†ç‰‡: {shards}\u0026#34;) for shard_id in shards: nodes = router.get_shard_nodes(shard_id) print(f\u0026#34; åˆ†ç‰‡ {shard_id} èŠ‚ç‚¹: {[node.node_id for node in nodes]}\u0026#34;) # æ‰“å°é›†ç¾¤æ‹“æ‰‘ topology = router.get_cluster_topology() print(\u0026#34;\\né›†ç¾¤æ‹“æ‰‘:\u0026#34;) print(yaml.dump(topology, default_flow_style=False, allow_unicode=True)) if __name__ == \u0026#34;__main__\u0026#34;: main() 2. åˆ†å¸ƒå¼äº‹åŠ¡åè°ƒå™¨ #!/usr/bin/env python3 # src/transaction/distributed_transaction.py import uuid import time import logging import threading from typing import Dict, List, Any, Optional, Callable from enum import Enum from dataclasses import dataclass, field import json from concurrent.futures import ThreadPoolExecutor, Future class TransactionState(Enum): ACTIVE = \u0026#34;active\u0026#34; PREPARING = \u0026#34;preparing\u0026#34; PREPARED = \u0026#34;prepared\u0026#34; COMMITTING = \u0026#34;committing\u0026#34; COMMITTED = \u0026#34;committed\u0026#34; ABORTING = \u0026#34;aborting\u0026#34; ABORTED = \u0026#34;aborted\u0026#34; class ParticipantState(Enum): ACTIVE = \u0026#34;active\u0026#34; PREPARED = \u0026#34;prepared\u0026#34; COMMITTED = \u0026#34;committed\u0026#34; ABORTED = \u0026#34;aborted\u0026#34; UNKNOWN = \u0026#34;unknown\u0026#34; @dataclass class TransactionParticipant: participant_id: str node_id: str shard_id: str operations: List[Dict[str, Any]] = field(default_factory=list) state: ParticipantState = ParticipantState.ACTIVE prepare_timestamp: Optional[float] = None commit_timestamp: Optional[float] = None @dataclass class DistributedTransaction: transaction_id: str coordinator_id: str participants: Dict[str, TransactionParticipant] = field(default_factory=dict) state: TransactionState = TransactionState.ACTIVE start_timestamp: float = field(default_factory=time.time) timeout: float = 30.0 # 30ç§’è¶…æ—¶ isolation_level: str = \u0026#34;read_committed\u0026#34; def is_expired(self) -\u0026gt; bool: return time.time() - self.start_timestamp \u0026gt; self.timeout class DistributedTransactionCoordinator: def __init__(self, coordinator_id: str, shard_router): self.coordinator_id = coordinator_id self.shard_router = shard_router self.logger = self._setup_logging() # æ´»è·ƒäº‹åŠ¡ self.active_transactions: Dict[str, DistributedTransaction] = {} # çº¿ç¨‹æ±  self.executor = ThreadPoolExecutor(max_workers=50) # é” self.lock = threading.RLock() # å¯åŠ¨æ¸…ç†çº¿ç¨‹ self.cleanup_thread = threading.Thread(target=self._cleanup_expired_transactions) self.cleanup_thread.daemon = True self.cleanup_thread.start() def _setup_logging(self) -\u0026gt; logging.Logger: \u0026#34;\u0026#34;\u0026#34;è®¾ç½®æ—¥å¿—è®°å½•\u0026#34;\u0026#34;\u0026#34; logger = logging.getLogger(f\u0026#39;TransactionCoordinator-{self.coordinator_id}\u0026#39;) logger.setLevel(logging.INFO) handler = logging.StreamHandler() formatter = logging.Formatter( \u0026#39;%(asctime)s - %(name)s - %(levelname)s - %(message)s\u0026#39; ) handler.setFormatter(formatter) logger.addHandler(handler) return logger def begin_transaction(self, isolation_level: str = \u0026#34;read_committed\u0026#34;, timeout: float = 30.0) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;å¼€å§‹åˆ†å¸ƒå¼äº‹åŠ¡\u0026#34;\u0026#34;\u0026#34; transaction_id = str(uuid.uuid4()) with self.lock: transaction = DistributedTransaction( transaction_id=transaction_id, coordinator_id=self.coordinator_id, isolation_level=isolation_level, timeout=timeout ) self.active_transactions[transaction_id] = transaction self.logger.info(f\u0026#34;å¼€å§‹åˆ†å¸ƒå¼äº‹åŠ¡: {transaction_id}\u0026#34;) return transaction_id def add_operation(self, transaction_id: str, table: str, operation: str, data: Dict[str, Any]) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ·»åŠ äº‹åŠ¡æ“ä½œ\u0026#34;\u0026#34;\u0026#34; with self.lock: if transaction_id not in self.active_transactions: self.logger.error(f\u0026#34;äº‹åŠ¡ä¸å­˜åœ¨: {transaction_id}\u0026#34;) return False transaction = self.active_transactions[transaction_id] if transaction.state != TransactionState.ACTIVE: self.logger.error(f\u0026#34;äº‹åŠ¡çŠ¶æ€æ— æ•ˆ: {transaction_id}, çŠ¶æ€: {transaction.state}\u0026#34;) return False if transaction.is_expired(): self.logger.error(f\u0026#34;äº‹åŠ¡å·²è¶…æ—¶: {transaction_id}\u0026#34;) self._abort_transaction(transaction_id) return False # è·¯ç”±æ“ä½œåˆ°ç›¸åº”çš„åˆ†ç‰‡ shards = self.shard_router.route_query(table, data) for shard_id in shards: nodes = self.shard_router.get_shard_nodes(shard_id) if not nodes: continue # é€‰æ‹©ä¸»èŠ‚ç‚¹ primary_node = next((node for node in nodes if node.role == \u0026#39;primary\u0026#39;), nodes[0]) participant_id = f\u0026#34;{shard_id}_{primary_node.node_id}\u0026#34; with self.lock: if participant_id not in transaction.participants: transaction.participants[participant_id] = TransactionParticipant( participant_id=participant_id, node_id=primary_node.node_id, shard_id=shard_id ) transaction.participants[participant_id].operations.append({ \u0026#39;table\u0026#39;: table, \u0026#39;operation\u0026#39;: operation, \u0026#39;data\u0026#39;: data, \u0026#39;timestamp\u0026#39;: time.time() }) self.logger.info(f\u0026#34;æ·»åŠ äº‹åŠ¡æ“ä½œ: {transaction_id}, è¡¨: {table}, æ“ä½œ: {operation}\u0026#34;) return True def commit_transaction(self, transaction_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æäº¤åˆ†å¸ƒå¼äº‹åŠ¡ï¼ˆä¸¤é˜¶æ®µæäº¤ï¼‰\u0026#34;\u0026#34;\u0026#34; with self.lock: if transaction_id not in self.active_transactions: self.logger.error(f\u0026#34;äº‹åŠ¡ä¸å­˜åœ¨: {transaction_id}\u0026#34;) return False transaction = self.active_transactions[transaction_id] if transaction.state != TransactionState.ACTIVE: self.logger.error(f\u0026#34;äº‹åŠ¡çŠ¶æ€æ— æ•ˆ: {transaction_id}\u0026#34;) return False if transaction.is_expired(): self.logger.error(f\u0026#34;äº‹åŠ¡å·²è¶…æ—¶: {transaction_id}\u0026#34;) self._abort_transaction(transaction_id) return False transaction.state = TransactionState.PREPARING self.logger.info(f\u0026#34;å¼€å§‹æäº¤äº‹åŠ¡: {transaction_id}\u0026#34;) # ç¬¬ä¸€é˜¶æ®µï¼šå‡†å¤‡é˜¶æ®µ if not self._prepare_phase(transaction): self.logger.error(f\u0026#34;å‡†å¤‡é˜¶æ®µå¤±è´¥ï¼Œä¸­æ­¢äº‹åŠ¡: {transaction_id}\u0026#34;) self._abort_transaction(transaction_id) return False # ç¬¬äºŒé˜¶æ®µï¼šæäº¤é˜¶æ®µ if not self._commit_phase(transaction): self.logger.error(f\u0026#34;æäº¤é˜¶æ®µå¤±è´¥: {transaction_id}\u0026#34;) return False with self.lock: transaction.state = TransactionState.COMMITTED del self.active_transactions[transaction_id] self.logger.info(f\u0026#34;äº‹åŠ¡æäº¤æˆåŠŸ: {transaction_id}\u0026#34;) return True def _prepare_phase(self, transaction: DistributedTransaction) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;ä¸¤é˜¶æ®µæäº¤çš„å‡†å¤‡é˜¶æ®µ\u0026#34;\u0026#34;\u0026#34; self.logger.info(f\u0026#34;æ‰§è¡Œå‡†å¤‡é˜¶æ®µ: {transaction.transaction_id}\u0026#34;) # å¹¶è¡Œå‘æ‰€æœ‰å‚ä¸è€…å‘é€å‡†å¤‡è¯·æ±‚ prepare_futures = [] for participant in transaction.participants.values(): future = self.executor.submit(self._prepare_participant, participant) prepare_futures.append((participant.participant_id, future)) # ç­‰å¾…æ‰€æœ‰å‚ä¸è€…å“åº” all_prepared = True for participant_id, future in prepare_futures: try: result = future.result(timeout=10.0) # 10ç§’è¶…æ—¶ if not result: self.logger.error(f\u0026#34;å‚ä¸è€…å‡†å¤‡å¤±è´¥: {participant_id}\u0026#34;) all_prepared = False else: transaction.participants[participant_id].state = ParticipantState.PREPARED transaction.participants[participant_id].prepare_timestamp = time.time() except Exception as e: self.logger.error(f\u0026#34;å‚ä¸è€…å‡†å¤‡å¼‚å¸¸: {participant_id}, é”™è¯¯: {e}\u0026#34;) all_prepared = False if all_prepared: transaction.state = TransactionState.PREPARED self.logger.info(f\u0026#34;æ‰€æœ‰å‚ä¸è€…å‡†å¤‡å®Œæˆ: {transaction.transaction_id}\u0026#34;) return all_prepared def _commit_phase(self, transaction: DistributedTransaction) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;ä¸¤é˜¶æ®µæäº¤çš„æäº¤é˜¶æ®µ\u0026#34;\u0026#34;\u0026#34; self.logger.info(f\u0026#34;æ‰§è¡Œæäº¤é˜¶æ®µ: {transaction.transaction_id}\u0026#34;) transaction.state = TransactionState.COMMITTING # å¹¶è¡Œå‘æ‰€æœ‰å‚ä¸è€…å‘é€æäº¤è¯·æ±‚ commit_futures = [] for participant in transaction.participants.values(): future = self.executor.submit(self._commit_participant, participant) commit_futures.append((participant.participant_id, future)) # ç­‰å¾…æ‰€æœ‰å‚ä¸è€…å“åº” all_committed = True for participant_id, future in commit_futures: try: result = future.result(timeout=10.0) # 10ç§’è¶…æ—¶ if result: transaction.participants[participant_id].state = ParticipantState.COMMITTED transaction.participants[participant_id].commit_timestamp = time.time() else: self.logger.error(f\u0026#34;å‚ä¸è€…æäº¤å¤±è´¥: {participant_id}\u0026#34;) all_committed = False except Exception as e: self.logger.error(f\u0026#34;å‚ä¸è€…æäº¤å¼‚å¸¸: {participant_id}, é”™è¯¯: {e}\u0026#34;) all_committed = False return all_committed def _prepare_participant(self, participant: TransactionParticipant) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;å‘å‚ä¸è€…å‘é€å‡†å¤‡è¯·æ±‚\u0026#34;\u0026#34;\u0026#34; try: # è¿™é‡Œåº”è¯¥å®é™…è°ƒç”¨å‚ä¸è€…èŠ‚ç‚¹çš„å‡†å¤‡æ¥å£ # ç®€åŒ–å®ç°ï¼Œæ¨¡æ‹Ÿç½‘ç»œè°ƒç”¨ self.logger.info(f\u0026#34;å‘å‚ä¸è€…å‘é€å‡†å¤‡è¯·æ±‚: {participant.participant_id}\u0026#34;) # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ time.sleep(0.1) # æ¨¡æ‹Ÿå‡†å¤‡æ“ä½œ # å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šï¼š # 1. é”å®šç›¸å…³èµ„æº # 2. éªŒè¯æ“ä½œçš„æœ‰æ•ˆæ€§ # 3. å‡†å¤‡å›æ»šæ—¥å¿— # 4. è¿”å›å‡†å¤‡ç»“æœ # ç®€åŒ–ï¼š90%çš„æ¦‚ç‡æˆåŠŸ import random return random.random() \u0026gt; 0.1 except Exception as e: self.logger.error(f\u0026#34;å‡†å¤‡å‚ä¸è€…å¤±è´¥: {participant.participant_id}, é”™è¯¯: {e}\u0026#34;) return False def _commit_participant(self, participant: TransactionParticipant) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;å‘å‚ä¸è€…å‘é€æäº¤è¯·æ±‚\u0026#34;\u0026#34;\u0026#34; try: self.logger.info(f\u0026#34;å‘å‚ä¸è€…å‘é€æäº¤è¯·æ±‚: {participant.participant_id}\u0026#34;) # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ time.sleep(0.1) # æ¨¡æ‹Ÿæäº¤æ“ä½œ # å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šï¼š # 1. æ‰§è¡Œå®é™…çš„æ•°æ®ä¿®æ”¹ # 2. é‡Šæ”¾é”å®šçš„èµ„æº # 3. æ¸…ç†äº‹åŠ¡æ—¥å¿— # 4. è¿”å›æäº¤ç»“æœ # ç®€åŒ–ï¼š95%çš„æ¦‚ç‡æˆåŠŸ import random return random.random() \u0026gt; 0.05 except Exception as e: self.logger.error(f\u0026#34;æäº¤å‚ä¸è€…å¤±è´¥: {participant.participant_id}, é”™è¯¯: {e}\u0026#34;) return False def _abort_transaction(self, transaction_id: str): \u0026#34;\u0026#34;\u0026#34;ä¸­æ­¢äº‹åŠ¡\u0026#34;\u0026#34;\u0026#34; with self.lock: if transaction_id not in self.active_transactions: return transaction = self.active_transactions[transaction_id] transaction.state = TransactionState.ABORTING self.logger.info(f\u0026#34;ä¸­æ­¢äº‹åŠ¡: {transaction_id}\u0026#34;) # å¹¶è¡Œå‘æ‰€æœ‰å‚ä¸è€…å‘é€ä¸­æ­¢è¯·æ±‚ abort_futures = [] for participant in transaction.participants.values(): future = self.executor.submit(self._abort_participant, participant) abort_futures.append(future) # ç­‰å¾…æ‰€æœ‰ä¸­æ­¢æ“ä½œå®Œæˆ for future in abort_futures: try: future.result(timeout=5.0) except Exception as e: self.logger.error(f\u0026#34;ä¸­æ­¢å‚ä¸è€…å¼‚å¸¸: {e}\u0026#34;) with self.lock: transaction.state = TransactionState.ABORTED del self.active_transactions[transaction_id] self.logger.info(f\u0026#34;äº‹åŠ¡ä¸­æ­¢å®Œæˆ: {transaction_id}\u0026#34;) def _abort_participant(self, participant: TransactionParticipant) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;å‘å‚ä¸è€…å‘é€ä¸­æ­¢è¯·æ±‚\u0026#34;\u0026#34;\u0026#34; try: self.logger.info(f\u0026#34;å‘å‚ä¸è€…å‘é€ä¸­æ­¢è¯·æ±‚: {participant.participant_id}\u0026#34;) # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ time.sleep(0.05) # æ¨¡æ‹Ÿä¸­æ­¢æ“ä½œ # å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šï¼š # 1. å›æ»šæ‰€æœ‰ä¿®æ”¹ # 2. é‡Šæ”¾é”å®šçš„èµ„æº # 3. æ¸…ç†äº‹åŠ¡æ—¥å¿— participant.state = ParticipantState.ABORTED return True except Exception as e: self.logger.error(f\u0026#34;ä¸­æ­¢å‚ä¸è€…å¤±è´¥: {participant.participant_id}, é”™è¯¯: {e}\u0026#34;) return False def rollback_transaction(self, transaction_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;å›æ»šäº‹åŠ¡\u0026#34;\u0026#34;\u0026#34; self.logger.info(f\u0026#34;å›æ»šäº‹åŠ¡: {transaction_id}\u0026#34;) self._abort_transaction(transaction_id) return True def get_transaction_status(self, transaction_id: str) -\u0026gt; Optional[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;è·å–äº‹åŠ¡çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; with self.lock: if transaction_id not in self.active_transactions: return None transaction = self.active_transactions[transaction_id] return { \u0026#39;transaction_id\u0026#39;: transaction.transaction_id, \u0026#39;state\u0026#39;: transaction.state.value, \u0026#39;start_timestamp\u0026#39;: transaction.start_timestamp, \u0026#39;timeout\u0026#39;: transaction.timeout, \u0026#39;participants\u0026#39;: { p_id: { \u0026#39;node_id\u0026#39;: p.node_id, \u0026#39;shard_id\u0026#39;: p.shard_id, \u0026#39;state\u0026#39;: p.state.value, \u0026#39;operations_count\u0026#39;: len(p.operations) } for p_id, p in transaction.participants.items() } } def _cleanup_expired_transactions(self): \u0026#34;\u0026#34;\u0026#34;æ¸…ç†è¿‡æœŸäº‹åŠ¡\u0026#34;\u0026#34;\u0026#34; while True: try: time.sleep(10) # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡ expired_transactions = [] with self.lock: for transaction_id, transaction in self.active_transactions.items(): if transaction.is_expired(): expired_transactions.append(transaction_id) for transaction_id in expired_transactions: self.logger.warning(f\u0026#34;æ¸…ç†è¿‡æœŸäº‹åŠ¡: {transaction_id}\u0026#34;) self._abort_transaction(transaction_id) except Exception as e: self.logger.error(f\u0026#34;æ¸…ç†è¿‡æœŸäº‹åŠ¡å¼‚å¸¸: {e}\u0026#34;) def main(): # ç¤ºä¾‹ç”¨æ³• from sharding.shard_router import ShardRouter # åˆ›å»ºåˆ†ç‰‡è·¯ç”±å™¨ router = ShardRouter(\u0026#39;config/distributed_database.yaml\u0026#39;) # åˆ›å»ºäº‹åŠ¡åè°ƒå™¨ coordinator = DistributedTransactionCoordinator(\u0026#39;coordinator_1\u0026#39;, router) # å¼€å§‹äº‹åŠ¡ tx_id = coordinator.begin_transaction() print(f\u0026#34;å¼€å§‹äº‹åŠ¡: {tx_id}\u0026#34;) # æ·»åŠ æ“ä½œ coordinator.add_operation(tx_id, \u0026#39;users\u0026#39;, \u0026#39;insert\u0026#39;, {\u0026#39;user_id\u0026#39;: 12345, \u0026#39;name\u0026#39;: \u0026#39;John\u0026#39;}) coordinator.add_operation(tx_id, \u0026#39;orders\u0026#39;, \u0026#39;insert\u0026#39;, { \u0026#39;user_id\u0026#39;: 12345, \u0026#39;order_id\u0026#39;: 67890, \u0026#39;order_date\u0026#39;: \u0026#39;2024-01-15\u0026#39; }) # è·å–äº‹åŠ¡çŠ¶æ€ status = coordinator.get_transaction_status(tx_id) print(f\u0026#34;äº‹åŠ¡çŠ¶æ€: {json.dumps(status, indent=2, ensure_ascii=False)}\u0026#34;) # æäº¤äº‹åŠ¡ success = coordinator.commit_transaction(tx_id) print(f\u0026#34;äº‹åŠ¡æäº¤ç»“æœ: {success}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() ä¸€è‡´æ€§åè®®å®ç° 1. Raftå…±è¯†ç®—æ³• #!/usr/bin/env python3 # src/consensus/raft_consensus.py import time import random import threading import logging import json from typing import Dict, List, Any, Optional, Callable from enum import Enum from dataclasses import dataclass, field import socket import pickle class NodeState(Enum): FOLLOWER = \u0026#34;follower\u0026#34; CANDIDATE = \u0026#34;candidate\u0026#34; LEADER = \u0026#34;leader\u0026#34; @dataclass class LogEntry: term: int index: int command: Dict[str, Any] timestamp: float = field(default_factory=time.time) @dataclass class VoteRequest: term: int candidate_id: str last_log_index: int last_log_term: int @dataclass class VoteResponse: term: int vote_granted: bool @dataclass class AppendEntriesRequest: term: int leader_id: str prev_log_index: int prev_log_term: int entries: List[LogEntry] leader_commit: int @dataclass class AppendEntriesResponse: term: int success: bool match_index: int = 0 class RaftNode: def __init__(self, node_id: str, cluster_nodes: List[str], host: str = \u0026#34;localhost\u0026#34;, port: int = 8000): self.node_id = node_id self.cluster_nodes = cluster_nodes self.host = host self.port = port # RaftçŠ¶æ€ self.current_term = 0 self.voted_for: Optional[str] = None self.log: List[LogEntry] = [] self.state = NodeState.FOLLOWER # æ˜“å¤±æ€§çŠ¶æ€ self.commit_index = 0 self.last_applied = 0 # LeaderçŠ¶æ€ self.next_index: Dict[str, int] = {} self.match_index: Dict[str, int] = {} # å®šæ—¶å™¨ self.election_timeout = self._random_election_timeout() self.last_heartbeat = time.time() self.heartbeat_interval = 0.1 # 100ms # çº¿ç¨‹å’Œé” self.lock = threading.RLock() self.running = False self.election_thread: Optional[threading.Thread] = None self.heartbeat_thread: Optional[threading.Thread] = None # æ—¥å¿—è®°å½• self.logger = self._setup_logging() # çŠ¶æ€æœº self.state_machine: Dict[str, Any] = {} self.state_machine_callbacks: List[Callable] = [] def _setup_logging(self) -\u0026gt; logging.Logger: \u0026#34;\u0026#34;\u0026#34;è®¾ç½®æ—¥å¿—è®°å½•\u0026#34;\u0026#34;\u0026#34; logger = logging.getLogger(f\u0026#39;RaftNode-{self.node_id}\u0026#39;) logger.setLevel(logging.INFO) handler = logging.StreamHandler() formatter = logging.Formatter( f\u0026#39;%(asctime)s - {self.node_id} - %(levelname)s - %(message)s\u0026#39; ) handler.setFormatter(formatter) logger.addHandler(handler) return logger def _random_election_timeout(self) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆéšæœºé€‰ä¸¾è¶…æ—¶æ—¶é—´ï¼ˆ150-300msï¼‰\u0026#34;\u0026#34;\u0026#34; return random.uniform(0.15, 0.3) def start(self): \u0026#34;\u0026#34;\u0026#34;å¯åŠ¨RaftèŠ‚ç‚¹\u0026#34;\u0026#34;\u0026#34; with self.lock: if self.running: return self.running = True self.last_heartbeat = time.time() # å¯åŠ¨é€‰ä¸¾å®šæ—¶å™¨çº¿ç¨‹ self.election_thread = threading.Thread(target=self._election_timer) self.election_thread.daemon = True self.election_thread.start() self.logger.info(f\u0026#34;RaftèŠ‚ç‚¹å¯åŠ¨: {self.node_id}\u0026#34;) def stop(self): \u0026#34;\u0026#34;\u0026#34;åœæ­¢RaftèŠ‚ç‚¹\u0026#34;\u0026#34;\u0026#34; with self.lock: self.running = False if self.heartbeat_thread: self.heartbeat_thread.join(timeout=1.0) self.logger.info(f\u0026#34;RaftèŠ‚ç‚¹åœæ­¢: {self.node_id}\u0026#34;) def _election_timer(self): \u0026#34;\u0026#34;\u0026#34;é€‰ä¸¾å®šæ—¶å™¨\u0026#34;\u0026#34;\u0026#34; while self.running: time.sleep(0.01) # 10msæ£€æŸ¥é—´éš” with self.lock: if self.state == NodeState.LEADER: continue # æ£€æŸ¥æ˜¯å¦è¶…æ—¶ if time.time() - self.last_heartbeat \u0026gt; self.election_timeout: self._start_election() def _start_election(self): \u0026#34;\u0026#34;\u0026#34;å¼€å§‹é€‰ä¸¾\u0026#34;\u0026#34;\u0026#34; self.logger.info(f\u0026#34;å¼€å§‹é€‰ä¸¾ï¼Œä»»æœŸ: {self.current_term + 1}\u0026#34;) # è½¬æ¢ä¸ºå€™é€‰äººçŠ¶æ€ self.state = NodeState.CANDIDATE self.current_term += 1 self.voted_for = self.node_id self.last_heartbeat = time.time() self.election_timeout = self._random_election_timeout() # è·å–æœ€åæ—¥å¿—ä¿¡æ¯ last_log_index = len(self.log) - 1 last_log_term = self.log[last_log_index].term if self.log else 0 # å‘æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹å‘é€æŠ•ç¥¨è¯·æ±‚ vote_request = VoteRequest( term=self.current_term, candidate_id=self.node_id, last_log_index=last_log_index, last_log_term=last_log_term ) votes_received = 1 # è‡ªå·±çš„ç¥¨ for node_id in self.cluster_nodes: if node_id == self.node_id: continue try: response = self._send_vote_request(node_id, vote_request) if response and response.vote_granted: votes_received += 1 elif response and response.term \u0026gt; self.current_term: # å‘ç°æ›´é«˜çš„ä»»æœŸï¼Œè½¬ä¸ºè·Ÿéšè€… self._become_follower(response.term) return except Exception as e: self.logger.error(f\u0026#34;å‘é€æŠ•ç¥¨è¯·æ±‚å¤±è´¥: {node_id}, é”™è¯¯: {e}\u0026#34;) # æ£€æŸ¥æ˜¯å¦è·å¾—å¤šæ•°ç¥¨ majority = len(self.cluster_nodes) // 2 + 1 if votes_received \u0026gt;= majority: self._become_leader() else: self._become_follower(self.current_term) def _become_leader(self): \u0026#34;\u0026#34;\u0026#34;æˆä¸ºLeader\u0026#34;\u0026#34;\u0026#34; self.logger.info(f\u0026#34;æˆä¸ºLeaderï¼Œä»»æœŸ: {self.current_term}\u0026#34;) self.state = NodeState.LEADER # åˆå§‹åŒ–LeaderçŠ¶æ€ for node_id in self.cluster_nodes: if node_id != self.node_id: self.next_index[node_id] = len(self.log) self.match_index[node_id] = 0 # å¯åŠ¨å¿ƒè·³çº¿ç¨‹ if self.heartbeat_thread: self.heartbeat_thread.join(timeout=0.1) self.heartbeat_thread = threading.Thread(target=self._send_heartbeats) self.heartbeat_thread.daemon = True self.heartbeat_thread.start() # å‘é€ç©ºçš„AppendEntriesä½œä¸ºå¿ƒè·³ self._send_append_entries() def _become_follower(self, term: int): \u0026#34;\u0026#34;\u0026#34;æˆä¸ºFollower\u0026#34;\u0026#34;\u0026#34; if term \u0026gt; self.current_term: self.current_term = term self.voted_for = None self.state = NodeState.FOLLOWER self.last_heartbeat = time.time() # åœæ­¢å¿ƒè·³çº¿ç¨‹ if self.heartbeat_thread: self.heartbeat_thread = None def _send_heartbeats(self): \u0026#34;\u0026#34;\u0026#34;å‘é€å¿ƒè·³\u0026#34;\u0026#34;\u0026#34; while self.running and self.state == NodeState.LEADER: self._send_append_entries() time.sleep(self.heartbeat_interval) def _send_append_entries(self): \u0026#34;\u0026#34;\u0026#34;å‘é€AppendEntries RPC\u0026#34;\u0026#34;\u0026#34; for node_id in self.cluster_nodes: if node_id == self.node_id: continue try: # å‡†å¤‡AppendEntriesè¯·æ±‚ next_index = self.next_index.get(node_id, len(self.log)) prev_log_index = next_index - 1 prev_log_term = 0 if prev_log_index \u0026gt;= 0 and prev_log_index \u0026lt; len(self.log): prev_log_term = self.log[prev_log_index].term # å‘é€çš„æ—¥å¿—æ¡ç›® entries = self.log[next_index:] if next_index \u0026lt; len(self.log) else [] request = AppendEntriesRequest( term=self.current_term, leader_id=self.node_id, prev_log_index=prev_log_index, prev_log_term=prev_log_term, entries=entries, leader_commit=self.commit_index ) response = self._send_append_entries_request(node_id, request) if response: if response.term \u0026gt; self.current_term: # å‘ç°æ›´é«˜çš„ä»»æœŸï¼Œè½¬ä¸ºè·Ÿéšè€… self._become_follower(response.term) return if response.success: # æ›´æ–°next_indexå’Œmatch_index self.match_index[node_id] = prev_log_index + len(entries) self.next_index[node_id] = self.match_index[node_id] + 1 else: # å‡å°‘next_indexå¹¶é‡è¯• self.next_index[node_id] = max(0, self.next_index[node_id] - 1) except Exception as e: self.logger.error(f\u0026#34;å‘é€AppendEntrieså¤±è´¥: {node_id}, é”™è¯¯: {e}\u0026#34;) # æ›´æ–°commit_index self._update_commit_index() def _update_commit_index(self): \u0026#34;\u0026#34;\u0026#34;æ›´æ–°commit_index\u0026#34;\u0026#34;\u0026#34; if self.state != NodeState.LEADER: return # æ‰¾åˆ°å¤§å¤šæ•°èŠ‚ç‚¹éƒ½å·²å¤åˆ¶çš„æœ€å¤§ç´¢å¼• for i in range(len(self.log) - 1, self.commit_index, -1): if self.log[i].term == self.current_term: count = 1 # Leaderè‡ªå·± for node_id in self.cluster_nodes: if node_id != self.node_id and self.match_index.get(node_id, 0) \u0026gt;= i: count += 1 majority = len(self.cluster_nodes) // 2 + 1 if count \u0026gt;= majority: self.commit_index = i self._apply_committed_entries() break def _apply_committed_entries(self): \u0026#34;\u0026#34;\u0026#34;åº”ç”¨å·²æäº¤çš„æ—¥å¿—æ¡ç›®åˆ°çŠ¶æ€æœº\u0026#34;\u0026#34;\u0026#34; while self.last_applied \u0026lt; self.commit_index: self.last_applied += 1 entry = self.log[self.last_applied] # åº”ç”¨åˆ°çŠ¶æ€æœº self._apply_to_state_machine(entry.command) self.logger.info(f\u0026#34;åº”ç”¨æ—¥å¿—æ¡ç›®: {self.last_applied}, å‘½ä»¤: {entry.command}\u0026#34;) def _apply_to_state_machine(self, command: Dict[str, Any]): \u0026#34;\u0026#34;\u0026#34;åº”ç”¨å‘½ä»¤åˆ°çŠ¶æ€æœº\u0026#34;\u0026#34;\u0026#34; operation = command.get(\u0026#39;operation\u0026#39;) key = command.get(\u0026#39;key\u0026#39;) value = command.get(\u0026#39;value\u0026#39;) if operation == \u0026#39;set\u0026#39;: self.state_machine[key] = value elif operation == \u0026#39;delete\u0026#39;: self.state_machine.pop(key, None) # è°ƒç”¨å›è°ƒå‡½æ•° for callback in self.state_machine_callbacks: try: callback(command, self.state_machine) except Exception as e: self.logger.error(f\u0026#34;çŠ¶æ€æœºå›è°ƒå¼‚å¸¸: {e}\u0026#34;) def append_entry(self, command: Dict[str, Any]) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;è¿½åŠ æ—¥å¿—æ¡ç›®ï¼ˆä»…Leaderå¯è°ƒç”¨ï¼‰\u0026#34;\u0026#34;\u0026#34; with self.lock: if self.state != NodeState.LEADER: return False entry = LogEntry( term=self.current_term, index=len(self.log), command=command ) self.log.append(entry) self.logger.info(f\u0026#34;è¿½åŠ æ—¥å¿—æ¡ç›®: {entry.index}, å‘½ä»¤: {command}\u0026#34;) return True def _send_vote_request(self, node_id: str, request: VoteRequest) -\u0026gt; Optional[VoteResponse]: \u0026#34;\u0026#34;\u0026#34;å‘é€æŠ•ç¥¨è¯·æ±‚ï¼ˆæ¨¡æ‹Ÿç½‘ç»œè°ƒç”¨ï¼‰\u0026#34;\u0026#34;\u0026#34; # è¿™é‡Œåº”è¯¥å®é™…å‘é€ç½‘ç»œè¯·æ±‚ # ç®€åŒ–å®ç°ï¼Œæ¨¡æ‹Ÿç½‘ç»œè°ƒç”¨ time.sleep(0.01) # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ # æ¨¡æ‹Ÿå“åº” return VoteResponse( term=self.current_term, vote_granted=random.random() \u0026gt; 0.3 # 70%æ¦‚ç‡åŒæ„æŠ•ç¥¨ ) def _send_append_entries_request(self, node_id: str, request: AppendEntriesRequest) -\u0026gt; Optional[AppendEntriesResponse]: \u0026#34;\u0026#34;\u0026#34;å‘é€AppendEntriesè¯·æ±‚ï¼ˆæ¨¡æ‹Ÿç½‘ç»œè°ƒç”¨ï¼‰\u0026#34;\u0026#34;\u0026#34; # è¿™é‡Œåº”è¯¥å®é™…å‘é€ç½‘ç»œè¯·æ±‚ # ç®€åŒ–å®ç°ï¼Œæ¨¡æ‹Ÿç½‘ç»œè°ƒç”¨ time.sleep(0.005) # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ # æ¨¡æ‹Ÿå“åº” return AppendEntriesResponse( term=self.current_term, success=random.random() \u0026gt; 0.1, # 90%æ¦‚ç‡æˆåŠŸ match_index=request.prev_log_index + len(request.entries) ) def handle_vote_request(self, request: VoteRequest) -\u0026gt; VoteResponse: \u0026#34;\u0026#34;\u0026#34;å¤„ç†æŠ•ç¥¨è¯·æ±‚\u0026#34;\u0026#34;\u0026#34; with self.lock: # å¦‚æœè¯·æ±‚çš„ä»»æœŸå°äºå½“å‰ä»»æœŸï¼Œæ‹’ç»æŠ•ç¥¨ if request.term \u0026lt; self.current_term: return VoteResponse(term=self.current_term, vote_granted=False) # å¦‚æœè¯·æ±‚çš„ä»»æœŸå¤§äºå½“å‰ä»»æœŸï¼Œæ›´æ–°ä»»æœŸå¹¶è½¬ä¸ºè·Ÿéšè€… if request.term \u0026gt; self.current_term: self.current_term = request.term self.voted_for = None self._become_follower(request.term) # æ£€æŸ¥æ˜¯å¦å¯ä»¥æŠ•ç¥¨ vote_granted = False if (self.voted_for is None or self.voted_for == request.candidate_id): # æ£€æŸ¥å€™é€‰äººçš„æ—¥å¿—æ˜¯å¦è‡³å°‘å’Œè‡ªå·±ä¸€æ ·æ–° last_log_index = len(self.log) - 1 last_log_term = self.log[last_log_index].term if self.log else 0 if (request.last_log_term \u0026gt; last_log_term or (request.last_log_term == last_log_term and request.last_log_index \u0026gt;= last_log_index)): vote_granted = True self.voted_for = request.candidate_id self.last_heartbeat = time.time() return VoteResponse(term=self.current_term, vote_granted=vote_granted) def handle_append_entries(self, request: AppendEntriesRequest) -\u0026gt; AppendEntriesResponse: \u0026#34;\u0026#34;\u0026#34;å¤„ç†AppendEntriesè¯·æ±‚\u0026#34;\u0026#34;\u0026#34; with self.lock: # å¦‚æœè¯·æ±‚çš„ä»»æœŸå°äºå½“å‰ä»»æœŸï¼Œæ‹’ç» if request.term \u0026lt; self.current_term: return AppendEntriesResponse(term=self.current_term, success=False) # æ›´æ–°ä»»æœŸå¹¶è½¬ä¸ºè·Ÿéšè€… if request.term \u0026gt;= self.current_term: self.current_term = request.term self._become_follower(request.term) self.last_heartbeat = time.time() # æ£€æŸ¥å‰ä¸€ä¸ªæ—¥å¿—æ¡ç›®æ˜¯å¦åŒ¹é… if (request.prev_log_index \u0026gt;= 0 and (request.prev_log_index \u0026gt;= len(self.log) or self.log[request.prev_log_index].term != request.prev_log_term)): return AppendEntriesResponse(term=self.current_term, success=False) # åˆ é™¤å†²çªçš„æ—¥å¿—æ¡ç›® if request.entries: # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå†²çªçš„æ¡ç›® conflict_index = request.prev_log_index + 1 for i, entry in enumerate(request.entries): index = conflict_index + i if index \u0026lt; len(self.log) and self.log[index].term != entry.term: # åˆ é™¤ä»è¿™ä¸ªä½ç½®å¼€å§‹çš„æ‰€æœ‰æ¡ç›® self.log = self.log[:index] break # è¿½åŠ æ–°æ¡ç›® for entry in request.entries: if conflict_index \u0026gt;= len(self.log): self.log.append(entry) conflict_index += 1 # æ›´æ–°commit_index if request.leader_commit \u0026gt; self.commit_index: self.commit_index = min(request.leader_commit, len(self.log) - 1) self._apply_committed_entries() return AppendEntriesResponse( term=self.current_term, success=True, match_index=request.prev_log_index + len(request.entries) ) def get_state(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è·å–èŠ‚ç‚¹çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; with self.lock: return { \u0026#39;node_id\u0026#39;: self.node_id, \u0026#39;state\u0026#39;: self.state.value, \u0026#39;current_term\u0026#39;: self.current_term, \u0026#39;voted_for\u0026#39;: self.voted_for, \u0026#39;log_length\u0026#39;: len(self.log), \u0026#39;commit_index\u0026#39;: self.commit_index, \u0026#39;last_applied\u0026#39;: self.last_applied, \u0026#39;state_machine\u0026#39;: dict(self.state_machine) } def add_state_machine_callback(self, callback: Callable): \u0026#34;\u0026#34;\u0026#34;æ·»åŠ çŠ¶æ€æœºå›è°ƒå‡½æ•°\u0026#34;\u0026#34;\u0026#34; self.state_machine_callbacks.append(callback) def main(): # ç¤ºä¾‹ç”¨æ³• cluster_nodes = [\u0026#39;node1\u0026#39;, \u0026#39;node2\u0026#39;, \u0026#39;node3\u0026#39;] # åˆ›å»ºRaftèŠ‚ç‚¹ nodes = [] for i, node_id in enumerate(cluster_nodes): node = RaftNode(node_id, cluster_nodes, port=8000 + i) nodes.append(node) # å¯åŠ¨æ‰€æœ‰èŠ‚ç‚¹ for node in nodes: node.start() # ç­‰å¾…é€‰ä¸¾å®Œæˆ time.sleep(2) # æŸ¥æ‰¾Leader leader = None for node in nodes: state = node.get_state() print(f\u0026#34;èŠ‚ç‚¹ {state[\u0026#39;node_id\u0026#39;]}: {state[\u0026#39;state\u0026#39;]}, ä»»æœŸ: {state[\u0026#39;current_term\u0026#39;]}\u0026#34;) if state[\u0026#39;state\u0026#39;] == \u0026#39;leader\u0026#39;: leader = node if leader: print(f\u0026#34;\\nLeader: {leader.node_id}\u0026#34;) # åœ¨Leaderä¸Šè¿½åŠ ä¸€äº›æ—¥å¿—æ¡ç›® commands = [ {\u0026#39;operation\u0026#39;: \u0026#39;set\u0026#39;, \u0026#39;key\u0026#39;: \u0026#39;user:1\u0026#39;, \u0026#39;value\u0026#39;: \u0026#39;Alice\u0026#39;}, {\u0026#39;operation\u0026#39;: \u0026#39;set\u0026#39;, \u0026#39;key\u0026#39;: \u0026#39;user:2\u0026#39;, \u0026#39;value\u0026#39;: \u0026#39;Bob\u0026#39;}, {\u0026#39;operation\u0026#39;: \u0026#39;set\u0026#39;, \u0026#39;key\u0026#39;: \u0026#39;counter\u0026#39;, \u0026#39;value\u0026#39;: 100}, {\u0026#39;operation\u0026#39;: \u0026#39;delete\u0026#39;, \u0026#39;key\u0026#39;: \u0026#39;user:1\u0026#39;} ] for command in commands: success = leader.append_entry(command) print(f\u0026#34;è¿½åŠ å‘½ä»¤: {command}, ç»“æœ: {success}\u0026#34;) time.sleep(0.5) # ç­‰å¾…æ—¥å¿—å¤åˆ¶ time.sleep(2) # æŸ¥çœ‹æ‰€æœ‰èŠ‚ç‚¹çš„çŠ¶æ€æœº print(\u0026#34;\\næ‰€æœ‰èŠ‚ç‚¹çš„çŠ¶æ€æœº:\u0026#34;) for node in nodes: state = node.get_state() print(f\u0026#34;èŠ‚ç‚¹ {state[\u0026#39;node_id\u0026#39;]}: {state[\u0026#39;state_machine\u0026#39;]}\u0026#34;) # åœæ­¢æ‰€æœ‰èŠ‚ç‚¹ for node in nodes: node.stop() if __name__ == \u0026#34;__main__\u0026#34;: main() æ•…éšœæ£€æµ‹ä¸æ¢å¤ 1. é›†ç¾¤å¥åº·ç›‘æ§ç³»ç»Ÿ #!/bin/bash # scripts/cluster_health_monitor.sh # é›†ç¾¤å¥åº·ç›‘æ§è„šæœ¬ # é…ç½®æ–‡ä»¶è·¯å¾„ CONFIG_FILE=\u0026#34;${CONFIG_FILE:-/etc/distributed_db/cluster.conf}\u0026#34; LOG_FILE=\u0026#34;${LOG_FILE:-/var/log/cluster_health.log}\u0026#34; ALERT_SCRIPT=\u0026#34;${ALERT_SCRIPT:-/usr/local/bin/send_alert.sh}\u0026#34; # ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰ MONITOR_INTERVAL=\u0026#34;${MONITOR_INTERVAL:-30}\u0026#34; # æ•…éšœé˜ˆå€¼ FAILURE_THRESHOLD=\u0026#34;${FAILURE_THRESHOLD:-3}\u0026#34; RESPONSE_TIMEOUT=\u0026#34;${RESPONSE_TIMEOUT:-5}\u0026#34; # å…¨å±€å˜é‡ declare -A NODE_FAILURE_COUNT declare -A NODE_STATUS declare -A SHARD_STATUS # æ—¥å¿—å‡½æ•° log() { echo \u0026#34;[$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)] $1\u0026#34; | tee -a \u0026#34;$LOG_FILE\u0026#34; } error() { echo \u0026#34;[$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)] ERROR: $1\u0026#34; | tee -a \u0026#34;$LOG_FILE\u0026#34; \u0026gt;\u0026amp;2 } # åŠ è½½é…ç½® load_config() { if [[ ! -f \u0026#34;$CONFIG_FILE\u0026#34; ]]; then error \u0026#34;é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $CONFIG_FILE\u0026#34; exit 1 fi source \u0026#34;$CONFIG_FILE\u0026#34; log \u0026#34;åŠ è½½é…ç½®æ–‡ä»¶: $CONFIG_FILE\u0026#34; } # æ£€æŸ¥èŠ‚ç‚¹å¥åº·çŠ¶æ€ check_node_health() { local node_id=\u0026#34;$1\u0026#34; local host=\u0026#34;$2\u0026#34; local port=\u0026#34;$3\u0026#34; # TCPè¿æ¥æ£€æŸ¥ if timeout \u0026#34;$RESPONSE_TIMEOUT\u0026#34; bash -c \u0026#34;\u0026lt;/dev/tcp/$host/$port\u0026#34; 2\u0026gt;/dev/null; then # åº”ç”¨å±‚å¥åº·æ£€æŸ¥ local health_response health_response=$(curl -s --max-time \u0026#34;$RESPONSE_TIMEOUT\u0026#34; \\ \u0026#34;http://$host:$((port + 1000))/health\u0026#34; 2\u0026gt;/dev/null) if [[ \u0026#34;$health_response\u0026#34; == *\u0026#34;healthy\u0026#34;* ]]; then NODE_STATUS[\u0026#34;$node_id\u0026#34;]=\u0026#34;healthy\u0026#34; NODE_FAILURE_COUNT[\u0026#34;$node_id\u0026#34;]=0 return 0 else NODE_STATUS[\u0026#34;$node_id\u0026#34;]=\u0026#34;unhealthy\u0026#34; ((NODE_FAILURE_COUNT[\u0026#34;$node_id\u0026#34;]++)) return 1 fi else NODE_STATUS[\u0026#34;$node_id\u0026#34;]=\u0026#34;unreachable\u0026#34; ((NODE_FAILURE_COUNT[\u0026#34;$node_id\u0026#34;]++)) return 1 fi } # æ£€æŸ¥åˆ†ç‰‡çŠ¶æ€ check_shard_health() { local shard_id=\u0026#34;$1\u0026#34; local primary_node=\u0026#34;$2\u0026#34; local replica_nodes=\u0026#34;$3\u0026#34; local healthy_replicas=0 local total_replicas=0 # æ£€æŸ¥ä¸»èŠ‚ç‚¹ if [[ \u0026#34;${NODE_STATUS[$primary_node]}\u0026#34; == \u0026#34;healthy\u0026#34; ]]; then ((healthy_replicas++)) fi ((total_replicas++)) # æ£€æŸ¥å‰¯æœ¬èŠ‚ç‚¹ IFS=\u0026#39;,\u0026#39; read -ra REPLICAS \u0026lt;\u0026lt;\u0026lt; \u0026#34;$replica_nodes\u0026#34; for replica in \u0026#34;${REPLICAS[@]}\u0026#34;; do if [[ \u0026#34;${NODE_STATUS[$replica]}\u0026#34; == \u0026#34;healthy\u0026#34; ]]; then ((healthy_replicas++)) fi ((total_replicas++)) done # è®¡ç®—å¥åº·æ¯”ä¾‹ local health_ratio=$((healthy_replicas * 100 / total_replicas)) if [[ $health_ratio -ge 67 ]]; then # è‡³å°‘2/3èŠ‚ç‚¹å¥åº· SHARD_STATUS[\u0026#34;$shard_id\u0026#34;]=\u0026#34;healthy\u0026#34; elif [[ $health_ratio -ge 50 ]]; then SHARD_STATUS[\u0026#34;$shard_id\u0026#34;]=\u0026#34;degraded\u0026#34; else SHARD_STATUS[\u0026#34;$shard_id\u0026#34;]=\u0026#34;critical\u0026#34; fi log \u0026#34;åˆ†ç‰‡ $shard_id å¥åº·çŠ¶æ€: ${SHARD_STATUS[$shard_id]} ($healthy_replicas/$total_replicas)\u0026#34; } # æ•…éšœæ¢å¤ handle_node_failure() { local failed_node=\u0026#34;$1\u0026#34; log \u0026#34;å¤„ç†èŠ‚ç‚¹æ•…éšœ: $failed_node\u0026#34; # å‘é€å‘Šè­¦ if [[ -x \u0026#34;$ALERT_SCRIPT\u0026#34; ]]; then \u0026#34;$ALERT_SCRIPT\u0026#34; \u0026#34;NODE_FAILURE\u0026#34; \u0026#34;$failed_node\u0026#34; \u0026#34;èŠ‚ç‚¹ $failed_node è¿ç»­å¤±è´¥ ${NODE_FAILURE_COUNT[$failed_node]} æ¬¡\u0026#34; fi # å°è¯•è‡ªåŠ¨æ¢å¤ attempt_node_recovery \u0026#34;$failed_node\u0026#34; # å¦‚æœæ˜¯ä¸»èŠ‚ç‚¹æ•…éšœï¼Œè§¦å‘æ•…éšœè½¬ç§» if is_primary_node \u0026#34;$failed_node\u0026#34;; then trigger_failover \u0026#34;$failed_node\u0026#34; fi } # å°è¯•èŠ‚ç‚¹æ¢å¤ attempt_node_recovery() { local node_id=\u0026#34;$1\u0026#34; log \u0026#34;å°è¯•æ¢å¤èŠ‚ç‚¹: $node_id\u0026#34; # é‡å¯èŠ‚ç‚¹æœåŠ¡ if systemctl is-active --quiet \u0026#34;distributed-db-$node_id\u0026#34;; then systemctl restart \u0026#34;distributed-db-$node_id\u0026#34; sleep 10 # é‡æ–°æ£€æŸ¥å¥åº·çŠ¶æ€ local host port get_node_info \u0026#34;$node_id\u0026#34; host port if check_node_health \u0026#34;$node_id\u0026#34; \u0026#34;$host\u0026#34; \u0026#34;$port\u0026#34;; then log \u0026#34;èŠ‚ç‚¹æ¢å¤æˆåŠŸ: $node_id\u0026#34; return 0 fi fi log \u0026#34;èŠ‚ç‚¹æ¢å¤å¤±è´¥: $node_id\u0026#34; return 1 } # æ£€æŸ¥æ˜¯å¦ä¸ºä¸»èŠ‚ç‚¹ is_primary_node() { local node_id=\u0026#34;$1\u0026#34; # ä»é…ç½®ä¸­æŸ¥æ‰¾è¯¥èŠ‚ç‚¹æ˜¯å¦ä¸ºæŸä¸ªåˆ†ç‰‡çš„ä¸»èŠ‚ç‚¹ grep -q \u0026#34;primary.*$node_id\u0026#34; \u0026#34;$CONFIG_FILE\u0026#34; } # è§¦å‘æ•…éšœè½¬ç§» trigger_failover() { local failed_primary=\u0026#34;$1\u0026#34; log \u0026#34;è§¦å‘æ•…éšœè½¬ç§»: $failed_primary\u0026#34; # æŸ¥æ‰¾å—å½±å“çš„åˆ†ç‰‡ local affected_shards affected_shards=$(grep -l \u0026#34;primary.*$failed_primary\u0026#34; \u0026#34;$CONFIG_FILE\u0026#34; | \\ sed \u0026#39;s/.*shard_\\([0-9]*\\).*/\\1/\u0026#39;) for shard_id in $affected_shards; do log \u0026#34;ä¸ºåˆ†ç‰‡ $shard_id æ‰§è¡Œæ•…éšœè½¬ç§»\u0026#34; # é€‰æ‹©æ–°çš„ä¸»èŠ‚ç‚¹ï¼ˆé€‰æ‹©ç¬¬ä¸€ä¸ªå¥åº·çš„å‰¯æœ¬ï¼‰ local replica_nodes replica_nodes=$(get_shard_replicas \u0026#34;$shard_id\u0026#34;) IFS=\u0026#39;,\u0026#39; read -ra REPLICAS \u0026lt;\u0026lt;\u0026lt; \u0026#34;$replica_nodes\u0026#34; for replica in \u0026#34;${REPLICAS[@]}\u0026#34;; do if [[ \u0026#34;${NODE_STATUS[$replica]}\u0026#34; == \u0026#34;healthy\u0026#34; ]]; then promote_to_primary \u0026#34;$replica\u0026#34; \u0026#34;$shard_id\u0026#34; break fi done done } # æå‡å‰¯æœ¬ä¸ºä¸»èŠ‚ç‚¹ promote_to_primary() { local new_primary=\u0026#34;$1\u0026#34; local shard_id=\u0026#34;$2\u0026#34; log \u0026#34;æå‡èŠ‚ç‚¹ $new_primary ä¸ºåˆ†ç‰‡ $shard_id çš„ä¸»èŠ‚ç‚¹\u0026#34; # è°ƒç”¨ç®¡ç†APIè¿›è¡Œæ•…éšœè½¬ç§» local host port get_node_info \u0026#34;$new_primary\u0026#34; host port curl -X POST \u0026#34;http://$host:$((port + 1000))/admin/promote\u0026#34; \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#34;{\\\u0026#34;shard_id\\\u0026#34;: \\\u0026#34;$shard_id\\\u0026#34;}\u0026#34; \\ --max-time 10 2\u0026gt;/dev/null if [[ $? -eq 0 ]]; then log \u0026#34;æ•…éšœè½¬ç§»æˆåŠŸ: åˆ†ç‰‡ $shard_id, æ–°ä¸»èŠ‚ç‚¹: $new_primary\u0026#34; # æ›´æ–°é…ç½®æ–‡ä»¶ update_config_primary \u0026#34;$shard_id\u0026#34; \u0026#34;$new_primary\u0026#34; # å‘é€å‘Šè­¦ if [[ -x \u0026#34;$ALERT_SCRIPT\u0026#34; ]]; then \u0026#34;$ALERT_SCRIPT\u0026#34; \u0026#34;FAILOVER_SUCCESS\u0026#34; \u0026#34;$shard_id\u0026#34; \\ \u0026#34;åˆ†ç‰‡ $shard_id æ•…éšœè½¬ç§»æˆåŠŸï¼Œæ–°ä¸»èŠ‚ç‚¹: $new_primary\u0026#34; fi else error \u0026#34;æ•…éšœè½¬ç§»å¤±è´¥: åˆ†ç‰‡ $shard_id, ç›®æ ‡èŠ‚ç‚¹: $new_primary\u0026#34; fi } # è·å–èŠ‚ç‚¹ä¿¡æ¯ get_node_info() { local node_id=\u0026#34;$1\u0026#34; local -n host_ref=\u0026#34;$2\u0026#34; local -n port_ref=\u0026#34;$3\u0026#34; # ä»é…ç½®æ–‡ä»¶è§£æèŠ‚ç‚¹ä¿¡æ¯ local node_line node_line=$(grep \u0026#34;^$node_id=\u0026#34; \u0026#34;$CONFIG_FILE\u0026#34;) if [[ -n \u0026#34;$node_line\u0026#34; ]]; then host_ref=$(echo \u0026#34;$node_line\u0026#34; | cut -d\u0026#39;=\u0026#39; -f2 | cut -d\u0026#39;:\u0026#39; -f1) port_ref=$(echo \u0026#34;$node_line\u0026#34; | cut -d\u0026#39;=\u0026#39; -f2 | cut -d\u0026#39;:\u0026#39; -f2) fi } # è·å–åˆ†ç‰‡å‰¯æœ¬èŠ‚ç‚¹ get_shard_replicas() { local shard_id=\u0026#34;$1\u0026#34; grep \u0026#34;^shard_${shard_id}_replicas=\u0026#34; \u0026#34;$CONFIG_FILE\u0026#34; | cut -d\u0026#39;=\u0026#39; -f2 } # æ›´æ–°é…ç½®æ–‡ä»¶ä¸­çš„ä¸»èŠ‚ç‚¹ update_config_primary() { local shard_id=\u0026#34;$1\u0026#34; local new_primary=\u0026#34;$2\u0026#34; sed -i \u0026#34;s/^shard_${shard_id}_primary=.*/shard_${shard_id}_primary=$new_primary/\u0026#34; \u0026#34;$CONFIG_FILE\u0026#34; } # ç”Ÿæˆå¥åº·æŠ¥å‘Š generate_health_report() { local report_file=\u0026#34;/tmp/cluster_health_$(date +%Y%m%d_%H%M%S).json\u0026#34; cat \u0026gt; \u0026#34;$report_file\u0026#34; \u0026lt;\u0026lt; EOF { \u0026#34;timestamp\u0026#34;: \u0026#34;$(date -Iseconds)\u0026#34;, \u0026#34;cluster_status\u0026#34;: \u0026#34;$(get_cluster_status)\u0026#34;, \u0026#34;nodes\u0026#34;: { EOF local first_node=true for node_id in \u0026#34;${!NODE_STATUS[@]}\u0026#34;; do if [[ \u0026#34;$first_node\u0026#34; == true ]]; then first_node=false else echo \u0026#34;,\u0026#34; \u0026gt;\u0026gt; \u0026#34;$report_file\u0026#34; fi cat \u0026gt;\u0026gt; \u0026#34;$report_file\u0026#34; \u0026lt;\u0026lt; EOF \u0026#34;$node_id\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;${NODE_STATUS[$node_id]}\u0026#34;, \u0026#34;failure_count\u0026#34;: ${NODE_FAILURE_COUNT[$node_id]:-0} } EOF done cat \u0026gt;\u0026gt; \u0026#34;$report_file\u0026#34; \u0026lt;\u0026lt; EOF }, \u0026#34;shards\u0026#34;: { EOF local first_shard=true for shard_id in \u0026#34;${!SHARD_STATUS[@]}\u0026#34;; do if [[ \u0026#34;$first_shard\u0026#34; == true ]]; then first_shard=false else echo \u0026#34;,\u0026#34; \u0026gt;\u0026gt; \u0026#34;$report_file\u0026#34; fi cat \u0026gt;\u0026gt; \u0026#34;$report_file\u0026#34; \u0026lt;\u0026lt; EOF \u0026#34;$shard_id\u0026#34;: { \u0026#34;status\u0026#34;: \u0026#34;${SHARD_STATUS[$shard_id]}\u0026#34; } EOF done cat \u0026gt;\u0026gt; \u0026#34;$report_file\u0026#34; \u0026lt;\u0026lt; EOF } } EOF echo \u0026#34;$report_file\u0026#34; } # è·å–é›†ç¾¤æ•´ä½“çŠ¶æ€ get_cluster_status() { local healthy_nodes=0 local total_nodes=0 local critical_shards=0 for status in \u0026#34;${NODE_STATUS[@]}\u0026#34;; do if [[ \u0026#34;$status\u0026#34; == \u0026#34;healthy\u0026#34; ]]; then ((healthy_nodes++)) fi ((total_nodes++)) done for status in \u0026#34;${SHARD_STATUS[@]}\u0026#34;; do if [[ \u0026#34;$status\u0026#34; == \u0026#34;critical\u0026#34; ]]; then ((critical_shards++)) fi done if [[ $critical_shards -gt 0 ]]; then echo \u0026#34;critical\u0026#34; elif [[ $((healthy_nodes * 100 / total_nodes)) -lt 80 ]]; then echo \u0026#34;degraded\u0026#34; else echo \u0026#34;healthy\u0026#34; fi } # ä¸»ç›‘æ§å¾ªç¯ main_monitor_loop() { log \u0026#34;å¯åŠ¨é›†ç¾¤å¥åº·ç›‘æ§\u0026#34; while true; do log \u0026#34;å¼€å§‹å¥åº·æ£€æŸ¥å¾ªç¯\u0026#34; # æ£€æŸ¥æ‰€æœ‰èŠ‚ç‚¹ while IFS=\u0026#39;=\u0026#39; read -r node_id node_addr; do if [[ -n \u0026#34;$node_id\u0026#34; \u0026amp;\u0026amp; \u0026#34;$node_id\u0026#34; != \\#* ]]; then local host port host=$(echo \u0026#34;$node_addr\u0026#34; | cut -d\u0026#39;:\u0026#39; -f1) port=$(echo \u0026#34;$node_addr\u0026#34; | cut -d\u0026#39;:\u0026#39; -f2) if ! check_node_health \u0026#34;$node_id\u0026#34; \u0026#34;$host\u0026#34; \u0026#34;$port\u0026#34;; then if [[ ${NODE_FAILURE_COUNT[$node_id]} -ge $FAILURE_THRESHOLD ]]; then handle_node_failure \u0026#34;$node_id\u0026#34; fi fi fi done \u0026lt; \u0026lt;(grep \u0026#34;^[^#].*=\u0026#34; \u0026#34;$CONFIG_FILE\u0026#34; | grep -v \u0026#34;shard_\u0026#34;) # æ£€æŸ¥æ‰€æœ‰åˆ†ç‰‡ while IFS=\u0026#39;=\u0026#39; read -r shard_key shard_primary; do if [[ \u0026#34;$shard_key\u0026#34; == shard_*_primary ]]; then local shard_id shard_id=$(echo \u0026#34;$shard_key\u0026#34; | sed \u0026#39;s/shard_\\(.*\\)_primary/\\1/\u0026#39;) local replica_nodes replica_nodes=$(get_shard_replicas \u0026#34;$shard_id\u0026#34;) check_shard_health \u0026#34;$shard_id\u0026#34; \u0026#34;$shard_primary\u0026#34; \u0026#34;$replica_nodes\u0026#34; fi done \u0026lt; \u0026lt;(grep \u0026#34;^shard_.*_primary=\u0026#34; \u0026#34;$CONFIG_FILE\u0026#34;) # ç”Ÿæˆå¥åº·æŠ¥å‘Š local report_file report_file=$(generate_health_report) log \u0026#34;å¥åº·æŠ¥å‘Šå·²ç”Ÿæˆ: $report_file\u0026#34; # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥ sleep \u0026#34;$MONITOR_INTERVAL\u0026#34; done } # ä¿¡å·å¤„ç† cleanup() { log \u0026#34;æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨æ¸…ç†...\u0026#34; exit 0 } trap cleanup SIGTERM SIGINT # ä¸»å‡½æ•° main() { # æ£€æŸ¥ä¾èµ– for cmd in curl timeout systemctl; do if ! command -v \u0026#34;$cmd\u0026#34; \u0026amp;\u0026gt; /dev/null; then error \u0026#34;ç¼ºå°‘ä¾èµ–å‘½ä»¤: $cmd\u0026#34; exit 1 fi done # åŠ è½½é…ç½® load_config # åˆ›å»ºæ—¥å¿—ç›®å½• mkdir -p \u0026#34;$(dirname \u0026#34;$LOG_FILE\u0026#34;)\u0026#34; # å¯åŠ¨ç›‘æ§ main_monitor_loop } # å¦‚æœç›´æ¥æ‰§è¡Œè„šæœ¬ if [[ \u0026#34;${BASH_SOURCE[0]}\u0026#34; == \u0026#34;${0}\u0026#34; ]]; then main \u0026#34;$@\u0026#34; fi æ€§èƒ½ä¼˜åŒ–ä¸è°ƒä¼˜ 1. æŸ¥è¯¢ä¼˜åŒ–å™¨ #!/usr/bin/env python3 # src/optimizer/query_optimizer.py import re import time import logging from typing import Dict, List, Any, Optional, Tuple from dataclasses import dataclass from enum import Enum import sqlparse from sqlparse.sql import Statement, Token from sqlparse.tokens import Keyword, Name class QueryType(Enum): SELECT = \u0026#34;select\u0026#34; INSERT = \u0026#34;insert\u0026#34; UPDATE = \u0026#34;update\u0026#34; DELETE = \u0026#34;delete\u0026#34; JOIN = \u0026#34;join\u0026#34; @dataclass class QueryPlan: query_id: str original_query: str optimized_query: str execution_plan: List[Dict[str, Any]] estimated_cost: float target_shards: List[str] optimization_hints: List[str] @dataclass class TableStats: table_name: str row_count: int avg_row_size: int index_info: Dict[str, Any] partition_info: Dict[str, Any] last_updated: float class DistributedQueryOptimizer: def __init__(self, shard_router, stats_collector): self.shard_router = shard_router self.stats_collector = stats_collector self.logger = self._setup_logging() # æŸ¥è¯¢ç¼“å­˜ self.query_cache: Dict[str, QueryPlan] = {} self.cache_size_limit = 1000 # ç»Ÿè®¡ä¿¡æ¯ç¼“å­˜ self.table_stats: Dict[str, TableStats] = {} self.stats_ttl = 3600 # 1å°æ—¶ # ä¼˜åŒ–è§„åˆ™ self.optimization_rules = [ self._optimize_predicate_pushdown, self._optimize_join_order, self._optimize_index_selection, self._optimize_partition_pruning, self._optimize_aggregation_pushdown ] def _setup_logging(self) -\u0026gt; logging.Logger: \u0026#34;\u0026#34;\u0026#34;è®¾ç½®æ—¥å¿—è®°å½•\u0026#34;\u0026#34;\u0026#34; logger = logging.getLogger(\u0026#39;QueryOptimizer\u0026#39;) logger.setLevel(logging.INFO) handler = logging.StreamHandler() formatter = logging.Formatter( \u0026#39;%(asctime)s - %(name)s - %(levelname)s - %(message)s\u0026#39; ) handler.setFormatter(formatter) logger.addHandler(handler) return logger def optimize_query(self, query: str, query_params: Dict[str, Any] = None) -\u0026gt; QueryPlan: \u0026#34;\u0026#34;\u0026#34;ä¼˜åŒ–æŸ¥è¯¢\u0026#34;\u0026#34;\u0026#34; query_id = self._generate_query_id(query, query_params) # æ£€æŸ¥ç¼“å­˜ if query_id in self.query_cache: self.logger.info(f\u0026#34;ä½¿ç”¨ç¼“å­˜çš„æŸ¥è¯¢è®¡åˆ’: {query_id}\u0026#34;) return self.query_cache[query_id] # è§£ææŸ¥è¯¢ parsed_query = self._parse_query(query) if not parsed_query: return self._create_fallback_plan(query_id, query) # åˆ†ææŸ¥è¯¢ query_analysis = self._analyze_query(parsed_query, query_params) # åº”ç”¨ä¼˜åŒ–è§„åˆ™ optimized_query = query optimization_hints = [] for rule in self.optimization_rules: try: result = rule(parsed_query, query_analysis, query_params) if result: optimized_query = result.get(\u0026#39;query\u0026#39;, optimized_query) optimization_hints.extend(result.get(\u0026#39;hints\u0026#39;, [])) except Exception as e: self.logger.error(f\u0026#34;ä¼˜åŒ–è§„åˆ™æ‰§è¡Œå¤±è´¥: {rule.__name__}, é”™è¯¯: {e}\u0026#34;) # ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ execution_plan = self._generate_execution_plan(optimized_query, query_analysis) # ä¼°ç®—æˆæœ¬ estimated_cost = self._estimate_query_cost(execution_plan, query_analysis) # ç¡®å®šç›®æ ‡åˆ†ç‰‡ target_shards = self._determine_target_shards(query_analysis, query_params) # åˆ›å»ºæŸ¥è¯¢è®¡åˆ’ query_plan = QueryPlan( query_id=query_id, original_query=query, optimized_query=optimized_query, execution_plan=execution_plan, estimated_cost=estimated_cost, target_shards=target_shards, optimization_hints=optimization_hints ) # ç¼“å­˜æŸ¥è¯¢è®¡åˆ’ self._cache_query_plan(query_plan) return query_plan def _parse_query(self, query: str) -\u0026gt; Optional[Statement]: \u0026#34;\u0026#34;\u0026#34;è§£æSQLæŸ¥è¯¢\u0026#34;\u0026#34;\u0026#34; try: parsed = sqlparse.parse(query) return parsed[0] if parsed else None except Exception as e: self.logger.error(f\u0026#34;æŸ¥è¯¢è§£æå¤±è´¥: {e}\u0026#34;) return None def _analyze_query(self, parsed_query: Statement, query_params: Dict[str, Any] = None) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;åˆ†ææŸ¥è¯¢ç»“æ„\u0026#34;\u0026#34;\u0026#34; analysis = { \u0026#39;query_type\u0026#39;: self._get_query_type(parsed_query), \u0026#39;tables\u0026#39;: self._extract_tables(parsed_query), \u0026#39;columns\u0026#39;: self._extract_columns(parsed_query), \u0026#39;where_conditions\u0026#39;: self._extract_where_conditions(parsed_query), \u0026#39;join_conditions\u0026#39;: self._extract_join_conditions(parsed_query), \u0026#39;group_by\u0026#39;: self._extract_group_by(parsed_query), \u0026#39;order_by\u0026#39;: self._extract_order_by(parsed_query), \u0026#39;limit\u0026#39;: self._extract_limit(parsed_query), \u0026#39;aggregations\u0026#39;: self._extract_aggregations(parsed_query) } return analysis def _get_query_type(self, parsed_query: Statement) -\u0026gt; QueryType: \u0026#34;\u0026#34;\u0026#34;è·å–æŸ¥è¯¢ç±»å‹\u0026#34;\u0026#34;\u0026#34; first_token = parsed_query.token_first(skip_ws=True, skip_cm=True) if first_token and first_token.ttype is Keyword: keyword = first_token.value.upper() if keyword == \u0026#39;SELECT\u0026#39;: return QueryType.SELECT elif keyword == \u0026#39;INSERT\u0026#39;: return QueryType.INSERT elif keyword == \u0026#39;UPDATE\u0026#39;: return QueryType.UPDATE elif keyword == \u0026#39;DELETE\u0026#39;: return QueryType.DELETE return QueryType.SELECT def _extract_tables(self, parsed_query: Statement) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;æå–è¡¨å\u0026#34;\u0026#34;\u0026#34; tables = [] # ç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„è§£æé€»è¾‘ query_str = str(parsed_query).upper() # æŸ¥æ‰¾FROMå­å¥ä¸­çš„è¡¨å from_match = re.search(r\u0026#39;FROM\\s+(\\w+)\u0026#39;, query_str) if from_match: tables.append(from_match.group(1).lower()) # æŸ¥æ‰¾JOINå­å¥ä¸­çš„è¡¨å join_matches = re.findall(r\u0026#39;JOIN\\s+(\\w+)\u0026#39;, query_str) for match in join_matches: tables.append(match.lower()) return list(set(tables)) def _extract_columns(self, parsed_query: Statement) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;æå–åˆ—å\u0026#34;\u0026#34;\u0026#34; columns = [] # ç®€åŒ–å®ç° query_str = str(parsed_query) # æå–SELECTå­å¥ä¸­çš„åˆ—å select_match = re.search(r\u0026#39;SELECT\\s+(.*?)\\s+FROM\u0026#39;, query_str, re.IGNORECASE | re.DOTALL) if select_match: select_clause = select_match.group(1) # ç®€å•åˆ†å‰²ï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„è§£æ for col in select_clause.split(\u0026#39;,\u0026#39;): col = col.strip() if col and col != \u0026#39;*\u0026#39;: columns.append(col) return columns def _extract_where_conditions(self, parsed_query: Statement) -\u0026gt; List[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;æå–WHEREæ¡ä»¶\u0026#34;\u0026#34;\u0026#34; conditions = [] query_str = str(parsed_query) where_match = re.search(r\u0026#39;WHERE\\s+(.*?)(?:\\s+GROUP\\s+BY|\\s+ORDER\\s+BY|\\s+LIMIT|$)\u0026#39;, query_str, re.IGNORECASE | re.DOTALL) if where_match: where_clause = where_match.group(1).strip() # ç®€åŒ–è§£æï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„é€»è¾‘ # æŸ¥æ‰¾ç­‰å€¼æ¡ä»¶ eq_conditions = re.findall(r\u0026#39;(\\w+)\\s*=\\s*[\\\u0026#39;\u0026#34;]?([^\\\u0026#39;\u0026#34;\\s]+)[\\\u0026#39;\u0026#34;]?\u0026#39;, where_clause) for column, value in eq_conditions: conditions.append({ \u0026#39;column\u0026#39;: column, \u0026#39;operator\u0026#39;: \u0026#39;=\u0026#39;, \u0026#39;value\u0026#39;: value, \u0026#39;type\u0026#39;: \u0026#39;equality\u0026#39; }) # æŸ¥æ‰¾èŒƒå›´æ¡ä»¶ range_conditions = re.findall(r\u0026#39;(\\w+)\\s*(\u0026gt;|\u0026lt;|\u0026gt;=|\u0026lt;=)\\s*[\\\u0026#39;\u0026#34;]?([^\\\u0026#39;\u0026#34;\\s]+)[\\\u0026#39;\u0026#34;]?\u0026#39;, where_clause) for column, operator, value in range_conditions: conditions.append({ \u0026#39;column\u0026#39;: column, \u0026#39;operator\u0026#39;: operator, \u0026#39;value\u0026#39;: value, \u0026#39;type\u0026#39;: \u0026#39;range\u0026#39; }) return conditions def _extract_join_conditions(self, parsed_query: Statement) -\u0026gt; List[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;æå–JOINæ¡ä»¶\u0026#34;\u0026#34;\u0026#34; joins = [] query_str = str(parsed_query) join_matches = re.findall(r\u0026#39;(\\w+\\s+)?JOIN\\s+(\\w+)\\s+ON\\s+(.*?)(?:\\s+(?:INNER|LEFT|RIGHT|FULL)\\s+JOIN|\\s+WHERE|\\s+GROUP\\s+BY|\\s+ORDER\\s+BY|$)\u0026#39;, query_str, re.IGNORECASE) for join_type, table, condition in join_matches: joins.append({ \u0026#39;type\u0026#39;: join_type.strip() if join_type else \u0026#39;INNER\u0026#39;, \u0026#39;table\u0026#39;: table, \u0026#39;condition\u0026#39;: condition.strip() }) return joins def _extract_group_by(self, parsed_query: Statement) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;æå–GROUP BYåˆ—\u0026#34;\u0026#34;\u0026#34; query_str = str(parsed_query) group_match = re.search(r\u0026#39;GROUP\\s+BY\\s+(.*?)(?:\\s+ORDER\\s+BY|\\s+LIMIT|$)\u0026#39;, query_str, re.IGNORECASE) if group_match: group_clause = group_match.group(1).strip() return [col.strip() for col in group_clause.split(\u0026#39;,\u0026#39;)] return [] def _extract_order_by(self, parsed_query: Statement) -\u0026gt; List[Dict[str, str]]: \u0026#34;\u0026#34;\u0026#34;æå–ORDER BYåˆ—\u0026#34;\u0026#34;\u0026#34; query_str = str(parsed_query) order_match = re.search(r\u0026#39;ORDER\\s+BY\\s+(.*?)(?:\\s+LIMIT|$)\u0026#39;, query_str, re.IGNORECASE) order_columns = [] if order_match: order_clause = order_match.group(1).strip() for item in order_clause.split(\u0026#39;,\u0026#39;): item = item.strip() if \u0026#39; DESC\u0026#39; in item.upper(): column = item.replace(\u0026#39; DESC\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39; desc\u0026#39;, \u0026#39;\u0026#39;).strip() order_columns.append({\u0026#39;column\u0026#39;: column, \u0026#39;direction\u0026#39;: \u0026#39;DESC\u0026#39;}) else: column = item.replace(\u0026#39; ASC\u0026#39;, \u0026#39;\u0026#39;).replace(\u0026#39; asc\u0026#39;, \u0026#39;\u0026#39;).strip() order_columns.append({\u0026#39;column\u0026#39;: column, \u0026#39;direction\u0026#39;: \u0026#39;ASC\u0026#39;}) return order_columns def _extract_limit(self, parsed_query: Statement) -\u0026gt; Optional[int]: \u0026#34;\u0026#34;\u0026#34;æå–LIMITå€¼\u0026#34;\u0026#34;\u0026#34; query_str = str(parsed_query) limit_match = re.search(r\u0026#39;LIMIT\\s+(\\d+)\u0026#39;, query_str, re.IGNORECASE) if limit_match: return int(limit_match.group(1)) return None def _extract_aggregations(self, parsed_query: Statement) -\u0026gt; List[Dict[str, str]]: \u0026#34;\u0026#34;\u0026#34;æå–èšåˆå‡½æ•°\u0026#34;\u0026#34;\u0026#34; aggregations = [] query_str = str(parsed_query) # æŸ¥æ‰¾èšåˆå‡½æ•° agg_matches = re.findall(r\u0026#39;(COUNT|SUM|AVG|MIN|MAX)\\s*\\(\\s*([^)]+)\\s*\\)\u0026#39;, query_str, re.IGNORECASE) for func, column in agg_matches: aggregations.append({ \u0026#39;function\u0026#39;: func.upper(), \u0026#39;column\u0026#39;: column.strip() }) return aggregations def _optimize_predicate_pushdown(self, parsed_query: Statement, analysis: Dict[str, Any], query_params: Dict[str, Any] = None) -\u0026gt; Optional[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;è°“è¯ä¸‹æ¨ä¼˜åŒ–\u0026#34;\u0026#34;\u0026#34; hints = [] # æ£€æŸ¥WHEREæ¡ä»¶æ˜¯å¦å¯ä»¥ä¸‹æ¨åˆ°åˆ†ç‰‡ where_conditions = analysis.get(\u0026#39;where_conditions\u0026#39;, []) tables = analysis.get(\u0026#39;tables\u0026#39;, []) for condition in where_conditions: column = condition[\u0026#39;column\u0026#39;] # æ£€æŸ¥æ˜¯å¦ä¸ºåˆ†ç‰‡é”® for table in tables: if table in self.shard_router.shard_keys: shard_keys = self.shard_router.shard_keys[table] if column in shard_keys: hints.append(f\u0026#34;è°“è¯ä¸‹æ¨: {column} æ¡ä»¶å¯ä»¥ä¸‹æ¨åˆ°åˆ†ç‰‡çº§åˆ«\u0026#34;) return {\u0026#39;hints\u0026#39;: hints} if hints else None def _optimize_join_order(self, parsed_query: Statement, analysis: Dict[str, Any], query_params: Dict[str, Any] = None) -\u0026gt; Optional[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;JOINé¡ºåºä¼˜åŒ–\u0026#34;\u0026#34;\u0026#34; hints = [] joins = analysis.get(\u0026#39;join_conditions\u0026#39;, []) if len(joins) \u0026gt; 1: # è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯ tables = analysis.get(\u0026#39;tables\u0026#39;, []) table_sizes = {} for table in tables: stats = self._get_table_stats(table) if stats: table_sizes[table] = stats.row_count # å»ºè®®å°è¡¨åœ¨å‰çš„JOINé¡ºåº if table_sizes: sorted_tables = sorted(table_sizes.items(), key=lambda x: x[1]) hints.append(f\u0026#34;å»ºè®®JOINé¡ºåº: {\u0026#39; -\u0026gt; \u0026#39;.join([t[0] for t in sorted_tables])}\u0026#34;) return {\u0026#39;hints\u0026#39;: hints} if hints else None def _optimize_index_selection(self, parsed_query: Statement, analysis: Dict[str, Any], query_params: Dict[str, Any] = None) -\u0026gt; Optional[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;ç´¢å¼•é€‰æ‹©ä¼˜åŒ–\u0026#34;\u0026#34;\u0026#34; hints = [] where_conditions = analysis.get(\u0026#39;where_conditions\u0026#39;, []) tables = analysis.get(\u0026#39;tables\u0026#39;, []) for table in tables: stats = self._get_table_stats(table) if not stats: continue # æ£€æŸ¥WHEREæ¡ä»¶ä¸­çš„åˆ—æ˜¯å¦æœ‰ç´¢å¼• for condition in where_conditions: column = condition[\u0026#39;column\u0026#39;] if column in stats.index_info: index_info = stats.index_info[column] hints.append(f\u0026#34;ä½¿ç”¨ç´¢å¼•: {table}.{column} ({index_info.get(\u0026#39;type\u0026#39;, \u0026#39;unknown\u0026#39;)})\u0026#34;) else: hints.append(f\u0026#34;å»ºè®®åˆ›å»ºç´¢å¼•: {table}.{column}\u0026#34;) return {\u0026#39;hints\u0026#39;: hints} if hints else None def _optimize_partition_pruning(self, parsed_query: Statement, analysis: Dict[str, Any], query_params: Dict[str, Any] = None) -\u0026gt; Optional[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;åˆ†åŒºè£å‰ªä¼˜åŒ–\u0026#34;\u0026#34;\u0026#34; hints = [] where_conditions = analysis.get(\u0026#39;where_conditions\u0026#39;, []) tables = analysis.get(\u0026#39;tables\u0026#39;, []) for table in tables: stats = self._get_table_stats(table) if not stats or not stats.partition_info: continue partition_column = stats.partition_info.get(\u0026#39;column\u0026#39;) if not partition_column: continue # æ£€æŸ¥WHEREæ¡ä»¶ä¸­æ˜¯å¦åŒ…å«åˆ†åŒºåˆ— for condition in where_conditions: if condition[\u0026#39;column\u0026#39;] == partition_column: hints.append(f\u0026#34;åˆ†åŒºè£å‰ª: åŸºäº {partition_column} æ¡ä»¶å¯ä»¥è£å‰ªåˆ†åŒº\u0026#34;) return {\u0026#39;hints\u0026#39;: hints} if hints else None def _optimize_aggregation_pushdown(self, parsed_query: Statement, analysis: Dict[str, Any], query_params: Dict[str, Any] = None) -\u0026gt; Optional[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;èšåˆä¸‹æ¨ä¼˜åŒ–\u0026#34;\u0026#34;\u0026#34; hints = [] aggregations = analysis.get(\u0026#39;aggregations\u0026#39;, []) group_by = analysis.get(\u0026#39;group_by\u0026#39;, []) if aggregations: # æ£€æŸ¥æ˜¯å¦å¯ä»¥å°†èšåˆä¸‹æ¨åˆ°åˆ†ç‰‡ pushdown_possible = True for agg in aggregations: func = agg[\u0026#39;function\u0026#39;] if func not in [\u0026#39;COUNT\u0026#39;, \u0026#39;SUM\u0026#39;, \u0026#39;MIN\u0026#39;, \u0026#39;MAX\u0026#39;]: pushdown_possible = False break if pushdown_possible: hints.append(\u0026#34;èšåˆä¸‹æ¨: å¯ä»¥å°†èšåˆæ“ä½œä¸‹æ¨åˆ°å„ä¸ªåˆ†ç‰‡å¹¶åœ¨åè°ƒèŠ‚ç‚¹åˆå¹¶ç»“æœ\u0026#34;) return {\u0026#39;hints\u0026#39;: hints} if hints else None def _generate_execution_plan(self, query: str, analysis: Dict[str, Any]) -\u0026gt; List[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆæ‰§è¡Œè®¡åˆ’\u0026#34;\u0026#34;\u0026#34; plan = [] query_type = analysis.get(\u0026#39;query_type\u0026#39;, QueryType.SELECT) tables = analysis.get(\u0026#39;tables\u0026#39;, []) joins = analysis.get(\u0026#39;join_conditions\u0026#39;, []) # è¡¨æ‰«ææ­¥éª¤ for table in tables: step = { \u0026#39;step_type\u0026#39;: \u0026#39;table_scan\u0026#39;, \u0026#39;table\u0026#39;: table, \u0026#39;estimated_rows\u0026#39;: self._estimate_table_rows(table, analysis), \u0026#39;cost\u0026#39;: 1.0 } plan.append(step) # JOINæ­¥éª¤ for join in joins: step = { \u0026#39;step_type\u0026#39;: \u0026#39;join\u0026#39;, \u0026#39;join_type\u0026#39;: join[\u0026#39;type\u0026#39;], \u0026#39;table\u0026#39;: join[\u0026#39;table\u0026#39;], \u0026#39;condition\u0026#39;: join[\u0026#39;condition\u0026#39;], \u0026#39;estimated_rows\u0026#39;: 1000, # ç®€åŒ–ä¼°ç®— \u0026#39;cost\u0026#39;: 2.0 } plan.append(step) # èšåˆæ­¥éª¤ if analysis.get(\u0026#39;aggregations\u0026#39;) or analysis.get(\u0026#39;group_by\u0026#39;): step = { \u0026#39;step_type\u0026#39;: \u0026#39;aggregation\u0026#39;, \u0026#39;estimated_rows\u0026#39;: 100, # ç®€åŒ–ä¼°ç®— \u0026#39;cost\u0026#39;: 1.5 } plan.append(step) # æ’åºæ­¥éª¤ if analysis.get(\u0026#39;order_by\u0026#39;): step = { \u0026#39;step_type\u0026#39;: \u0026#39;sort\u0026#39;, \u0026#39;columns\u0026#39;: analysis[\u0026#39;order_by\u0026#39;], \u0026#39;estimated_rows\u0026#39;: 1000, # ç®€åŒ–ä¼°ç®— \u0026#39;cost\u0026#39;: 2.5 } plan.append(step) return plan def _estimate_query_cost(self, execution_plan: List[Dict[str, Any]], analysis: Dict[str, Any]) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;ä¼°ç®—æŸ¥è¯¢æˆæœ¬\u0026#34;\u0026#34;\u0026#34; total_cost = 0.0 for step in execution_plan: step_cost = step.get(\u0026#39;cost\u0026#39;, 1.0) estimated_rows = step.get(\u0026#39;estimated_rows\u0026#39;, 1000) # åŸºäºè¡Œæ•°è°ƒæ•´æˆæœ¬ row_factor = min(estimated_rows / 1000.0, 10.0) # æœ€å¤§10å€ total_cost += step_cost * row_factor return total_cost def _determine_target_shards(self, analysis: Dict[str, Any], query_params: Dict[str, Any] = None) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;ç¡®å®šç›®æ ‡åˆ†ç‰‡\u0026#34;\u0026#34;\u0026#34; tables = analysis.get(\u0026#39;tables\u0026#39;, []) where_conditions = analysis.get(\u0026#39;where_conditions\u0026#39;, []) target_shards = set() for table in tables: # æ„å»ºæŸ¥è¯¢å‚æ•° table_params = {} for condition in where_conditions: table_params[condition[\u0026#39;column\u0026#39;]] = condition[\u0026#39;value\u0026#39;] # å¦‚æœæœ‰æŸ¥è¯¢å‚æ•°ï¼Œåˆå¹¶ if query_params: table_params.update(query_params) # è·¯ç”±åˆ°åˆ†ç‰‡ shards = self.shard_router.route_query(table, table_params) target_shards.update(shards) return list(target_shards) def _get_table_stats(self, table_name: str) -\u0026gt; Optional[TableStats]: \u0026#34;\u0026#34;\u0026#34;è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯\u0026#34;\u0026#34;\u0026#34; if table_name in self.table_stats: stats = self.table_stats[table_name] if time.time() - stats.last_updated \u0026lt; self.stats_ttl: return stats # ä»ç»Ÿè®¡æ”¶é›†å™¨è·å–æœ€æ–°ç»Ÿè®¡ä¿¡æ¯ try: stats_data = self.stats_collector.get_table_stats(table_name) if stats_data: stats = TableStats( table_name=table_name, row_count=stats_data.get(\u0026#39;row_count\u0026#39;, 0), avg_row_size=stats_data.get(\u0026#39;avg_row_size\u0026#39;, 0), index_info=stats_data.get(\u0026#39;index_info\u0026#39;, {}), partition_info=stats_data.get(\u0026#39;partition_info\u0026#39;, {}), last_updated=time.time() ) self.table_stats[table_name] = stats return stats except Exception as e: self.logger.error(f\u0026#34;è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {table_name}, é”™è¯¯: {e}\u0026#34;) return None def _estimate_table_rows(self, table_name: str, analysis: Dict[str, Any]) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;ä¼°ç®—è¡¨è¡Œæ•°\u0026#34;\u0026#34;\u0026#34; stats = self._get_table_stats(table_name) if stats: base_rows = stats.row_count # æ ¹æ®WHEREæ¡ä»¶è°ƒæ•´ä¼°ç®— where_conditions = analysis.get(\u0026#39;where_conditions\u0026#39;, []) selectivity = 1.0 for condition in where_conditions: if condition[\u0026#39;type\u0026#39;] == \u0026#39;equality\u0026#39;: selectivity *= 0.1 # ç­‰å€¼æ¡ä»¶é€‰æ‹©æ€§10% elif condition[\u0026#39;type\u0026#39;] == \u0026#39;range\u0026#39;: selectivity *= 0.3 # èŒƒå›´æ¡ä»¶é€‰æ‹©æ€§30% return int(base_rows * selectivity) return 1000 # é»˜è®¤ä¼°ç®— def _generate_query_id(self, query: str, query_params: Dict[str, Any] = None) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆæŸ¥è¯¢ID\u0026#34;\u0026#34;\u0026#34; import hashlib content = query if query_params: content += str(sorted(query_params.items())) return hashlib.md5(content.encode()).hexdigest()[:16] def _create_fallback_plan(self, query_id: str, query: str) -\u0026gt; QueryPlan: \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºå›é€€æŸ¥è¯¢è®¡åˆ’\u0026#34;\u0026#34;\u0026#34; return QueryPlan( query_id=query_id, original_query=query, optimized_query=query, execution_plan=[{ \u0026#39;step_type\u0026#39;: \u0026#39;fallback\u0026#39;, \u0026#39;estimated_rows\u0026#39;: 1000, \u0026#39;cost\u0026#39;: 10.0 }], estimated_cost=10.0, target_shards=[], optimization_hints=[\u0026#39;æŸ¥è¯¢è§£æå¤±è´¥ï¼Œä½¿ç”¨å›é€€è®¡åˆ’\u0026#39;] ) def _cache_query_plan(self, query_plan: QueryPlan): \u0026#34;\u0026#34;\u0026#34;ç¼“å­˜æŸ¥è¯¢è®¡åˆ’\u0026#34;\u0026#34;\u0026#34; if len(self.query_cache) \u0026gt;= self.cache_size_limit: # ç§»é™¤æœ€æ—§çš„ç¼“å­˜é¡¹ oldest_key = next(iter(self.query_cache)) del self.query_cache[oldest_key] self.query_cache[query_plan.query_id] = query_plan def get_cache_stats(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯\u0026#34;\u0026#34;\u0026#34; return { \u0026#39;cache_size\u0026#39;: len(self.query_cache), \u0026#39;cache_limit\u0026#39;: self.cache_size_limit, \u0026#39;hit_rate\u0026#39;: 0.85 # ç®€åŒ–å®ç° } def main(): # ç¤ºä¾‹ç”¨æ³• from sharding.shard_router import ShardRouter class MockStatsCollector: def get_table_stats(self, table_name): return { \u0026#39;row_count\u0026#39;: 100000, \u0026#39;avg_row_size\u0026#39;: 256, \u0026#39;index_info\u0026#39;: { \u0026#39;user_id\u0026#39;: {\u0026#39;type\u0026#39;: \u0026#39;btree\u0026#39;}, \u0026#39;email\u0026#39;: {\u0026#39;type\u0026#39;: \u0026#39;hash\u0026#39;} }, \u0026#39;partition_info\u0026#39;: { \u0026#39;column\u0026#39;: \u0026#39;created_date\u0026#39;, \u0026#39;type\u0026#39;: \u0026#39;range\u0026#39; } } # åˆ›å»ºä¼˜åŒ–å™¨ router = ShardRouter(\u0026#39;config/distributed_database.yaml\u0026#39;) stats_collector = MockStatsCollector() optimizer = DistributedQueryOptimizer(router, stats_collector) # æµ‹è¯•æŸ¥è¯¢ä¼˜åŒ– test_queries = [ \u0026#34;SELECT * FROM users WHERE user_id = 12345\u0026#34;, \u0026#34;SELECT u.name, o.total FROM users u JOIN orders o ON u.user_id = o.user_id WHERE u.user_id = 12345\u0026#34;, \u0026#34;SELECT category_id, COUNT(*) FROM products GROUP BY category_id ORDER BY COUNT(*) DESC LIMIT 10\u0026#34; ] for query in test_queries: print(f\u0026#34;\\nåŸå§‹æŸ¥è¯¢: {query}\u0026#34;) plan = optimizer.optimize_query(query) print(f\u0026#34;æŸ¥è¯¢ID: {plan.query_id}\u0026#34;) print(f\u0026#34;ä¼˜åŒ–åæŸ¥è¯¢: {plan.optimized_query}\u0026#34;) print(f\u0026#34;ç›®æ ‡åˆ†ç‰‡: {plan.target_shards}\u0026#34;) print(f\u0026#34;ä¼°ç®—æˆæœ¬: {plan.estimated_cost:.2f}\u0026#34;) print(\u0026#34;ä¼˜åŒ–å»ºè®®:\u0026#34;) for hint in plan.optimization_hints: print(f\u0026#34; - {hint}\u0026#34;) print(\u0026#34;æ‰§è¡Œè®¡åˆ’:\u0026#34;) for i, step in enumerate(plan.execution_plan): print(f\u0026#34; {i+1}. {step[\u0026#39;step_type\u0026#39;]}: ä¼°ç®—è¡Œæ•°={step[\u0026#39;estimated_rows\u0026#39;]}, æˆæœ¬={step[\u0026#39;cost\u0026#39;]}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() æ€»ç»“ åˆ†å¸ƒå¼æ•°æ®åº“æ¶æ„è®¾è®¡æ˜¯ä¸€ä¸ªå¤æ‚çš„ç³»ç»Ÿå·¥ç¨‹ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘æ•°æ®åˆ†ç‰‡ã€ä¸€è‡´æ€§ä¿è¯ã€æ•…éšœæ¢å¤ã€æ€§èƒ½ä¼˜åŒ–ç­‰å¤šä¸ªæ–¹é¢ã€‚æœ¬æ–‡æä¾›çš„å®Œæ•´æ–¹æ¡ˆåŒ…æ‹¬ï¼š\n","content":"åˆ†å¸ƒå¼æ•°æ®åº“æ¶æ„è®¾è®¡ä¸å®è·µï¼šä»åˆ†ç‰‡ç­–ç•¥åˆ°ä¸€è‡´æ€§ä¿è¯çš„å®Œæ•´æ–¹æ¡ˆ å¼•è¨€ éšç€æ•°æ®é‡çš„çˆ†ç‚¸å¼å¢é•¿å’Œä¸šåŠ¡å¤æ‚åº¦çš„ä¸æ–­æå‡ï¼Œä¼ ç»Ÿçš„å•æœºæ•°æ®åº“å·²ç»æ— æ³•æ»¡è¶³ç°ä»£åº”ç”¨çš„éœ€æ±‚ã€‚åˆ†å¸ƒå¼æ•°æ®åº“ä½œä¸ºè§£å†³å¤§è§„æ¨¡æ•°æ®å­˜å‚¨å’Œå¤„ç†çš„å…³é”®æŠ€æœ¯ï¼Œåœ¨ä¿è¯é«˜å¯ç”¨æ€§ã€å¯æ‰©å±•æ€§å’Œä¸€è‡´æ€§æ–¹é¢å‘æŒ¥ç€é‡è¦ä½œç”¨ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨åˆ†å¸ƒå¼æ•°æ®åº“çš„æ¶æ„è®¾è®¡åŸç†å’Œå®è·µæ–¹æ¡ˆã€‚\nåˆ†å¸ƒå¼æ•°æ®åº“æ¶æ„æ¦‚è¿° 1. æ ¸å¿ƒæ¶æ„ç»„ä»¶ graph TB subgraph \u0026amp;#34;å®¢æˆ·ç«¯å±‚\u0026amp;#34; A[åº”ç”¨ç¨‹åº] --\u0026amp;gt; B[æ•°æ®åº“ä»£ç†] B --\u0026amp;gt; C[è¿æ¥æ± ç®¡ç†å™¨] end subgraph \u0026amp;#34;è·¯ç”±å±‚\u0026amp;#34; C --\u0026amp;gt; D[åˆ†ç‰‡è·¯ç”±å™¨] D --\u0026amp;gt; E[è´Ÿè½½å‡è¡¡å™¨] E --\u0026amp;gt; F[æŸ¥è¯¢ä¼˜åŒ–å™¨] end subgraph \u0026amp;#34;æ•°æ®å±‚\u0026amp;#34; F --\u0026amp;gt; G[åˆ†ç‰‡1] F --\u0026amp;gt; H[åˆ†ç‰‡2] F --\u0026amp;gt; I[åˆ†ç‰‡3] F --\u0026amp;gt; J[åˆ†ç‰‡N] end subgraph \u0026amp;#34;å…ƒæ•°æ®å±‚\u0026amp;#34; K[é…ç½®ä¸­å¿ƒ] --\u0026amp;gt; D L[åˆ†ç‰‡æ˜ å°„è¡¨] --\u0026amp;gt; D M[èŠ‚ç‚¹çŠ¶æ€ç›‘æ§] â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["æ•°æ®åº“","åˆ†å¸ƒå¼ç³»ç»Ÿ","æ¶æ„è®¾è®¡","æ•°æ®åˆ†ç‰‡","ä¸€è‡´æ€§","é«˜å¯ç”¨"],"categories":["æ•°æ®åº“æŠ€æœ¯"],"author":"æ•°æ®åº“æ¶æ„å¸ˆ","readingTime":28,"wordCount":5769,"section":"posts","type":"posts","draft":false,"featured":false,"series":null},{"title":"å…³äºæˆ‘","url":"https://www.dishuihengxin.com/about/","summary":"å…³äºæˆ‘ ä½ å¥½ï¼æˆ‘æ˜¯æ»´æ°´æ’å¿ƒï¼Œä¸€åä¸“æ³¨äºäº‘åŸç”Ÿæ¶æ„å’Œ DevOps å®è·µçš„æŠ€æœ¯ä¸“å®¶ã€‚\næˆ‘çš„æŠ€æœ¯ä¹‹è·¯ ä»æ•°æ®åº“å·¥ç¨‹å¸ˆèµ·æ­¥ï¼Œé€æ­¥æ·±å…¥åˆ°äº‘åŸç”Ÿæ¶æ„å’Œ DevOps é¢†åŸŸã€‚å¤šå¹´çš„å®æˆ˜ç»éªŒè®©æˆ‘åœ¨æ•°æ®åº“ä¼˜åŒ–ã€å®¹å™¨ç¼–æ’ã€CI/CD æµç¨‹è®¾è®¡ç­‰æ–¹é¢ç§¯ç´¯äº†ä¸°å¯Œçš„ç»éªŒã€‚\næ ¸å¿ƒèƒ½åŠ› äº‘åŸç”Ÿæ¶æ„ï¼šç²¾é€š Kubernetes é›†ç¾¤ç®¡ç†ï¼Œæ“…é•¿å¾®æœåŠ¡æ¶æ„è®¾è®¡å’ŒæœåŠ¡ç½‘æ ¼å®æ–½ æ•°æ®åº“æŠ€æœ¯ï¼šæ·±å…¥ç†è§£ PostgreSQL/MySQL å†…æ ¸ï¼Œå…·å¤‡å¤§è§„æ¨¡æ•°æ®åº“ä¼˜åŒ–å’Œé«˜å¯ç”¨æ¶æ„è®¾è®¡èƒ½åŠ› DevOps å®è·µï¼šç†Ÿç»ƒæŒæ¡ CI/CDã€åŸºç¡€è®¾æ–½å³ä»£ç ï¼ˆIaCï¼‰ã€è‡ªåŠ¨åŒ–è¿ç»´ç­‰å®è·µ ç³»ç»Ÿæ¶æ„ï¼šå…·å¤‡å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡ç»éªŒï¼Œæ³¨é‡é«˜å¯ç”¨ã€é«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§ æŠ€æœ¯ç†å¿µ æŒç»­å­¦ä¹ ï¼šæŠ€æœ¯æ—¥æ–°æœˆå¼‚ï¼Œä¿æŒå­¦ä¹ æ˜¯ä¿æŒç«äº‰åŠ›çš„å…³é”® å®è·µä¸ºå…ˆï¼šç†è®ºç»“åˆå®è·µï¼Œæ³¨é‡è§£å†³å®é™…é—®é¢˜ çŸ¥è¯†åˆ†äº«ï¼šé€šè¿‡åšå®¢å’ŒæŠ€æœ¯æ¼”è®²åˆ†äº«ç»éªŒï¼Œå¸®åŠ©ä»–äººæˆé•¿ å¼€æºè´¡çŒ®ï¼šç§¯æå‚ä¸å¼€æºç¤¾åŒºï¼Œå›é¦ˆæŠ€æœ¯ç”Ÿæ€ ä¸“ä¸šè®¤è¯ æˆ‘æŒæœ‰å¤šé¡¹ä¸šç•Œè®¤å¯çš„ä¸“ä¸šè®¤è¯ï¼ŒåŒ…æ‹¬ï¼š\nAWS è®¤è¯è§£å†³æ–¹æ¡ˆæ¶æ„å¸ˆ - ä¸“ä¸šçº§ Certified Kubernetes Administrator (CKA) Certified Kubernetes Application Developer (CKAD) PostgreSQL è®¤è¯ä¸“å®¶ é˜¿é‡Œäº‘äº‘è®¡ç®—ä¸“ä¸šè®¤è¯ DevOps å·¥ç¨‹å¸ˆè®¤è¯ æˆ‘çš„åšå®¢ åœ¨è¿™ä¸ªåšå®¢ä¸­ï¼Œæˆ‘åˆ†äº«å…³äºäº‘åŸç”Ÿã€æ•°æ®åº“ã€DevOps ç­‰é¢†åŸŸçš„æŠ€æœ¯æ–‡ç« å’Œå®æˆ˜ç»éªŒã€‚ä¸»è¦å†…å®¹åŒ…æ‹¬ï¼š\næ•°æ®åº“æŠ€æœ¯ï¼šPostgreSQL/MySQL ä¼˜åŒ–ã€é«˜å¯ç”¨æ¶æ„ã€åˆ†å¸ƒå¼æ•°æ®åº“ äº‘åŸç”Ÿï¼šKubernetes å®è·µã€å®¹å™¨åŒ–ã€å¾®æœåŠ¡æ¶æ„ DevOpsï¼šCI/CDã€è‡ªåŠ¨åŒ–è¿ç»´ã€åŸºç¡€è®¾æ–½å³ä»£ç  ç³»ç»Ÿæ¶æ„ï¼šåˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡ã€é«˜å¯ç”¨æ–¹æ¡ˆã€æ€§èƒ½ä¼˜åŒ– è”ç³»æ–¹å¼ å¦‚æœä½ å¯¹æŠ€æœ¯äº¤æµã€é¡¹ç›®åˆä½œæˆ–å’¨è¯¢æœåŠ¡æ„Ÿå…´è¶£ï¼Œæ¬¢è¿é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»æˆ‘ï¼š\nEmail: é€šè¿‡ç½‘ç«™è¡¨å•è”ç³» GitHub: å…³æ³¨é¡¹ç›®è·å–æœ€æ–°æ›´æ–° å¾®ä¿¡: æ‰«æé¡µé¢äºŒç»´ç æ·»åŠ  æœŸå¾…ä¸ä½ äº¤æµï¼\n","content":"å…³äºæˆ‘ ä½ å¥½ï¼æˆ‘æ˜¯æ»´æ°´æ’å¿ƒï¼Œä¸€åä¸“æ³¨äºäº‘åŸç”Ÿæ¶æ„å’Œ DevOps å®è·µçš„æŠ€æœ¯ä¸“å®¶ã€‚\næˆ‘çš„æŠ€æœ¯ä¹‹è·¯ ä»æ•°æ®åº“å·¥ç¨‹å¸ˆèµ·æ­¥ï¼Œé€æ­¥æ·±å…¥åˆ°äº‘åŸç”Ÿæ¶æ„å’Œ DevOps é¢†åŸŸã€‚å¤šå¹´çš„å®æˆ˜ç»éªŒè®©æˆ‘åœ¨æ•°æ®åº“ä¼˜åŒ–ã€å®¹å™¨ç¼–æ’ã€CI/CD æµç¨‹è®¾è®¡ç­‰æ–¹é¢ç§¯ç´¯äº†ä¸°å¯Œçš„ç»éªŒã€‚\næ ¸å¿ƒèƒ½åŠ› äº‘åŸç”Ÿæ¶æ„ï¼šç²¾é€š Kubernetes é›†ç¾¤ç®¡ç†ï¼Œæ“…é•¿å¾®æœåŠ¡æ¶æ„è®¾è®¡å’ŒæœåŠ¡ç½‘æ ¼å®æ–½ æ•°æ®åº“æŠ€æœ¯ï¼šæ·±å…¥ç†è§£ PostgreSQL/MySQL å†…æ ¸ï¼Œå…·å¤‡å¤§è§„æ¨¡æ•°æ®åº“ä¼˜åŒ–å’Œé«˜å¯ç”¨æ¶æ„è®¾è®¡èƒ½åŠ› DevOps å®è·µï¼šç†Ÿç»ƒæŒæ¡ CI/CDã€åŸºç¡€è®¾æ–½å³ä»£ç ï¼ˆIaCï¼‰ã€è‡ªåŠ¨åŒ–è¿ç»´ç­‰å®è·µ ç³»ç»Ÿæ¶æ„ï¼šå…·å¤‡å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿè®¾è®¡ç»éªŒï¼Œæ³¨é‡é«˜å¯ç”¨ã€é«˜æ€§èƒ½å’Œå¯æ‰©å±•æ€§ æŠ€æœ¯ç†å¿µ æŒç»­å­¦ä¹ ï¼šæŠ€æœ¯æ—¥æ–°æœˆå¼‚ï¼Œä¿æŒå­¦ä¹ æ˜¯ä¿æŒç«äº‰åŠ›çš„å…³é”® å®è·µä¸ºå…ˆï¼šç†è®ºç»“åˆå®è·µï¼Œæ³¨é‡è§£å†³å®é™…é—®é¢˜ çŸ¥è¯†åˆ†äº«ï¼šé€šè¿‡åšå®¢å’ŒæŠ€æœ¯æ¼”è®²åˆ†äº«ç»éªŒï¼Œå¸®åŠ©ä»–äººæˆé•¿ å¼€æºè´¡çŒ®ï¼šç§¯æå‚ä¸å¼€æºç¤¾åŒºï¼Œå›é¦ˆæŠ€æœ¯ç”Ÿæ€ ä¸“ä¸šè®¤è¯ æˆ‘æŒæœ‰å¤šé¡¹ä¸šç•Œè®¤å¯çš„ä¸“ä¸šè®¤è¯ï¼ŒåŒ…æ‹¬ï¼š\nAWS è®¤è¯è§£å†³æ–¹æ¡ˆæ¶æ„å¸ˆ - ä¸“ä¸šçº§ Certified Kubernetes Administrator (CKA) Certified â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":null,"categories":null,"author":"åšä¸»","readingTime":1,"wordCount":63,"section":"","type":"about","draft":false,"featured":false,"series":null},{"title":"åŸºç¡€è®¾æ–½å³ä»£ç (IaC)å®è·µï¼šTerraform + Ansible ä¼ä¸šçº§è‡ªåŠ¨åŒ–","url":"https://www.dishuihengxin.com/posts/devops-infrastructure-as-code/","summary":"åŸºç¡€è®¾æ–½å³ä»£ç (IaC)å®è·µï¼šTerraform + Ansible ä¼ä¸šçº§è‡ªåŠ¨åŒ– åŸºç¡€è®¾æ–½å³ä»£ç (Infrastructure as Code, IaC)æ˜¯ç°ä»£äº‘åŸç”Ÿæ¶æ„çš„æ ¸å¿ƒå®è·µä¹‹ä¸€ã€‚é€šè¿‡ä»£ç åŒ–ç®¡ç†åŸºç¡€è®¾æ–½ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°ç¯å¢ƒçš„ä¸€è‡´æ€§ã€å¯é‡å¤æ€§å’Œå¯å®¡è®¡æ€§ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨Terraformå’ŒAnsibleçš„ä¼ä¸šçº§å®è·µã€‚\nIaCæ¶æ„è®¾è®¡ æ•´ä½“æ¶æ„å›¾ graph TB A[å¼€å‘è€…] --\u0026gt; B[Git Repository] B --\u0026gt; C[CI/CD Pipeline] C --\u0026gt; D[Terraform Plan] D --\u0026gt; E[Review \u0026amp; Approval] E --\u0026gt; F[Terraform Apply] F --\u0026gt; G[AWS/Azure/GCP] G --\u0026gt; H[Infrastructure] H --\u0026gt; I[Ansible Playbooks] I --\u0026gt; J[Configuration Management] J --\u0026gt; K[Application Deployment] L[State Backend] --\u0026gt; F M[Terraform Modules] --\u0026gt; D N[Ansible Roles] --\u0026gt; I æŠ€æœ¯æ ˆç»„åˆ å±‚çº§ å·¥å…· èŒè´£ åŸºç¡€è®¾æ–½ä¾›åº” Terraform äº‘èµ„æºåˆ›å»ºå’Œç®¡ç† é…ç½®ç®¡ç† Ansible ç³»ç»Ÿé…ç½®å’Œåº”ç”¨éƒ¨ç½² çŠ¶æ€ç®¡ç† Terraform Backend çŠ¶æ€æ–‡ä»¶å­˜å‚¨å’Œé”å®š å¯†é’¥ç®¡ç† HashiCorp Vault æ•æ„Ÿä¿¡æ¯ç®¡ç† ç‰ˆæœ¬æ§åˆ¶ Git ä»£ç ç‰ˆæœ¬ç®¡ç† CI/CD GitLab CI/Jenkins è‡ªåŠ¨åŒ–æ‰§è¡Œ Terraformä¼ä¸šçº§å®è·µ é¡¹ç›®ç»“æ„è®¾è®¡ terraform-infrastructure/ â”œâ”€â”€ environments/ â”‚ â”œâ”€â”€ dev/ â”‚ â”‚ â”œâ”€â”€ main.tf â”‚ â”‚ â”œâ”€â”€ variables.tf â”‚ â”‚ â”œâ”€â”€ outputs.tf â”‚ â”‚ â””â”€â”€ terraform.tfvars â”‚ â”œâ”€â”€ staging/ â”‚ â””â”€â”€ production/ â”œâ”€â”€ modules/ â”‚ â”œâ”€â”€ vpc/ â”‚ â”‚ â”œâ”€â”€ main.tf â”‚ â”‚ â”œâ”€â”€ variables.tf â”‚ â”‚ â”œâ”€â”€ outputs.tf â”‚ â”‚ â””â”€â”€ README.md â”‚ â”œâ”€â”€ eks/ â”‚ â”œâ”€â”€ rds/ â”‚ â””â”€â”€ security-groups/ â”œâ”€â”€ shared/ â”‚ â”œâ”€â”€ backend.tf â”‚ â”œâ”€â”€ providers.tf â”‚ â””â”€â”€ versions.tf â””â”€â”€ scripts/ â”œâ”€â”€ plan.sh â”œâ”€â”€ apply.sh â””â”€â”€ destroy.sh æ ¸å¿ƒé…ç½®æ–‡ä»¶ 1. Provideré…ç½® # shared/providers.tf terraform { required_version = \u0026#34;\u0026gt;= 1.5.0\u0026#34; required_providers { aws = { source = \u0026#34;hashicorp/aws\u0026#34; version = \u0026#34;~\u0026gt; 5.0\u0026#34; } kubernetes = { source = \u0026#34;hashicorp/kubernetes\u0026#34; version = \u0026#34;~\u0026gt; 2.20\u0026#34; } helm = { source = \u0026#34;hashicorp/helm\u0026#34; version = \u0026#34;~\u0026gt; 2.10\u0026#34; } } } provider \u0026#34;aws\u0026#34; { region = var.aws_region default_tags { tags = { Environment = var.environment Project = var.project_name ManagedBy = \u0026#34;Terraform\u0026#34; Owner = var.team_name CostCenter = var.cost_center } } } provider \u0026#34;kubernetes\u0026#34; { host = module.eks.cluster_endpoint cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data) exec { api_version = \u0026#34;client.authentication.k8s.io/v1beta1\u0026#34; command = \u0026#34;aws\u0026#34; args = [\u0026#34;eks\u0026#34;, \u0026#34;get-token\u0026#34;, \u0026#34;--cluster-name\u0026#34;, module.eks.cluster_name] } } 2. åç«¯é…ç½® # shared/backend.tf terraform { backend \u0026#34;s3\u0026#34; { bucket = \u0026#34;company-terraform-state\u0026#34; key = \u0026#34;environments/${var.environment}/terraform.tfstate\u0026#34; region = \u0026#34;us-west-2\u0026#34; encrypt = true dynamodb_table = \u0026#34;terraform-state-lock\u0026#34; # å¯ç”¨çŠ¶æ€æ–‡ä»¶ç‰ˆæœ¬æ§åˆ¶ versioning = true } } # DynamoDBè¡¨ç”¨äºçŠ¶æ€é”å®š resource \u0026#34;aws_dynamodb_table\u0026#34; \u0026#34;terraform_state_lock\u0026#34; { name = \u0026#34;terraform-state-lock\u0026#34; billing_mode = \u0026#34;PAY_PER_REQUEST\u0026#34; hash_key = \u0026#34;LockID\u0026#34; attribute { name = \u0026#34;LockID\u0026#34; type = \u0026#34;S\u0026#34; } tags = { Name = \u0026#34;Terraform State Lock Table\u0026#34; Environment = \u0026#34;shared\u0026#34; } } 3. VPCæ¨¡å— # modules/vpc/main.tf locals { availability_zones = data.aws_availability_zones.available.names public_subnet_cidrs = [ for i, az in local.availability_zones : cidrsubnet(var.vpc_cidr, 8, i + 1) ] private_subnet_cidrs = [ for i, az in local.availability_zones : cidrsubnet(var.vpc_cidr, 8, i + 10) ] database_subnet_cidrs = [ for i, az in local.availability_zones : cidrsubnet(var.vpc_cidr, 8, i + 20) ] } data \u0026#34;aws_availability_zones\u0026#34; \u0026#34;available\u0026#34; { state = \u0026#34;available\u0026#34; } # VPC resource \u0026#34;aws_vpc\u0026#34; \u0026#34;main\u0026#34; { cidr_block = var.vpc_cidr enable_dns_hostnames = true enable_dns_support = true tags = { Name = \u0026#34;${var.project_name}-${var.environment}-vpc\u0026#34; } } # Internet Gateway resource \u0026#34;aws_internet_gateway\u0026#34; \u0026#34;main\u0026#34; { vpc_id = aws_vpc.main.id tags = { Name = \u0026#34;${var.project_name}-${var.environment}-igw\u0026#34; } } # Public Subnets resource \u0026#34;aws_subnet\u0026#34; \u0026#34;public\u0026#34; { count = length(local.availability_zones) vpc_id = aws_vpc.main.id cidr_block = local.public_subnet_cidrs[count.index] availability_zone = local.availability_zones[count.index] map_public_ip_on_launch = true tags = { Name = \u0026#34;${var.project_name}-${var.environment}-public-${count.index + 1}\u0026#34; Type = \u0026#34;Public\u0026#34; \u0026#34;kubernetes.io/role/elb\u0026#34; = \u0026#34;1\u0026#34; } } # Private Subnets resource \u0026#34;aws_subnet\u0026#34; \u0026#34;private\u0026#34; { count = length(local.availability_zones) vpc_id = aws_vpc.main.id cidr_block = local.private_subnet_cidrs[count.index] availability_zone = local.availability_zones[count.index] tags = { Name = \u0026#34;${var.project_name}-${var.environment}-private-${count.index + 1}\u0026#34; Type = \u0026#34;Private\u0026#34; \u0026#34;kubernetes.io/role/internal-elb\u0026#34; = \u0026#34;1\u0026#34; } } # Database Subnets resource \u0026#34;aws_subnet\u0026#34; \u0026#34;database\u0026#34; { count = length(local.availability_zones) vpc_id = aws_vpc.main.id cidr_block = local.database_subnet_cidrs[count.index] availability_zone = local.availability_zones[count.index] tags = { Name = \u0026#34;${var.project_name}-${var.environment}-database-${count.index + 1}\u0026#34; Type = \u0026#34;Database\u0026#34; } } # NAT Gateways resource \u0026#34;aws_eip\u0026#34; \u0026#34;nat\u0026#34; { count = var.enable_nat_gateway ? length(local.availability_zones) : 0 domain = \u0026#34;vpc\u0026#34; depends_on = [aws_internet_gateway.main] tags = { Name = \u0026#34;${var.project_name}-${var.environment}-nat-eip-${count.index + 1}\u0026#34; } } resource \u0026#34;aws_nat_gateway\u0026#34; \u0026#34;main\u0026#34; { count = var.enable_nat_gateway ? length(local.availability_zones) : 0 allocation_id = aws_eip.nat[count.index].id subnet_id = aws_subnet.public[count.index].id tags = { Name = \u0026#34;${var.project_name}-${var.environment}-nat-${count.index + 1}\u0026#34; } depends_on = [aws_internet_gateway.main] } # Route Tables resource \u0026#34;aws_route_table\u0026#34; \u0026#34;public\u0026#34; { vpc_id = aws_vpc.main.id route { cidr_block = \u0026#34;0.0.0.0/0\u0026#34; gateway_id = aws_internet_gateway.main.id } tags = { Name = \u0026#34;${var.project_name}-${var.environment}-public-rt\u0026#34; } } resource \u0026#34;aws_route_table\u0026#34; \u0026#34;private\u0026#34; { count = var.enable_nat_gateway ? length(local.availability_zones) : 1 vpc_id = aws_vpc.main.id dynamic \u0026#34;route\u0026#34; { for_each = var.enable_nat_gateway ? [1] : [] content { cidr_block = \u0026#34;0.0.0.0/0\u0026#34; nat_gateway_id = aws_nat_gateway.main[count.index].id } } tags = { Name = \u0026#34;${var.project_name}-${var.environment}-private-rt-${count.index + 1}\u0026#34; } } # Route Table Associations resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;public\u0026#34; { count = length(aws_subnet.public) subnet_id = aws_subnet.public[count.index].id route_table_id = aws_route_table.public.id } resource \u0026#34;aws_route_table_association\u0026#34; \u0026#34;private\u0026#34; { count = length(aws_subnet.private) subnet_id = aws_subnet.private[count.index].id route_table_id = var.enable_nat_gateway ? aws_route_table.private[count.index].id : aws_route_table.private[0].id } 4. EKSæ¨¡å— # modules/eks/main.tf data \u0026#34;aws_iam_policy_document\u0026#34; \u0026#34;eks_cluster_assume_role\u0026#34; { statement { effect = \u0026#34;Allow\u0026#34; principals { type = \u0026#34;Service\u0026#34; identifiers = [\u0026#34;eks.amazonaws.com\u0026#34;] } actions = [\u0026#34;sts:AssumeRole\u0026#34;] } } resource \u0026#34;aws_iam_role\u0026#34; \u0026#34;eks_cluster\u0026#34; { name = \u0026#34;${var.cluster_name}-cluster-role\u0026#34; assume_role_policy = data.aws_iam_policy_document.eks_cluster_assume_role.json } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;eks_cluster_policy\u0026#34; { policy_arn = \u0026#34;arn:aws:iam::aws:policy/AmazonEKSClusterPolicy\u0026#34; role = aws_iam_role.eks_cluster.name } resource \u0026#34;aws_iam_role_policy_attachment\u0026#34; \u0026#34;eks_vpc_resource_controller\u0026#34; { policy_arn = \u0026#34;arn:aws:iam::aws:policy/AmazonEKSVPCResourceController\u0026#34; role = aws_iam_role.eks_cluster.name } # EKS Cluster resource \u0026#34;aws_eks_cluster\u0026#34; \u0026#34;main\u0026#34; { name = var.cluster_name role_arn = aws_iam_role.eks_cluster.arn version = var.kubernetes_version vpc_config { subnet_ids = concat(var.private_subnet_ids, var.public_subnet_ids) endpoint_private_access = true endpoint_public_access = var.endpoint_public_access public_access_cidrs = var.endpoint_public_access_cidrs security_group_ids = [aws_security_group.eks_cluster.id] } encryption_config { provider { key_arn = aws_kms_key.eks.arn } resources = [\u0026#34;secrets\u0026#34;] } enabled_cluster_log_types = var.cluster_log_types depends_on = [ aws_iam_role_policy_attachment.eks_cluster_policy, aws_iam_role_policy_attachment.eks_vpc_resource_controller, aws_cloudwatch_log_group.eks_cluster, ] tags = var.tags } # CloudWatch Log Group resource \u0026#34;aws_cloudwatch_log_group\u0026#34; \u0026#34;eks_cluster\u0026#34; { name = \u0026#34;/aws/eks/${var.cluster_name}/cluster\u0026#34; retention_in_days = var.cluster_log_retention_days kms_key_id = aws_kms_key.eks.arn tags = var.tags } # KMS Key for EKS resource \u0026#34;aws_kms_key\u0026#34; \u0026#34;eks\u0026#34; { description = \u0026#34;EKS Secret Encryption Key\u0026#34; deletion_window_in_days = 7 enable_key_rotation = true tags = var.tags } resource \u0026#34;aws_kms_alias\u0026#34; \u0026#34;eks\u0026#34; { name = \u0026#34;alias/${var.cluster_name}-eks\u0026#34; target_key_id = aws_kms_key.eks.key_id } # Security Group for EKS Cluster resource \u0026#34;aws_security_group\u0026#34; \u0026#34;eks_cluster\u0026#34; { name_prefix = \u0026#34;${var.cluster_name}-cluster-\u0026#34; vpc_id = var.vpc_id ingress { description = \u0026#34;HTTPS\u0026#34; from_port = 443 to_port = 443 protocol = \u0026#34;tcp\u0026#34; cidr_blocks = [var.vpc_cidr] } egress { from_port = 0 to_port = 0 protocol = \u0026#34;-1\u0026#34; cidr_blocks = [\u0026#34;0.0.0.0/0\u0026#34;] } tags = merge(var.tags, { Name = \u0026#34;${var.cluster_name}-cluster-sg\u0026#34; }) } # Node Groups resource \u0026#34;aws_eks_node_group\u0026#34; \u0026#34;main\u0026#34; { for_each = var.node_groups cluster_name = aws_eks_cluster.main.name node_group_name = each.key node_role_arn = aws_iam_role.eks_node_group.arn subnet_ids = var.private_subnet_ids capacity_type = each.value.capacity_type instance_types = each.value.instance_types ami_type = each.value.ami_type disk_size = each.value.disk_size scaling_config { desired_size = each.value.desired_size max_size = each.value.max_size min_size = each.value.min_size } update_config { max_unavailable_percentage = each.value.max_unavailable_percentage } labels = each.value.labels dynamic \u0026#34;taint\u0026#34; { for_each = each.value.taints content { key = taint.value.key value = taint.value.value effect = taint.value.effect } } depends_on = [ aws_iam_role_policy_attachment.eks_worker_node_policy, aws_iam_role_policy_attachment.eks_cni_policy, aws_iam_role_policy_attachment.eks_container_registry_policy, ] tags = var.tags } ç¯å¢ƒé…ç½® # environments/production/main.tf module \u0026#34;vpc\u0026#34; { source = \u0026#34;../../modules/vpc\u0026#34; project_name = var.project_name environment = var.environment vpc_cidr = var.vpc_cidr enable_nat_gateway = true } module \u0026#34;eks\u0026#34; { source = \u0026#34;../../modules/eks\u0026#34; cluster_name = \u0026#34;${var.project_name}-${var.environment}\u0026#34; kubernetes_version = var.kubernetes_version vpc_id = module.vpc.vpc_id vpc_cidr = module.vpc.vpc_cidr private_subnet_ids = module.vpc.private_subnet_ids public_subnet_ids = module.vpc.public_subnet_ids endpoint_public_access = false endpoint_public_access_cidrs = var.allowed_cidr_blocks node_groups = { general = { capacity_type = \u0026#34;ON_DEMAND\u0026#34; instance_types = [\u0026#34;t3.medium\u0026#34;] ami_type = \u0026#34;AL2_x86_64\u0026#34; disk_size = 50 desired_size = 3 max_size = 10 min_size = 3 max_unavailable_percentage = 25 labels = { role = \u0026#34;general\u0026#34; } taints = [] } spot = { capacity_type = \u0026#34;SPOT\u0026#34; instance_types = [\u0026#34;t3.medium\u0026#34;, \u0026#34;t3.large\u0026#34;] ami_type = \u0026#34;AL2_x86_64\u0026#34; disk_size = 50 desired_size = 2 max_size = 20 min_size = 0 max_unavailable_percentage = 50 labels = { role = \u0026#34;spot\u0026#34; } taints = [ { key = \u0026#34;spot\u0026#34; value = \u0026#34;true\u0026#34; effect = \u0026#34;NO_SCHEDULE\u0026#34; } ] } } tags = local.common_tags } # environments/production/variables.tf variable \u0026#34;project_name\u0026#34; { description = \u0026#34;Name of the project\u0026#34; type = string default = \u0026#34;myapp\u0026#34; } variable \u0026#34;environment\u0026#34; { description = \u0026#34;Environment name\u0026#34; type = string default = \u0026#34;production\u0026#34; } variable \u0026#34;vpc_cidr\u0026#34; { description = \u0026#34;CIDR block for VPC\u0026#34; type = string default = \u0026#34;10.0.0.0/16\u0026#34; } variable \u0026#34;kubernetes_version\u0026#34; { description = \u0026#34;Kubernetes version\u0026#34; type = string default = \u0026#34;1.27\u0026#34; } variable \u0026#34;allowed_cidr_blocks\u0026#34; { description = \u0026#34;CIDR blocks allowed to access EKS API\u0026#34; type = list(string) default = [\u0026#34;10.0.0.0/8\u0026#34;] } # environments/production/terraform.tfvars project_name = \u0026#34;myapp\u0026#34; environment = \u0026#34;production\u0026#34; vpc_cidr = \u0026#34;10.0.0.0/16\u0026#34; kubernetes_version = \u0026#34;1.27\u0026#34; allowed_cidr_blocks = [ \u0026#34;10.0.0.0/8\u0026#34;, \u0026#34;172.16.0.0/12\u0026#34; ] Ansibleé…ç½®ç®¡ç† é¡¹ç›®ç»“æ„ ansible-configuration/ â”œâ”€â”€ inventories/ â”‚ â”œâ”€â”€ dev/ â”‚ â”‚ â”œâ”€â”€ hosts.yml â”‚ â”‚ â””â”€â”€ group_vars/ â”‚ â”œâ”€â”€ staging/ â”‚ â””â”€â”€ production/ â”œâ”€â”€ roles/ â”‚ â”œâ”€â”€ common/ â”‚ â”‚ â”œâ”€â”€ tasks/main.yml â”‚ â”‚ â”œâ”€â”€ handlers/main.yml â”‚ â”‚ â”œâ”€â”€ templates/ â”‚ â”‚ â”œâ”€â”€ files/ â”‚ â”‚ â””â”€â”€ vars/main.yml â”‚ â”œâ”€â”€ docker/ â”‚ â”œâ”€â”€ kubernetes/ â”‚ â””â”€â”€ monitoring/ â”œâ”€â”€ playbooks/ â”‚ â”œâ”€â”€ site.yml â”‚ â”œâ”€â”€ deploy.yml â”‚ â””â”€â”€ maintenance.yml â”œâ”€â”€ group_vars/ â”‚ â”œâ”€â”€ all.yml â”‚ â””â”€â”€ production.yml â””â”€â”€ ansible.cfg æ ¸å¿ƒPlaybook # playbooks/site.yml --- - name: Configure all servers hosts: all become: yes gather_facts: yes pre_tasks: - name: Update package cache package: update_cache: yes when: ansible_os_family in [\u0026#39;Debian\u0026#39;, \u0026#39;RedHat\u0026#39;] roles: - common - security - monitoring - name: Configure Kubernetes nodes hosts: kubernetes become: yes roles: - docker - kubernetes - cni - name: Configure database servers hosts: database become: yes roles: - postgresql - backup - monitoring # roles/common/tasks/main.yml --- - name: Install essential packages package: name: - curl - wget - git - htop - vim - unzip - jq state: present - name: Configure timezone timezone: name: \u0026#34;{{ system_timezone | default(\u0026#39;UTC\u0026#39;) }}\u0026#34; - name: Configure NTP template: src: ntp.conf.j2 dest: /etc/ntp.conf backup: yes notify: restart ntp - name: Create application user user: name: \u0026#34;{{ app_user }}\u0026#34; shell: /bin/bash home: \u0026#34;/home/{{ app_user }}\u0026#34; create_home: yes groups: docker append: yes - name: Configure SSH template: src: sshd_config.j2 dest: /etc/ssh/sshd_config backup: yes validate: sshd -t -f %s notify: restart ssh - name: Configure firewall ufw: rule: \u0026#34;{{ item.rule }}\u0026#34; port: \u0026#34;{{ item.port }}\u0026#34; proto: \u0026#34;{{ item.proto | default(\u0026#39;tcp\u0026#39;) }}\u0026#34; loop: - { rule: \u0026#39;allow\u0026#39;, port: \u0026#39;22\u0026#39; } - { rule: \u0026#39;allow\u0026#39;, port: \u0026#39;80\u0026#39; } - { rule: \u0026#39;allow\u0026#39;, port: \u0026#39;443\u0026#39; } notify: enable ufw # roles/docker/tasks/main.yml --- - name: Add Docker GPG key apt_key: url: https://download.docker.com/linux/ubuntu/gpg state: present - name: Add Docker repository apt_repository: repo: \u0026#34;deb [arch=amd64] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} stable\u0026#34; state: present - name: Install Docker package: name: - docker-ce - docker-ce-cli - containerd.io - docker-compose-plugin state: present - name: Configure Docker daemon template: src: daemon.json.j2 dest: /etc/docker/daemon.json notify: restart docker - name: Start and enable Docker systemd: name: docker state: started enabled: yes - name: Add users to docker group user: name: \u0026#34;{{ item }}\u0026#34; groups: docker append: yes loop: \u0026#34;{{ docker_users }}\u0026#34; # roles/kubernetes/tasks/main.yml --- - name: Add Kubernetes GPG key apt_key: url: https://packages.cloud.google.com/apt/doc/apt-key.gpg state: present - name: Add Kubernetes repository apt_repository: repo: \u0026#34;deb https://apt.kubernetes.io/ kubernetes-xenial main\u0026#34; state: present - name: Install Kubernetes components package: name: - kubelet={{ kubernetes_version }} - kubeadm={{ kubernetes_version }} - kubectl={{ kubernetes_version }} state: present allow_downgrade: yes - name: Hold Kubernetes packages dpkg_selections: name: \u0026#34;{{ item }}\u0026#34; selection: hold loop: - kubelet - kubeadm - kubectl - name: Configure kubelet template: src: kubelet-config.yaml.j2 dest: /var/lib/kubelet/config.yaml notify: restart kubelet - name: Start and enable kubelet systemd: name: kubelet state: started enabled: yes åŠ¨æ€Inventory #!/usr/bin/env python3 # inventories/aws_ec2.py import boto3 import json import sys def get_ec2_instances(): ec2 = boto3.client(\u0026#39;ec2\u0026#39;) response = ec2.describe_instances( Filters=[ {\u0026#39;Name\u0026#39;: \u0026#39;instance-state-name\u0026#39;, \u0026#39;Values\u0026#39;: [\u0026#39;running\u0026#39;]}, {\u0026#39;Name\u0026#39;: \u0026#39;tag:Environment\u0026#39;, \u0026#39;Values\u0026#39;: [sys.argv[1] if len(sys.argv) \u0026gt; 1 else \u0026#39;production\u0026#39;]} ] ) inventory = { \u0026#39;_meta\u0026#39;: { \u0026#39;hostvars\u0026#39;: {} }, \u0026#39;all\u0026#39;: { \u0026#39;children\u0026#39;: [\u0026#39;ungrouped\u0026#39;] } } for reservation in response[\u0026#39;Reservations\u0026#39;]: for instance in reservation[\u0026#39;Instances\u0026#39;]: instance_id = instance[\u0026#39;InstanceId\u0026#39;] private_ip = instance.get(\u0026#39;PrivateIpAddress\u0026#39;, \u0026#39;\u0026#39;) public_ip = instance.get(\u0026#39;PublicIpAddress\u0026#39;, \u0026#39;\u0026#39;) # è·å–æ ‡ç­¾ tags = {tag[\u0026#39;Key\u0026#39;]: tag[\u0026#39;Value\u0026#39;] for tag in instance.get(\u0026#39;Tags\u0026#39;, [])} # ä¸»æœºå˜é‡ hostvars = { \u0026#39;ansible_host\u0026#39;: public_ip or private_ip, \u0026#39;ansible_user\u0026#39;: \u0026#39;ubuntu\u0026#39;, \u0026#39;instance_id\u0026#39;: instance_id, \u0026#39;instance_type\u0026#39;: instance[\u0026#39;InstanceType\u0026#39;], \u0026#39;private_ip\u0026#39;: private_ip, \u0026#39;public_ip\u0026#39;: public_ip, \u0026#39;tags\u0026#39;: tags } inventory[\u0026#39;_meta\u0026#39;][\u0026#39;hostvars\u0026#39;][instance_id] = hostvars # æ ¹æ®æ ‡ç­¾åˆ†ç»„ role = tags.get(\u0026#39;Role\u0026#39;, \u0026#39;ungrouped\u0026#39;) if role not in inventory: inventory[role] = {\u0026#39;hosts\u0026#39;: []} inventory[role][\u0026#39;hosts\u0026#39;].append(instance_id) # æ ¹æ®ç¯å¢ƒåˆ†ç»„ env = tags.get(\u0026#39;Environment\u0026#39;, \u0026#39;unknown\u0026#39;) env_group = f\u0026#34;env_{env}\u0026#34; if env_group not in inventory: inventory[env_group] = {\u0026#39;hosts\u0026#39;: []} inventory[env_group][\u0026#39;hosts\u0026#39;].append(instance_id) return inventory if __name__ == \u0026#39;__main__\u0026#39;: if len(sys.argv) == 2 and sys.argv[1] == \u0026#39;--list\u0026#39;: print(json.dumps(get_ec2_instances(), indent=2)) elif len(sys.argv) == 3 and sys.argv[1] == \u0026#39;--host\u0026#39;: print(json.dumps({})) else: print(\u0026#34;Usage: %s --list or %s --host \u0026lt;hostname\u0026gt;\u0026#34; % (sys.argv[0], sys.argv[0])) CI/CDé›†æˆ GitLab CIé…ç½® # .gitlab-ci.yml for Infrastructure stages: - validate - plan - apply - configure variables: TF_ROOT: ${CI_PROJECT_DIR}/environments/${ENVIRONMENT} TF_ADDRESS: ${CI_API_V4_URL}/projects/${CI_PROJECT_ID}/terraform/state/${ENVIRONMENT} cache: key: \u0026#34;${ENVIRONMENT}\u0026#34; paths: - ${TF_ROOT}/.terraform before_script: - cd ${TF_ROOT} - terraform --version - terraform init -backend-config=\u0026#34;address=${TF_ADDRESS}\u0026#34; -backend-config=\u0026#34;lock_address=${TF_ADDRESS}/lock\u0026#34; -backend-config=\u0026#34;unlock_address=${TF_ADDRESS}/lock\u0026#34; -backend-config=\u0026#34;username=${GITLAB_USER_LOGIN}\u0026#34; -backend-config=\u0026#34;password=${CI_JOB_TOKEN}\u0026#34; -backend-config=\u0026#34;lock_method=POST\u0026#34; -backend-config=\u0026#34;unlock_method=DELETE\u0026#34; -backend-config=\u0026#34;retry_wait_min=5\u0026#34; validate: stage: validate script: - terraform validate - terraform fmt -check only: - merge_requests - main plan: stage: plan script: - terraform plan -out=\u0026#34;planfile\u0026#34; artifacts: name: plan paths: - ${TF_ROOT}/planfile expire_in: 1 week only: - merge_requests - main apply: stage: apply script: - terraform apply -input=false \u0026#34;planfile\u0026#34; dependencies: - plan when: manual only: - main environment: name: ${ENVIRONMENT} configure: stage: configure image: ansible/ansible:latest script: - cd ansible-configuration - ansible-playbook -i inventories/${ENVIRONMENT}/hosts.yml playbooks/site.yml dependencies: - apply only: - main è‡ªåŠ¨åŒ–è„šæœ¬ #!/bin/bash # scripts/deploy.sh set -euo pipefail ENVIRONMENT=${1:-dev} ACTION=${2:-plan} echo \u0026#34;ğŸš€ Starting infrastructure deployment for environment: $ENVIRONMENT\u0026#34; # éªŒè¯ç¯å¢ƒ if [[ ! -d \u0026#34;environments/$ENVIRONMENT\u0026#34; ]]; then echo \u0026#34;âŒ Environment $ENVIRONMENT does not exist\u0026#34; exit 1 fi cd \u0026#34;environments/$ENVIRONMENT\u0026#34; # åˆå§‹åŒ–Terraform echo \u0026#34;ğŸ“¦ Initializing Terraform...\u0026#34; terraform init # éªŒè¯é…ç½® echo \u0026#34;âœ… Validating Terraform configuration...\u0026#34; terraform validate # æ ¼å¼åŒ–æ£€æŸ¥ echo \u0026#34;ğŸ¨ Checking Terraform formatting...\u0026#34; terraform fmt -check case $ACTION in \u0026#34;plan\u0026#34;) echo \u0026#34;ğŸ“‹ Creating Terraform plan...\u0026#34; terraform plan -out=tfplan ;; \u0026#34;apply\u0026#34;) echo \u0026#34;ğŸ”¨ Applying Terraform changes...\u0026#34; if [[ -f \u0026#34;tfplan\u0026#34; ]]; then terraform apply tfplan else terraform apply -auto-approve fi echo \u0026#34;âš™ï¸ Running Ansible configuration...\u0026#34; cd ../../ansible-configuration ansible-playbook -i inventories/$ENVIRONMENT/hosts.yml playbooks/site.yml ;; \u0026#34;destroy\u0026#34;) echo \u0026#34;ğŸ’¥ Destroying infrastructure...\u0026#34; terraform destroy -auto-approve ;; *) echo \u0026#34;âŒ Unknown action: $ACTION\u0026#34; echo \u0026#34;Usage: $0 \u0026lt;environment\u0026gt; \u0026lt;plan|apply|destroy\u0026gt;\u0026#34; exit 1 ;; esac echo \u0026#34;âœ… Infrastructure deployment completed successfully!\u0026#34; æœ€ä½³å®è·µæ€»ç»“ 1. çŠ¶æ€ç®¡ç† # è¿œç¨‹çŠ¶æ€é…ç½® terraform { backend \u0026#34;s3\u0026#34; { bucket = \u0026#34;terraform-state-bucket\u0026#34; key = \u0026#34;path/to/terraform.tfstate\u0026#34; region = \u0026#34;us-west-2\u0026#34; encrypt = true dynamodb_table = \u0026#34;terraform-locks\u0026#34; } } # çŠ¶æ€å¯¼å…¥ç¤ºä¾‹ terraform import aws_instance.example i-1234567890abcdef0 2. å®‰å…¨æœ€ä½³å®è·µ # å¯†é’¥ç®¡ç† export AWS_ACCESS_KEY_ID=$(vault kv get -field=access_key secret/aws/terraform) export AWS_SECRET_ACCESS_KEY=$(vault kv get -field=secret_key secret/aws/terraform) # Terraformå˜é‡åŠ å¯† terraform plan -var-file=\u0026#34;secrets.tfvars.encrypted\u0026#34; # Ansible Vault ansible-vault encrypt group_vars/production/secrets.yml ansible-playbook --ask-vault-pass playbooks/site.yml 3. ç›‘æ§å’Œå‘Šè­¦ # Prometheusç›‘æ§è§„åˆ™ groups: - name: infrastructure rules: - alert: TerraformDrift expr: terraform_state_drift \u0026gt; 0 for: 5m labels: severity: warning annotations: summary: \u0026#34;Terraform state drift detected\u0026#34; - alert: AnsiblePlaybookFailed expr: ansible_playbook_failures \u0026gt; 0 for: 1m labels: severity: critical annotations: summary: \u0026#34;Ansible playbook execution failed\u0026#34; é€šè¿‡æœ¬æ–‡çš„å®è·µæŒ‡å—ï¼Œæ‚¨å¯ä»¥æ„å»ºä¸€ä¸ªå®Œæ•´çš„åŸºç¡€è®¾æ–½å³ä»£ç è§£å†³æ–¹æ¡ˆï¼Œå®ç°åŸºç¡€è®¾æ–½çš„è‡ªåŠ¨åŒ–ç®¡ç†å’Œé…ç½®ï¼Œæé«˜è¿ç»´æ•ˆç‡å’Œç³»ç»Ÿå¯é æ€§ã€‚\n","content":"åŸºç¡€è®¾æ–½å³ä»£ç (IaC)å®è·µï¼šTerraform + Ansible ä¼ä¸šçº§è‡ªåŠ¨åŒ– åŸºç¡€è®¾æ–½å³ä»£ç (Infrastructure as Code, IaC)æ˜¯ç°ä»£äº‘åŸç”Ÿæ¶æ„çš„æ ¸å¿ƒå®è·µä¹‹ä¸€ã€‚é€šè¿‡ä»£ç åŒ–ç®¡ç†åŸºç¡€è®¾æ–½ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°ç¯å¢ƒçš„ä¸€è‡´æ€§ã€å¯é‡å¤æ€§å’Œå¯å®¡è®¡æ€§ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨Terraformå’ŒAnsibleçš„ä¼ä¸šçº§å®è·µã€‚\nIaCæ¶æ„è®¾è®¡ æ•´ä½“æ¶æ„å›¾ graph TB A[å¼€å‘è€…] --\u0026amp;gt; B[Git Repository] B --\u0026amp;gt; C[CI/CD Pipeline] C --\u0026amp;gt; D[Terraform Plan] D --\u0026amp;gt; E[Review \u0026amp;amp; Approval] E --\u0026amp;gt; F[Terraform Apply] F --\u0026amp;gt; G[AWS/Azure/GCP] G --\u0026amp;gt; H[Infrastructure] H --\u0026amp;gt; I[Ansible Playbooks] I --\u0026amp;gt; J[Configuration Management] J --\u0026amp;gt; K[Application Deployment] L[State â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["IaC","Terraform","Ansible","è‡ªåŠ¨åŒ–","äº‘åŸºç¡€è®¾æ–½","DevOps"],"categories":["è¿ç»´"],"author":"åŸºç¡€è®¾æ–½ä¸“å®¶","readingTime":11,"wordCount":2301,"section":"posts","type":"posts","draft":false,"featured":false,"series":["è‡ªåŠ¨åŒ–è¿ç»´"]},{"title":"ä¼ä¸šçº§CI/CDæµæ°´çº¿è®¾è®¡ä¸å®è·µï¼šGitLab + Jenkins + Kubernetes","url":"https://www.dishuihengxin.com/posts/devops-ci-cd-pipeline/","summary":"ä¼ä¸šçº§CI/CDæµæ°´çº¿è®¾è®¡ä¸å®è·µï¼šGitLab + Jenkins + Kubernetes åœ¨ç°ä»£è½¯ä»¶å¼€å‘ä¸­ï¼ŒCI/CDï¼ˆæŒç»­é›†æˆ/æŒç»­éƒ¨ç½²ï¼‰å·²æˆä¸ºæé«˜å¼€å‘æ•ˆç‡ã€ä¿éšœä»£ç è´¨é‡çš„æ ¸å¿ƒå®è·µã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨ä¼ä¸šçº§CI/CDæµæ°´çº¿çš„è®¾è®¡åŸåˆ™ã€æŠ€æœ¯æ¶æ„å’Œå®æ–½ç­–ç•¥ã€‚\nCI/CDæ¶æ„è®¾è®¡ æ•´ä½“æ¶æ„å›¾ graph TB A[å¼€å‘è€…] --\u0026gt; B[Git Repository] B --\u0026gt; C[GitLab CI] C --\u0026gt; D[ä»£ç è´¨é‡æ£€æŸ¥] C --\u0026gt; E[å•å…ƒæµ‹è¯•] C --\u0026gt; F[æ„å»ºé•œåƒ] F --\u0026gt; G[é•œåƒä»“åº“] G --\u0026gt; H[Jenkins Pipeline] H --\u0026gt; I[é›†æˆæµ‹è¯•] H --\u0026gt; J[å®‰å…¨æ‰«æ] H --\u0026gt; K[éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ] K --\u0026gt; L[è‡ªåŠ¨åŒ–æµ‹è¯•] L --\u0026gt; M[éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ] M --\u0026gt; N[Kubernetesé›†ç¾¤] æŠ€æœ¯æ ˆé€‰æ‹© ç»„ä»¶ æŠ€æœ¯é€‰å‹ ä½œç”¨ ä»£ç ä»“åº“ GitLab æºç ç®¡ç†ã€CIè§¦å‘ CIå¼•æ“ GitLab CI + Jenkins æ„å»ºã€æµ‹è¯•ã€éƒ¨ç½² å®¹å™¨åŒ– Docker åº”ç”¨æ‰“åŒ… é•œåƒä»“åº“ Harbor é•œåƒå­˜å‚¨ç®¡ç† ç¼–æ’å¹³å° Kubernetes å®¹å™¨ç¼–æ’éƒ¨ç½² ç›‘æ§å‘Šè­¦ Prometheus + Grafana æµæ°´çº¿ç›‘æ§ GitLab CIé…ç½® .gitlab-ci.ymlæ ¸å¿ƒé…ç½® # .gitlab-ci.yml stages: - validate - test - build - security - deploy-test - integration-test - deploy-prod variables: DOCKER_DRIVER: overlay2 DOCKER_TLS_CERTDIR: \u0026#34;/certs\u0026#34; MAVEN_OPTS: \u0026#34;-Dmaven.repo.local=$CI_PROJECT_DIR/.m2/repository\u0026#34; MAVEN_CLI_OPTS: \u0026#34;--batch-mode --errors --fail-at-end --show-version\u0026#34; cache: paths: - .m2/repository/ - node_modules/ - target/ # ä»£ç è´¨é‡æ£€æŸ¥ code-quality: stage: validate image: sonarsource/sonar-scanner-cli:latest script: - sonar-scanner -Dsonar.projectKey=$CI_PROJECT_NAME -Dsonar.sources=src/ -Dsonar.host.url=$SONAR_HOST_URL -Dsonar.login=$SONAR_TOKEN -Dsonar.qualitygate.wait=true only: - merge_requests - main - develop # å•å…ƒæµ‹è¯• unit-test: stage: test image: maven:3.8.6-openjdk-11 script: - mvn $MAVEN_CLI_OPTS clean test - mvn jacoco:report coverage: \u0026#39;/Total.*?([0-9]{1,3})%/\u0026#39; artifacts: reports: junit: - target/surefire-reports/TEST-*.xml coverage_report: coverage_format: cobertura path: target/site/jacoco/jacoco.xml paths: - target/ expire_in: 1 hour # æ„å»ºDockeré•œåƒ build-image: stage: build image: docker:20.10.16 services: - docker:20.10.16-dind before_script: - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY script: - docker build -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA . - docker build -t $CI_REGISTRY_IMAGE:latest . - docker push $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA - docker push $CI_REGISTRY_IMAGE:latest only: - main - develop # å®‰å…¨æ‰«æ security-scan: stage: security image: aquasec/trivy:latest script: - trivy image --exit-code 0 --severity HIGH,CRITICAL $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA - trivy image --exit-code 1 --severity CRITICAL $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA artifacts: reports: container_scanning: trivy-report.json only: - main - develop # éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ deploy-test: stage: deploy-test image: bitnami/kubectl:latest script: - kubectl config use-context $KUBE_CONTEXT_TEST - envsubst \u0026lt; k8s/deployment-test.yaml | kubectl apply -f - - kubectl rollout status deployment/$CI_PROJECT_NAME-test -n test environment: name: test url: https://test.example.com only: - develop # é›†æˆæµ‹è¯• integration-test: stage: integration-test image: postman/newman:latest script: - newman run tests/integration/api-tests.json --environment tests/integration/test-env.json --reporters cli,junit --reporter-junit-export newman-report.xml artifacts: reports: junit: newman-report.xml dependencies: - deploy-test only: - develop # ç”Ÿäº§ç¯å¢ƒéƒ¨ç½² deploy-prod: stage: deploy-prod image: bitnami/kubectl:latest script: - kubectl config use-context $KUBE_CONTEXT_PROD - envsubst \u0026lt; k8s/deployment-prod.yaml | kubectl apply -f - - kubectl rollout status deployment/$CI_PROJECT_NAME -n production environment: name: production url: https://api.example.com when: manual only: - main Dockerfileæœ€ä½³å®è·µ # å¤šé˜¶æ®µæ„å»ºDockerfile FROM maven:3.8.6-openjdk-11-slim AS builder WORKDIR /app COPY pom.xml . RUN mvn dependency:go-offline -B COPY src ./src RUN mvn clean package -DskipTests # è¿è¡Œæ—¶é•œåƒ FROM openjdk:11-jre-slim # åˆ›å»ºérootç”¨æˆ· RUN groupadd -r appuser \u0026amp;\u0026amp; useradd -r -g appuser appuser # å®‰è£…å¿…è¦çš„å·¥å…· RUN apt-get update \u0026amp;\u0026amp; apt-get install -y \\ curl \\ jq \\ \u0026amp;\u0026amp; rm -rf /var/lib/apt/lists/* WORKDIR /app # å¤åˆ¶åº”ç”¨æ–‡ä»¶ COPY --from=builder /app/target/*.jar app.jar COPY --chown=appuser:appuser scripts/ ./scripts/ # è®¾ç½®æƒé™ RUN chmod +x scripts/*.sh # å¥åº·æ£€æŸ¥ HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\ CMD curl -f http://localhost:8080/actuator/health || exit 1 # åˆ‡æ¢åˆ°érootç”¨æˆ· USER appuser # æš´éœ²ç«¯å£ EXPOSE 8080 # å¯åŠ¨å‘½ä»¤ ENTRYPOINT [\u0026#34;java\u0026#34;, \u0026#34;-XX:+UseContainerSupport\u0026#34;, \u0026#34;-XX:MaxRAMPercentage=75.0\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;app.jar\u0026#34;] Jenkins Pipelineé…ç½® Jenkinsfileå£°æ˜å¼æµæ°´çº¿ // Jenkinsfile pipeline { agent { kubernetes { yaml \u0026#34;\u0026#34;\u0026#34; apiVersion: v1 kind: Pod spec: containers: - name: maven image: maven:3.8.6-openjdk-11 command: - cat tty: true volumeMounts: - name: maven-cache mountPath: /root/.m2 - name: docker image: docker:20.10.16 command: - cat tty: true volumeMounts: - name: docker-sock mountPath: /var/run/docker.sock - name: kubectl image: bitnami/kubectl:latest command: - cat tty: true volumes: - name: maven-cache persistentVolumeClaim: claimName: maven-cache-pvc - name: docker-sock hostPath: path: /var/run/docker.sock \u0026#34;\u0026#34;\u0026#34; } } environment { REGISTRY = \u0026#39;harbor.example.com\u0026#39; IMAGE_NAME = \u0026#34;${REGISTRY}/library/${env.JOB_NAME}\u0026#34; KUBECONFIG = credentials(\u0026#39;kubeconfig\u0026#39;) HARBOR_CREDS = credentials(\u0026#39;harbor-credentials\u0026#39;) } stages { stage(\u0026#39;Checkout\u0026#39;) { steps { checkout scm script { env.GIT_COMMIT_SHORT = sh( script: \u0026#34;git rev-parse --short HEAD\u0026#34;, returnStdout: true ).trim() env.BUILD_VERSION = \u0026#34;${env.BUILD_NUMBER}-${env.GIT_COMMIT_SHORT}\u0026#34; } } } stage(\u0026#39;Code Quality\u0026#39;) { parallel { stage(\u0026#39;SonarQube Analysis\u0026#39;) { steps { container(\u0026#39;maven\u0026#39;) { withSonarQubeEnv(\u0026#39;SonarQube\u0026#39;) { sh \u0026#39;\u0026#39;\u0026#39; mvn clean compile sonar:sonar \\ -Dsonar.projectKey=${JOB_NAME} \\ -Dsonar.projectName=${JOB_NAME} \\ -Dsonar.projectVersion=${BUILD_VERSION} \u0026#39;\u0026#39;\u0026#39; } } } } stage(\u0026#39;Dependency Check\u0026#39;) { steps { container(\u0026#39;maven\u0026#39;) { sh \u0026#39;mvn dependency-check:check\u0026#39; publishHTML([ allowMissing: false, alwaysLinkToLastBuild: true, keepAll: true, reportDir: \u0026#39;target\u0026#39;, reportFiles: \u0026#39;dependency-check-report.html\u0026#39;, reportName: \u0026#39;Dependency Check Report\u0026#39; ]) } } } } } stage(\u0026#39;Quality Gate\u0026#39;) { steps { timeout(time: 5, unit: \u0026#39;MINUTES\u0026#39;) { waitForQualityGate abortPipeline: true } } } stage(\u0026#39;Test\u0026#39;) { parallel { stage(\u0026#39;Unit Tests\u0026#39;) { steps { container(\u0026#39;maven\u0026#39;) { sh \u0026#39;mvn test\u0026#39; publishTestResults testResultsPattern: \u0026#39;target/surefire-reports/*.xml\u0026#39; publishCoverage adapters: [ jacocoAdapter(\u0026#39;target/site/jacoco/jacoco.xml\u0026#39;) ], sourceFileResolver: sourceFiles(\u0026#39;STORE_LAST_BUILD\u0026#39;) } } } stage(\u0026#39;Integration Tests\u0026#39;) { steps { container(\u0026#39;maven\u0026#39;) { sh \u0026#39;mvn verify -Dskip.unit.tests=true\u0026#39; } } } } } stage(\u0026#39;Build \u0026amp; Push Image\u0026#39;) { when { anyOf { branch \u0026#39;main\u0026#39; branch \u0026#39;develop\u0026#39; } } steps { container(\u0026#39;docker\u0026#39;) { script { docker.withRegistry(\u0026#34;https://${REGISTRY}\u0026#34;, \u0026#39;harbor-credentials\u0026#39;) { def image = docker.build(\u0026#34;${IMAGE_NAME}:${BUILD_VERSION}\u0026#34;) image.push() image.push(\u0026#39;latest\u0026#39;) } } } } } stage(\u0026#39;Security Scan\u0026#39;) { when { anyOf { branch \u0026#39;main\u0026#39; branch \u0026#39;develop\u0026#39; } } steps { container(\u0026#39;docker\u0026#39;) { sh \u0026#34;\u0026#34;\u0026#34; docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\ aquasec/trivy:latest image \\ --exit-code 0 \\ --severity HIGH,CRITICAL \\ --format json \\ --output trivy-report.json \\ ${IMAGE_NAME}:${BUILD_VERSION} \u0026#34;\u0026#34;\u0026#34; archiveArtifacts artifacts: \u0026#39;trivy-report.json\u0026#39;, fingerprint: true } } } stage(\u0026#39;Deploy to Test\u0026#39;) { when { branch \u0026#39;develop\u0026#39; } steps { container(\u0026#39;kubectl\u0026#39;) { sh \u0026#34;\u0026#34;\u0026#34; envsubst \u0026lt; k8s/test/deployment.yaml | kubectl apply -f - kubectl set image deployment/\\${JOB_NAME} \\ \\${JOB_NAME}=${IMAGE_NAME}:${BUILD_VERSION} \\ -n test kubectl rollout status deployment/\\${JOB_NAME} -n test --timeout=300s \u0026#34;\u0026#34;\u0026#34; } } } stage(\u0026#39;Smoke Tests\u0026#39;) { when { branch \u0026#39;develop\u0026#39; } steps { script { def testResult = sh( script: \u0026#39;\u0026#39;\u0026#39; curl -f http://test.example.com/actuator/health newman run tests/smoke/smoke-tests.json \\ --environment tests/smoke/test-env.json \\ --reporters cli,junit \\ --reporter-junit-export smoke-test-results.xml \u0026#39;\u0026#39;\u0026#39;, returnStatus: true ) publishTestResults testResultsPattern: \u0026#39;smoke-test-results.xml\u0026#39; if (testResult != 0) { error(\u0026#34;Smoke tests failed\u0026#34;) } } } } stage(\u0026#39;Deploy to Production\u0026#39;) { when { allOf { branch \u0026#39;main\u0026#39; expression { return params.DEPLOY_TO_PROD == true } } } steps { script { def deployApproval = input( message: \u0026#39;Deploy to Production?\u0026#39;, parameters: [ choice( name: \u0026#39;DEPLOYMENT_STRATEGY\u0026#39;, choices: [\u0026#39;rolling\u0026#39;, \u0026#39;blue-green\u0026#39;, \u0026#39;canary\u0026#39;], description: \u0026#39;Select deployment strategy\u0026#39; ) ] ) container(\u0026#39;kubectl\u0026#39;) { if (deployApproval == \u0026#39;blue-green\u0026#39;) { sh \u0026#39;\u0026#39;\u0026#39; # Blue-Greenéƒ¨ç½²é€»è¾‘ kubectl apply -f k8s/prod/deployment-green.yaml kubectl set image deployment/${JOB_NAME}-green \\ ${JOB_NAME}=${IMAGE_NAME}:${BUILD_VERSION} \\ -n production kubectl rollout status deployment/${JOB_NAME}-green -n production # åˆ‡æ¢æµé‡ kubectl patch service ${JOB_NAME} \\ -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;selector\u0026#34;:{\u0026#34;version\u0026#34;:\u0026#34;green\u0026#34;}}}\u0026#39; \\ -n production \u0026#39;\u0026#39;\u0026#39; } else if (deployApproval == \u0026#39;canary\u0026#39;) { sh \u0026#39;\u0026#39;\u0026#39; # Canaryéƒ¨ç½²é€»è¾‘ kubectl apply -f k8s/prod/deployment-canary.yaml kubectl set image deployment/${JOB_NAME}-canary \\ ${JOB_NAME}=${IMAGE_NAME}:${BUILD_VERSION} \\ -n production kubectl rollout status deployment/${JOB_NAME}-canary -n production \u0026#39;\u0026#39;\u0026#39; } else { sh \u0026#39;\u0026#39;\u0026#39; # æ»šåŠ¨æ›´æ–° kubectl set image deployment/${JOB_NAME} \\ ${JOB_NAME}=${IMAGE_NAME}:${BUILD_VERSION} \\ -n production kubectl rollout status deployment/${JOB_NAME} -n production \u0026#39;\u0026#39;\u0026#39; } } } } } } post { always { cleanWs() } success { script { if (env.BRANCH_NAME == \u0026#39;main\u0026#39;) { slackSend( channel: \u0026#39;#deployments\u0026#39;, color: \u0026#39;good\u0026#39;, message: \u0026#34;\u0026#34;\u0026#34; âœ… Production deployment successful! Project: ${env.JOB_NAME} Version: ${env.BUILD_VERSION} Build: ${env.BUILD_URL} \u0026#34;\u0026#34;\u0026#34; ) } } } failure { slackSend( channel: \u0026#39;#alerts\u0026#39;, color: \u0026#39;danger\u0026#39;, message: \u0026#34;\u0026#34;\u0026#34; âŒ Pipeline failed! Project: ${env.JOB_NAME} Branch: ${env.BRANCH_NAME} Build: ${env.BUILD_URL} \u0026#34;\u0026#34;\u0026#34; ) } } } Kuberneteséƒ¨ç½²é…ç½® ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æ¸…å• # k8s/prod/namespace.yaml apiVersion: v1 kind: Namespace metadata: name: production labels: name: production environment: prod --- # k8s/prod/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: app-config namespace: production data: application.yml: | server: port: 8080 spring: profiles: active: prod datasource: url: jdbc:postgresql://postgres:5432/appdb username: ${DB_USERNAME} password: ${DB_PASSWORD} logging: level: com.example: INFO pattern: console: \u0026#34;%d{yyyy-MM-dd HH:mm:ss} - %msg%n\u0026#34; --- # k8s/prod/secret.yaml apiVersion: v1 kind: Secret metadata: name: app-secrets namespace: production type: Opaque data: DB_USERNAME: \u0026lt;base64-encoded-username\u0026gt; DB_PASSWORD: \u0026lt;base64-encoded-password\u0026gt; JWT_SECRET: \u0026lt;base64-encoded-jwt-secret\u0026gt; --- # k8s/prod/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: ${CI_PROJECT_NAME} namespace: production labels: app: ${CI_PROJECT_NAME} version: ${CI_COMMIT_SHA} spec: replicas: 3 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 selector: matchLabels: app: ${CI_PROJECT_NAME} template: metadata: labels: app: ${CI_PROJECT_NAME} version: ${CI_COMMIT_SHA} annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/port: \u0026#34;8080\u0026#34; prometheus.io/path: \u0026#34;/actuator/prometheus\u0026#34; spec: serviceAccountName: app-service-account securityContext: runAsNonRoot: true runAsUser: 1000 fsGroup: 1000 containers: - name: ${CI_PROJECT_NAME} image: ${CI_REGISTRY_IMAGE}:${CI_COMMIT_SHA} imagePullPolicy: Always ports: - containerPort: 8080 name: http env: - name: SPRING_PROFILES_ACTIVE value: \u0026#34;prod\u0026#34; envFrom: - secretRef: name: app-secrets - configMapRef: name: app-config resources: requests: memory: \u0026#34;512Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;1Gi\u0026#34; cpu: \u0026#34;500m\u0026#34; livenessProbe: httpGet: path: /actuator/health/liveness port: 8080 initialDelaySeconds: 60 periodSeconds: 30 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: httpGet: path: /actuator/health/readiness port: 8080 initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 volumeMounts: - name: config-volume mountPath: /app/config - name: logs-volume mountPath: /app/logs volumes: - name: config-volume configMap: name: app-config - name: logs-volume emptyDir: {} imagePullSecrets: - name: harbor-secret --- # k8s/prod/service.yaml apiVersion: v1 kind: Service metadata: name: ${CI_PROJECT_NAME} namespace: production labels: app: ${CI_PROJECT_NAME} spec: type: ClusterIP ports: - port: 80 targetPort: 8080 protocol: TCP name: http selector: app: ${CI_PROJECT_NAME} --- # k8s/prod/ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: ${CI_PROJECT_NAME} namespace: production annotations: kubernetes.io/ingress.class: nginx cert-manager.io/cluster-issuer: letsencrypt-prod nginx.ingress.kubernetes.io/rate-limit: \u0026#34;100\u0026#34; nginx.ingress.kubernetes.io/rate-limit-window: \u0026#34;1m\u0026#34; spec: tls: - hosts: - api.example.com secretName: api-tls rules: - host: api.example.com http: paths: - path: / pathType: Prefix backend: service: name: ${CI_PROJECT_NAME} port: number: 80 --- # k8s/prod/hpa.yaml apiVersion: autoscaling/v2 kind: HorizontalPodAutoscaler metadata: name: ${CI_PROJECT_NAME} namespace: production spec: scaleTargetRef: apiVersion: apps/v1 kind: Deployment name: ${CI_PROJECT_NAME} minReplicas: 3 maxReplicas: 10 metrics: - type: Resource resource: name: cpu target: type: Utilization averageUtilization: 70 - type: Resource resource: name: memory target: type: Utilization averageUtilization: 80 æµæ°´çº¿ç›‘æ§ä¸å‘Šè­¦ Prometheusç›‘æ§é…ç½® # prometheus-rules.yaml groups: - name: cicd-pipeline rules: - alert: PipelineFailureRate expr: | ( sum(rate(jenkins_builds_failed_total[5m])) / sum(rate(jenkins_builds_total[5m])) ) * 100 \u0026gt; 10 for: 5m labels: severity: warning annotations: summary: \u0026#34;High pipeline failure rate\u0026#34; description: \u0026#34;Pipeline failure rate is {{ $value }}% over the last 5 minutes\u0026#34; - alert: LongRunningPipeline expr: jenkins_builds_duration_milliseconds \u0026gt; 1800000 # 30 minutes for: 0m labels: severity: warning annotations: summary: \u0026#34;Pipeline running too long\u0026#34; description: \u0026#34;Pipeline {{ $labels.job }} has been running for more than 30 minutes\u0026#34; - alert: DeploymentFailure expr: | increase(kube_deployment_status_replicas_unavailable[5m]) \u0026gt; 0 for: 2m labels: severity: critical annotations: summary: \u0026#34;Deployment failure detected\u0026#34; description: \u0026#34;Deployment {{ $labels.deployment }} in namespace {{ $labels.namespace }} has unavailable replicas\u0026#34; Grafanaä»ªè¡¨æ¿ { \u0026#34;dashboard\u0026#34;: { \u0026#34;title\u0026#34;: \u0026#34;CI/CD Pipeline Dashboard\u0026#34;, \u0026#34;panels\u0026#34;: [ { \u0026#34;title\u0026#34;: \u0026#34;Pipeline Success Rate\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;stat\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(rate(jenkins_builds_success_total[24h])) / sum(rate(jenkins_builds_total[24h])) * 100\u0026#34; } ] }, { \u0026#34;title\u0026#34;: \u0026#34;Average Build Time\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;stat\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;avg(jenkins_builds_duration_milliseconds) / 1000\u0026#34; } ] }, { \u0026#34;title\u0026#34;: \u0026#34;Deployment Frequency\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;graph\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(rate(jenkins_builds_success_total{job=~\\\u0026#34;.*deploy.*\\\u0026#34;}[1h]))\u0026#34; } ] } ] } } æœ€ä½³å®è·µæ€»ç»“ 1. æµæ°´çº¿è®¾è®¡åŸåˆ™ å¿«é€Ÿåé¦ˆï¼šä¼˜åŒ–æ„å»ºæ—¶é—´ï¼Œå°½æ—©å‘ç°é—®é¢˜ å¹¶è¡Œæ‰§è¡Œï¼šåˆç†è®¾è®¡å¹¶è¡Œä»»åŠ¡ï¼Œæé«˜æ•ˆç‡ å¤±è´¥å¿«é€Ÿï¼šé‡åˆ°é”™è¯¯ç«‹å³åœæ­¢ï¼Œé¿å…èµ„æºæµªè´¹ å¯é‡å¤æ€§ï¼šç¡®ä¿æµæ°´çº¿åœ¨ä»»ä½•ç¯å¢ƒä¸‹éƒ½èƒ½ç¨³å®šè¿è¡Œ 2. å®‰å…¨æœ€ä½³å®è·µ # å¯†é’¥ç®¡ç† kubectl create secret generic app-secrets \\ --from-literal=db-password=\u0026#34;$(openssl rand -base64 32)\u0026#34; \\ --from-literal=jwt-secret=\u0026#34;$(openssl rand -base64 64)\u0026#34; # é•œåƒç­¾åéªŒè¯ cosign sign --key cosign.key ${IMAGE_NAME}:${TAG} cosign verify --key cosign.pub ${IMAGE_NAME}:${TAG} # ç½‘ç»œç­–ç•¥ apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: deny-all spec: podSelector: {} policyTypes: - Ingress - Egress 3. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ ç¼“å­˜ç­–ç•¥ï¼šåˆç†ä½¿ç”¨æ„å»ºç¼“å­˜å’Œä¾èµ–ç¼“å­˜ é•œåƒä¼˜åŒ–ï¼šä½¿ç”¨å¤šé˜¶æ®µæ„å»ºï¼Œå‡å°é•œåƒä½“ç§¯ èµ„æºé™åˆ¶ï¼šè®¾ç½®åˆç†çš„CPUå’Œå†…å­˜é™åˆ¶ å¹¶å‘æ§åˆ¶ï¼šé¿å…è¿‡å¤šå¹¶å‘æ„å»ºå½±å“ç³»ç»Ÿæ€§èƒ½ é€šè¿‡æœ¬æ–‡çš„å®è·µæŒ‡å—ï¼Œæ‚¨å¯ä»¥æ„å»ºä¸€ä¸ªé«˜æ•ˆã€å®‰å…¨ã€å¯é çš„ä¼ä¸šçº§CI/CDæµæ°´çº¿ï¼Œæ˜¾è‘—æå‡è½¯ä»¶äº¤ä»˜çš„è´¨é‡å’Œæ•ˆç‡ã€‚\n","content":"ä¼ä¸šçº§CI/CDæµæ°´çº¿è®¾è®¡ä¸å®è·µï¼šGitLab + Jenkins + Kubernetes åœ¨ç°ä»£è½¯ä»¶å¼€å‘ä¸­ï¼ŒCI/CDï¼ˆæŒç»­é›†æˆ/æŒç»­éƒ¨ç½²ï¼‰å·²æˆä¸ºæé«˜å¼€å‘æ•ˆç‡ã€ä¿éšœä»£ç è´¨é‡çš„æ ¸å¿ƒå®è·µã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨ä¼ä¸šçº§CI/CDæµæ°´çº¿çš„è®¾è®¡åŸåˆ™ã€æŠ€æœ¯æ¶æ„å’Œå®æ–½ç­–ç•¥ã€‚\nCI/CDæ¶æ„è®¾è®¡ æ•´ä½“æ¶æ„å›¾ graph TB A[å¼€å‘è€…] --\u0026amp;gt; B[Git Repository] B --\u0026amp;gt; C[GitLab CI] C --\u0026amp;gt; D[ä»£ç è´¨é‡æ£€æŸ¥] C --\u0026amp;gt; E[å•å…ƒæµ‹è¯•] C --\u0026amp;gt; F[æ„å»ºé•œåƒ] F --\u0026amp;gt; G[é•œåƒä»“åº“] G --\u0026amp;gt; H[Jenkins Pipeline] H --\u0026amp;gt; I[é›†æˆæµ‹è¯•] H --\u0026amp;gt; J[å®‰å…¨æ‰«æ] H --\u0026amp;gt; K[éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ] K --\u0026amp;gt; L[è‡ªåŠ¨åŒ–æµ‹è¯•] L --\u0026amp;gt; M[éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ] M --\u0026amp;gt; N[Kubernetesé›†ç¾¤] æŠ€æœ¯æ ˆé€‰æ‹© ç»„ä»¶ æŠ€æœ¯é€‰å‹ ä½œç”¨ ä»£ç ä»“åº“ GitLab æºç ç®¡ç†ã€CIè§¦å‘ CIå¼•æ“ GitLab CI + Jenkins æ„å»ºã€æµ‹è¯•ã€ â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["CI/CD","Jenkins","GitLab","Kubernetes","DevOps","è‡ªåŠ¨åŒ–éƒ¨ç½²"],"categories":["è¿ç»´"],"author":"DevOpsæ¶æ„å¸ˆ","readingTime":8,"wordCount":1687,"section":"posts","type":"posts","draft":false,"featured":false,"series":["DevOpså®è·µ"]},{"title":"ä¼ä¸šçº§å¯è§‚æµ‹æ€§å¹³å°å»ºè®¾ï¼šPrometheus + Grafana + Jaeger + ELK å®Œæ•´æ–¹æ¡ˆ","url":"https://www.dishuihengxin.com/posts/devops-observability-platform/","summary":"ä¼ä¸šçº§å¯è§‚æµ‹æ€§å¹³å°å»ºè®¾ï¼šPrometheus + Grafana + Jaeger + ELK å®Œæ•´æ–¹æ¡ˆ å¯è§‚æµ‹æ€§(Observability)æ˜¯ç°ä»£åˆ†å¸ƒå¼ç³»ç»Ÿè¿ç»´çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œé€šè¿‡æŒ‡æ ‡(Metrics)ã€æ—¥å¿—(Logs)å’Œé“¾è·¯è¿½è¸ª(Traces)ä¸‰å¤§æ”¯æŸ±ï¼Œä¸ºç³»ç»Ÿæä¾›å…¨æ–¹ä½çš„ç›‘æ§å’Œè¯Šæ–­èƒ½åŠ›ã€‚æœ¬æ–‡å°†è¯¦ç»†ä»‹ç»å¦‚ä½•æ„å»ºä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§å¯è§‚æµ‹æ€§å¹³å°ã€‚\nå¯è§‚æµ‹æ€§æ¶æ„è®¾è®¡ æ•´ä½“æ¶æ„å›¾ graph TB subgraph \u0026#34;æ•°æ®æºå±‚\u0026#34; A1[åº”ç”¨æœåŠ¡] --\u0026gt; B1[Metrics] A1 --\u0026gt; B2[Logs] A1 --\u0026gt; B3[Traces] A2[åŸºç¡€è®¾æ–½] --\u0026gt; B1 A2 --\u0026gt; B2 A3[ä¸­é—´ä»¶] --\u0026gt; B1 A3 --\u0026gt; B2 A3 --\u0026gt; B3 end subgraph \u0026#34;æ•°æ®æ”¶é›†å±‚\u0026#34; B1 --\u0026gt; C1[Prometheus] B2 --\u0026gt; C2[Filebeat/Fluentd] B3 --\u0026gt; C3[Jaeger Agent] C2 --\u0026gt; C4[Logstash] C3 --\u0026gt; C5[Jaeger Collector] end subgraph \u0026#34;æ•°æ®å­˜å‚¨å±‚\u0026#34; C1 --\u0026gt; D1[Prometheus TSDB] C4 --\u0026gt; D2[Elasticsearch] C5 --\u0026gt; D3[Jaeger Storage] D1 --\u0026gt; D4[Thanos/VictoriaMetrics] end subgraph \u0026#34;æ•°æ®å¤„ç†å±‚\u0026#34; D4 --\u0026gt; E1[PromQLæŸ¥è¯¢] D2 --\u0026gt; E2[ElasticsearchæŸ¥è¯¢] D3 --\u0026gt; E3[JaegeræŸ¥è¯¢] end subgraph \u0026#34;å¯è§†åŒ–å±‚\u0026#34; E1 --\u0026gt; F1[Grafana] E2 --\u0026gt; F2[Kibana] E3 --\u0026gt; F3[Jaeger UI] F1 --\u0026gt; F4[ç»Ÿä¸€Dashboard] end subgraph \u0026#34;å‘Šè­¦å±‚\u0026#34; E1 --\u0026gt; G1[AlertManager] G1 --\u0026gt; G2[é€šçŸ¥æ¸ é“] end æŠ€æœ¯æ ˆé€‰å‹ ç»„ä»¶ç±»å‹ æŠ€æœ¯é€‰æ‹© èŒè´£ ä¼˜åŠ¿ æŒ‡æ ‡ç›‘æ§ Prometheus + Thanos æ—¶åºæ•°æ®æ”¶é›†å’Œå­˜å‚¨ é«˜æ€§èƒ½ã€ç”Ÿæ€ä¸°å¯Œ æ—¥å¿—èšåˆ ELK Stack æ—¥å¿—æ”¶é›†ã€å¤„ç†ã€å­˜å‚¨ æˆç†Ÿç¨³å®šã€åŠŸèƒ½å¼ºå¤§ é“¾è·¯è¿½è¸ª Jaeger åˆ†å¸ƒå¼è¿½è¸ª äº‘åŸç”Ÿã€æ€§èƒ½ä¼˜ç§€ å¯è§†åŒ– Grafana ç»Ÿä¸€ç›‘æ§é¢æ¿ æ’ä»¶ä¸°å¯Œã€ç•Œé¢å‹å¥½ å‘Šè­¦ AlertManager å‘Šè­¦è·¯ç”±å’Œé€šçŸ¥ çµæ´»çš„å‘Šè­¦è§„åˆ™ æœåŠ¡å‘ç° Consul/Kubernetes åŠ¨æ€æœåŠ¡å‘ç° è‡ªåŠ¨åŒ–é…ç½® Prometheusç›‘æ§ä½“ç³» Prometheusé›†ç¾¤é…ç½® # prometheus-config.yaml global: scrape_interval: 15s evaluation_interval: 15s external_labels: cluster: \u0026#39;production\u0026#39; region: \u0026#39;us-west-2\u0026#39; # å‘Šè­¦è§„åˆ™æ–‡ä»¶ rule_files: - \u0026#34;/etc/prometheus/rules/*.yml\u0026#34; # å‘Šè­¦ç®¡ç†å™¨é…ç½® alerting: alertmanagers: - static_configs: - targets: - alertmanager-1:9093 - alertmanager-2:9093 - alertmanager-3:9093 # æŠ“å–é…ç½® scrape_configs: # Prometheusè‡ªç›‘æ§ - job_name: \u0026#39;prometheus\u0026#39; static_configs: - targets: [\u0026#39;localhost:9090\u0026#39;] scrape_interval: 5s metrics_path: /metrics # Kubernetes API Server - job_name: \u0026#39;kubernetes-apiservers\u0026#39; kubernetes_sd_configs: - role: endpoints namespaces: names: - default scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name] action: keep regex: default;kubernetes;https # KubernetesèŠ‚ç‚¹ç›‘æ§ - job_name: \u0026#39;kubernetes-nodes\u0026#39; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics # cAdvisorå®¹å™¨ç›‘æ§ - job_name: \u0026#39;kubernetes-cadvisor\u0026#39; kubernetes_sd_configs: - role: node scheme: https tls_config: ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt insecure_skip_verify: true bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token relabel_configs: - action: labelmap regex: __meta_kubernetes_node_label_(.+) - target_label: __address__ replacement: kubernetes.default.svc:443 - source_labels: [__meta_kubernetes_node_name] regex: (.+) target_label: __metrics_path__ replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor # Podç›‘æ§ - job_name: \u0026#39;kubernetes-pods\u0026#39; kubernetes_sd_configs: - role: pod relabel_configs: - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port] action: replace regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 target_label: __address__ - action: labelmap regex: __meta_kubernetes_pod_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_pod_name] action: replace target_label: kubernetes_pod_name # æœåŠ¡ç›‘æ§ - job_name: \u0026#39;kubernetes-services\u0026#39; kubernetes_sd_configs: - role: service relabel_configs: - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape] action: keep regex: true - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme] action: replace target_label: __scheme__ regex: (https?) - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path] action: replace target_label: __metrics_path__ regex: (.+) - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port] action: replace target_label: __address__ regex: ([^:]+)(?::\\d+)?;(\\d+) replacement: $1:$2 - action: labelmap regex: __meta_kubernetes_service_label_(.+) - source_labels: [__meta_kubernetes_namespace] action: replace target_label: kubernetes_namespace - source_labels: [__meta_kubernetes_service_name] action: replace target_label: kubernetes_name # åº”ç”¨è‡ªå®šä¹‰ç›‘æ§ - job_name: \u0026#39;application-metrics\u0026#39; consul_sd_configs: - server: \u0026#39;consul.service.consul:8500\u0026#39; services: [\u0026#39;app-metrics\u0026#39;] relabel_configs: - source_labels: [__meta_consul_tags] regex: \u0026#39;.*,metrics,.*\u0026#39; action: keep - source_labels: [__meta_consul_service] target_label: job - source_labels: [__meta_consul_node] target_label: instance # è¿œç¨‹å†™å…¥é…ç½®ï¼ˆç”¨äºé•¿æœŸå­˜å‚¨ï¼‰ remote_write: - url: \u0026#34;http://thanos-receive:19291/api/v1/receive\u0026#34; queue_config: max_samples_per_send: 1000 max_shards: 200 capacity: 2500 # è¿œç¨‹è¯»å–é…ç½® remote_read: - url: \u0026#34;http://thanos-query:9090/api/v1/query\u0026#34; read_recent: true å‘Šè­¦è§„åˆ™é…ç½® # alerts/infrastructure.yml groups: - name: infrastructure rules: # èŠ‚ç‚¹å®•æœºå‘Šè­¦ - alert: NodeDown expr: up{job=\u0026#34;kubernetes-nodes\u0026#34;} == 0 for: 1m labels: severity: critical team: infrastructure annotations: summary: \u0026#34;Node {{ $labels.instance }} is down\u0026#34; description: \u0026#34;Node {{ $labels.instance }} has been down for more than 1 minute.\u0026#34; runbook_url: \u0026#34;https://runbooks.company.com/node-down\u0026#34; # CPUä½¿ç”¨ç‡è¿‡é«˜ - alert: HighCPUUsage expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}[5m])) * 100) \u0026gt; 80 for: 5m labels: severity: warning team: infrastructure annotations: summary: \u0026#34;High CPU usage on {{ $labels.instance }}\u0026#34; description: \u0026#34;CPU usage is above 80% for more than 5 minutes on {{ $labels.instance }}.\u0026#34; # å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜ - alert: HighMemoryUsage expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 \u0026gt; 85 for: 5m labels: severity: warning team: infrastructure annotations: summary: \u0026#34;High memory usage on {{ $labels.instance }}\u0026#34; description: \u0026#34;Memory usage is above 85% for more than 5 minutes on {{ $labels.instance }}.\u0026#34; # ç£ç›˜ç©ºé—´ä¸è¶³ - alert: DiskSpaceLow expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 \u0026gt; 85 for: 5m labels: severity: warning team: infrastructure annotations: summary: \u0026#34;Disk space low on {{ $labels.instance }}\u0026#34; description: \u0026#34;Disk usage is above 85% on {{ $labels.instance }} mount {{ $labels.mountpoint }}.\u0026#34; # ç£ç›˜IOç­‰å¾…æ—¶é—´è¿‡é•¿ - alert: HighDiskIOWait expr: irate(node_cpu_seconds_total{mode=\u0026#34;iowait\u0026#34;}[5m]) * 100 \u0026gt; 20 for: 5m labels: severity: warning team: infrastructure annotations: summary: \u0026#34;High disk I/O wait on {{ $labels.instance }}\u0026#34; description: \u0026#34;Disk I/O wait time is above 20% for more than 5 minutes on {{ $labels.instance }}.\u0026#34; - name: kubernetes rules: # Podé‡å¯é¢‘ç¹ - alert: PodRestartingTooOften expr: increase(kube_pod_container_status_restarts_total[1h]) \u0026gt; 5 for: 5m labels: severity: warning team: platform annotations: summary: \u0026#34;Pod {{ $labels.namespace }}/{{ $labels.pod }} restarting too often\u0026#34; description: \u0026#34;Pod {{ $labels.namespace }}/{{ $labels.pod }} has restarted {{ $value }} times in the last hour.\u0026#34; # Podå¤„äºPendingçŠ¶æ€ - alert: PodStuckInPending expr: kube_pod_status_phase{phase=\u0026#34;Pending\u0026#34;} == 1 for: 10m labels: severity: warning team: platform annotations: summary: \u0026#34;Pod {{ $labels.namespace }}/{{ $labels.pod }} stuck in Pending\u0026#34; description: \u0026#34;Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in Pending state for more than 10 minutes.\u0026#34; # Deploymentå‰¯æœ¬æ•°ä¸åŒ¹é… - alert: DeploymentReplicasMismatch expr: kube_deployment_spec_replicas != kube_deployment_status_available_replicas for: 5m labels: severity: warning team: platform annotations: summary: \u0026#34;Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replicas mismatch\u0026#34; description: \u0026#34;Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has {{ $value }} available replicas, expected {{ $labels.spec_replicas }}.\u0026#34; - name: application rules: # HTTPé”™è¯¯ç‡è¿‡é«˜ - alert: HighHTTPErrorRate expr: (sum(rate(http_requests_total{status=~\u0026#34;5..\u0026#34;}[5m])) by (service) / sum(rate(http_requests_total[5m])) by (service)) * 100 \u0026gt; 5 for: 5m labels: severity: critical team: application annotations: summary: \u0026#34;High HTTP error rate for {{ $labels.service }}\u0026#34; description: \u0026#34;HTTP error rate is {{ $value }}% for service {{ $labels.service }}.\u0026#34; # å“åº”æ—¶é—´è¿‡é•¿ - alert: HighResponseTime expr: histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service)) \u0026gt; 1 for: 5m labels: severity: warning team: application annotations: summary: \u0026#34;High response time for {{ $labels.service }}\u0026#34; description: \u0026#34;95th percentile response time is {{ $value }}s for service {{ $labels.service }}.\u0026#34; # æ•°æ®åº“è¿æ¥æ± è€—å°½ - alert: DatabaseConnectionPoolExhausted expr: db_connection_pool_active / db_connection_pool_max \u0026gt; 0.9 for: 2m labels: severity: critical team: database annotations: summary: \u0026#34;Database connection pool nearly exhausted\u0026#34; description: \u0026#34;Database connection pool usage is {{ $value | humanizePercentage }} for {{ $labels.database }}.\u0026#34; Thanosé•¿æœŸå­˜å‚¨é…ç½® # thanos-sidecar.yaml apiVersion: apps/v1 kind: Deployment metadata: name: prometheus-with-thanos spec: replicas: 1 selector: matchLabels: app: prometheus template: metadata: labels: app: prometheus spec: containers: - name: prometheus image: prom/prometheus:v2.45.0 args: - \u0026#39;--config.file=/etc/prometheus/prometheus.yml\u0026#39; - \u0026#39;--storage.tsdb.path=/prometheus\u0026#39; - \u0026#39;--storage.tsdb.retention.time=2h\u0026#39; # æœ¬åœ°åªä¿ç•™2å°æ—¶ - \u0026#39;--storage.tsdb.min-block-duration=2h\u0026#39; - \u0026#39;--storage.tsdb.max-block-duration=2h\u0026#39; - \u0026#39;--web.enable-lifecycle\u0026#39; - \u0026#39;--web.enable-admin-api\u0026#39; ports: - containerPort: 9090 volumeMounts: - name: config mountPath: /etc/prometheus - name: storage mountPath: /prometheus - name: thanos-sidecar image: thanosio/thanos:v0.32.0 args: - sidecar - --tsdb.path=/prometheus - --prometheus.url=http://localhost:9090 - --grpc-address=0.0.0.0:10901 - --http-address=0.0.0.0:10902 - --objstore.config-file=/etc/thanos/objstore.yml ports: - containerPort: 10901 name: grpc - containerPort: 10902 name: http volumeMounts: - name: storage mountPath: /prometheus - name: objstore-config mountPath: /etc/thanos volumes: - name: config configMap: name: grafana-config - name: storage persistentVolumeClaim: claimName: grafana-storage - name: dashboards configMap: name: grafana-dashboards æ ¸å¿ƒDashboardé…ç½® { \u0026#34;dashboard\u0026#34;: { \u0026#34;id\u0026#34;: null, \u0026#34;title\u0026#34;: \u0026#34;å¯è§‚æµ‹æ€§æ€»è§ˆ\u0026#34;, \u0026#34;tags\u0026#34;: [\u0026#34;observability\u0026#34;, \u0026#34;overview\u0026#34;], \u0026#34;timezone\u0026#34;: \u0026#34;browser\u0026#34;, \u0026#34;panels\u0026#34;: [ { \u0026#34;id\u0026#34;: 1, \u0026#34;title\u0026#34;: \u0026#34;ç³»ç»Ÿå¥åº·çŠ¶æ€\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;stat\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;up\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;{{instance}}\u0026#34; } ], \u0026#34;fieldConfig\u0026#34;: { \u0026#34;defaults\u0026#34;: { \u0026#34;color\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;thresholds\u0026#34; }, \u0026#34;thresholds\u0026#34;: { \u0026#34;steps\u0026#34;: [ {\u0026#34;color\u0026#34;: \u0026#34;red\u0026#34;, \u0026#34;value\u0026#34;: 0}, {\u0026#34;color\u0026#34;: \u0026#34;green\u0026#34;, \u0026#34;value\u0026#34;: 1} ] } } } }, { \u0026#34;id\u0026#34;: 2, \u0026#34;title\u0026#34;: \u0026#34;è¯·æ±‚å“åº”æ—¶é—´\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;timeseries\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, service))\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;95th percentile - {{service}}\u0026#34; } ] }, { \u0026#34;id\u0026#34;: 3, \u0026#34;title\u0026#34;: \u0026#34;é”™è¯¯ç‡è¶‹åŠ¿\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;timeseries\u0026#34;, \u0026#34;targets\u0026#34;: [ { \u0026#34;expr\u0026#34;: \u0026#34;sum(rate(http_requests_total{status=~\\\u0026#34;5..\\\u0026#34;}[5m])) by (service) / sum(rate(http_requests_total[5m])) by (service)\u0026#34;, \u0026#34;legendFormat\u0026#34;: \u0026#34;Error Rate - {{service}}\u0026#34; } ] } ], \u0026#34;time\u0026#34;: { \u0026#34;from\u0026#34;: \u0026#34;now-1h\u0026#34;, \u0026#34;to\u0026#34;: \u0026#34;now\u0026#34; }, \u0026#34;refresh\u0026#34;: \u0026#34;30s\u0026#34; } } å‘Šè­¦ç³»ç»Ÿé…ç½® AlertManageré…ç½® # alertmanager-config.yaml apiVersion: v1 kind: ConfigMap metadata: name: alertmanager-config data: alertmanager.yml: | global: smtp_smarthost: \u0026#39;smtp.company.com:587\u0026#39; smtp_from: \u0026#39;alerts@company.com\u0026#39; smtp_auth_username: \u0026#39;alerts@company.com\u0026#39; smtp_auth_password: \u0026#39;${SMTP_PASSWORD}\u0026#39; slack_api_url: \u0026#39;${SLACK_API_URL}\u0026#39; route: group_by: [\u0026#39;alertname\u0026#39;, \u0026#39;cluster\u0026#39;, \u0026#39;service\u0026#39;] group_wait: 10s group_interval: 10s repeat_interval: 1h receiver: \u0026#39;default\u0026#39; routes: - match: severity: critical receiver: \u0026#39;critical-alerts\u0026#39; group_wait: 5s repeat_interval: 30m - match: team: infrastructure receiver: \u0026#39;infrastructure-team\u0026#39; - match: team: application receiver: \u0026#39;application-team\u0026#39; - match: team: database receiver: \u0026#39;database-team\u0026#39; receivers: - name: \u0026#39;default\u0026#39; email_configs: - to: \u0026#39;ops@company.com\u0026#39; subject: \u0026#39;[{{ .Status | toUpper }}] {{ .GroupLabels.alertname }}\u0026#39; body: | {{ range .Alerts }} Alert: {{ .Annotations.summary }} Description: {{ .Annotations.description }} Labels: {{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }} {{ end }} {{ end }} - name: \u0026#39;critical-alerts\u0026#39; email_configs: - to: \u0026#39;critical-alerts@company.com\u0026#39; subject: \u0026#39;[CRITICAL] {{ .GroupLabels.alertname }}\u0026#39; body: | CRITICAL ALERT TRIGGERED {{ range .Alerts }} Alert: {{ .Annotations.summary }} Description: {{ .Annotations.description }} Severity: {{ .Labels.severity }} Service: {{ .Labels.service }} Instance: {{ .Labels.instance }} Time: {{ .StartsAt }} {{ end }} slack_configs: - channel: \u0026#39;#critical-alerts\u0026#39; title: \u0026#39;Critical Alert: {{ .GroupLabels.alertname }}\u0026#39; text: | {{ range .Alerts }} *Alert:* {{ .Annotations.summary }} *Description:* {{ .Annotations.description }} *Severity:* {{ .Labels.severity }} *Service:* {{ .Labels.service }} {{ end }} send_resolved: true pagerduty_configs: - routing_key: \u0026#39;${PAGERDUTY_ROUTING_KEY}\u0026#39; description: \u0026#39;{{ .GroupLabels.alertname }}: {{ .Annotations.summary }}\u0026#39; - name: \u0026#39;infrastructure-team\u0026#39; email_configs: - to: \u0026#39;infrastructure@company.com\u0026#39; subject: \u0026#39;[{{ .Status | toUpper }}] Infrastructure Alert\u0026#39; slack_configs: - channel: \u0026#39;#infrastructure-alerts\u0026#39; send_resolved: true - name: \u0026#39;application-team\u0026#39; email_configs: - to: \u0026#39;application@company.com\u0026#39; subject: \u0026#39;[{{ .Status | toUpper }}] Application Alert\u0026#39; slack_configs: - channel: \u0026#39;#application-alerts\u0026#39; send_resolved: true - name: \u0026#39;database-team\u0026#39; email_configs: - to: \u0026#39;database@company.com\u0026#39; subject: \u0026#39;[{{ .Status | toUpper }}] Database Alert\u0026#39; slack_configs: - channel: \u0026#39;#database-alerts\u0026#39; send_resolved: true inhibit_rules: - source_match: severity: \u0026#39;critical\u0026#39; target_match: severity: \u0026#39;warning\u0026#39; equal: [\u0026#39;alertname\u0026#39;, \u0026#39;instance\u0026#39;] - source_match: alertname: \u0026#39;NodeDown\u0026#39; target_match_re: alertname: \u0026#39;(HighCPUUsage|HighMemoryUsage|DiskSpaceLow)\u0026#39; equal: [\u0026#39;instance\u0026#39;] æœ€ä½³å®è·µä¸ä¼˜åŒ– 1. æ€§èƒ½ä¼˜åŒ–ç­–ç•¥ æ•°æ®ä¿ç•™ç­–ç•¥: Prometheusæœ¬åœ°ä¿ç•™2å°æ—¶ï¼ŒThanosé•¿æœŸå­˜å‚¨ é‡‡æ ·ç­–ç•¥: æ ¹æ®æœåŠ¡é‡è¦æ€§è°ƒæ•´Jaegeré‡‡æ ·ç‡ ç´¢å¼•ä¼˜åŒ–: ElasticsearchæŒ‰æ—¥æœŸåˆ†ç‰‡ï¼Œå®šæœŸæ¸…ç†æ—§æ•°æ® æŸ¥è¯¢ä¼˜åŒ–: ä½¿ç”¨PromQLæœ€ä½³å®è·µï¼Œé¿å…é«˜åŸºæ•°æ ‡ç­¾ 2. å®‰å…¨åŠ å›ºæªæ–½ ä¼ è¾“åŠ å¯†: æ‰€æœ‰ç»„ä»¶é—´é€šä¿¡ä½¿ç”¨TLS èº«ä»½è®¤è¯: é›†æˆLDAP/ADè¿›è¡Œç»Ÿä¸€è®¤è¯ æƒé™æ§åˆ¶: åŸºäºRBACçš„ç»†ç²’åº¦æƒé™ç®¡ç† æ•°æ®è„±æ•: æ—¥å¿—ä¸­æ•æ„Ÿä¿¡æ¯è‡ªåŠ¨è„±æ• 3. é«˜å¯ç”¨è®¾è®¡ å¤šå‰¯æœ¬éƒ¨ç½²: å…³é”®ç»„ä»¶å¤šå‰¯æœ¬éƒ¨ç½² è·¨åŒºåŸŸå¤‡ä»½: æ•°æ®è·¨å¯ç”¨åŒºå¤‡ä»½ æ•…éšœè½¬ç§»: è‡ªåŠ¨æ•…éšœæ£€æµ‹å’Œåˆ‡æ¢ å®¹é‡è§„åˆ’: åŸºäºå†å²æ•°æ®è¿›è¡Œå®¹é‡é¢„æµ‹ 4. è¿ç»´è‡ªåŠ¨åŒ– è‡ªåŠ¨æ‰©ç¼©å®¹: åŸºäºè´Ÿè½½è‡ªåŠ¨è°ƒæ•´èµ„æº æ™ºèƒ½å‘Šè­¦: åŸºäºæœºå™¨å­¦ä¹ çš„å¼‚å¸¸æ£€æµ‹ è‡ªæ„ˆæœºåˆ¶: å¸¸è§æ•…éšœè‡ªåŠ¨ä¿®å¤ å˜æ›´ç®¡ç†: GitOpsæµç¨‹ç®¡ç†é…ç½®å˜æ›´ æ€»ç»“ ä¼ä¸šçº§å¯è§‚æµ‹æ€§å¹³å°çš„å»ºè®¾æ˜¯ä¸€ä¸ªç³»ç»Ÿå·¥ç¨‹ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘æŠ€æœ¯é€‰å‹ã€æ¶æ„è®¾è®¡ã€æ€§èƒ½ä¼˜åŒ–ã€å®‰å…¨åŠ å›ºç­‰å¤šä¸ªæ–¹é¢ã€‚é€šè¿‡Prometheus + Grafana + Jaeger + ELKçš„æŠ€æœ¯æ ˆç»„åˆï¼Œå¯ä»¥æ„å»ºä¸€ä¸ªåŠŸèƒ½å®Œæ•´ã€æ€§èƒ½ä¼˜ç§€ã€å®‰å…¨å¯é çš„å¯è§‚æµ‹æ€§å¹³å°ï¼Œä¸ºä¼ä¸šçš„æ•°å­—åŒ–è½¬å‹æä¾›å¼ºæœ‰åŠ›çš„æŠ€æœ¯æ”¯æ’‘ã€‚\n","content":"ä¼ä¸šçº§å¯è§‚æµ‹æ€§å¹³å°å»ºè®¾ï¼šPrometheus + Grafana + Jaeger + ELK å®Œæ•´æ–¹æ¡ˆ å¯è§‚æµ‹æ€§(Observability)æ˜¯ç°ä»£åˆ†å¸ƒå¼ç³»ç»Ÿè¿ç»´çš„æ ¸å¿ƒèƒ½åŠ›ï¼Œé€šè¿‡æŒ‡æ ‡(Metrics)ã€æ—¥å¿—(Logs)å’Œé“¾è·¯è¿½è¸ª(Traces)ä¸‰å¤§æ”¯æŸ±ï¼Œä¸ºç³»ç»Ÿæä¾›å…¨æ–¹ä½çš„ç›‘æ§å’Œè¯Šæ–­èƒ½åŠ›ã€‚æœ¬æ–‡å°†è¯¦ç»†ä»‹ç»å¦‚ä½•æ„å»ºä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§å¯è§‚æµ‹æ€§å¹³å°ã€‚\nå¯è§‚æµ‹æ€§æ¶æ„è®¾è®¡ æ•´ä½“æ¶æ„å›¾ graph TB subgraph \u0026amp;#34;æ•°æ®æºå±‚\u0026amp;#34; A1[åº”ç”¨æœåŠ¡] --\u0026amp;gt; B1[Metrics] A1 --\u0026amp;gt; B2[Logs] A1 --\u0026amp;gt; B3[Traces] A2[åŸºç¡€è®¾æ–½] --\u0026amp;gt; B1 A2 --\u0026amp;gt; B2 A3[ä¸­é—´ä»¶] --\u0026amp;gt; B1 A3 --\u0026amp;gt; B2 A3 --\u0026amp;gt; B3 end subgraph \u0026amp;#34;æ•°æ®æ”¶é›†å±‚\u0026amp;#34; B1 --\u0026amp;gt; C1[Prometheus] B2 --\u0026amp;gt; C2[Filebeat/Fluentd] B3 --\u0026amp;gt; C3[Jaeger Agent] C2 --\u0026amp;gt; C4[Logstash] â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["å¯è§‚æµ‹æ€§","Prometheus","Grafana","Jaeger","ELK","ç›‘æ§","APM"],"categories":["è¿ç»´"],"author":"å¯è§‚æµ‹æ€§ä¸“å®¶","readingTime":20,"wordCount":4249,"section":"posts","type":"posts","draft":false,"featured":false,"series":["ç›‘æ§è¿ç»´"]},{"title":"å®¹å™¨ç¼–æ’ä¸ç®¡ç†å®æˆ˜ï¼šä»Dockeråˆ°Kubernetesçš„ä¼ä¸šçº§å®è·µ","url":"https://www.dishuihengxin.com/posts/devops-container-orchestration/","summary":"å®¹å™¨ç¼–æ’ä¸ç®¡ç†å®æˆ˜ï¼šä»Dockeråˆ°Kubernetesçš„ä¼ä¸šçº§å®è·µ å®¹å™¨æŠ€æœ¯å·²ç»æˆä¸ºç°ä»£åº”ç”¨éƒ¨ç½²å’Œç®¡ç†çš„æ ¸å¿ƒæŠ€æœ¯æ ˆã€‚æœ¬æ–‡å°†ä»DockeråŸºç¡€å¼€å§‹ï¼Œæ·±å…¥æ¢è®¨Kubernetesé›†ç¾¤çš„è®¾è®¡ã€éƒ¨ç½²å’Œç®¡ç†ï¼Œä¸ºä¼ä¸šçº§å®¹å™¨åŒ–æä¾›å®Œæ•´çš„å®è·µæŒ‡å—ã€‚\nå®¹å™¨æŠ€æœ¯æ¶æ„æ¦‚è§ˆ æ•´ä½“æŠ€æœ¯æ ˆ graph TB subgraph \u0026#34;å¼€å‘å±‚\u0026#34; A1[åº”ç”¨ä»£ç ] --\u0026gt; A2[Dockerfile] A2 --\u0026gt; A3[å®¹å™¨é•œåƒ] end subgraph \u0026#34;é•œåƒç®¡ç†å±‚\u0026#34; A3 --\u0026gt; B1[Harbor Registry] B1 --\u0026gt; B2[é•œåƒæ‰«æ] B1 --\u0026gt; B3[é•œåƒç­¾å] end subgraph \u0026#34;ç¼–æ’å±‚\u0026#34; B1 --\u0026gt; C1[Kubernetes] C1 --\u0026gt; C2[Podç®¡ç†] C1 --\u0026gt; C3[Serviceç½‘ç»œ] C1 --\u0026gt; C4[å­˜å‚¨ç®¡ç†] end subgraph \u0026#34;è¿è¡Œæ—¶å±‚\u0026#34; C1 --\u0026gt; D1[containerd] D1 --\u0026gt; D2[runc] D2 --\u0026gt; D3[Linux Namespace] D2 --\u0026gt; D4[cgroups] end subgraph \u0026#34;åŸºç¡€è®¾æ–½å±‚\u0026#34; D3 --\u0026gt; E1[è®¡ç®—èµ„æº] D4 --\u0026gt; E2[ç½‘ç»œèµ„æº] C4 --\u0026gt; E3[å­˜å‚¨èµ„æº] end subgraph \u0026#34;ç›‘æ§å±‚\u0026#34; C1 --\u0026gt; F1[Prometheus] F1 --\u0026gt; F2[Grafana] C1 --\u0026gt; F3[Jaeger] C1 --\u0026gt; F4[ELK Stack] end Dockerä¼ä¸šçº§å®è·µ å¤šé˜¶æ®µæ„å»ºä¼˜åŒ– # Dockerfile.multi-stage # æ„å»ºé˜¶æ®µ FROM golang:1.21-alpine AS builder # è®¾ç½®å·¥ä½œç›®å½• WORKDIR /app # å®‰è£…æ„å»ºä¾èµ– RUN apk add --no-cache git ca-certificates tzdata # å¤åˆ¶ä¾èµ–æ–‡ä»¶ COPY go.mod go.sum ./ RUN go mod download # å¤åˆ¶æºä»£ç  COPY . . # æ„å»ºåº”ç”¨ RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build \\ -ldflags=\u0026#39;-w -s -extldflags \u0026#34;-static\u0026#34;\u0026#39; \\ -a -installsuffix cgo \\ -o main ./cmd/server # è¿è¡Œé˜¶æ®µ FROM scratch # ä»æ„å»ºé˜¶æ®µå¤åˆ¶å¿…è¦æ–‡ä»¶ COPY --from=builder /etc/ssl/certs/ca-certificates.crt /etc/ssl/certs/ COPY --from=builder /usr/share/zoneinfo /usr/share/zoneinfo COPY --from=builder /app/main /main # è®¾ç½®æ—¶åŒº ENV TZ=Asia/Shanghai # åˆ›å»ºérootç”¨æˆ· USER 65534:65534 # æš´éœ²ç«¯å£ EXPOSE 8080 # å¥åº·æ£€æŸ¥ HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\ CMD [\u0026#34;/main\u0026#34;, \u0026#34;healthcheck\u0026#34;] # å¯åŠ¨åº”ç”¨ ENTRYPOINT [\u0026#34;/main\u0026#34;] Dockerå®‰å…¨æœ€ä½³å®è·µ # Dockerfile.secure FROM alpine:3.18 # åˆ›å»ºéç‰¹æƒç”¨æˆ· RUN addgroup -g 1001 -S appgroup \u0026amp;\u0026amp; \\ adduser -u 1001 -S appuser -G appgroup # å®‰è£…å¿…è¦çš„å®‰å…¨æ›´æ–° RUN apk update \u0026amp;\u0026amp; \\ apk upgrade \u0026amp;\u0026amp; \\ apk add --no-cache \\ ca-certificates \\ dumb-init \u0026amp;\u0026amp; \\ rm -rf /var/cache/apk/* # è®¾ç½®å·¥ä½œç›®å½• WORKDIR /app # å¤åˆ¶åº”ç”¨æ–‡ä»¶å¹¶è®¾ç½®æƒé™ COPY --chown=appuser:appgroup ./app /app/ RUN chmod +x /app/main # åˆ‡æ¢åˆ°éç‰¹æƒç”¨æˆ· USER appuser # ä½¿ç”¨dumb-initä½œä¸ºPID 1 ENTRYPOINT [\u0026#34;dumb-init\u0026#34;, \u0026#34;--\u0026#34;] CMD [\u0026#34;./main\u0026#34;] # å®‰å…¨æ ‡ç­¾ LABEL security.scan=\u0026#34;enabled\u0026#34; \\ security.policy=\u0026#34;restricted\u0026#34; \\ maintainer=\u0026#34;security@company.com\u0026#34; Docker Composeç”Ÿäº§ç¯å¢ƒé…ç½® # docker-compose.prod.yml version: \u0026#39;3.8\u0026#39; services: app: image: myapp:${APP_VERSION:-latest} build: context: . dockerfile: Dockerfile.multi-stage target: production restart: unless-stopped environment: - NODE_ENV=production - DATABASE_URL=${DATABASE_URL} - REDIS_URL=${REDIS_URL} - JWT_SECRET=${JWT_SECRET} ports: - \u0026#34;8080:8080\u0026#34; volumes: - app-logs:/app/logs - app-uploads:/app/uploads networks: - app-network depends_on: - database - redis healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;curl\u0026#34;, \u0026#34;-f\u0026#34;, \u0026#34;http://localhost:8080/health\u0026#34;] interval: 30s timeout: 10s retries: 3 start_period: 40s deploy: resources: limits: cpus: \u0026#39;2.0\u0026#39; memory: 2G reservations: cpus: \u0026#39;0.5\u0026#39; memory: 512M restart_policy: condition: on-failure delay: 5s max_attempts: 3 window: 120s logging: driver: \u0026#34;json-file\u0026#34; options: max-size: \u0026#34;10m\u0026#34; max-file: \u0026#34;3\u0026#34; security_opt: - no-new-privileges:true read_only: true tmpfs: - /tmp:noexec,nosuid,size=100m database: image: postgres:15-alpine restart: unless-stopped environment: - POSTGRES_DB=${DB_NAME} - POSTGRES_USER=${DB_USER} - POSTGRES_PASSWORD=${DB_PASSWORD} volumes: - postgres-data:/var/lib/postgresql/data - ./init.sql:/docker-entrypoint-initdb.d/init.sql:ro networks: - app-network ports: - \u0026#34;127.0.0.1:5432:5432\u0026#34; command: \u0026gt; postgres -c shared_preload_libraries=pg_stat_statements -c pg_stat_statements.track=all -c max_connections=200 -c shared_buffers=256MB -c effective_cache_size=1GB -c work_mem=4MB -c maintenance_work_mem=64MB healthcheck: test: [\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;pg_isready -U ${DB_USER} -d ${DB_NAME}\u0026#34;] interval: 30s timeout: 10s retries: 5 deploy: resources: limits: cpus: \u0026#39;1.0\u0026#39; memory: 1G reservations: cpus: \u0026#39;0.25\u0026#39; memory: 256M redis: image: redis:7-alpine restart: unless-stopped command: \u0026gt; redis-server --appendonly yes --appendfsync everysec --maxmemory 512mb --maxmemory-policy allkeys-lru volumes: - redis-data:/data networks: - app-network ports: - \u0026#34;127.0.0.1:6379:6379\u0026#34; healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;redis-cli\u0026#34;, \u0026#34;ping\u0026#34;] interval: 30s timeout: 10s retries: 3 deploy: resources: limits: cpus: \u0026#39;0.5\u0026#39; memory: 512M reservations: cpus: \u0026#39;0.1\u0026#39; memory: 128M nginx: image: nginx:1.25-alpine restart: unless-stopped ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - ./nginx.conf:/etc/nginx/nginx.conf:ro - ./ssl:/etc/nginx/ssl:ro - nginx-logs:/var/log/nginx networks: - app-network depends_on: - app healthcheck: test: [\u0026#34;CMD\u0026#34;, \u0026#34;wget\u0026#34;, \u0026#34;--quiet\u0026#34;, \u0026#34;--tries=1\u0026#34;, \u0026#34;--spider\u0026#34;, \u0026#34;http://localhost/health\u0026#34;] interval: 30s timeout: 10s retries: 3 deploy: resources: limits: cpus: \u0026#39;0.5\u0026#39; memory: 256M reservations: cpus: \u0026#39;0.1\u0026#39; memory: 64M volumes: postgres-data: driver: local driver_opts: type: none o: bind device: /data/postgres redis-data: driver: local driver_opts: type: none o: bind device: /data/redis app-logs: driver: local app-uploads: driver: local nginx-logs: driver: local networks: app-network: driver: bridge ipam: config: - subnet: 172.20.0.0/16 Kubernetesé›†ç¾¤æ¶æ„è®¾è®¡ é«˜å¯ç”¨é›†ç¾¤é…ç½® # kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta3 kind: InitConfiguration localAPIEndpoint: advertiseAddress: 10.0.1.10 bindPort: 6443 nodeRegistration: criSocket: unix:///var/run/containerd/containerd.sock kubeletExtraArgs: cloud-provider: external container-runtime: remote container-runtime-endpoint: unix:///var/run/containerd/containerd.sock --- apiVersion: kubeadm.k8s.io/v1beta3 kind: ClusterConfiguration kubernetesVersion: v1.28.0 clusterName: production-cluster controlPlaneEndpoint: k8s-api.company.com:6443 apiServer: advertiseAddress: 10.0.1.10 bindPort: 6443 certSANs: - k8s-api.company.com - 10.0.1.10 - 10.0.1.11 - 10.0.1.12 - 127.0.0.1 extraArgs: audit-log-maxage: \u0026#34;30\u0026#34; audit-log-maxbackup: \u0026#34;10\u0026#34; audit-log-maxsize: \u0026#34;100\u0026#34; audit-log-path: /var/log/audit.log audit-policy-file: /etc/kubernetes/audit-policy.yaml enable-admission-plugins: NodeRestriction,ResourceQuota,PodSecurityPolicy encryption-provider-config: /etc/kubernetes/encryption-config.yaml extraVolumes: - name: audit-policy hostPath: /etc/kubernetes/audit-policy.yaml mountPath: /etc/kubernetes/audit-policy.yaml readOnly: true pathType: File - name: encryption-config hostPath: /etc/kubernetes/encryption-config.yaml mountPath: /etc/kubernetes/encryption-config.yaml readOnly: true pathType: File etcd: local: dataDir: /var/lib/etcd extraArgs: listen-metrics-urls: http://0.0.0.0:2381 auto-compaction-mode: periodic auto-compaction-retention: \u0026#34;1\u0026#34; max-request-bytes: \u0026#34;33554432\u0026#34; quota-backend-bytes: \u0026#34;6442450944\u0026#34; networking: serviceSubnet: 10.96.0.0/12 podSubnet: 10.244.0.0/16 dnsDomain: cluster.local controllerManager: extraArgs: bind-address: 0.0.0.0 secure-port: \u0026#34;10257\u0026#34; cluster-signing-duration: \u0026#34;8760h\u0026#34; scheduler: extraArgs: bind-address: 0.0.0.0 secure-port: \u0026#34;10259\u0026#34; --- apiVersion: kubeadm.k8s.io/v1beta3 kind: KubeletConfiguration cgroupDriver: systemd containerRuntimeEndpoint: unix:///var/run/containerd/containerd.sock resolvConf: /run/systemd/resolve/resolv.conf runtimeRequestTimeout: \u0026#34;15m\u0026#34; tlsCipherSuites: - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 - TLS_RSA_WITH_AES_256_GCM_SHA384 - TLS_RSA_WITH_AES_128_GCM_SHA256 protectKernelDefaults: true makeIPTablesUtilChains: true eventRecordQPS: 0 shutdownGracePeriod: 60s shutdownGracePeriodCriticalPods: 20s --- apiVersion: kubeproxy.config.k8s.io/v1alpha1 kind: KubeProxyConfiguration bindAddress: 0.0.0.0 metricsBindAddress: 0.0.0.0:10249 mode: ipvs ipvs: strictARP: true scheduler: rr iptables: masqueradeAll: true ç½‘ç»œæ’ä»¶é…ç½® (Calico) # calico-config.yaml apiVersion: operator.tigera.io/v1 kind: Installation metadata: name: default spec: calicoNetwork: ipPools: - blockSize: 26 cidr: 10.244.0.0/16 encapsulation: VXLANCrossSubnet natOutgoing: Enabled nodeSelector: all() nodeAddressAutodetectionV4: interface: \u0026#34;eth0\u0026#34; mtu: 1440 registry: quay.io/ imagePullSecrets: - name: tigera-pull-secret --- apiVersion: projectcalico.org/v3 kind: BGPConfiguration metadata: name: default spec: logSeverityScreen: Info nodeToNodeMeshEnabled: true asNumber: 64512 --- apiVersion: projectcalico.org/v3 kind: IPPool metadata: name: default-ipv4-ippool spec: cidr: 10.244.0.0/16 ipipMode: CrossSubnet vxlanMode: Never natOutgoing: true disabled: false nodeSelector: all() --- apiVersion: projectcalico.org/v3 kind: NetworkPolicy metadata: name: default-deny-all namespace: default spec: selector: all() types: - Ingress - Egress --- apiVersion: projectcalico.org/v3 kind: NetworkPolicy metadata: name: allow-dns namespace: default spec: selector: all() types: - Egress egress: - action: Allow protocol: UDP destination: selector: k8s-app == \u0026#34;kube-dns\u0026#34; ports: - 53 - action: Allow protocol: TCP destination: selector: k8s-app == \u0026#34;kube-dns\u0026#34; ports: - 53 å­˜å‚¨ç±»é…ç½® # storage-classes.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: fast-ssd annotations: storageclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34; provisioner: kubernetes.io/aws-ebs parameters: type: gp3 iops: \u0026#34;3000\u0026#34; throughput: \u0026#34;125\u0026#34; encrypted: \u0026#34;true\u0026#34; kmsKeyId: arn:aws:kms:us-west-2:123456789012:key/12345678-1234-1234-1234-123456789012 volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true reclaimPolicy: Delete --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: standard-hdd provisioner: kubernetes.io/aws-ebs parameters: type: gp2 encrypted: \u0026#34;true\u0026#34; volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true reclaimPolicy: Delete --- apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: high-iops-ssd provisioner: kubernetes.io/aws-ebs parameters: type: io2 iops: \u0026#34;10000\u0026#34; encrypted: \u0026#34;true\u0026#34; volumeBindingMode: WaitForFirstConsumer allowVolumeExpansion: true reclaimPolicy: Retain --- # æœ¬åœ°å­˜å‚¨ç±» apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: local-storage provisioner: kubernetes.io/no-provisioner volumeBindingMode: WaitForFirstConsumer reclaimPolicy: Delete --- apiVersion: v1 kind: PersistentVolume metadata: name: local-pv-1 spec: capacity: storage: 100Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Delete storageClassName: local-storage local: path: /mnt/disks/ssd1 nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - worker-node-1 åº”ç”¨éƒ¨ç½²ä¸ç®¡ç† å¾®æœåŠ¡éƒ¨ç½²æ¨¡æ¿ # microservice-template.yaml apiVersion: apps/v1 kind: Deployment metadata: name: user-service namespace: production labels: app: user-service version: v1.2.3 component: backend spec: replicas: 3 strategy: type: RollingUpdate rollingUpdate: maxSurge: 1 maxUnavailable: 0 selector: matchLabels: app: user-service template: metadata: labels: app: user-service version: v1.2.3 component: backend annotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/port: \u0026#34;8080\u0026#34; prometheus.io/path: \u0026#34;/metrics\u0026#34; spec: serviceAccountName: user-service securityContext: runAsNonRoot: true runAsUser: 1001 fsGroup: 1001 containers: - name: user-service image: myregistry.com/user-service:v1.2.3 imagePullPolicy: Always ports: - containerPort: 8080 name: http protocol: TCP - containerPort: 8081 name: grpc protocol: TCP env: - name: DATABASE_URL valueFrom: secretKeyRef: name: user-service-secrets key: database-url - name: REDIS_URL valueFrom: configMapKeyRef: name: user-service-config key: redis-url - name: LOG_LEVEL value: \u0026#34;info\u0026#34; - name: JAEGER_AGENT_HOST valueFrom: fieldRef: fieldPath: status.hostIP resources: requests: memory: \u0026#34;256Mi\u0026#34; cpu: \u0026#34;250m\u0026#34; limits: memory: \u0026#34;512Mi\u0026#34; cpu: \u0026#34;500m\u0026#34; livenessProbe: httpGet: path: /health port: 8080 initialDelaySeconds: 30 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 3 readinessProbe: httpGet: path: /ready port: 8080 initialDelaySeconds: 5 periodSeconds: 5 timeoutSeconds: 3 failureThreshold: 3 startupProbe: httpGet: path: /startup port: 8080 initialDelaySeconds: 10 periodSeconds: 10 timeoutSeconds: 5 failureThreshold: 30 volumeMounts: - name: config mountPath: /app/config readOnly: true - name: secrets mountPath: /app/secrets readOnly: true - name: tmp mountPath: /tmp securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true capabilities: drop: - ALL volumes: - name: config configMap: name: user-service-config - name: secrets secret: secretName: user-service-secrets - name: tmp emptyDir: {} imagePullSecrets: - name: registry-secret nodeSelector: node-type: application tolerations: - key: \u0026#34;application\u0026#34; operator: \u0026#34;Equal\u0026#34; value: \u0026#34;true\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - user-service topologyKey: kubernetes.io/hostname --- apiVersion: v1 kind: Service metadata: name: user-service namespace: production labels: app: user-service annotations: service.beta.kubernetes.io/aws-load-balancer-type: nlb service.beta.kubernetes.io/aws-load-balancer-internal: \u0026#34;true\u0026#34; spec: type: LoadBalancer ports: - port: 80 targetPort: 8080 protocol: TCP name: http - port: 8081 targetPort: 8081 protocol: TCP name: grpc selector: app: user-service --- apiVersion: v1 kind: ConfigMap metadata: name: user-service-config namespace: production data: redis-url: \u0026#34;redis://redis-cluster:6379\u0026#34; log-format: \u0026#34;json\u0026#34; metrics-enabled: \u0026#34;true\u0026#34; tracing-enabled: \u0026#34;true\u0026#34; app.yaml: | server: port: 8080 timeout: 30s database: max_connections: 100 idle_timeout: 300s cache: ttl: 3600s max_size: 1000 --- apiVersion: v1 kind: Secret metadata: name: user-service-secrets namespace: production type: Opaque data: database-url: cG9zdGdyZXNxbDovL3VzZXI6cGFzc3dvcmRAZGI6NTQzMi9kYg== jwt-secret: bXlfc3VwZXJfc2VjcmV0X2tleQ== api-key: YWJjZGVmZ2hpams= --- apiVersion: v1 kind: ServiceAccount metadata: name: user-service namespace: production annotations: eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/user-service-role --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: user-service-netpol namespace: production spec: podSelector: matchLabels: app: user-service policyTypes: - Ingress - Egress ingress: - from: - namespaceSelector: matchLabels: name: production - podSelector: matchLabels: component: frontend ports: - protocol: TCP port: 8080 egress: - to: - namespaceSelector: matchLabels: name: production - podSelector: matchLabels: app: database ports: - protocol: TCP port: 5432 - to: - namespaceSelector: matchLabels: name: production - podSelector: matchLabels: app: redis ports: - protocol: TCP port: 6379 - to: [] ports: - protocol: UDP port: 53 Ingressé…ç½® # ingress-nginx.yaml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: app-ingress namespace: production annotations: kubernetes.io/ingress.class: nginx nginx.ingress.kubernetes.io/ssl-redirect: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/force-ssl-redirect: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/proxy-body-size: \u0026#34;50m\u0026#34; nginx.ingress.kubernetes.io/proxy-connect-timeout: \u0026#34;60\u0026#34; nginx.ingress.kubernetes.io/proxy-send-timeout: \u0026#34;60\u0026#34; nginx.ingress.kubernetes.io/proxy-read-timeout: \u0026#34;60\u0026#34; nginx.ingress.kubernetes.io/rate-limit: \u0026#34;100\u0026#34; nginx.ingress.kubernetes.io/rate-limit-window: \u0026#34;1m\u0026#34; nginx.ingress.kubernetes.io/enable-cors: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/cors-allow-origin: \u0026#34;https://app.company.com\u0026#34; nginx.ingress.kubernetes.io/cors-allow-methods: \u0026#34;GET, POST, PUT, DELETE, OPTIONS\u0026#34; nginx.ingress.kubernetes.io/cors-allow-headers: \u0026#34;DNT,X-CustomHeader,Keep-Alive,User-Agent,X-Requested-With,If-Modified-Since,Cache-Control,Content-Type,Authorization\u0026#34; cert-manager.io/cluster-issuer: \u0026#34;letsencrypt-prod\u0026#34; nginx.ingress.kubernetes.io/configuration-snippet: | more_set_headers \u0026#34;X-Frame-Options: DENY\u0026#34;; more_set_headers \u0026#34;X-Content-Type-Options: nosniff\u0026#34;; more_set_headers \u0026#34;X-XSS-Protection: 1; mode=block\u0026#34;; more_set_headers \u0026#34;Strict-Transport-Security: max-age=31536000; includeSubDomains\u0026#34;; spec: tls: - hosts: - api.company.com - app.company.com secretName: app-tls-secret rules: - host: api.company.com http: paths: - path: /api/v1/users pathType: Prefix backend: service: name: user-service port: number: 80 - path: /api/v1/orders pathType: Prefix backend: service: name: order-service port: number: 80 - path: /api/v1/payments pathType: Prefix backend: service: name: payment-service port: number: 80 - host: app.company.com http: paths: - path: / pathType: Prefix backend: service: name: frontend-service port: number: 80 --- apiVersion: networking.k8s.io/v1 kind: IngressClass metadata: name: nginx annotations: ingressclass.kubernetes.io/is-default-class: \u0026#34;true\u0026#34; spec: controller: k8s.io/ingress-nginx å®‰å…¨åŠ å›ºä¸ç­–ç•¥ Pod Security Standards # pod-security-policy.yaml apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: restricted spec: privileged: false allowPrivilegeEscalation: false requiredDropCapabilities: - ALL volumes: - \u0026#39;configMap\u0026#39; - \u0026#39;emptyDir\u0026#39; - \u0026#39;projected\u0026#39; - \u0026#39;secret\u0026#39; - \u0026#39;downwardAPI\u0026#39; - \u0026#39;persistentVolumeClaim\u0026#39; runAsUser: rule: \u0026#39;MustRunAsNonRoot\u0026#39; seLinux: rule: \u0026#39;RunAsAny\u0026#39; fsGroup: rule: \u0026#39;RunAsAny\u0026#39; readOnlyRootFilesystem: true --- apiVersion: v1 kind: Namespace metadata: name: production labels: pod-security.kubernetes.io/enforce: restricted pod-security.kubernetes.io/audit: restricted pod-security.kubernetes.io/warn: restricted --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: psp-restricted rules: - apiGroups: [\u0026#39;policy\u0026#39;] resources: [\u0026#39;podsecuritypolicies\u0026#39;] verbs: [\u0026#39;use\u0026#39;] resourceNames: - restricted --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: psp-restricted roleRef: kind: ClusterRole name: psp-restricted apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount name: default namespace: production OPA Gatekeeperç­–ç•¥ # gatekeeper-policies.yaml apiVersion: templates.gatekeeper.sh/v1beta1 kind: ConstraintTemplate metadata: name: k8srequiredlabels spec: crd: spec: names: kind: K8sRequiredLabels validation: openAPIV3Schema: type: object properties: labels: type: array items: type: string targets: - target: admission.k8s.gatekeeper.sh rego: | package k8srequiredlabels violation[{\u0026#34;msg\u0026#34;: msg}] { required := input.parameters.labels provided := input.review.object.metadata.labels missing := required[_] not provided[missing] msg := sprintf(\u0026#34;Missing required label: %v\u0026#34;, [missing]) } --- apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sRequiredLabels metadata: name: must-have-app-label spec: match: kinds: - apiGroups: [\u0026#34;apps\u0026#34;] kinds: [\u0026#34;Deployment\u0026#34;, \u0026#34;StatefulSet\u0026#34;, \u0026#34;DaemonSet\u0026#34;] namespaces: [\u0026#34;production\u0026#34;] parameters: labels: [\u0026#34;app\u0026#34;, \u0026#34;version\u0026#34;, \u0026#34;component\u0026#34;] --- apiVersion: templates.gatekeeper.sh/v1beta1 kind: ConstraintTemplate metadata: name: k8scontainerresources spec: crd: spec: names: kind: K8sContainerResources validation: openAPIV3Schema: type: object properties: cpu: type: string memory: type: string targets: - target: admission.k8s.gatekeeper.sh rego: | package k8scontainerresources violation[{\u0026#34;msg\u0026#34;: msg}] { container := input.review.object.spec.template.spec.containers[_] not container.resources.requests.cpu msg := \u0026#34;Container must specify CPU requests\u0026#34; } violation[{\u0026#34;msg\u0026#34;: msg}] { container := input.review.object.spec.template.spec.containers[_] not container.resources.requests.memory msg := \u0026#34;Container must specify memory requests\u0026#34; } violation[{\u0026#34;msg\u0026#34;: msg}] { container := input.review.object.spec.template.spec.containers[_] not container.resources.limits.cpu msg := \u0026#34;Container must specify CPU limits\u0026#34; } violation[{\u0026#34;msg\u0026#34;: msg}] { container := input.review.object.spec.template.spec.containers[_] not container.resources.limits.memory msg := \u0026#34;Container must specify memory limits\u0026#34; } --- apiVersion: constraints.gatekeeper.sh/v1beta1 kind: K8sContainerResources metadata: name: must-have-resources spec: match: kinds: - apiGroups: [\u0026#34;apps\u0026#34;] kinds: [\u0026#34;Deployment\u0026#34;, \u0026#34;StatefulSet\u0026#34;, \u0026#34;DaemonSet\u0026#34;] namespaces: [\u0026#34;production\u0026#34;] CI/CDé›†æˆ GitLab CIå®¹å™¨åŒ–æµæ°´çº¿ # .gitlab-ci.yml stages: - build - test - security - deploy-staging - deploy-production variables: DOCKER_DRIVER: overlay2 DOCKER_TLS_CERTDIR: \u0026#34;/certs\u0026#34; REGISTRY: registry.company.com IMAGE_NAME: $REGISTRY/$CI_PROJECT_PATH KUBECONFIG: /tmp/kubeconfig before_script: - echo $CI_REGISTRY_PASSWORD | docker login -u $CI_REGISTRY_USER --password-stdin $CI_REGISTRY build: stage: build image: docker:20.10.16 services: - docker:20.10.16-dind script: - docker build --build-arg BUILD_DATE=$(date -u +\u0026#39;%Y-%m-%dT%H:%M:%SZ\u0026#39;) --build-arg VCS_REF=$CI_COMMIT_SHA --build-arg VERSION=$CI_COMMIT_TAG -t $IMAGE_NAME:$CI_COMMIT_SHA -t $IMAGE_NAME:latest . - docker push $IMAGE_NAME:$CI_COMMIT_SHA - docker push $IMAGE_NAME:latest only: - main - develop - tags test: stage: test image: $IMAGE_NAME:$CI_COMMIT_SHA script: - go test -v -race -coverprofile=coverage.out ./... - go tool cover -html=coverage.out -o coverage.html artifacts: reports: coverage_report: coverage_format: cobertura path: coverage.xml paths: - coverage.html coverage: \u0026#39;/coverage: \\d+\\.\\d+% of statements/\u0026#39; only: - main - develop - merge_requests security-scan: stage: security image: aquasec/trivy:latest script: - trivy image --exit-code 0 --severity HIGH,CRITICAL --format template --template \u0026#34;@contrib/sarif.tpl\u0026#34; -o trivy-results.sarif $IMAGE_NAME:$CI_COMMIT_SHA - trivy image --exit-code 1 --severity CRITICAL $IMAGE_NAME:$CI_COMMIT_SHA artifacts: reports: sast: trivy-results.sarif only: - main - develop - tags deploy-staging: stage: deploy-staging image: bitnami/kubectl:latest environment: name: staging url: https://staging.company.com script: - echo $KUBE_CONFIG_STAGING | base64 -d \u0026gt; $KUBECONFIG - kubectl config use-context staging - envsubst \u0026lt; k8s/deployment.yaml | kubectl apply -f - - kubectl rollout status deployment/app -n staging --timeout=300s - kubectl get pods -n staging -l app=myapp variables: NAMESPACE: staging REPLICAS: 2 IMAGE_TAG: $CI_COMMIT_SHA only: - develop deploy-production: stage: deploy-production image: bitnami/kubectl:latest environment: name: production url: https://app.company.com script: - echo $KUBE_CONFIG_PRODUCTION | base64 -d \u0026gt; $KUBECONFIG - kubectl config use-context production - envsubst \u0026lt; k8s/deployment.yaml | kubectl apply -f - - kubectl rollout status deployment/app -n production --timeout=600s - kubectl get pods -n production -l app=myapp variables: NAMESPACE: production REPLICAS: 5 IMAGE_TAG: $CI_COMMIT_SHA when: manual only: - main - tags Helm Chartæ¨¡æ¿ # Chart.yaml apiVersion: v2 name: microservice description: A Helm chart for microservice deployment type: application version: 0.1.0 appVersion: \u0026#34;1.0.0\u0026#34; dependencies: - name: postgresql version: 11.9.13 repository: https://charts.bitnami.com/bitnami condition: postgresql.enabled - name: redis version: 17.3.7 repository: https://charts.bitnami.com/bitnami condition: redis.enabled --- # values.yaml replicaCount: 3 image: repository: myregistry.com/myapp pullPolicy: IfNotPresent tag: \u0026#34;\u0026#34; imagePullSecrets: - name: registry-secret nameOverride: \u0026#34;\u0026#34; fullnameOverride: \u0026#34;\u0026#34; serviceAccount: create: true annotations: {} name: \u0026#34;\u0026#34; podAnnotations: prometheus.io/scrape: \u0026#34;true\u0026#34; prometheus.io/port: \u0026#34;8080\u0026#34; prometheus.io/path: \u0026#34;/metrics\u0026#34; podSecurityContext: fsGroup: 1001 runAsNonRoot: true runAsUser: 1001 securityContext: allowPrivilegeEscalation: false readOnlyRootFilesystem: true capabilities: drop: - ALL service: type: ClusterIP port: 80 targetPort: 8080 ingress: enabled: true className: \u0026#34;nginx\u0026#34; annotations: cert-manager.io/cluster-issuer: letsencrypt-prod nginx.ingress.kubernetes.io/ssl-redirect: \u0026#34;true\u0026#34; hosts: - host: api.company.com paths: - path: /api/v1 pathType: Prefix tls: - secretName: api-tls hosts: - api.company.com resources: limits: cpu: 500m memory: 512Mi requests: cpu: 250m memory: 256Mi autoscaling: enabled: true minReplicas: 3 maxReplicas: 10 targetCPUUtilizationPercentage: 70 targetMemoryUtilizationPercentage: 80 nodeSelector: {} tolerations: [] affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app.kubernetes.io/name operator: In values: - microservice topologyKey: kubernetes.io/hostname postgresql: enabled: true auth: postgresPassword: \u0026#34;changeme\u0026#34; database: \u0026#34;myapp\u0026#34; primary: persistence: enabled: true size: 20Gi redis: enabled: true auth: enabled: false master: persistence: enabled: true size: 8Gi config: logLevel: info database: maxConnections: 100 cache: ttl: 3600 secrets: databaseUrl: \u0026#34;\u0026#34; jwtSecret: \u0026#34;\u0026#34; apiKey: \u0026#34;\u0026#34; æ€»ç»“ å®¹å™¨ç¼–æ’ä¸ç®¡ç†æ˜¯ç°ä»£äº‘åŸç”Ÿåº”ç”¨çš„æ ¸å¿ƒæŠ€æœ¯ã€‚é€šè¿‡Dockerçš„æ ‡å‡†åŒ–æ‰“åŒ…ã€Kubernetesçš„å¼ºå¤§ç¼–æ’èƒ½åŠ›ï¼Œä»¥åŠå®Œå–„çš„å®‰å…¨ç­–ç•¥å’ŒCI/CDé›†æˆï¼Œä¼ä¸šå¯ä»¥æ„å»ºé«˜æ•ˆã€å¯é ã€å®‰å…¨çš„å®¹å™¨åŒ–å¹³å°ã€‚\n","content":"å®¹å™¨ç¼–æ’ä¸ç®¡ç†å®æˆ˜ï¼šä»Dockeråˆ°Kubernetesçš„ä¼ä¸šçº§å®è·µ å®¹å™¨æŠ€æœ¯å·²ç»æˆä¸ºç°ä»£åº”ç”¨éƒ¨ç½²å’Œç®¡ç†çš„æ ¸å¿ƒæŠ€æœ¯æ ˆã€‚æœ¬æ–‡å°†ä»DockeråŸºç¡€å¼€å§‹ï¼Œæ·±å…¥æ¢è®¨Kubernetesé›†ç¾¤çš„è®¾è®¡ã€éƒ¨ç½²å’Œç®¡ç†ï¼Œä¸ºä¼ä¸šçº§å®¹å™¨åŒ–æä¾›å®Œæ•´çš„å®è·µæŒ‡å—ã€‚\nå®¹å™¨æŠ€æœ¯æ¶æ„æ¦‚è§ˆ æ•´ä½“æŠ€æœ¯æ ˆ graph TB subgraph \u0026amp;#34;å¼€å‘å±‚\u0026amp;#34; A1[åº”ç”¨ä»£ç ] --\u0026amp;gt; A2[Dockerfile] A2 --\u0026amp;gt; A3[å®¹å™¨é•œåƒ] end subgraph \u0026amp;#34;é•œåƒç®¡ç†å±‚\u0026amp;#34; A3 --\u0026amp;gt; B1[Harbor Registry] B1 --\u0026amp;gt; B2[é•œåƒæ‰«æ] B1 --\u0026amp;gt; B3[é•œåƒç­¾å] end subgraph \u0026amp;#34;ç¼–æ’å±‚\u0026amp;#34; B1 --\u0026amp;gt; C1[Kubernetes] C1 --\u0026amp;gt; C2[Podç®¡ç†] C1 --\u0026amp;gt; C3[Serviceç½‘ç»œ] C1 --\u0026amp;gt; C4[å­˜å‚¨ç®¡ç†] end subgraph \u0026amp;#34;è¿è¡Œæ—¶å±‚\u0026amp;#34; C1 --\u0026amp;gt; D1[containerd] D1 --\u0026amp;gt; D2[runc] â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["å®¹å™¨","Docker","Kubernetes","ç¼–æ’","å¾®æœåŠ¡","DevOps"],"categories":["è¿ç»´"],"author":"å®¹å™¨æŠ€æœ¯ä¸“å®¶","readingTime":12,"wordCount":2400,"section":"posts","type":"posts","draft":false,"featured":false,"series":["å®¹å™¨åŒ–è¿ç»´"]},{"title":"ä½¿ç”¨æ¡æ¬¾","url":"https://www.dishuihengxin.com/terms/","summary":"ä½¿ç”¨æ¡æ¬¾ æœ€åæ›´æ–°æ—¶é—´ï¼š2025å¹´12æœˆ31æ—¥\næ¬¢è¿è®¿é—®æœ¬æŠ€æœ¯åšå®¢ã€‚ä½¿ç”¨æœ¬ç½‘ç«™å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä»¥ä¸‹ä½¿ç”¨æ¡æ¬¾ã€‚å¦‚æœæ‚¨ä¸åŒæ„è¿™äº›æ¡æ¬¾ï¼Œè¯·ä¸è¦ä½¿ç”¨æœ¬ç½‘ç«™ã€‚\n1. æœåŠ¡è¯´æ˜ 1.1 ç½‘ç«™æ€§è´¨ æœ¬ç½‘ç«™æ˜¯ä¸€ä¸ªä¸ªäººæŠ€æœ¯åšå®¢ï¼Œæ—¨åœ¨åˆ†äº«æŠ€æœ¯çŸ¥è¯†ã€ç»éªŒå’Œè§è§£ã€‚å†…å®¹åŒ…æ‹¬ä½†ä¸é™äºï¼š\næŠ€æœ¯æ–‡ç« å’Œæ•™ç¨‹ ç¼–ç¨‹ç»éªŒåˆ†äº« æŠ€æœ¯å·¥å…·æ¨è è¡Œä¸šåŠ¨æ€è¯„è®º 1.2 æœåŠ¡æä¾› æœ¬ç½‘ç«™å…è´¹æä¾›å†…å®¹è®¿é—® æˆ‘ä»¬ä¿ç•™éšæ—¶ä¿®æ”¹ã€æš‚åœæˆ–ç»ˆæ­¢æœåŠ¡çš„æƒåˆ© æˆ‘ä»¬ä¸ä¿è¯æœåŠ¡çš„æŒç»­å¯ç”¨æ€§ 2. çŸ¥è¯†äº§æƒ 2.1 åŸåˆ›å†…å®¹ æœ¬ç½‘ç«™çš„åŸåˆ›å†…å®¹ï¼ˆåŒ…æ‹¬æ–‡ç« ã€å›¾ç‰‡ã€ä»£ç ç­‰ï¼‰å—ç‰ˆæƒä¿æŠ¤ï¼Œç‰ˆæƒå½’ä½œè€…æ‰€æœ‰ã€‚æœªç»è®¸å¯ï¼Œä¸å¾—ï¼š\nå•†ä¸šæ€§ä½¿ç”¨ ä¿®æ”¹æˆ–åˆ›å»ºè¡ç”Ÿä½œå“ åˆ é™¤ç‰ˆæƒå£°æ˜ 2.2 åˆç†ä½¿ç”¨ æ‚¨å¯ä»¥ï¼š\nä¸ªäººå­¦ä¹ å’Œç ”ç©¶ éå•†ä¸šæ€§åˆ†äº«ï¼ˆéœ€æ³¨æ˜å‡ºå¤„ï¼‰ åœ¨æŠ€æœ¯è®¨è®ºä¸­å¼•ç”¨ï¼ˆéœ€æ³¨æ˜æ¥æºï¼‰ 2.3 ç¬¬ä¸‰æ–¹å†…å®¹ æœ¬ç½‘ç«™å¯èƒ½åŒ…å«ç¬¬ä¸‰æ–¹å†…å®¹æˆ–é“¾æ¥ã€‚è¿™äº›å†…å®¹çš„ç‰ˆæƒå½’å„è‡ªæ‰€æœ‰è€…æ‰€æœ‰ã€‚\n2.4 å¼€æºä»£ç  æœ¬ç½‘ç«™åˆ†äº«çš„ä»£ç ç¤ºä¾‹ï¼Œé™¤éå¦æœ‰è¯´æ˜ï¼Œé€šå¸¸é‡‡ç”¨MITæˆ–Apache 2.0è®¸å¯è¯ã€‚å…·ä½“è®¸å¯ä¿¡æ¯è¯·æŸ¥çœ‹å„ä»£ç ç‰‡æ®µçš„è¯´æ˜ã€‚\n3. ç”¨æˆ·è´£ä»» 3.1 è´¦å·å’Œè¯„è®º å¦‚æœæ‚¨åˆ›å»ºè´¦å·æˆ–å‘è¡¨è¯„è®ºï¼Œæ‚¨éœ€è¦ï¼š\næä¾›å‡†ç¡®çš„ä¿¡æ¯ ä¿æŠ¤è´¦å·å®‰å…¨ å¯¹æ‚¨çš„è¡Œä¸ºè´Ÿè´£ 3.2 ç¦æ­¢è¡Œä¸º æ‚¨ä¸å¾—ï¼š\nå‘å¸ƒéæ³•ã€æœ‰å®³ã€å¨èƒã€è¾±éª‚ã€éªšæ‰°ã€è¯½è°¤ã€ç²—ä¿—ã€æ·«ç§½æˆ–å…¶ä»–ä»¤äººåæ„Ÿçš„å†…å®¹ å†’å……ä»–äººæˆ–è™šå‡é™ˆè¿°ä¸ä»–äººçš„å…³ç³» ä¸Šä¼ å«æœ‰ç—…æ¯’æˆ–æ¶æ„ä»£ç çš„å†…å®¹ å¹²æ‰°æˆ–ç ´åç½‘ç«™æœåŠ¡ æ”¶é›†å…¶ä»–ç”¨æˆ·çš„ä¸ªäººä¿¡æ¯ è¿åä»»ä½•é€‚ç”¨çš„æ³•å¾‹æ³•è§„ 3.3 å†…å®¹å®¡æ ¸ æˆ‘ä»¬ä¿ç•™å®¡æ ¸ã€ç¼–è¾‘æˆ–åˆ é™¤ä»»ä½•è¿åæœ¬æ¡æ¬¾çš„å†…å®¹çš„æƒåˆ©ã€‚\n4. å…è´£å£°æ˜ 4.1 å†…å®¹å‡†ç¡®æ€§ æœ¬ç½‘ç«™å†…å®¹\u0026quot;æŒ‰åŸæ ·\u0026quot;æä¾›ï¼Œä¸ä½œä»»ä½•æ˜ç¤ºæˆ–æš—ç¤ºçš„ä¿è¯ æˆ‘ä»¬åŠªåŠ›ç¡®ä¿ä¿¡æ¯å‡†ç¡®ï¼Œä½†ä¸ä¿è¯å†…å®¹çš„å®Œæ•´æ€§ã€å‡†ç¡®æ€§æˆ–æ—¶æ•ˆæ€§ æŠ€æœ¯å‘å±•è¿…é€Ÿï¼ŒæŸäº›å†…å®¹å¯èƒ½å·²è¿‡æ—¶ 4.2 ä½¿ç”¨é£é™© æ‚¨ä½¿ç”¨æœ¬ç½‘ç«™çš„ä¿¡æ¯å’Œå»ºè®®éœ€è‡ªè¡Œæ‰¿æ‹…é£é™© æˆ‘ä»¬ä¸å¯¹å› ä½¿ç”¨æˆ–ä¾èµ–ç½‘ç«™å†…å®¹è€Œå¯¼è‡´çš„ä»»ä½•æŸå¤±è´Ÿè´£ å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ä»»ä½•ä»£ç æˆ–æ–¹æ¡ˆå‰è¿›è¡Œå……åˆ†æµ‹è¯• 4.3 å¤–éƒ¨é“¾æ¥ æœ¬ç½‘ç«™å¯èƒ½åŒ…å«æŒ‡å‘ç¬¬ä¸‰æ–¹ç½‘ç«™çš„é“¾æ¥ æˆ‘ä»¬ä¸å¯¹è¿™äº›ç½‘ç«™çš„å†…å®¹ã€éšç§æ”¿ç­–æˆ–å®è·µè´Ÿè´£ è®¿é—®å¤–éƒ¨é“¾æ¥éœ€è‡ªè¡Œæ‰¿æ‹…é£é™© 5. æŠ€æœ¯å£°æ˜ 5.1 ä»£ç ä½¿ç”¨ æœ¬ç½‘ç«™æä¾›çš„ä»£ç ç¤ºä¾‹ä»…ä¾›å­¦ä¹ å’Œå‚è€ƒ åœ¨ç”Ÿäº§ç¯å¢ƒä½¿ç”¨å‰ï¼Œè¯·è¿›è¡Œå½»åº•æµ‹è¯• æˆ‘ä»¬ä¸å¯¹ä»£ç ä½¿ç”¨é€ æˆçš„ä»»ä½•é—®é¢˜è´Ÿè´£ 5.2 æŠ€æœ¯å»ºè®® æœ¬ç½‘ç«™çš„æŠ€æœ¯å»ºè®®åŸºäºä½œè€…çš„ç»éªŒå’Œç†è§£ å¯èƒ½ä¸é€‚ç”¨äºæ‰€æœ‰æƒ…å†µ è¯·æ ¹æ®å…·ä½“æƒ…å†µè¿›è¡Œè¯„ä¼°å’Œè°ƒæ•´ 6. éšç§ä¿æŠ¤ æœ¬ç½‘ç«™çš„éšç§å®è·µåœ¨éšç§æ”¿ç­–ä¸­è¯¦ç»†è¯´æ˜ã€‚ä½¿ç”¨æœ¬ç½‘ç«™å³è¡¨ç¤ºæ‚¨åŒæ„éšç§æ”¿ç­–ã€‚\n","content":"ä½¿ç”¨æ¡æ¬¾ æœ€åæ›´æ–°æ—¶é—´ï¼š2025å¹´12æœˆ31æ—¥\næ¬¢è¿è®¿é—®æœ¬æŠ€æœ¯åšå®¢ã€‚ä½¿ç”¨æœ¬ç½‘ç«™å³è¡¨ç¤ºæ‚¨åŒæ„éµå®ˆä»¥ä¸‹ä½¿ç”¨æ¡æ¬¾ã€‚å¦‚æœæ‚¨ä¸åŒæ„è¿™äº›æ¡æ¬¾ï¼Œè¯·ä¸è¦ä½¿ç”¨æœ¬ç½‘ç«™ã€‚\n1. æœåŠ¡è¯´æ˜ 1.1 ç½‘ç«™æ€§è´¨ æœ¬ç½‘ç«™æ˜¯ä¸€ä¸ªä¸ªäººæŠ€æœ¯åšå®¢ï¼Œæ—¨åœ¨åˆ†äº«æŠ€æœ¯çŸ¥è¯†ã€ç»éªŒå’Œè§è§£ã€‚å†…å®¹åŒ…æ‹¬ä½†ä¸é™äºï¼š\næŠ€æœ¯æ–‡ç« å’Œæ•™ç¨‹ ç¼–ç¨‹ç»éªŒåˆ†äº« æŠ€æœ¯å·¥å…·æ¨è è¡Œä¸šåŠ¨æ€è¯„è®º 1.2 æœåŠ¡æä¾› æœ¬ç½‘ç«™å…è´¹æä¾›å†…å®¹è®¿é—® æˆ‘ä»¬ä¿ç•™éšæ—¶ä¿®æ”¹ã€æš‚åœæˆ–ç»ˆæ­¢æœåŠ¡çš„æƒåˆ© æˆ‘ä»¬ä¸ä¿è¯æœåŠ¡çš„æŒç»­å¯ç”¨æ€§ 2. çŸ¥è¯†äº§æƒ 2.1 åŸåˆ›å†…å®¹ æœ¬ç½‘ç«™çš„åŸåˆ›å†…å®¹ï¼ˆåŒ…æ‹¬æ–‡ç« ã€å›¾ç‰‡ã€ä»£ç ç­‰ï¼‰å—ç‰ˆæƒä¿æŠ¤ï¼Œç‰ˆæƒå½’ä½œè€…æ‰€æœ‰ã€‚æœªç»è®¸å¯ï¼Œä¸å¾—ï¼š\nå•†ä¸šæ€§ä½¿ç”¨ ä¿®æ”¹æˆ–åˆ›å»ºè¡ç”Ÿä½œå“ åˆ é™¤ç‰ˆæƒå£°æ˜ 2.2 åˆç†ä½¿ç”¨ æ‚¨å¯ä»¥ï¼š\nä¸ªäººå­¦ä¹ å’Œç ”ç©¶ éå•†ä¸šæ€§åˆ†äº«ï¼ˆéœ€æ³¨æ˜å‡ºå¤„ï¼‰ åœ¨æŠ€æœ¯è®¨è®ºä¸­å¼•ç”¨ï¼ˆéœ€æ³¨æ˜æ¥æºï¼‰ 2.3 ç¬¬ä¸‰æ–¹å†…å®¹ æœ¬ç½‘ç«™å¯èƒ½åŒ…å«ç¬¬ä¸‰æ–¹å†…å®¹æˆ–é“¾æ¥ã€‚è¿™äº›å†…å®¹çš„ç‰ˆæƒå½’å„è‡ªæ‰€æœ‰è€…æ‰€æœ‰ã€‚\n2.4 å¼€æºä»£ç  æœ¬ç½‘ç«™åˆ†äº«çš„ä»£ç ç¤ºä¾‹ï¼Œé™¤éå¦æœ‰è¯´æ˜ï¼Œé€šå¸¸é‡‡ç”¨MITæˆ–Apache 2.0è®¸å¯è¯ã€‚å…·ä½“è®¸å¯ä¿¡æ¯è¯·æŸ¥çœ‹å„ä»£ç ç‰‡æ®µçš„è¯´æ˜ã€‚\n3. ç”¨æˆ·è´£ä»» 3.1 è´¦å·å’Œè¯„è®º å¦‚æœæ‚¨åˆ›å»ºè´¦å·æˆ–å‘è¡¨è¯„è®ºï¼Œæ‚¨éœ€è¦ï¼š\næä¾›å‡† â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":null,"categories":null,"author":"åšä¸»","readingTime":1,"wordCount":144,"section":"","type":"page","draft":false,"featured":false,"series":null},{"title":"æ•°æ®åº“å®‰å…¨ä¸æƒé™ç®¡ç†ï¼šä»åŸºç¡€é˜²æŠ¤åˆ°ä¼ä¸šçº§å®‰å…¨ç­–ç•¥çš„å®Œæ•´æŒ‡å—","url":"https://www.dishuihengxin.com/posts/database-security-management/","summary":"æ•°æ®åº“å®‰å…¨ä¸æƒé™ç®¡ç†ï¼šä»åŸºç¡€é˜²æŠ¤åˆ°ä¼ä¸šçº§å®‰å…¨ç­–ç•¥çš„å®Œæ•´æŒ‡å— å¼•è¨€ æ•°æ®åº“å®‰å…¨æ˜¯ç°ä»£ä¼ä¸šä¿¡æ¯å®‰å…¨ä½“ç³»çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚éšç€æ•°æ®æ³„éœ²äº‹ä»¶é¢‘å‘å’Œç›‘ç®¡è¦æ±‚æ—¥è¶‹ä¸¥æ ¼ï¼Œå»ºç«‹å®Œå–„çš„æ•°æ®åº“å®‰å…¨é˜²æŠ¤ä½“ç³»å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨æ•°æ®åº“å®‰å…¨çš„å„ä¸ªå±‚é¢ï¼Œä»åŸºç¡€çš„è®¿é—®æ§åˆ¶åˆ°é«˜çº§çš„å®‰å…¨ç­–ç•¥å®æ–½ï¼Œä¸ºä¼ä¸šæä¾›å…¨é¢çš„æ•°æ®åº“å®‰å…¨è§£å†³æ–¹æ¡ˆã€‚\næ•°æ®åº“å®‰å…¨æ¶æ„æ¦‚è¿° å®‰å…¨é˜²æŠ¤å±‚æ¬¡æ¨¡å‹ graph TB subgraph \u0026#34;æ•°æ®åº“å®‰å…¨æ¶æ„\u0026#34; A[ç½‘ç»œå®‰å…¨å±‚] --\u0026gt; B[ä¸»æœºå®‰å…¨å±‚] B --\u0026gt; C[æ•°æ®åº“å®‰å…¨å±‚] C --\u0026gt; D[åº”ç”¨å®‰å…¨å±‚] D --\u0026gt; E[æ•°æ®å®‰å…¨å±‚] subgraph \u0026#34;ç½‘ç»œå®‰å…¨\u0026#34; F[é˜²ç«å¢™] G[VPN] H[ç½‘ç»œéš”ç¦»] end subgraph \u0026#34;ä¸»æœºå®‰å…¨\u0026#34; I[æ“ä½œç³»ç»ŸåŠ å›º] J[è®¿é—®æ§åˆ¶] K[æ—¥å¿—å®¡è®¡] end subgraph \u0026#34;æ•°æ®åº“å®‰å…¨\u0026#34; L[èº«ä»½è®¤è¯] M[æƒé™ç®¡ç†] N[æ•°æ®åŠ å¯†] O[å®¡è®¡ç›‘æ§] end subgraph \u0026#34;åº”ç”¨å®‰å…¨\u0026#34; P[SQLæ³¨å…¥é˜²æŠ¤] Q[è¿æ¥æ± å®‰å…¨] R[APIå®‰å…¨] end subgraph \u0026#34;æ•°æ®å®‰å…¨\u0026#34; S[æ•æ„Ÿæ•°æ®è¯†åˆ«] T[æ•°æ®è„±æ•] U[æ•°æ®åˆ†ç±»åˆ†çº§] end end èº«ä»½è®¤è¯ä¸è®¿é—®æ§åˆ¶ 1. å¤šå±‚èº«ä»½è®¤è¯ç³»ç»Ÿ #!/usr/bin/env python3 # scripts/database_auth_manager.py import hashlib import secrets import time import json import logging from datetime import datetime, timedelta from typing import Dict, List, Optional, Tuple import mysql.connector import psycopg2 from cryptography.fernet import Fernet import ldap3 class DatabaseAuthManager: def __init__(self, config_file: str): with open(config_file, \u0026#39;r\u0026#39;) as f: self.config = json.load(f) self.logger = self._setup_logging() self.encryption_key = Fernet.generate_key() self.cipher_suite = Fernet(self.encryption_key) # è¿æ¥æ•°æ®åº“ self.db_connections = self._initialize_db_connections() # LDAPé…ç½® self.ldap_server = self.config.get(\u0026#39;ldap\u0026#39;, {}).get(\u0026#39;server\u0026#39;) self.ldap_base_dn = self.config.get(\u0026#39;ldap\u0026#39;, {}).get(\u0026#39;base_dn\u0026#39;) def _setup_logging(self) -\u0026gt; logging.Logger: \u0026#34;\u0026#34;\u0026#34;è®¾ç½®æ—¥å¿—è®°å½•\u0026#34;\u0026#34;\u0026#34; logger = logging.getLogger(\u0026#39;DatabaseAuthManager\u0026#39;) logger.setLevel(logging.INFO) handler = logging.FileHandler(\u0026#39;/var/log/database_auth.log\u0026#39;) formatter = logging.Formatter( \u0026#39;%(asctime)s - %(name)s - %(levelname)s - %(message)s\u0026#39; ) handler.setFormatter(formatter) logger.addHandler(handler) return logger def _initialize_db_connections(self) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;åˆå§‹åŒ–æ•°æ®åº“è¿æ¥\u0026#34;\u0026#34;\u0026#34; connections = {} for db_name, db_config in self.config[\u0026#39;databases\u0026#39;].items(): try: if db_config[\u0026#39;type\u0026#39;] == \u0026#39;mysql\u0026#39;: conn = mysql.connector.connect( host=db_config[\u0026#39;host\u0026#39;], port=db_config[\u0026#39;port\u0026#39;], user=db_config[\u0026#39;admin_user\u0026#39;], password=db_config[\u0026#39;admin_password\u0026#39;], database=db_config[\u0026#39;database\u0026#39;] ) elif db_config[\u0026#39;type\u0026#39;] == \u0026#39;postgresql\u0026#39;: conn = psycopg2.connect( host=db_config[\u0026#39;host\u0026#39;], port=db_config[\u0026#39;port\u0026#39;], user=db_config[\u0026#39;admin_user\u0026#39;], password=db_config[\u0026#39;admin_password\u0026#39;], database=db_config[\u0026#39;database\u0026#39;] ) connections[db_name] = conn self.logger.info(f\u0026#34;æˆåŠŸè¿æ¥åˆ°æ•°æ®åº“: {db_name}\u0026#34;) except Exception as e: self.logger.error(f\u0026#34;è¿æ¥æ•°æ®åº“ {db_name} å¤±è´¥: {e}\u0026#34;) return connections def create_user_account(self, username: str, password: str, email: str, role: str, database: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºç”¨æˆ·è´¦æˆ·\u0026#34;\u0026#34;\u0026#34; try: # ç”Ÿæˆå¯†ç å“ˆå¸Œ salt = secrets.token_hex(16) password_hash = hashlib.pbkdf2_hmac(\u0026#39;sha256\u0026#39;, password.encode(\u0026#39;utf-8\u0026#39;), salt.encode(\u0026#39;utf-8\u0026#39;), 100000) # åœ¨è®¤è¯æ•°æ®åº“ä¸­åˆ›å»ºç”¨æˆ·è®°å½• auth_conn = self.db_connections.get(\u0026#39;auth_db\u0026#39;) if not auth_conn: raise Exception(\u0026#34;è®¤è¯æ•°æ®åº“è¿æ¥ä¸å¯ç”¨\u0026#34;) cursor = auth_conn.cursor() # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å·²å­˜åœ¨ cursor.execute( \u0026#34;SELECT username FROM users WHERE username = %s\u0026#34;, (username,) ) if cursor.fetchone(): self.logger.warning(f\u0026#34;ç”¨æˆ· {username} å·²å­˜åœ¨\u0026#34;) return False # æ’å…¥ç”¨æˆ·è®°å½• cursor.execute(\u0026#34;\u0026#34;\u0026#34; INSERT INTO users (username, password_hash, salt, email, role, created_at, last_login, is_active, failed_attempts) VALUES (%s, %s, %s, %s, %s, %s, NULL, TRUE, 0) \u0026#34;\u0026#34;\u0026#34;, (username, password_hash.hex(), salt, email, role, datetime.now())) auth_conn.commit() # åœ¨ç›®æ ‡æ•°æ®åº“ä¸­åˆ›å»ºæ•°æ®åº“ç”¨æˆ· self._create_database_user(username, password, role, database) self.logger.info(f\u0026#34;æˆåŠŸåˆ›å»ºç”¨æˆ·: {username}\u0026#34;) return True except Exception as e: self.logger.error(f\u0026#34;åˆ›å»ºç”¨æˆ· {username} å¤±è´¥: {e}\u0026#34;) return False def _create_database_user(self, username: str, password: str, role: str, database: str): \u0026#34;\u0026#34;\u0026#34;åœ¨æ•°æ®åº“ä¸­åˆ›å»ºç”¨æˆ·\u0026#34;\u0026#34;\u0026#34; conn = self.db_connections.get(database) if not conn: raise Exception(f\u0026#34;æ•°æ®åº“ {database} è¿æ¥ä¸å¯ç”¨\u0026#34;) cursor = conn.cursor() # æ ¹æ®æ•°æ®åº“ç±»å‹æ‰§è¡Œä¸åŒçš„åˆ›å»ºç”¨æˆ·å‘½ä»¤ db_config = self.config[\u0026#39;databases\u0026#39;][database] if db_config[\u0026#39;type\u0026#39;] == \u0026#39;mysql\u0026#39;: # MySQLç”¨æˆ·åˆ›å»º cursor.execute(f\u0026#34;CREATE USER \u0026#39;{username}\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;{password}\u0026#39;\u0026#34;) # æ ¹æ®è§’è‰²åˆ†é…æƒé™ if role == \u0026#39;admin\u0026#39;: cursor.execute(f\u0026#34;GRANT ALL PRIVILEGES ON *.* TO \u0026#39;{username}\u0026#39;@\u0026#39;%\u0026#39;\u0026#34;) elif role == \u0026#39;developer\u0026#39;: cursor.execute(f\u0026#34;GRANT SELECT, INSERT, UPDATE, DELETE ON {db_config[\u0026#39;database\u0026#39;]}.* TO \u0026#39;{username}\u0026#39;@\u0026#39;%\u0026#39;\u0026#34;) elif role == \u0026#39;readonly\u0026#39;: cursor.execute(f\u0026#34;GRANT SELECT ON {db_config[\u0026#39;database\u0026#39;]}.* TO \u0026#39;{username}\u0026#39;@\u0026#39;%\u0026#39;\u0026#34;) cursor.execute(\u0026#34;FLUSH PRIVILEGES\u0026#34;) elif db_config[\u0026#39;type\u0026#39;] == \u0026#39;postgresql\u0026#39;: # PostgreSQLç”¨æˆ·åˆ›å»º cursor.execute(f\u0026#34;CREATE USER {username} WITH PASSWORD \u0026#39;{password}\u0026#39;\u0026#34;) # æ ¹æ®è§’è‰²åˆ†é…æƒé™ if role == \u0026#39;admin\u0026#39;: cursor.execute(f\u0026#34;ALTER USER {username} CREATEDB CREATEROLE\u0026#34;) cursor.execute(f\u0026#34;GRANT ALL PRIVILEGES ON DATABASE {db_config[\u0026#39;database\u0026#39;]} TO {username}\u0026#34;) elif role == \u0026#39;developer\u0026#39;: cursor.execute(f\u0026#34;GRANT CONNECT ON DATABASE {db_config[\u0026#39;database\u0026#39;]} TO {username}\u0026#34;) cursor.execute(f\u0026#34;GRANT USAGE ON SCHEMA public TO {username}\u0026#34;) cursor.execute(f\u0026#34;GRANT SELECT, INSERT, UPDATE, DELETE ON ALL TABLES IN SCHEMA public TO {username}\u0026#34;) elif role == \u0026#39;readonly\u0026#39;: cursor.execute(f\u0026#34;GRANT CONNECT ON DATABASE {db_config[\u0026#39;database\u0026#39;]} TO {username}\u0026#34;) cursor.execute(f\u0026#34;GRANT USAGE ON SCHEMA public TO {username}\u0026#34;) cursor.execute(f\u0026#34;GRANT SELECT ON ALL TABLES IN SCHEMA public TO {username}\u0026#34;) conn.commit() def authenticate_user(self, username: str, password: str, client_ip: str = None) -\u0026gt; Tuple[bool, Dict]: \u0026#34;\u0026#34;\u0026#34;ç”¨æˆ·èº«ä»½è®¤è¯\u0026#34;\u0026#34;\u0026#34; try: auth_conn = self.db_connections.get(\u0026#39;auth_db\u0026#39;) cursor = auth_conn.cursor(dictionary=True) # è·å–ç”¨æˆ·ä¿¡æ¯ cursor.execute(\u0026#34;\u0026#34;\u0026#34; SELECT username, password_hash, salt, role, is_active, failed_attempts, last_failed_attempt FROM users WHERE username = %s \u0026#34;\u0026#34;\u0026#34;, (username,)) user = cursor.fetchone() if not user: self.logger.warning(f\u0026#34;ç”¨æˆ· {username} ä¸å­˜åœ¨ï¼ŒIP: {client_ip}\u0026#34;) return False, {\u0026#34;error\u0026#34;: \u0026#34;ç”¨æˆ·ä¸å­˜åœ¨\u0026#34;} if not user[\u0026#39;is_active\u0026#39;]: self.logger.warning(f\u0026#34;ç”¨æˆ· {username} å·²è¢«ç¦ç”¨ï¼ŒIP: {client_ip}\u0026#34;) return False, {\u0026#34;error\u0026#34;: \u0026#34;ç”¨æˆ·å·²è¢«ç¦ç”¨\u0026#34;} # æ£€æŸ¥è´¦æˆ·é”å®šçŠ¶æ€ if user[\u0026#39;failed_attempts\u0026#39;] \u0026gt;= 5: last_failed = user[\u0026#39;last_failed_attempt\u0026#39;] if last_failed and (datetime.now() - last_failed).seconds \u0026lt; 1800: # 30åˆ†é’Ÿé”å®š self.logger.warning(f\u0026#34;ç”¨æˆ· {username} è´¦æˆ·è¢«é”å®šï¼ŒIP: {client_ip}\u0026#34;) return False, {\u0026#34;error\u0026#34;: \u0026#34;è´¦æˆ·å·²è¢«é”å®š\u0026#34;} # éªŒè¯å¯†ç  password_hash = hashlib.pbkdf2_hmac(\u0026#39;sha256\u0026#39;, password.encode(\u0026#39;utf-8\u0026#39;), user[\u0026#39;salt\u0026#39;].encode(\u0026#39;utf-8\u0026#39;), 100000) if password_hash.hex() != user[\u0026#39;password_hash\u0026#39;]: # è®°å½•å¤±è´¥å°è¯• cursor.execute(\u0026#34;\u0026#34;\u0026#34; UPDATE users SET failed_attempts = failed_attempts + 1, last_failed_attempt = %s WHERE username = %s \u0026#34;\u0026#34;\u0026#34;, (datetime.now(), username)) auth_conn.commit() self.logger.warning(f\u0026#34;ç”¨æˆ· {username} å¯†ç é”™è¯¯ï¼ŒIP: {client_ip}\u0026#34;) return False, {\u0026#34;error\u0026#34;: \u0026#34;å¯†ç é”™è¯¯\u0026#34;} # è®¤è¯æˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°å¹¶æ›´æ–°æœ€åç™»å½•æ—¶é—´ cursor.execute(\u0026#34;\u0026#34;\u0026#34; UPDATE users SET failed_attempts = 0, last_login = %s, last_failed_attempt = NULL WHERE username = %s \u0026#34;\u0026#34;\u0026#34;, (datetime.now(), username)) auth_conn.commit() # è®°å½•ç™»å½•æ—¥å¿— self._log_user_activity(username, \u0026#39;login\u0026#39;, client_ip, True) self.logger.info(f\u0026#34;ç”¨æˆ· {username} è®¤è¯æˆåŠŸï¼ŒIP: {client_ip}\u0026#34;) return True, { \u0026#34;username\u0026#34;: user[\u0026#39;username\u0026#39;], \u0026#34;role\u0026#34;: user[\u0026#39;role\u0026#39;], \u0026#34;session_token\u0026#34;: self._generate_session_token(username) } except Exception as e: self.logger.error(f\u0026#34;è®¤è¯ç”¨æˆ· {username} æ—¶å‘ç”Ÿé”™è¯¯: {e}\u0026#34;) return False, {\u0026#34;error\u0026#34;: \u0026#34;è®¤è¯æœåŠ¡å¼‚å¸¸\u0026#34;} def _generate_session_token(self, username: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆä¼šè¯ä»¤ç‰Œ\u0026#34;\u0026#34;\u0026#34; token_data = { \u0026#34;username\u0026#34;: username, \u0026#34;timestamp\u0026#34;: datetime.now().isoformat(), \u0026#34;random\u0026#34;: secrets.token_hex(16) } token_json = json.dumps(token_data) encrypted_token = self.cipher_suite.encrypt(token_json.encode()) return encrypted_token.decode() def validate_session_token(self, token: str) -\u0026gt; Tuple[bool, str]: \u0026#34;\u0026#34;\u0026#34;éªŒè¯ä¼šè¯ä»¤ç‰Œ\u0026#34;\u0026#34;\u0026#34; try: decrypted_token = self.cipher_suite.decrypt(token.encode()) token_data = json.loads(decrypted_token.decode()) # æ£€æŸ¥ä»¤ç‰Œæ—¶æ•ˆæ€§ï¼ˆ24å°æ—¶ï¼‰ token_time = datetime.fromisoformat(token_data[\u0026#39;timestamp\u0026#39;]) if (datetime.now() - token_time).total_seconds() \u0026gt; 86400: return False, \u0026#34;ä»¤ç‰Œå·²è¿‡æœŸ\u0026#34; return True, token_data[\u0026#39;username\u0026#39;] except Exception as e: self.logger.error(f\u0026#34;éªŒè¯ä¼šè¯ä»¤ç‰Œå¤±è´¥: {e}\u0026#34;) return False, \u0026#34;æ— æ•ˆä»¤ç‰Œ\u0026#34; def ldap_authenticate(self, username: str, password: str) -\u0026gt; Tuple[bool, Dict]: \u0026#34;\u0026#34;\u0026#34;LDAPèº«ä»½è®¤è¯\u0026#34;\u0026#34;\u0026#34; if not self.ldap_server: return False, {\u0026#34;error\u0026#34;: \u0026#34;LDAPæœªé…ç½®\u0026#34;} try: server = ldap3.Server(self.ldap_server) user_dn = f\u0026#34;uid={username},{self.ldap_base_dn}\u0026#34; conn = ldap3.Connection(server, user_dn, password, auto_bind=True) # è·å–ç”¨æˆ·å±æ€§ conn.search(user_dn, \u0026#39;(objectClass=*)\u0026#39;, attributes=[\u0026#39;cn\u0026#39;, \u0026#39;mail\u0026#39;, \u0026#39;memberOf\u0026#39;]) if conn.entries: user_info = conn.entries[0] groups = user_info.memberOf.values if hasattr(user_info, \u0026#39;memberOf\u0026#39;) else [] # æ ¹æ®LDAPç»„æ˜ å°„æ•°æ®åº“è§’è‰² role = self._map_ldap_groups_to_role(groups) self.logger.info(f\u0026#34;LDAPè®¤è¯æˆåŠŸ: {username}\u0026#34;) return True, { \u0026#34;username\u0026#34;: username, \u0026#34;email\u0026#34;: str(user_info.mail) if hasattr(user_info, \u0026#39;mail\u0026#39;) else \u0026#34;\u0026#34;, \u0026#34;role\u0026#34;: role, \u0026#34;groups\u0026#34;: groups } return False, {\u0026#34;error\u0026#34;: \u0026#34;LDAPè®¤è¯å¤±è´¥\u0026#34;} except Exception as e: self.logger.error(f\u0026#34;LDAPè®¤è¯é”™è¯¯: {e}\u0026#34;) return False, {\u0026#34;error\u0026#34;: \u0026#34;LDAPæœåŠ¡å¼‚å¸¸\u0026#34;} def _map_ldap_groups_to_role(self, groups: List[str]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;æ˜ å°„LDAPç»„åˆ°æ•°æ®åº“è§’è‰²\u0026#34;\u0026#34;\u0026#34; group_role_mapping = self.config.get(\u0026#39;ldap\u0026#39;, {}).get(\u0026#39;group_mapping\u0026#39;, {}) for group in groups: if group in group_role_mapping: return group_role_mapping[group] return \u0026#39;readonly\u0026#39; # é»˜è®¤åªè¯»æƒé™ def _log_user_activity(self, username: str, activity: str, client_ip: str, success: bool): \u0026#34;\u0026#34;\u0026#34;è®°å½•ç”¨æˆ·æ´»åŠ¨æ—¥å¿—\u0026#34;\u0026#34;\u0026#34; try: auth_conn = self.db_connections.get(\u0026#39;auth_db\u0026#39;) cursor = auth_conn.cursor() cursor.execute(\u0026#34;\u0026#34;\u0026#34; INSERT INTO user_activity_log (username, activity, client_ip, success, timestamp) VALUES (%s, %s, %s, %s, %s) \u0026#34;\u0026#34;\u0026#34;, (username, activity, client_ip, success, datetime.now())) auth_conn.commit() except Exception as e: self.logger.error(f\u0026#34;è®°å½•ç”¨æˆ·æ´»åŠ¨æ—¥å¿—å¤±è´¥: {e}\u0026#34;) def get_user_permissions(self, username: str, database: str) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;è·å–ç”¨æˆ·æƒé™åˆ—è¡¨\u0026#34;\u0026#34;\u0026#34; try: conn = self.db_connections.get(database) cursor = conn.cursor() db_config = self.config[\u0026#39;databases\u0026#39;][database] if db_config[\u0026#39;type\u0026#39;] == \u0026#39;mysql\u0026#39;: cursor.execute(\u0026#34;\u0026#34;\u0026#34; SELECT PRIVILEGE_TYPE, TABLE_SCHEMA, TABLE_NAME FROM information_schema.USER_PRIVILEGES WHERE GRANTEE = %s UNION SELECT PRIVILEGE_TYPE, TABLE_SCHEMA, TABLE_NAME FROM information_schema.SCHEMA_PRIVILEGES WHERE GRANTEE = %s UNION SELECT PRIVILEGE_TYPE, TABLE_SCHEMA, TABLE_NAME FROM information_schema.TABLE_PRIVILEGES WHERE GRANTEE = %s \u0026#34;\u0026#34;\u0026#34;, (f\u0026#34;\u0026#39;{username}\u0026#39;@\u0026#39;%\u0026#39;\u0026#34;, f\u0026#34;\u0026#39;{username}\u0026#39;@\u0026#39;%\u0026#39;\u0026#34;, f\u0026#34;\u0026#39;{username}\u0026#39;@\u0026#39;%\u0026#39;\u0026#34;)) elif db_config[\u0026#39;type\u0026#39;] == \u0026#39;postgresql\u0026#39;: cursor.execute(\u0026#34;\u0026#34;\u0026#34; SELECT privilege_type, table_schema, table_name FROM information_schema.table_privileges WHERE grantee = %s \u0026#34;\u0026#34;\u0026#34;, (username,)) permissions = cursor.fetchall() return [f\u0026#34;{perm[0]} on {perm[1]}.{perm[2]}\u0026#34; for perm in permissions] except Exception as e: self.logger.error(f\u0026#34;è·å–ç”¨æˆ· {username} æƒé™å¤±è´¥: {e}\u0026#34;) return [] def revoke_user_access(self, username: str, database: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ’¤é”€ç”¨æˆ·è®¿é—®æƒé™\u0026#34;\u0026#34;\u0026#34; try: conn = self.db_connections.get(database) cursor = conn.cursor() db_config = self.config[\u0026#39;databases\u0026#39;][database] if db_config[\u0026#39;type\u0026#39;] == \u0026#39;mysql\u0026#39;: cursor.execute(f\u0026#34;DROP USER \u0026#39;{username}\u0026#39;@\u0026#39;%\u0026#39;\u0026#34;) cursor.execute(\u0026#34;FLUSH PRIVILEGES\u0026#34;) elif db_config[\u0026#39;type\u0026#39;] == \u0026#39;postgresql\u0026#39;: cursor.execute(f\u0026#34;DROP USER {username}\u0026#34;) conn.commit() # åœ¨è®¤è¯æ•°æ®åº“ä¸­æ ‡è®°ç”¨æˆ·ä¸ºéæ´»è·ƒ auth_conn = self.db_connections.get(\u0026#39;auth_db\u0026#39;) auth_cursor = auth_conn.cursor() auth_cursor.execute( \u0026#34;UPDATE users SET is_active = FALSE WHERE username = %s\u0026#34;, (username,) ) auth_conn.commit() self.logger.info(f\u0026#34;æˆåŠŸæ’¤é”€ç”¨æˆ· {username} çš„è®¿é—®æƒé™\u0026#34;) return True except Exception as e: self.logger.error(f\u0026#34;æ’¤é”€ç”¨æˆ· {username} æƒé™å¤±è´¥: {e}\u0026#34;) return False def main(): # é…ç½®æ–‡ä»¶ç¤ºä¾‹ config = { \u0026#34;databases\u0026#34;: { \u0026#34;auth_db\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mysql\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 3306, \u0026#34;database\u0026#34;: \u0026#34;auth_system\u0026#34;, \u0026#34;admin_user\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;admin_password\u0026#34;: \u0026#34;admin_password\u0026#34; }, \u0026#34;production_db\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;postgresql\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 5432, \u0026#34;database\u0026#34;: \u0026#34;production\u0026#34;, \u0026#34;admin_user\u0026#34;: \u0026#34;postgres\u0026#34;, \u0026#34;admin_password\u0026#34;: \u0026#34;postgres_password\u0026#34; } }, \u0026#34;ldap\u0026#34;: { \u0026#34;server\u0026#34;: \u0026#34;ldap://ldap.company.com\u0026#34;, \u0026#34;base_dn\u0026#34;: \u0026#34;ou=users,dc=company,dc=com\u0026#34;, \u0026#34;group_mapping\u0026#34;: { \u0026#34;cn=dba,ou=groups,dc=company,dc=com\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;cn=developers,ou=groups,dc=company,dc=com\u0026#34;: \u0026#34;developer\u0026#34;, \u0026#34;cn=analysts,ou=groups,dc=company,dc=com\u0026#34;: \u0026#34;readonly\u0026#34; } } } # ä¿å­˜é…ç½®æ–‡ä»¶ with open(\u0026#39;/tmp/auth_config.json\u0026#39;, \u0026#39;w\u0026#39;) as f: json.dump(config, f, indent=2) # åˆå§‹åŒ–è®¤è¯ç®¡ç†å™¨ auth_manager = DatabaseAuthManager(\u0026#39;/tmp/auth_config.json\u0026#39;) # ç¤ºä¾‹ï¼šåˆ›å»ºç”¨æˆ· auth_manager.create_user_account( username=\u0026#34;john_doe\u0026#34;, password=\u0026#34;secure_password123\u0026#34;, email=\u0026#34;john@company.com\u0026#34;, role=\u0026#34;developer\u0026#34;, database=\u0026#34;production_db\u0026#34; ) # ç¤ºä¾‹ï¼šç”¨æˆ·è®¤è¯ success, result = auth_manager.authenticate_user(\u0026#34;john_doe\u0026#34;, \u0026#34;secure_password123\u0026#34;, \u0026#34;192.168.1.100\u0026#34;) if success: print(f\u0026#34;è®¤è¯æˆåŠŸ: {result}\u0026#34;) else: print(f\u0026#34;è®¤è¯å¤±è´¥: {result}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() 2. åŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶(RBAC) -- åˆ›å»ºè§’è‰²æƒé™ç®¡ç†ç³»ç»Ÿ -- åˆ›å»ºè§’è‰²è¡¨ CREATE TABLE roles ( role_id INT PRIMARY KEY AUTO_INCREMENT, role_name VARCHAR(50) UNIQUE NOT NULL, description TEXT, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP ); -- åˆ›å»ºæƒé™è¡¨ CREATE TABLE permissions ( permission_id INT PRIMARY KEY AUTO_INCREMENT, permission_name VARCHAR(100) UNIQUE NOT NULL, resource_type VARCHAR(50) NOT NULL, action VARCHAR(50) NOT NULL, description TEXT, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ); -- åˆ›å»ºè§’è‰²æƒé™å…³è”è¡¨ CREATE TABLE role_permissions ( role_id INT, permission_id INT, granted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, granted_by VARCHAR(50), PRIMARY KEY (role_id, permission_id), FOREIGN KEY (role_id) REFERENCES roles(role_id) ON DELETE CASCADE, FOREIGN KEY (permission_id) REFERENCES permissions(permission_id) ON DELETE CASCADE ); -- åˆ›å»ºç”¨æˆ·è§’è‰²å…³è”è¡¨ CREATE TABLE user_roles ( user_id INT, role_id INT, assigned_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, assigned_by VARCHAR(50), expires_at TIMESTAMP NULL, is_active BOOLEAN DEFAULT TRUE, PRIMARY KEY (user_id, role_id), FOREIGN KEY (user_id) REFERENCES users(user_id) ON DELETE CASCADE, FOREIGN KEY (role_id) REFERENCES roles(role_id) ON DELETE CASCADE ); -- æ’å…¥åŸºç¡€è§’è‰² INSERT INTO roles (role_name, description) VALUES (\u0026#39;super_admin\u0026#39;, \u0026#39;è¶…çº§ç®¡ç†å‘˜ï¼Œæ‹¥æœ‰æ‰€æœ‰æƒé™\u0026#39;), (\u0026#39;dba\u0026#39;, \u0026#39;æ•°æ®åº“ç®¡ç†å‘˜ï¼Œè´Ÿè´£æ•°æ®åº“ç»´æŠ¤å’Œç®¡ç†\u0026#39;), (\u0026#39;developer\u0026#39;, \u0026#39;å¼€å‘äººå‘˜ï¼Œå¯ä»¥è¿›è¡Œæ•°æ®çš„å¢åˆ æ”¹æŸ¥\u0026#39;), (\u0026#39;analyst\u0026#39;, \u0026#39;æ•°æ®åˆ†æå¸ˆï¼Œåªèƒ½æŸ¥è¯¢å’Œå¯¼å‡ºæ•°æ®\u0026#39;), (\u0026#39;readonly\u0026#39;, \u0026#39;åªè¯»ç”¨æˆ·ï¼Œä»…èƒ½æŸ¥çœ‹æ•°æ®\u0026#39;), (\u0026#39;auditor\u0026#39;, \u0026#39;å®¡è®¡å‘˜ï¼Œå¯ä»¥æŸ¥çœ‹æ—¥å¿—å’Œå®¡è®¡ä¿¡æ¯\u0026#39;); -- æ’å…¥åŸºç¡€æƒé™ INSERT INTO permissions (permission_name, resource_type, action, description) VALUES -- æ•°æ®åº“çº§æƒé™ (\u0026#39;database.create\u0026#39;, \u0026#39;database\u0026#39;, \u0026#39;create\u0026#39;, \u0026#39;åˆ›å»ºæ•°æ®åº“\u0026#39;), (\u0026#39;database.drop\u0026#39;, \u0026#39;database\u0026#39;, \u0026#39;drop\u0026#39;, \u0026#39;åˆ é™¤æ•°æ®åº“\u0026#39;), (\u0026#39;database.alter\u0026#39;, \u0026#39;database\u0026#39;, \u0026#39;alter\u0026#39;, \u0026#39;ä¿®æ”¹æ•°æ®åº“ç»“æ„\u0026#39;), -- è¡¨çº§æƒé™ (\u0026#39;table.create\u0026#39;, \u0026#39;table\u0026#39;, \u0026#39;create\u0026#39;, \u0026#39;åˆ›å»ºè¡¨\u0026#39;), (\u0026#39;table.drop\u0026#39;, \u0026#39;table\u0026#39;, \u0026#39;drop\u0026#39;, \u0026#39;åˆ é™¤è¡¨\u0026#39;), (\u0026#39;table.alter\u0026#39;, \u0026#39;table\u0026#39;, \u0026#39;alter\u0026#39;, \u0026#39;ä¿®æ”¹è¡¨ç»“æ„\u0026#39;), (\u0026#39;table.select\u0026#39;, \u0026#39;table\u0026#39;, \u0026#39;select\u0026#39;, \u0026#39;æŸ¥è¯¢è¡¨æ•°æ®\u0026#39;), (\u0026#39;table.insert\u0026#39;, \u0026#39;table\u0026#39;, \u0026#39;insert\u0026#39;, \u0026#39;æ’å…¥è¡¨æ•°æ®\u0026#39;), (\u0026#39;table.update\u0026#39;, \u0026#39;table\u0026#39;, \u0026#39;update\u0026#39;, \u0026#39;æ›´æ–°è¡¨æ•°æ®\u0026#39;), (\u0026#39;table.delete\u0026#39;, \u0026#39;table\u0026#39;, \u0026#39;delete\u0026#39;, \u0026#39;åˆ é™¤è¡¨æ•°æ®\u0026#39;), -- ç”¨æˆ·ç®¡ç†æƒé™ (\u0026#39;user.create\u0026#39;, \u0026#39;user\u0026#39;, \u0026#39;create\u0026#39;, \u0026#39;åˆ›å»ºç”¨æˆ·\u0026#39;), (\u0026#39;user.drop\u0026#39;, \u0026#39;user\u0026#39;, \u0026#39;drop\u0026#39;, \u0026#39;åˆ é™¤ç”¨æˆ·\u0026#39;), (\u0026#39;user.grant\u0026#39;, \u0026#39;user\u0026#39;, \u0026#39;grant\u0026#39;, \u0026#39;æˆäºˆæƒé™\u0026#39;), (\u0026#39;user.revoke\u0026#39;, \u0026#39;user\u0026#39;, \u0026#39;revoke\u0026#39;, \u0026#39;æ’¤é”€æƒé™\u0026#39;), -- ç³»ç»Ÿç®¡ç†æƒé™ (\u0026#39;system.backup\u0026#39;, \u0026#39;system\u0026#39;, \u0026#39;backup\u0026#39;, \u0026#39;æ•°æ®åº“å¤‡ä»½\u0026#39;), (\u0026#39;system.restore\u0026#39;, \u0026#39;system\u0026#39;, \u0026#39;restore\u0026#39;, \u0026#39;æ•°æ®åº“æ¢å¤\u0026#39;), (\u0026#39;system.monitor\u0026#39;, \u0026#39;system\u0026#39;, \u0026#39;monitor\u0026#39;, \u0026#39;ç³»ç»Ÿç›‘æ§\u0026#39;), (\u0026#39;system.audit\u0026#39;, \u0026#39;system\u0026#39;, \u0026#39;audit\u0026#39;, \u0026#39;å®¡è®¡æ—¥å¿—æŸ¥çœ‹\u0026#39;); -- ä¸ºè§’è‰²åˆ†é…æƒé™ -- è¶…çº§ç®¡ç†å‘˜æ‹¥æœ‰æ‰€æœ‰æƒé™ INSERT INTO role_permissions (role_id, permission_id, granted_by) SELECT r.role_id, p.permission_id, \u0026#39;system\u0026#39; FROM roles r, permissions p WHERE r.role_name = \u0026#39;super_admin\u0026#39;; -- DBAæƒé™ INSERT INTO role_permissions (role_id, permission_id, granted_by) SELECT r.role_id, p.permission_id, \u0026#39;system\u0026#39; FROM roles r, permissions p WHERE r.role_name = \u0026#39;dba\u0026#39; AND p.permission_name IN ( \u0026#39;database.create\u0026#39;, \u0026#39;database.alter\u0026#39;, \u0026#39;table.create\u0026#39;, \u0026#39;table.drop\u0026#39;, \u0026#39;table.alter\u0026#39;, \u0026#39;table.select\u0026#39;, \u0026#39;table.insert\u0026#39;, \u0026#39;table.update\u0026#39;, \u0026#39;table.delete\u0026#39;, \u0026#39;user.create\u0026#39;, \u0026#39;user.drop\u0026#39;, \u0026#39;user.grant\u0026#39;, \u0026#39;user.revoke\u0026#39;, \u0026#39;system.backup\u0026#39;, \u0026#39;system.restore\u0026#39;, \u0026#39;system.monitor\u0026#39; ); -- å¼€å‘äººå‘˜æƒé™ INSERT INTO role_permissions (role_id, permission_id, granted_by) SELECT r.role_id, p.permission_id, \u0026#39;system\u0026#39; FROM roles r, permissions p WHERE r.role_name = \u0026#39;developer\u0026#39; AND p.permission_name IN ( \u0026#39;table.select\u0026#39;, \u0026#39;table.insert\u0026#39;, \u0026#39;table.update\u0026#39;, \u0026#39;table.delete\u0026#39;, \u0026#39;table.create\u0026#39;, \u0026#39;table.alter\u0026#39; ); -- åˆ†æå¸ˆæƒé™ INSERT INTO role_permissions (role_id, permission_id, granted_by) SELECT r.role_id, p.permission_id, \u0026#39;system\u0026#39; FROM roles r, permissions p WHERE r.role_name = \u0026#39;analyst\u0026#39; AND p.permission_name IN (\u0026#39;table.select\u0026#39;); -- åªè¯»ç”¨æˆ·æƒé™ INSERT INTO role_permissions (role_id, permission_id, granted_by) SELECT r.role_id, p.permission_id, \u0026#39;system\u0026#39; FROM roles r, permissions p WHERE r.role_name = \u0026#39;readonly\u0026#39; AND p.permission_name IN (\u0026#39;table.select\u0026#39;); -- å®¡è®¡å‘˜æƒé™ INSERT INTO role_permissions (role_id, permission_id, granted_by) SELECT r.role_id, p.permission_id, \u0026#39;system\u0026#39; FROM roles r, permissions p WHERE r.role_name = \u0026#39;auditor\u0026#39; AND p.permission_name IN (\u0026#39;table.select\u0026#39;, \u0026#39;system.audit\u0026#39;); -- åˆ›å»ºæƒé™æ£€æŸ¥å‡½æ•° DELIMITER // CREATE FUNCTION check_user_permission( p_username VARCHAR(50), p_permission VARCHAR(100) ) RETURNS BOOLEAN READS SQL DATA DETERMINISTIC BEGIN DECLARE permission_count INT DEFAULT 0; SELECT COUNT(*) INTO permission_count FROM users u JOIN user_roles ur ON u.user_id = ur.user_id JOIN role_permissions rp ON ur.role_id = rp.role_id JOIN permissions p ON rp.permission_id = p.permission_id WHERE u.username = p_username AND p.permission_name = p_permission AND ur.is_active = TRUE AND (ur.expires_at IS NULL OR ur.expires_at \u0026gt; NOW()) AND u.is_active = TRUE; RETURN permission_count \u0026gt; 0; END // DELIMITER ; -- åˆ›å»ºç”¨æˆ·æƒé™è§†å›¾ CREATE VIEW user_permissions_view AS SELECT u.username, u.email, r.role_name, p.permission_name, p.resource_type, p.action, ur.assigned_at, ur.expires_at, ur.is_active FROM users u JOIN user_roles ur ON u.user_id = ur.user_id JOIN roles r ON ur.role_id = r.role_id JOIN role_permissions rp ON r.role_id = rp.role_id JOIN permissions p ON rp.permission_id = p.permission_id WHERE u.is_active = TRUE AND ur.is_active = TRUE AND (ur.expires_at IS NULL OR ur.expires_at \u0026gt; NOW()); æ•°æ®åŠ å¯†ä¸è„±æ• 1. æ•°æ®åŠ å¯†å®ç° #!/usr/bin/env python3 # scripts/database_encryption.py import os import json import base64 import hashlib from typing import Dict, Any, Optional, List from cryptography.fernet import Fernet from cryptography.hazmat.primitives import hashes from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes import mysql.connector import psycopg2 class DatabaseEncryption: def __init__(self, config_file: str): with open(config_file, \u0026#39;r\u0026#39;) as f: self.config = json.load(f) # åˆå§‹åŒ–åŠ å¯†å¯†é’¥ self.master_key = self._derive_master_key() self.field_keys = self._generate_field_keys() # æ•°æ®åº“è¿æ¥ self.db_connections = self._initialize_db_connections() def _derive_master_key(self) -\u0026gt; bytes: \u0026#34;\u0026#34;\u0026#34;æ´¾ç”Ÿä¸»å¯†é’¥\u0026#34;\u0026#34;\u0026#34; password = self.config[\u0026#39;encryption\u0026#39;][\u0026#39;master_password\u0026#39;].encode() salt = self.config[\u0026#39;encryption\u0026#39;][\u0026#39;salt\u0026#39;].encode() kdf = PBKDF2HMAC( algorithm=hashes.SHA256(), length=32, salt=salt, iterations=100000, ) return kdf.derive(password) def _generate_field_keys(self) -\u0026gt; Dict[str, Fernet]: \u0026#34;\u0026#34;\u0026#34;ä¸ºä¸åŒå­—æ®µç”ŸæˆåŠ å¯†å¯†é’¥\u0026#34;\u0026#34;\u0026#34; field_keys = {} for field_name in self.config[\u0026#39;encryption\u0026#39;][\u0026#39;encrypted_fields\u0026#39;]: # ä¸ºæ¯ä¸ªå­—æ®µç”Ÿæˆå”¯ä¸€çš„å¯†é’¥ field_salt = hashlib.sha256(f\u0026#34;{field_name}_{self.master_key.hex()}\u0026#34;.encode()).digest() kdf = PBKDF2HMAC( algorithm=hashes.SHA256(), length=32, salt=field_salt, iterations=100000, ) field_key = kdf.derive(self.master_key) field_keys[field_name] = Fernet(base64.urlsafe_b64encode(field_key)) return field_keys def _initialize_db_connections(self) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;åˆå§‹åŒ–æ•°æ®åº“è¿æ¥\u0026#34;\u0026#34;\u0026#34; connections = {} for db_name, db_config in self.config[\u0026#39;databases\u0026#39;].items(): try: if db_config[\u0026#39;type\u0026#39;] == \u0026#39;mysql\u0026#39;: conn = mysql.connector.connect( host=db_config[\u0026#39;host\u0026#39;], port=db_config[\u0026#39;port\u0026#39;], user=db_config[\u0026#39;user\u0026#39;], password=db_config[\u0026#39;password\u0026#39;], database=db_config[\u0026#39;database\u0026#39;] ) elif db_config[\u0026#39;type\u0026#39;] == \u0026#39;postgresql\u0026#39;: conn = psycopg2.connect( host=db_config[\u0026#39;host\u0026#39;], port=db_config[\u0026#39;port\u0026#39;], user=db_config[\u0026#39;user\u0026#39;], password=db_config[\u0026#39;password\u0026#39;], database=db_config[\u0026#39;database\u0026#39;] ) connections[db_name] = conn except Exception as e: print(f\u0026#34;è¿æ¥æ•°æ®åº“ {db_name} å¤±è´¥: {e}\u0026#34;) return connections def encrypt_field(self, field_name: str, value: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;åŠ å¯†å­—æ®µå€¼\u0026#34;\u0026#34;\u0026#34; if field_name not in self.field_keys: raise ValueError(f\u0026#34;å­—æ®µ {field_name} æœªé…ç½®åŠ å¯†\u0026#34;) if value is None: return None encrypted_value = self.field_keys[field_name].encrypt(value.encode()) return base64.b64encode(encrypted_value).decode() def decrypt_field(self, field_name: str, encrypted_value: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;è§£å¯†å­—æ®µå€¼\u0026#34;\u0026#34;\u0026#34; if field_name not in self.field_keys: raise ValueError(f\u0026#34;å­—æ®µ {field_name} æœªé…ç½®åŠ å¯†\u0026#34;) if encrypted_value is None: return None try: encrypted_bytes = base64.b64decode(encrypted_value.encode()) decrypted_value = self.field_keys[field_name].decrypt(encrypted_bytes) return decrypted_value.decode() except Exception as e: raise ValueError(f\u0026#34;è§£å¯†å¤±è´¥: {e}\u0026#34;) def encrypt_sensitive_data(self, table_name: str, data: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;åŠ å¯†æ•æ„Ÿæ•°æ®\u0026#34;\u0026#34;\u0026#34; encrypted_data = data.copy() table_config = self.config[\u0026#39;tables\u0026#39;].get(table_name, {}) encrypted_fields = table_config.get(\u0026#39;encrypted_fields\u0026#39;, []) for field_name in encrypted_fields: if field_name in encrypted_data and encrypted_data[field_name] is not None: encrypted_data[field_name] = self.encrypt_field(field_name, str(encrypted_data[field_name])) return encrypted_data def decrypt_sensitive_data(self, table_name: str, data: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è§£å¯†æ•æ„Ÿæ•°æ®\u0026#34;\u0026#34;\u0026#34; decrypted_data = data.copy() table_config = self.config[\u0026#39;tables\u0026#39;].get(table_name, {}) encrypted_fields = table_config.get(\u0026#39;encrypted_fields\u0026#39;, []) for field_name in encrypted_fields: if field_name in decrypted_data and decrypted_data[field_name] is not None: decrypted_data[field_name] = self.decrypt_field(field_name, decrypted_data[field_name]) return decrypted_data def create_encrypted_table(self, database: str, table_name: str, schema: Dict[str, str]): \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºæ”¯æŒåŠ å¯†çš„è¡¨\u0026#34;\u0026#34;\u0026#34; conn = self.db_connections[database] cursor = conn.cursor() # æ„å»ºCREATE TABLEè¯­å¥ columns = [] for column_name, column_type in schema.items(): # å¦‚æœæ˜¯åŠ å¯†å­—æ®µï¼Œä½¿ç”¨TEXTç±»å‹å­˜å‚¨åŠ å¯†æ•°æ® if column_name in self.config[\u0026#39;tables\u0026#39;].get(table_name, {}).get(\u0026#39;encrypted_fields\u0026#39;, []): columns.append(f\u0026#34;{column_name} TEXT\u0026#34;) else: columns.append(f\u0026#34;{column_name} {column_type}\u0026#34;) create_sql = f\u0026#34;CREATE TABLE {table_name} ({\u0026#39;, \u0026#39;.join(columns)})\u0026#34; cursor.execute(create_sql) conn.commit() print(f\u0026#34;åˆ›å»ºåŠ å¯†è¡¨ {table_name} æˆåŠŸ\u0026#34;) def insert_encrypted_data(self, database: str, table_name: str, data: Dict[str, Any]): \u0026#34;\u0026#34;\u0026#34;æ’å…¥åŠ å¯†æ•°æ®\u0026#34;\u0026#34;\u0026#34; conn = self.db_connections[database] cursor = conn.cursor() # åŠ å¯†æ•æ„Ÿå­—æ®µ encrypted_data = self.encrypt_sensitive_data(table_name, data) # æ„å»ºINSERTè¯­å¥ columns = list(encrypted_data.keys()) placeholders = [\u0026#39;%s\u0026#39;] * len(columns) values = list(encrypted_data.values()) insert_sql = f\u0026#34;INSERT INTO {table_name} ({\u0026#39;, \u0026#39;.join(columns)}) VALUES ({\u0026#39;, \u0026#39;.join(placeholders)})\u0026#34; cursor.execute(insert_sql, values) conn.commit() print(f\u0026#34;æ’å…¥åŠ å¯†æ•°æ®åˆ°è¡¨ {table_name} æˆåŠŸ\u0026#34;) def select_encrypted_data(self, database: str, table_name: str, where_clause: str = \u0026#34;\u0026#34;, decrypt: bool = True) -\u0026gt; List[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;æŸ¥è¯¢å¹¶è§£å¯†æ•°æ®\u0026#34;\u0026#34;\u0026#34; conn = self.db_connections[database] cursor = conn.cursor(dictionary=True) select_sql = f\u0026#34;SELECT * FROM {table_name}\u0026#34; if where_clause: select_sql += f\u0026#34; WHERE {where_clause}\u0026#34; cursor.execute(select_sql) results = cursor.fetchall() if decrypt: # è§£å¯†æ•æ„Ÿå­—æ®µ decrypted_results = [] for row in results: decrypted_row = self.decrypt_sensitive_data(table_name, row) decrypted_results.append(decrypted_row) return decrypted_results return results def mask_sensitive_data(self, value: str, mask_type: str = \u0026#39;partial\u0026#39;) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;æ•°æ®è„±æ•\u0026#34;\u0026#34;\u0026#34; if value is None: return None if mask_type == \u0026#39;full\u0026#39;: return \u0026#39;*\u0026#39; * len(value) elif mask_type == \u0026#39;partial\u0026#39;: if len(value) \u0026lt;= 4: return \u0026#39;*\u0026#39; * len(value) else: return value[:2] + \u0026#39;*\u0026#39; * (len(value) - 4) + value[-2:] elif mask_type == \u0026#39;email\u0026#39;: if \u0026#39;@\u0026#39; in value: local, domain = value.split(\u0026#39;@\u0026#39;, 1) masked_local = local[0] + \u0026#39;*\u0026#39; * (len(local) - 1) if len(local) \u0026gt; 1 else \u0026#39;*\u0026#39; return f\u0026#34;{masked_local}@{domain}\u0026#34; else: return self.mask_sensitive_data(value, \u0026#39;partial\u0026#39;) elif mask_type == \u0026#39;phone\u0026#39;: if len(value) \u0026gt;= 7: return value[:3] + \u0026#39;*\u0026#39; * (len(value) - 6) + value[-3:] else: return \u0026#39;*\u0026#39; * len(value) elif mask_type == \u0026#39;id_card\u0026#39;: if len(value) \u0026gt;= 8: return value[:4] + \u0026#39;*\u0026#39; * (len(value) - 8) + value[-4:] else: return \u0026#39;*\u0026#39; * len(value) return value def get_masked_data(self, database: str, table_name: str, where_clause: str = \u0026#34;\u0026#34;) -\u0026gt; List[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;è·å–è„±æ•æ•°æ®\u0026#34;\u0026#34;\u0026#34; # å…ˆè·å–è§£å¯†æ•°æ® decrypted_data = self.select_encrypted_data(database, table_name, where_clause, decrypt=True) # åº”ç”¨è„±æ•è§„åˆ™ table_config = self.config[\u0026#39;tables\u0026#39;].get(table_name, {}) masking_rules = table_config.get(\u0026#39;masking_rules\u0026#39;, {}) masked_data = [] for row in decrypted_data: masked_row = row.copy() for field_name, mask_type in masking_rules.items(): if field_name in masked_row and masked_row[field_name] is not None: masked_row[field_name] = self.mask_sensitive_data(str(masked_row[field_name]), mask_type) masked_data.append(masked_row) return masked_data def rotate_encryption_keys(self): \u0026#34;\u0026#34;\u0026#34;è½®æ¢åŠ å¯†å¯†é’¥\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;å¼€å§‹å¯†é’¥è½®æ¢...\u0026#34;) # ç”Ÿæˆæ–°çš„ä¸»å¯†é’¥ new_master_key = os.urandom(32) # ä¸ºæ¯ä¸ªåŠ å¯†å­—æ®µç”Ÿæˆæ–°å¯†é’¥ new_field_keys = {} for field_name in self.config[\u0026#39;encryption\u0026#39;][\u0026#39;encrypted_fields\u0026#39;]: field_salt = hashlib.sha256(f\u0026#34;{field_name}_{new_master_key.hex()}\u0026#34;.encode()).digest() kdf = PBKDF2HMAC( algorithm=hashes.SHA256(), length=32, salt=field_salt, iterations=100000, ) field_key = kdf.derive(new_master_key) new_field_keys[field_name] = Fernet(base64.urlsafe_b64encode(field_key)) # é‡æ–°åŠ å¯†æ‰€æœ‰æ•°æ® for database_name in self.db_connections.keys(): for table_name, table_config in self.config[\u0026#39;tables\u0026#39;].items(): encrypted_fields = table_config.get(\u0026#39;encrypted_fields\u0026#39;, []) if not encrypted_fields: continue print(f\u0026#34;é‡æ–°åŠ å¯†è¡¨ {table_name} çš„æ•°æ®...\u0026#34;) # è·å–æ‰€æœ‰æ•°æ®å¹¶è§£å¯† old_data = self.select_encrypted_data(database_name, table_name, decrypt=True) # ä½¿ç”¨æ–°å¯†é’¥é‡æ–°åŠ å¯† conn = self.db_connections[database_name] cursor = conn.cursor() for row in old_data: # ä½¿ç”¨æ–°å¯†é’¥åŠ å¯† for field_name in encrypted_fields: if field_name in row and row[field_name] is not None: encrypted_value = new_field_keys[field_name].encrypt(str(row[field_name]).encode()) row[field_name] = base64.b64encode(encrypted_value).decode() # æ›´æ–°æ•°æ®åº“è®°å½• set_clause = \u0026#39;, \u0026#39;.join([f\u0026#34;{k} = %s\u0026#34; for k in row.keys() if k != \u0026#39;id\u0026#39;]) update_sql = f\u0026#34;UPDATE {table_name} SET {set_clause} WHERE id = %s\u0026#34; values = [row[k] for k in row.keys() if k != \u0026#39;id\u0026#39;] + [row[\u0026#39;id\u0026#39;]] cursor.execute(update_sql, values) conn.commit() # æ›´æ–°å¯†é’¥ self.master_key = new_master_key self.field_keys = new_field_keys print(\u0026#34;å¯†é’¥è½®æ¢å®Œæˆ\u0026#34;) def main(): # é…ç½®æ–‡ä»¶ç¤ºä¾‹ config = { \u0026#34;encryption\u0026#34;: { \u0026#34;master_password\u0026#34;: \u0026#34;your_very_secure_master_password\u0026#34;, \u0026#34;salt\u0026#34;: \u0026#34;your_unique_salt_value\u0026#34;, \u0026#34;encrypted_fields\u0026#34;: [\u0026#34;email\u0026#34;, \u0026#34;phone\u0026#34;, \u0026#34;id_card\u0026#34;, \u0026#34;credit_card\u0026#34;] }, \u0026#34;databases\u0026#34;: { \u0026#34;user_db\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mysql\u0026#34;, \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 3306, \u0026#34;user\u0026#34;: \u0026#34;app_user\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;app_password\u0026#34;, \u0026#34;database\u0026#34;: \u0026#34;user_data\u0026#34; } }, \u0026#34;tables\u0026#34;: { \u0026#34;users\u0026#34;: { \u0026#34;encrypted_fields\u0026#34;: [\u0026#34;email\u0026#34;, \u0026#34;phone\u0026#34;, \u0026#34;id_card\u0026#34;], \u0026#34;masking_rules\u0026#34;: { \u0026#34;email\u0026#34;: \u0026#34;email\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;phone\u0026#34;, \u0026#34;id_card\u0026#34;: \u0026#34;id_card\u0026#34; } }, \u0026#34;payments\u0026#34;: { \u0026#34;encrypted_fields\u0026#34;: [\u0026#34;credit_card\u0026#34;], \u0026#34;masking_rules\u0026#34;: { \u0026#34;credit_card\u0026#34;: \u0026#34;partial\u0026#34; } } } } # ä¿å­˜é…ç½®æ–‡ä»¶ with open(\u0026#39;/tmp/encryption_config.json\u0026#39;, \u0026#39;w\u0026#39;) as f: json.dump(config, f, indent=2) # åˆå§‹åŒ–åŠ å¯†ç®¡ç†å™¨ encryption = DatabaseEncryption(\u0026#39;/tmp/encryption_config.json\u0026#39;) # ç¤ºä¾‹ï¼šåˆ›å»ºåŠ å¯†è¡¨ user_schema = { \u0026#34;id\u0026#34;: \u0026#34;INT PRIMARY KEY AUTO_INCREMENT\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;VARCHAR(50) NOT NULL\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;VARCHAR(100)\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;VARCHAR(20)\u0026#34;, \u0026#34;id_card\u0026#34;: \u0026#34;VARCHAR(20)\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;TIMESTAMP DEFAULT CURRENT_TIMESTAMP\u0026#34; } encryption.create_encrypted_table(\u0026#34;user_db\u0026#34;, \u0026#34;users\u0026#34;, user_schema) # ç¤ºä¾‹ï¼šæ’å…¥åŠ å¯†æ•°æ® user_data = { \u0026#34;username\u0026#34;: \u0026#34;john_doe\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;john.doe@example.com\u0026#34;, \u0026#34;phone\u0026#34;: \u0026#34;13800138000\u0026#34;, \u0026#34;id_card\u0026#34;: \u0026#34;110101199001011234\u0026#34; } encryption.insert_encrypted_data(\u0026#34;user_db\u0026#34;, \u0026#34;users\u0026#34;, user_data) # ç¤ºä¾‹ï¼šæŸ¥è¯¢è„±æ•æ•°æ® masked_data = encryption.get_masked_data(\u0026#34;user_db\u0026#34;, \u0026#34;users\u0026#34;) print(\u0026#34;è„±æ•æ•°æ®:\u0026#34;, masked_data) if __name__ == \u0026#34;__main__\u0026#34;: main() å®¡è®¡æ—¥å¿—ä¸ç›‘æ§ 1. æ•°æ®åº“å®¡è®¡ç³»ç»Ÿ #!/bin/bash # scripts/database_audit_system.sh set -euo pipefail # é…ç½®å‚æ•° AUDIT_DB_HOST=\u0026#34;${AUDIT_DB_HOST:-localhost}\u0026#34; AUDIT_DB_PORT=\u0026#34;${AUDIT_DB_PORT:-3306}\u0026#34; AUDIT_DB_USER=\u0026#34;${AUDIT_DB_USER:-audit_user}\u0026#34; AUDIT_DB_PASSWORD=\u0026#34;${AUDIT_DB_PASSWORD:-audit_password}\u0026#34; AUDIT_DB_NAME=\u0026#34;${AUDIT_DB_NAME:-audit_system}\u0026#34; LOG_DIR=\u0026#34;${LOG_DIR:-/var/log/database_audit}\u0026#34; ALERT_EMAIL=\u0026#34;${ALERT_EMAIL:-admin@company.com}\u0026#34; RETENTION_DAYS=\u0026#34;${RETENTION_DAYS:-90}\u0026#34; # æ—¥å¿—å‡½æ•° log() { echo \u0026#34;[$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)] $1\u0026#34; \u0026gt;\u0026amp;2 } # åˆ›å»ºå®¡è®¡æ—¥å¿—ç›®å½• create_log_directory() { mkdir -p \u0026#34;$LOG_DIR\u0026#34; chmod 750 \u0026#34;$LOG_DIR\u0026#34; # åˆ›å»ºæ—¥å¿—è½®è½¬é…ç½® cat \u0026gt; /etc/logrotate.d/database_audit \u0026lt;\u0026lt; EOF $LOG_DIR/*.log { daily rotate 30 compress delaycompress missingok notifempty create 640 mysql mysql postrotate /usr/bin/systemctl reload rsyslog \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 || true endrotate } EOF log \u0026#34;å®¡è®¡æ—¥å¿—ç›®å½•åˆ›å»ºå®Œæˆ: $LOG_DIR\u0026#34; } # åˆ›å»ºå®¡è®¡æ•°æ®åº“è¡¨ç»“æ„ create_audit_tables() { log \u0026#34;åˆ›å»ºå®¡è®¡æ•°æ®åº“è¡¨ç»“æ„\u0026#34; mysql -h\u0026#34;$AUDIT_DB_HOST\u0026#34; -P\u0026#34;$AUDIT_DB_PORT\u0026#34; -u\u0026#34;$AUDIT_DB_USER\u0026#34; -p\u0026#34;$AUDIT_DB_PASSWORD\u0026#34; \u0026#34;$AUDIT_DB_NAME\u0026#34; \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; -- åˆ›å»ºè¿æ¥å®¡è®¡è¡¨ CREATE TABLE IF NOT EXISTS connection_audit ( id BIGINT PRIMARY KEY AUTO_INCREMENT, timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP, server_host VARCHAR(255), server_port INT, username VARCHAR(100), client_host VARCHAR(255), client_port INT, connection_type ENUM(\u0026#39;CONNECT\u0026#39;, \u0026#39;DISCONNECT\u0026#39;, \u0026#39;FAILED_CONNECT\u0026#39;), connection_id BIGINT, database_name VARCHAR(100), ssl_used BOOLEAN, error_code INT, error_message TEXT, INDEX idx_timestamp (timestamp), INDEX idx_username (username), INDEX idx_client_host (client_host), INDEX idx_connection_type (connection_type) ); -- åˆ›å»ºæŸ¥è¯¢å®¡è®¡è¡¨ CREATE TABLE IF NOT EXISTS query_audit ( id BIGINT PRIMARY KEY AUTO_INCREMENT, timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP, connection_id BIGINT, username VARCHAR(100), client_host VARCHAR(255), database_name VARCHAR(100), command_type VARCHAR(50), sql_text LONGTEXT, affected_rows BIGINT, execution_time_ms BIGINT, error_code INT, error_message TEXT, query_hash VARCHAR(64), INDEX idx_timestamp (timestamp), INDEX idx_username (username), INDEX idx_command_type (command_type), INDEX idx_query_hash (query_hash), INDEX idx_execution_time (execution_time_ms) ); -- åˆ›å»ºæƒé™å˜æ›´å®¡è®¡è¡¨ CREATE TABLE IF NOT EXISTS privilege_audit ( id BIGINT PRIMARY KEY AUTO_INCREMENT, timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP, admin_username VARCHAR(100), admin_host VARCHAR(255), target_username VARCHAR(100), operation_type ENUM(\u0026#39;GRANT\u0026#39;, \u0026#39;REVOKE\u0026#39;, \u0026#39;CREATE_USER\u0026#39;, \u0026#39;DROP_USER\u0026#39;, \u0026#39;ALTER_USER\u0026#39;), privilege_type VARCHAR(100), object_type VARCHAR(50), object_name VARCHAR(255), with_grant_option BOOLEAN, sql_text TEXT, INDEX idx_timestamp (timestamp), INDEX idx_admin_username (admin_username), INDEX idx_target_username (target_username), INDEX idx_operation_type (operation_type) ); -- åˆ›å»ºæ•°æ®å˜æ›´å®¡è®¡è¡¨ CREATE TABLE IF NOT EXISTS data_change_audit ( id BIGINT PRIMARY KEY AUTO_INCREMENT, timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP, username VARCHAR(100), client_host VARCHAR(255), database_name VARCHAR(100), table_name VARCHAR(100), operation_type ENUM(\u0026#39;INSERT\u0026#39;, \u0026#39;UPDATE\u0026#39;, \u0026#39;DELETE\u0026#39;), primary_key_values JSON, old_values JSON, new_values JSON, affected_columns JSON, transaction_id BIGINT, INDEX idx_timestamp (timestamp), INDEX idx_username (username), INDEX idx_table_name (table_name), INDEX idx_operation_type (operation_type) ); -- åˆ›å»ºå®‰å…¨äº‹ä»¶å®¡è®¡è¡¨ CREATE TABLE IF NOT EXISTS security_audit ( id BIGINT PRIMARY KEY AUTO_INCREMENT, timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP, event_type VARCHAR(50), severity ENUM(\u0026#39;LOW\u0026#39;, \u0026#39;MEDIUM\u0026#39;, \u0026#39;HIGH\u0026#39;, \u0026#39;CRITICAL\u0026#39;), username VARCHAR(100), client_host VARCHAR(255), database_name VARCHAR(100), description TEXT, additional_info JSON, resolved BOOLEAN DEFAULT FALSE, resolved_at TIMESTAMP NULL, resolved_by VARCHAR(100), INDEX idx_timestamp (timestamp), INDEX idx_event_type (event_type), INDEX idx_severity (severity), INDEX idx_resolved (resolved) ); -- åˆ›å»ºå®¡è®¡é…ç½®è¡¨ CREATE TABLE IF NOT EXISTS audit_config ( id INT PRIMARY KEY AUTO_INCREMENT, config_key VARCHAR(100) UNIQUE, config_value TEXT, description TEXT, updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP, updated_by VARCHAR(100) ); -- æ’å…¥é»˜è®¤é…ç½® INSERT IGNORE INTO audit_config (config_key, config_value, description, updated_by) VALUES (\u0026#39;audit_enabled\u0026#39;, \u0026#39;true\u0026#39;, \u0026#39;æ˜¯å¦å¯ç”¨å®¡è®¡åŠŸèƒ½\u0026#39;, \u0026#39;system\u0026#39;), (\u0026#39;log_connections\u0026#39;, \u0026#39;true\u0026#39;, \u0026#39;æ˜¯å¦è®°å½•è¿æ¥äº‹ä»¶\u0026#39;, \u0026#39;system\u0026#39;), (\u0026#39;log_queries\u0026#39;, \u0026#39;true\u0026#39;, \u0026#39;æ˜¯å¦è®°å½•æŸ¥è¯¢è¯­å¥\u0026#39;, \u0026#39;system\u0026#39;), (\u0026#39;log_privilege_changes\u0026#39;, \u0026#39;true\u0026#39;, \u0026#39;æ˜¯å¦è®°å½•æƒé™å˜æ›´\u0026#39;, \u0026#39;system\u0026#39;), (\u0026#39;log_data_changes\u0026#39;, \u0026#39;false\u0026#39;, \u0026#39;æ˜¯å¦è®°å½•æ•°æ®å˜æ›´ï¼ˆæ€§èƒ½å½±å“è¾ƒå¤§ï¼‰\u0026#39;, \u0026#39;system\u0026#39;), (\u0026#39;slow_query_threshold_ms\u0026#39;, \u0026#39;1000\u0026#39;, \u0026#39;æ…¢æŸ¥è¯¢é˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰\u0026#39;, \u0026#39;system\u0026#39;), (\u0026#39;retention_days\u0026#39;, \u0026#39;90\u0026#39;, \u0026#39;å®¡è®¡æ—¥å¿—ä¿ç•™å¤©æ•°\u0026#39;, \u0026#39;system\u0026#39;), (\u0026#39;alert_failed_login_threshold\u0026#39;, \u0026#39;5\u0026#39;, \u0026#39;ç™»å½•å¤±è´¥å‘Šè­¦é˜ˆå€¼\u0026#39;, \u0026#39;system\u0026#39;), (\u0026#39;alert_privilege_escalation\u0026#39;, \u0026#39;true\u0026#39;, \u0026#39;æ˜¯å¦å‘Šè­¦æƒé™æå‡\u0026#39;, \u0026#39;system\u0026#39;); EOF log \u0026#34;å®¡è®¡æ•°æ®åº“è¡¨ç»“æ„åˆ›å»ºå®Œæˆ\u0026#34; } # é…ç½®MySQLå®¡è®¡æ’ä»¶ configure_mysql_audit() { log \u0026#34;é…ç½®MySQLå®¡è®¡æ’ä»¶\u0026#34; # æ£€æŸ¥æ˜¯å¦å·²å®‰è£…auditæ’ä»¶ local plugin_status=$(mysql -h\u0026#34;$AUDIT_DB_HOST\u0026#34; -P\u0026#34;$AUDIT_DB_PORT\u0026#34; -u\u0026#34;$AUDIT_DB_USER\u0026#34; -p\u0026#34;$AUDIT_DB_PASSWORD\u0026#34; -e \u0026#34;SHOW PLUGINS LIKE \u0026#39;audit_log\u0026#39;;\u0026#34; --skip-column-names | wc -l) if [ \u0026#34;$plugin_status\u0026#34; -eq 0 ]; then log \u0026#34;å®‰è£…MySQLå®¡è®¡æ’ä»¶\u0026#34; mysql -h\u0026#34;$AUDIT_DB_HOST\u0026#34; -P\u0026#34;$AUDIT_DB_PORT\u0026#34; -u\u0026#34;$AUDIT_DB_USER\u0026#34; -p\u0026#34;$AUDIT_DB_PASSWORD\u0026#34; -e \u0026#34;INSTALL PLUGIN audit_log SONAME \u0026#39;audit_log.so\u0026#39;;\u0026#34; fi # é…ç½®å®¡è®¡å‚æ•° cat \u0026gt;\u0026gt; /etc/mysql/mysql.conf.d/audit.cnf \u0026lt;\u0026lt; EOF [mysqld] # å®¡è®¡æ—¥å¿—é…ç½® audit_log_policy=ALL audit_log_format=JSON audit_log_file=$LOG_DIR/mysql_audit.log audit_log_rotate_on_size=100M audit_log_rotations=10 # è¿æ¥å®¡è®¡ audit_log_connection_policy=ALL audit_log_statement_policy=ALL # æ’é™¤ç³»ç»Ÿç”¨æˆ· audit_log_exclude_accounts=\u0026#39;mysql.session@localhost,mysql.sys@localhost\u0026#39; # åŒ…å«æ•°æ®åº“ audit_log_include_databases=\u0026#39;\u0026#39; # æ€§èƒ½ä¼˜åŒ– audit_log_buffer_size=1M audit_log_flush=ON EOF log \u0026#34;MySQLå®¡è®¡é…ç½®å®Œæˆï¼Œéœ€è¦é‡å¯MySQLæœåŠ¡\u0026#34; } # é…ç½®PostgreSQLå®¡è®¡ configure_postgresql_audit() { log \u0026#34;é…ç½®PostgreSQLå®¡è®¡\u0026#34; # å®‰è£…pgauditæ‰©å±• sudo -u postgres psql -c \u0026#34;CREATE EXTENSION IF NOT EXISTS pgaudit;\u0026#34; # é…ç½®postgresql.conf cat \u0026gt;\u0026gt; /etc/postgresql/*/main/postgresql.conf \u0026lt;\u0026lt; EOF # pgAudité…ç½® shared_preload_libraries = \u0026#39;pgaudit\u0026#39; pgaudit.log = \u0026#39;all\u0026#39; pgaudit.log_catalog = on pgaudit.log_client = on pgaudit.log_level = log pgaudit.log_parameter = on pgaudit.log_relation = on pgaudit.log_statement_once = off # æ—¥å¿—é…ç½® log_destination = \u0026#39;csvlog\u0026#39; logging_collector = on log_directory = \u0026#39;$LOG_DIR\u0026#39; log_filename = \u0026#39;postgresql_audit_%Y%m%d_%H%M%S.log\u0026#39; log_rotation_age = 1d log_rotation_size = 100MB log_min_duration_statement = 1000 log_connections = on log_disconnections = on log_line_prefix = \u0026#39;%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h \u0026#39; EOF log \u0026#34;PostgreSQLå®¡è®¡é…ç½®å®Œæˆï¼Œéœ€è¦é‡å¯PostgreSQLæœåŠ¡\u0026#34; } # åˆ›å»ºå®¡è®¡æ—¥å¿—è§£æè„šæœ¬ create_log_parser() { cat \u0026gt; \u0026#34;$LOG_DIR/parse_audit_logs.py\u0026#34; \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; #!/usr/bin/env python3 import json import re import sys import mysql.connector from datetime import datetime import argparse class AuditLogParser: def __init__(self, db_config): self.db_config = db_config self.conn = mysql.connector.connect(**db_config) self.cursor = self.conn.cursor() def parse_mysql_audit_log(self, log_file): \u0026#34;\u0026#34;\u0026#34;è§£æMySQLå®¡è®¡æ—¥å¿—\u0026#34;\u0026#34;\u0026#34; with open(log_file, \u0026#39;r\u0026#39;) as f: for line in f: try: log_entry = json.loads(line.strip()) self.process_mysql_log_entry(log_entry) except json.JSONDecodeError: continue def process_mysql_log_entry(self, entry): \u0026#34;\u0026#34;\u0026#34;å¤„ç†MySQLæ—¥å¿—æ¡ç›®\u0026#34;\u0026#34;\u0026#34; timestamp = datetime.strptime(entry[\u0026#39;timestamp\u0026#39;], \u0026#39;%Y-%m-%dT%H:%M:%S\u0026#39;) if entry[\u0026#39;class\u0026#39;] == \u0026#39;connection\u0026#39;: self.insert_connection_audit( timestamp=timestamp, server_host=entry.get(\u0026#39;server_host\u0026#39;, \u0026#39;\u0026#39;), server_port=entry.get(\u0026#39;server_port\u0026#39;, 0), username=entry.get(\u0026#39;user\u0026#39;, \u0026#39;\u0026#39;), client_host=entry.get(\u0026#39;host\u0026#39;, \u0026#39;\u0026#39;), connection_type=entry.get(\u0026#39;event\u0026#39;, \u0026#39;\u0026#39;), connection_id=entry.get(\u0026#39;connection_id\u0026#39;, 0), database_name=entry.get(\u0026#39;db\u0026#39;, \u0026#39;\u0026#39;), error_code=entry.get(\u0026#39;error_code\u0026#39;, 0) ) elif entry[\u0026#39;class\u0026#39;] == \u0026#39;general\u0026#39;: self.insert_query_audit( timestamp=timestamp, connection_id=entry.get(\u0026#39;connection_id\u0026#39;, 0), username=entry.get(\u0026#39;user\u0026#39;, \u0026#39;\u0026#39;), client_host=entry.get(\u0026#39;host\u0026#39;, \u0026#39;\u0026#39;), database_name=entry.get(\u0026#39;db\u0026#39;, \u0026#39;\u0026#39;), command_type=entry.get(\u0026#39;command_class\u0026#39;, \u0026#39;\u0026#39;), sql_text=entry.get(\u0026#39;sql_text\u0026#39;, \u0026#39;\u0026#39;), error_code=entry.get(\u0026#39;error_code\u0026#39;, 0) ) def insert_connection_audit(self, **kwargs): \u0026#34;\u0026#34;\u0026#34;æ’å…¥è¿æ¥å®¡è®¡è®°å½•\u0026#34;\u0026#34;\u0026#34; sql = \u0026#34;\u0026#34;\u0026#34; INSERT INTO connection_audit (timestamp, server_host, server_port, username, client_host, connection_type, connection_id, database_name, error_code) VALUES (%(timestamp)s, %(server_host)s, %(server_port)s, %(username)s, %(client_host)s, %(connection_type)s, %(connection_id)s, %(database_name)s, %(error_code)s) \u0026#34;\u0026#34;\u0026#34; self.cursor.execute(sql, kwargs) self.conn.commit() def insert_query_audit(self, **kwargs): \u0026#34;\u0026#34;\u0026#34;æ’å…¥æŸ¥è¯¢å®¡è®¡è®°å½•\u0026#34;\u0026#34;\u0026#34; sql = \u0026#34;\u0026#34;\u0026#34; INSERT INTO query_audit (timestamp, connection_id, username, client_host, database_name, command_type, sql_text, error_code) VALUES (%(timestamp)s, %(connection_id)s, %(username)s, %(client_host)s, %(database_name)s, %(command_type)s, %(sql_text)s, %(error_code)s) \u0026#34;\u0026#34;\u0026#34; self.cursor.execute(sql, kwargs) self.conn.commit() def main(): parser = argparse.ArgumentParser(description=\u0026#39;è§£ææ•°æ®åº“å®¡è®¡æ—¥å¿—\u0026#39;) parser.add_argument(\u0026#39;--log-file\u0026#39;, required=True, help=\u0026#39;å®¡è®¡æ—¥å¿—æ–‡ä»¶è·¯å¾„\u0026#39;) parser.add_argument(\u0026#39;--db-host\u0026#39;, default=\u0026#39;localhost\u0026#39;, help=\u0026#39;å®¡è®¡æ•°æ®åº“ä¸»æœº\u0026#39;) parser.add_argument(\u0026#39;--db-port\u0026#39;, type=int, default=3306, help=\u0026#39;å®¡è®¡æ•°æ®åº“ç«¯å£\u0026#39;) parser.add_argument(\u0026#39;--db-user\u0026#39;, required=True, help=\u0026#39;å®¡è®¡æ•°æ®åº“ç”¨æˆ·\u0026#39;) parser.add_argument(\u0026#39;--db-password\u0026#39;, required=True, help=\u0026#39;å®¡è®¡æ•°æ®åº“å¯†ç \u0026#39;) parser.add_argument(\u0026#39;--db-name\u0026#39;, default=\u0026#39;audit_system\u0026#39;, help=\u0026#39;å®¡è®¡æ•°æ®åº“åç§°\u0026#39;) args = parser.parse_args() db_config = { \u0026#39;host\u0026#39;: args.db_host, \u0026#39;port\u0026#39;: args.db_port, \u0026#39;user\u0026#39;: args.db_user, \u0026#39;password\u0026#39;: args.db_password, \u0026#39;database\u0026#39;: args.db_name } parser = AuditLogParser(db_config) parser.parse_mysql_audit_log(args.log_file) print(f\u0026#34;å®¡è®¡æ—¥å¿—è§£æå®Œæˆ: {args.log_file}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() EOF chmod +x \u0026#34;$LOG_DIR/parse_audit_logs.py\u0026#34; log \u0026#34;å®¡è®¡æ—¥å¿—è§£æè„šæœ¬åˆ›å»ºå®Œæˆ\u0026#34; } # åˆ›å»ºå®‰å…¨ç›‘æ§è„šæœ¬ create_security_monitor() { cat \u0026gt; \u0026#34;$LOG_DIR/security_monitor.py\u0026#34; \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; #!/usr/bin/env python3 import mysql.connector import smtplib import json from datetime import datetime, timedelta from email.mime.text import MIMEText from email.mime.multipart import MIMEMultipart import logging class SecurityMonitor: def __init__(self, config_file): with open(config_file, \u0026#39;r\u0026#39;) as f: self.config = json.load(f) self.db_conn = mysql.connector.connect(**self.config[\u0026#39;database\u0026#39;]) self.cursor = self.db_conn.cursor(dictionary=True) logging.basicConfig( level=logging.INFO, format=\u0026#39;%(asctime)s - %(levelname)s - %(message)s\u0026#39;, handlers=[ logging.FileHandler(\u0026#39;/var/log/security_monitor.log\u0026#39;), logging.StreamHandler() ] ) self.logger = logging.getLogger(__name__) def check_failed_logins(self): \u0026#34;\u0026#34;\u0026#34;æ£€æŸ¥ç™»å½•å¤±è´¥æ¬¡æ•°\u0026#34;\u0026#34;\u0026#34; threshold = int(self.get_config_value(\u0026#39;alert_failed_login_threshold\u0026#39;, 5)) # æŸ¥è¯¢æœ€è¿‘1å°æ—¶å†…çš„ç™»å½•å¤±è´¥ self.cursor.execute(\u0026#34;\u0026#34;\u0026#34; SELECT username, client_host, COUNT(*) as failed_count FROM connection_audit WHERE connection_type = \u0026#39;FAILED_CONNECT\u0026#39; AND timestamp \u0026gt; DATE_SUB(NOW(), INTERVAL 1 HOUR) GROUP BY username, client_host HAVING failed_count \u0026gt;= %s \u0026#34;\u0026#34;\u0026#34;, (threshold,)) failed_logins = self.cursor.fetchall() for login in failed_logins: self.create_security_event( event_type=\u0026#39;FAILED_LOGIN_THRESHOLD\u0026#39;, severity=\u0026#39;HIGH\u0026#39;, username=login[\u0026#39;username\u0026#39;], client_host=login[\u0026#39;client_host\u0026#39;], description=f\u0026#34;ç”¨æˆ· {login[\u0026#39;username\u0026#39;]} ä» {login[\u0026#39;client_host\u0026#39;]} åœ¨1å°æ—¶å†…ç™»å½•å¤±è´¥ {login[\u0026#39;failed_count\u0026#39;]} æ¬¡\u0026#34; ) def check_privilege_escalation(self): \u0026#34;\u0026#34;\u0026#34;æ£€æŸ¥æƒé™æå‡\u0026#34;\u0026#34;\u0026#34; if self.get_config_value(\u0026#39;alert_privilege_escalation\u0026#39;, \u0026#39;true\u0026#39;) != \u0026#39;true\u0026#39;: return # æŸ¥è¯¢æœ€è¿‘24å°æ—¶å†…çš„æƒé™å˜æ›´ self.cursor.execute(\u0026#34;\u0026#34;\u0026#34; SELECT * FROM privilege_audit WHERE operation_type IN (\u0026#39;GRANT\u0026#39;, \u0026#39;CREATE_USER\u0026#39;) AND privilege_type IN (\u0026#39;ALL PRIVILEGES\u0026#39;, \u0026#39;SUPER\u0026#39;, \u0026#39;GRANT OPTION\u0026#39;) AND timestamp \u0026gt; DATE_SUB(NOW(), INTERVAL 24 HOUR) \u0026#34;\u0026#34;\u0026#34;) privilege_changes = self.cursor.fetchall() for change in privilege_changes: self.create_security_event( event_type=\u0026#39;PRIVILEGE_ESCALATION\u0026#39;, severity=\u0026#39;CRITICAL\u0026#39;, username=change[\u0026#39;target_username\u0026#39;], client_host=change[\u0026#39;admin_host\u0026#39;], description=f\u0026#34;ç®¡ç†å‘˜ {change[\u0026#39;admin_username\u0026#39;]} ä¸ºç”¨æˆ· {change[\u0026#39;target_username\u0026#39;]} æˆäºˆäº†é«˜çº§æƒé™: {change[\u0026#39;privilege_type\u0026#39;]}\u0026#34; ) def check_suspicious_queries(self): \u0026#34;\u0026#34;\u0026#34;æ£€æŸ¥å¯ç–‘æŸ¥è¯¢\u0026#34;\u0026#34;\u0026#34; suspicious_patterns = [ r\u0026#39;UNION.*SELECT.*FROM.*information_schema\u0026#39;, r\u0026#39;SELECT.*FROM.*mysql\\.user\u0026#39;, r\u0026#39;DROP\\s+DATABASE\u0026#39;, r\u0026#39;TRUNCATE\\s+TABLE\u0026#39;, r\u0026#39;DELETE\\s+FROM.*WHERE.*1\\s*=\\s*1\u0026#39;, r\u0026#39;UPDATE.*SET.*WHERE.*1\\s*=\\s*1\u0026#39; ] for pattern in suspicious_patterns: self.cursor.execute(\u0026#34;\u0026#34;\u0026#34; SELECT * FROM query_audit WHERE sql_text REGEXP %s AND timestamp \u0026gt; DATE_SUB(NOW(), INTERVAL 1 HOUR) \u0026#34;\u0026#34;\u0026#34;, (pattern,)) suspicious_queries = self.cursor.fetchall() for query in suspicious_queries: self.create_security_event( event_type=\u0026#39;SUSPICIOUS_QUERY\u0026#39;, severity=\u0026#39;HIGH\u0026#39;, username=query[\u0026#39;username\u0026#39;], client_host=query[\u0026#39;client_host\u0026#39;], description=f\u0026#34;æ£€æµ‹åˆ°å¯ç–‘æŸ¥è¯¢: {query[\u0026#39;sql_text\u0026#39;][:200]}...\u0026#34; ) def check_unusual_access_patterns(self): \u0026#34;\u0026#34;\u0026#34;æ£€æŸ¥å¼‚å¸¸è®¿é—®æ¨¡å¼\u0026#34;\u0026#34;\u0026#34; # æ£€æŸ¥éå·¥ä½œæ—¶é—´è®¿é—® self.cursor.execute(\u0026#34;\u0026#34;\u0026#34; SELECT username, client_host, COUNT(*) as access_count FROM connection_audit WHERE connection_type = \u0026#39;CONNECT\u0026#39; AND (HOUR(timestamp) \u0026lt; 8 OR HOUR(timestamp) \u0026gt; 18) AND WEEKDAY(timestamp) \u0026lt; 5 AND timestamp \u0026gt; DATE_SUB(NOW(), INTERVAL 24 HOUR) GROUP BY username, client_host HAVING access_count \u0026gt; 10 \u0026#34;\u0026#34;\u0026#34;) unusual_access = self.cursor.fetchall() for access in unusual_access: self.create_security_event( event_type=\u0026#39;UNUSUAL_ACCESS_TIME\u0026#39;, severity=\u0026#39;MEDIUM\u0026#39;, username=access[\u0026#39;username\u0026#39;], client_host=access[\u0026#39;client_host\u0026#39;], description=f\u0026#34;ç”¨æˆ· {access[\u0026#39;username\u0026#39;]} åœ¨éå·¥ä½œæ—¶é—´ä» {access[\u0026#39;client_host\u0026#39;]} è®¿é—®æ•°æ®åº“ {access[\u0026#39;access_count\u0026#39;]} æ¬¡\u0026#34; ) # æ£€æŸ¥å¼‚å¸¸IPè®¿é—® self.cursor.execute(\u0026#34;\u0026#34;\u0026#34; SELECT username, client_host, COUNT(*) as new_ip_count FROM connection_audit ca1 WHERE connection_type = \u0026#39;CONNECT\u0026#39; AND timestamp \u0026gt; DATE_SUB(NOW(), INTERVAL 24 HOUR) AND NOT EXISTS ( SELECT 1 FROM connection_audit ca2 WHERE ca2.username = ca1.username AND ca2.client_host = ca1.client_host AND ca2.timestamp \u0026lt; DATE_SUB(NOW(), INTERVAL 7 DAY) ) GROUP BY username, client_host \u0026#34;\u0026#34;\u0026#34;) new_ip_access = self.cursor.fetchall() for access in new_ip_access: self.create_security_event( event_type=\u0026#39;NEW_IP_ACCESS\u0026#39;, severity=\u0026#39;MEDIUM\u0026#39;, username=access[\u0026#39;username\u0026#39;], client_host=access[\u0026#39;client_host\u0026#39;], description=f\u0026#34;ç”¨æˆ· {access[\u0026#39;username\u0026#39;]} ä»æ–°IPåœ°å€ {access[\u0026#39;client_host\u0026#39;]} è®¿é—®æ•°æ®åº“\u0026#34; ) def create_security_event(self, event_type, severity, username, client_host, description): \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºå®‰å…¨äº‹ä»¶\u0026#34;\u0026#34;\u0026#34; self.cursor.execute(\u0026#34;\u0026#34;\u0026#34; INSERT INTO security_audit (event_type, severity, username, client_host, description) VALUES (%s, %s, %s, %s, %s) \u0026#34;\u0026#34;\u0026#34;, (event_type, severity, username, client_host, description)) self.db_conn.commit() self.logger.warning(f\u0026#34;å®‰å…¨äº‹ä»¶: {event_type} - {description}\u0026#34;) # å‘é€å‘Šè­¦é‚®ä»¶ if severity in [\u0026#39;HIGH\u0026#39;, \u0026#39;CRITICAL\u0026#39;]: self.send_alert_email(event_type, severity, description) def send_alert_email(self, event_type, severity, description): \u0026#34;\u0026#34;\u0026#34;å‘é€å‘Šè­¦é‚®ä»¶\u0026#34;\u0026#34;\u0026#34; try: smtp_config = self.config[\u0026#39;smtp\u0026#39;] msg = MIMEMultipart() msg[\u0026#39;From\u0026#39;] = smtp_config[\u0026#39;from_email\u0026#39;] msg[\u0026#39;To\u0026#39;] = smtp_config[\u0026#39;to_email\u0026#39;] msg[\u0026#39;Subject\u0026#39;] = f\u0026#34;æ•°æ®åº“å®‰å…¨å‘Šè­¦ - {severity} - {event_type}\u0026#34; body = f\u0026#34;\u0026#34;\u0026#34; æ•°æ®åº“å®‰å…¨å‘Šè­¦ äº‹ä»¶ç±»å‹: {event_type} ä¸¥é‡çº§åˆ«: {severity} æ—¶é—´: {datetime.now().strftime(\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;)} æè¿°: {description} è¯·åŠæ—¶å¤„ç†æ­¤å®‰å…¨äº‹ä»¶ã€‚ \u0026#34;\u0026#34;\u0026#34; msg.attach(MIMEText(body, \u0026#39;plain\u0026#39;, \u0026#39;utf-8\u0026#39;)) server = smtplib.SMTP(smtp_config[\u0026#39;server\u0026#39;], smtp_config[\u0026#39;port\u0026#39;]) if smtp_config.get(\u0026#39;use_tls\u0026#39;, False): server.starttls() if smtp_config.get(\u0026#39;username\u0026#39;): server.login(smtp_config[\u0026#39;username\u0026#39;], smtp_config[\u0026#39;password\u0026#39;]) server.send_message(msg) server.quit() self.logger.info(f\u0026#34;å‘Šè­¦é‚®ä»¶å‘é€æˆåŠŸ: {event_type}\u0026#34;) except Exception as e: self.logger.error(f\u0026#34;å‘é€å‘Šè­¦é‚®ä»¶å¤±è´¥: {e}\u0026#34;) def get_config_value(self, key, default_value): \u0026#34;\u0026#34;\u0026#34;è·å–é…ç½®å€¼\u0026#34;\u0026#34;\u0026#34; self.cursor.execute( \u0026#34;SELECT config_value FROM audit_config WHERE config_key = %s\u0026#34;, (key,) ) result = self.cursor.fetchone() return result[\u0026#39;config_value\u0026#39;] if result else default_value def run_security_checks(self): \u0026#34;\u0026#34;\u0026#34;è¿è¡Œæ‰€æœ‰å®‰å…¨æ£€æŸ¥\u0026#34;\u0026#34;\u0026#34; self.logger.info(\u0026#34;å¼€å§‹å®‰å…¨æ£€æŸ¥\u0026#34;) try: self.check_failed_logins() self.check_privilege_escalation() self.check_suspicious_queries() self.check_unusual_access_patterns() self.logger.info(\u0026#34;å®‰å…¨æ£€æŸ¥å®Œæˆ\u0026#34;) except Exception as e: self.logger.error(f\u0026#34;å®‰å…¨æ£€æŸ¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}\u0026#34;) def main(): config = { \u0026#34;database\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 3306, \u0026#34;user\u0026#34;: \u0026#34;audit_user\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;audit_password\u0026#34;, \u0026#34;database\u0026#34;: \u0026#34;audit_system\u0026#34; }, \u0026#34;smtp\u0026#34;: { \u0026#34;server\u0026#34;: \u0026#34;smtp.company.com\u0026#34;, \u0026#34;port\u0026#34;: 587, \u0026#34;use_tls\u0026#34;: True, \u0026#34;username\u0026#34;: \u0026#34;alert@company.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;smtp_password\u0026#34;, \u0026#34;from_email\u0026#34;: \u0026#34;alert@company.com\u0026#34;, \u0026#34;to_email\u0026#34;: \u0026#34;admin@company.com\u0026#34; } } with open(\u0026#39;/tmp/security_monitor_config.json\u0026#39;, \u0026#39;w\u0026#39;) as f: json.dump(config, f, indent=2) monitor = SecurityMonitor(\u0026#39;/tmp/security_monitor_config.json\u0026#39;) monitor.run_security_checks() if __name__ == \u0026#34;__main__\u0026#34;: main() EOF chmod +x \u0026#34;$LOG_DIR/security_monitor.py\u0026#34; log \u0026#34;å®‰å…¨ç›‘æ§è„šæœ¬åˆ›å»ºå®Œæˆ\u0026#34; } # åˆ›å»ºå®¡è®¡æŠ¥å‘Šç”Ÿæˆè„šæœ¬ create_audit_report() { cat \u0026gt; \u0026#34;$LOG_DIR/generate_audit_report.sh\u0026#34; \u0026lt;\u0026lt; \u0026#39;EOF\u0026#39; #!/bin/bash REPORT_DATE=\u0026#34;${1:-$(date -d \u0026#39;yesterday\u0026#39; \u0026#39;+%Y-%m-%d\u0026#39;)}\u0026#34; REPORT_DIR=\u0026#34;/var/reports/database_audit\u0026#34; REPORT_FILE=\u0026#34;$REPORT_DIR/audit_report_$REPORT_DATE.html\u0026#34; mkdir -p \u0026#34;$REPORT_DIR\u0026#34; # ç”ŸæˆHTMLæŠ¥å‘Š cat \u0026gt; \u0026#34;$REPORT_FILE\u0026#34; \u0026lt;\u0026lt; HTML_START \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;æ•°æ®åº“å®¡è®¡æŠ¥å‘Š - $REPORT_DATE\u0026lt;/title\u0026gt; \u0026lt;meta charset=\u0026#34;UTF-8\u0026#34;\u0026gt; \u0026lt;style\u0026gt; body { font-family: Arial, sans-serif; margin: 20px; } .header { background-color: #f0f0f0; padding: 20px; border-radius: 5px; } .section { margin: 20px 0; } .metric { display: inline-block; margin: 10px; padding: 15px; background-color: #e8f4f8; border-radius: 5px; } table { border-collapse: collapse; width: 100%; margin: 10px 0; } th, td { border: 1px solid #ddd; padding: 8px; text-align: left; } th { background-color: #f2f2f2; } .alert { color: red; font-weight: bold; } .warning { color: orange; font-weight: bold; } .info { color: blue; } \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;header\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;æ•°æ®åº“å®¡è®¡æŠ¥å‘Š\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;æŠ¥å‘Šæ—¥æœŸ: $REPORT_DATE\u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;ç”Ÿæˆæ—¶é—´: $(date)\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; HTML_START # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯ mysql -h\u0026#34;$AUDIT_DB_HOST\u0026#34; -P\u0026#34;$AUDIT_DB_PORT\u0026#34; -u\u0026#34;$AUDIT_DB_USER\u0026#34; -p\u0026#34;$AUDIT_DB_PASSWORD\u0026#34; \u0026#34;$AUDIT_DB_NAME\u0026#34; -H \u0026lt;\u0026lt; \u0026#39;SQL_STATS\u0026#39; \u0026gt;\u0026gt; \u0026#34;$REPORT_FILE\u0026#34; \u0026lt;div class=\u0026#34;section\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;ç»Ÿè®¡æ¦‚è§ˆ\u0026lt;/h2\u0026gt; \u0026lt;div class=\u0026#34;metric\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;è¿æ¥ç»Ÿè®¡\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;æ€»è¿æ¥æ•°: SELECT COUNT(*) as total_connections FROM connection_audit WHERE DATE(timestamp) = CURDATE() - INTERVAL 1 DAY; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;å¤±è´¥è¿æ¥æ•°: SELECT COUNT(*) as failed_connections FROM connection_audit WHERE DATE(timestamp) = CURDATE() - INTERVAL 1 DAY AND connection_type = \u0026#39;FAILED_CONNECT\u0026#39;; \u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;metric\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;æŸ¥è¯¢ç»Ÿè®¡\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;æ€»æŸ¥è¯¢æ•°: SELECT COUNT(*) as total_queries FROM query_audit WHERE DATE(timestamp) = CURDATE() - INTERVAL 1 DAY; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;æ…¢æŸ¥è¯¢æ•°: SELECT COUNT(*) as slow_queries FROM query_audit WHERE DATE(timestamp) = CURDATE() - INTERVAL 1 DAY AND execution_time_ms \u0026gt; 1000; \u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;div class=\u0026#34;metric\u0026#34;\u0026gt; \u0026lt;h3\u0026gt;å®‰å…¨äº‹ä»¶\u0026lt;/h3\u0026gt; \u0026lt;p\u0026gt;å®‰å…¨äº‹ä»¶æ€»æ•°: SELECT COUNT(*) as security_events FROM security_audit WHERE DATE(timestamp) = CURDATE() - INTERVAL 1 DAY; \u0026lt;/p\u0026gt; \u0026lt;p\u0026gt;é«˜å±äº‹ä»¶æ•°: SELECT COUNT(*) as critical_events FROM security_audit WHERE DATE(timestamp) = CURDATE() - INTERVAL 1 DAY AND severity IN (\u0026#39;HIGH\u0026#39;, \u0026#39;CRITICAL\u0026#39;); \u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; SQL_STATS # æ·»åŠ è¯¦ç»†æŠ¥å‘Š cat \u0026gt;\u0026gt; \u0026#34;$REPORT_FILE\u0026#34; \u0026lt;\u0026lt; \u0026#39;HTML_END\u0026#39; \u0026lt;div class=\u0026#34;section\u0026#34;\u0026gt; \u0026lt;h2\u0026gt;è¯¦ç»†åˆ†æ\u0026lt;/h2\u0026gt; \u0026lt;h3\u0026gt;Top 10 æ´»è·ƒç”¨æˆ·\u0026lt;/h3\u0026gt; \u0026lt;table\u0026gt; \u0026lt;tr\u0026gt;\u0026lt;th\u0026gt;ç”¨æˆ·å\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;è¿æ¥æ¬¡æ•°\u0026lt;/th\u0026gt;\u0026lt;th\u0026gt;æŸ¥è¯¢æ¬¡æ•°\u0026lt;/th\u0026gt;\u0026lt;/tr\u0026gt; HTML_END mysql -h\u0026#34;$AUDIT_DB_HOST\u0026#34; -P\u0026#34;$AUDIT_DB_PORT\u0026#34; -u\u0026#34;$AUDIT_DB_USER\u0026#34; -p\u0026#34;$AUDIT_DB_PASSWORD\u0026#34; \u0026#34;$AUDIT_DB_NAME\u0026#34; -H \u0026lt;\u0026lt; \u0026#39;SQL_USERS\u0026#39; \u0026gt;\u0026gt; \u0026#34;$REPORT_FILE\u0026#34; SELECT ca.username, COUNT(DISTINCT ca.id) as connection_count, COALESCE(qa.query_count, 0) as query_count FROM connection_audit ca LEFT JOIN ( SELECT username, COUNT(*) as query_count FROM query_audit WHERE DATE(timestamp) = CURDATE() - INTERVAL 1 DAY GROUP BY username ) qa ON ca.username = qa.username WHERE DATE(ca.timestamp) = CURDATE() - INTERVAL 1 DAY GROUP BY ca.username ORDER BY connection_count DESC LIMIT 10; SQL_USERS cat \u0026gt;\u0026gt; \u0026#34;$REPORT_FILE\u0026#34; \u0026lt;\u0026lt; \u0026#39;HTML_CLOSE\u0026#39; \u0026lt;/table\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; HTML_CLOSE echo \u0026#34;å®¡è®¡æŠ¥å‘Šç”Ÿæˆå®Œæˆ: $REPORT_FILE\u0026#34; EOF chmod +x \u0026#34;$LOG_DIR/generate_audit_report.sh\u0026#34; log \u0026#34;å®¡è®¡æŠ¥å‘Šç”Ÿæˆè„šæœ¬åˆ›å»ºå®Œæˆ\u0026#34; } # æ¸…ç†è¿‡æœŸå®¡è®¡æ—¥å¿— cleanup_old_logs() { log \u0026#34;æ¸…ç†è¿‡æœŸå®¡è®¡æ—¥å¿—\u0026#34; # æ¸…ç†æ–‡ä»¶ç³»ç»Ÿæ—¥å¿— find \u0026#34;$LOG_DIR\u0026#34; -name \u0026#34;*.log\u0026#34; -type f -mtime +$RETENTION_DAYS -delete # æ¸…ç†æ•°æ®åº“å®¡è®¡è®°å½• mysql -h\u0026#34;$AUDIT_DB_HOST\u0026#34; -P\u0026#34;$AUDIT_DB_PORT\u0026#34; -u\u0026#34;$AUDIT_DB_USER\u0026#34; -p\u0026#34;$AUDIT_DB_PASSWORD\u0026#34; \u0026#34;$AUDIT_DB_NAME\u0026#34; \u0026lt;\u0026lt; EOF DELETE FROM connection_audit WHERE timestamp \u0026lt; DATE_SUB(NOW(), INTERVAL $RETENTION_DAYS DAY); DELETE FROM query_audit WHERE timestamp \u0026lt; DATE_SUB(NOW(), INTERVAL $RETENTION_DAYS DAY); DELETE FROM privilege_audit WHERE timestamp \u0026lt; DATE_SUB(NOW(), INTERVAL $RETENTION_DAYS DAY); DELETE FROM data_change_audit WHERE timestamp \u0026lt; DATE_SUB(NOW(), INTERVAL $RETENTION_DAYS DAY); DELETE FROM security_audit WHERE timestamp \u0026lt; DATE_SUB(NOW(), INTERVAL $RETENTION_DAYS DAY) AND resolved = TRUE; EOF log \u0026#34;è¿‡æœŸå®¡è®¡æ—¥å¿—æ¸…ç†å®Œæˆ\u0026#34; } # åˆ›å»ºå®šæ—¶ä»»åŠ¡ setup_cron_jobs() { log \u0026#34;è®¾ç½®å®šæ—¶ä»»åŠ¡\u0026#34; # åˆ›å»ºcroné…ç½® cat \u0026gt; /tmp/database_audit_cron \u0026lt;\u0026lt; EOF # æ¯5åˆ†é’Ÿè¿è¡Œå®‰å…¨ç›‘æ§ */5 * * * * $LOG_DIR/security_monitor.py # æ¯å°æ—¶è§£æå®¡è®¡æ—¥å¿— 0 * * * * $LOG_DIR/parse_audit_logs.py --log-file $LOG_DIR/mysql_audit.log --db-user $AUDIT_DB_USER --db-password $AUDIT_DB_PASSWORD # æ¯å¤©ç”Ÿæˆå®¡è®¡æŠ¥å‘Š 0 6 * * * $LOG_DIR/generate_audit_report.sh # æ¯å‘¨æ¸…ç†è¿‡æœŸæ—¥å¿— 0 2 * * 0 $0 cleanup_old_logs EOF crontab /tmp/database_audit_cron rm /tmp/database_audit_cron log \u0026#34;å®šæ—¶ä»»åŠ¡è®¾ç½®å®Œæˆ\u0026#34; } # ä¸»å‡½æ•° main() { case \u0026#34;${1:-install}\u0026#34; in \u0026#34;install\u0026#34;) log \u0026#34;å¼€å§‹å®‰è£…æ•°æ®åº“å®¡è®¡ç³»ç»Ÿ\u0026#34; create_log_directory create_audit_tables configure_mysql_audit create_log_parser create_security_monitor create_audit_report setup_cron_jobs log \u0026#34;æ•°æ®åº“å®¡è®¡ç³»ç»Ÿå®‰è£…å®Œæˆ\u0026#34; ;; \u0026#34;cleanup\u0026#34;) cleanup_old_logs ;; \u0026#34;report\u0026#34;) \u0026#34;$LOG_DIR/generate_audit_report.sh\u0026#34; \u0026#34;${2:-}\u0026#34; ;; \u0026#34;monitor\u0026#34;) \u0026#34;$LOG_DIR/security_monitor.py\u0026#34; ;; *) echo \u0026#34;ç”¨æ³•: $0 {install|cleanup|report|monitor}\u0026#34; echo \u0026#34; install - å®‰è£…å®¡è®¡ç³»ç»Ÿ\u0026#34; echo \u0026#34; cleanup - æ¸…ç†è¿‡æœŸæ—¥å¿—\u0026#34; echo \u0026#34; report - ç”Ÿæˆå®¡è®¡æŠ¥å‘Š\u0026#34; echo \u0026#34; monitor - è¿è¡Œå®‰å…¨ç›‘æ§\u0026#34; exit 1 ;; esac } main \u0026#34;$@\u0026#34; æ¼æ´é˜²æŠ¤ä¸å®‰å…¨åŠ å›º 1. SQLæ³¨å…¥é˜²æŠ¤ #!/usr/bin/env python3 # scripts/sql_injection_protection.py import re import logging from typing import List, Dict, Any, Tuple import mysql.connector import psycopg2 from sqlparse import parse, tokens import hashlib class SQLInjectionProtector: def __init__(self): self.logger = self._setup_logging() # SQLæ³¨å…¥æ£€æµ‹è§„åˆ™ self.injection_patterns = [ # è”åˆæŸ¥è¯¢æ³¨å…¥ r\u0026#39;(?i)\\bunion\\s+select\\b\u0026#39;, r\u0026#39;(?i)\\bunion\\s+all\\s+select\\b\u0026#39;, # å¸ƒå°”ç›²æ³¨ r\u0026#39;(?i)\\b(and|or)\\s+\\d+\\s*=\\s*\\d+\u0026#39;, r\u0026#39;(?i)\\b(and|or)\\s+[\\\u0026#39;\u0026#34;]?\\w+[\\\u0026#39;\u0026#34;]?\\s*=\\s*[\\\u0026#39;\u0026#34;]?\\w+[\\\u0026#39;\u0026#34;]?\u0026#39;, # æ—¶é—´ç›²æ³¨ r\u0026#39;(?i)\\bsleep\\s*\\(\u0026#39;, r\u0026#39;(?i)\\bwaitfor\\s+delay\\b\u0026#39;, r\u0026#39;(?i)\\bbenchmark\\s*\\(\u0026#39;, # é”™è¯¯æ³¨å…¥ r\u0026#39;(?i)\\bextractvalue\\s*\\(\u0026#39;, r\u0026#39;(?i)\\bupdatexml\\s*\\(\u0026#39;, # å †å æŸ¥è¯¢ r\u0026#39;;\\s*drop\\s+\u0026#39;, r\u0026#39;;\\s*delete\\s+\u0026#39;, r\u0026#39;;\\s*insert\\s+\u0026#39;, r\u0026#39;;\\s*update\\s+\u0026#39;, # æ³¨é‡Šç»•è¿‡ r\u0026#39;/\\*.*?\\*/\u0026#39;, r\u0026#39;--\\s+\u0026#39;, r\u0026#39;#.*$\u0026#39;, # å‡½æ•°æ³¨å…¥ r\u0026#39;(?i)\\bload_file\\s*\\(\u0026#39;, r\u0026#39;(?i)\\binto\\s+outfile\\b\u0026#39;, r\u0026#39;(?i)\\binto\\s+dumpfile\\b\u0026#39;, # ä¿¡æ¯æ³„éœ² r\u0026#39;(?i)\\binformation_schema\\b\u0026#39;, r\u0026#39;(?i)\\bmysql\\.user\\b\u0026#39;, r\u0026#39;(?i)\\bpg_user\\b\u0026#39;, # ç‰¹æ®Šå­—ç¬¦ r\u0026#39;[\\\u0026#39;\u0026#34;;].*[\\\u0026#39;\u0026#34;;]\u0026#39;, r\u0026#39;\\bchar\\s*\\(\\s*\\d+\\s*\\)\u0026#39;, r\u0026#39;\\bhex\\s*\\(\u0026#39;, r\u0026#39;\\bascii\\s*\\(\u0026#39;, ] # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ self.compiled_patterns = [re.compile(pattern) for pattern in self.injection_patterns] # ç™½åå•å…³é”®å­— self.whitelist_keywords = { \u0026#39;SELECT\u0026#39;, \u0026#39;FROM\u0026#39;, \u0026#39;WHERE\u0026#39;, \u0026#39;ORDER\u0026#39;, \u0026#39;BY\u0026#39;, \u0026#39;GROUP\u0026#39;, \u0026#39;HAVING\u0026#39;, \u0026#39;INSERT\u0026#39;, \u0026#39;INTO\u0026#39;, \u0026#39;VALUES\u0026#39;, \u0026#39;UPDATE\u0026#39;, \u0026#39;SET\u0026#39;, \u0026#39;DELETE\u0026#39;, \u0026#39;JOIN\u0026#39;, \u0026#39;LEFT\u0026#39;, \u0026#39;RIGHT\u0026#39;, \u0026#39;INNER\u0026#39;, \u0026#39;OUTER\u0026#39;, \u0026#39;ON\u0026#39;, \u0026#39;AND\u0026#39;, \u0026#39;OR\u0026#39;, \u0026#39;NOT\u0026#39;, \u0026#39;IN\u0026#39;, \u0026#39;EXISTS\u0026#39;, \u0026#39;BETWEEN\u0026#39;, \u0026#39;LIKE\u0026#39;, \u0026#39;IS\u0026#39;, \u0026#39;NULL\u0026#39;, \u0026#39;ASC\u0026#39;, \u0026#39;DESC\u0026#39;, \u0026#39;LIMIT\u0026#39;, \u0026#39;OFFSET\u0026#39; } def _setup_logging(self) -\u0026gt; logging.Logger: \u0026#34;\u0026#34;\u0026#34;è®¾ç½®æ—¥å¿—è®°å½•\u0026#34;\u0026#34;\u0026#34; logger = logging.getLogger(\u0026#39;SQLInjectionProtector\u0026#39;) logger.setLevel(logging.INFO) handler = logging.FileHandler(\u0026#39;/var/log/sql_injection_protection.log\u0026#39;) formatter = logging.Formatter( \u0026#39;%(asctime)s - %(name)s - %(levelname)s - %(message)s\u0026#39; ) handler.setFormatter(formatter) logger.addHandler(handler) return logger def detect_sql_injection(self, sql_query: str, parameters: List[Any] = None) -\u0026gt; Tuple[bool, List[str]]: \u0026#34;\u0026#34;\u0026#34;æ£€æµ‹SQLæ³¨å…¥æ”»å‡»\u0026#34;\u0026#34;\u0026#34; threats = [] # åŸºæœ¬æ¨¡å¼åŒ¹é…æ£€æµ‹ for pattern in self.compiled_patterns: if pattern.search(sql_query): threats.append(f\u0026#34;æ£€æµ‹åˆ°å¯ç–‘æ¨¡å¼: {pattern.pattern}\u0026#34;) # è¯­æ³•åˆ†ææ£€æµ‹ syntax_threats = self._analyze_sql_syntax(sql_query) threats.extend(syntax_threats) # å‚æ•°åŒ–æŸ¥è¯¢æ£€æµ‹ if parameters: param_threats = self._check_parameters(parameters) threats.extend(param_threats) # é•¿åº¦æ£€æµ‹ if len(sql_query) \u0026gt; 10000: threats.append(\u0026#34;SQLæŸ¥è¯¢é•¿åº¦å¼‚å¸¸\u0026#34;) # å…³é”®å­—é¢‘ç‡æ£€æµ‹ keyword_threats = self._check_keyword_frequency(sql_query) threats.extend(keyword_threats) is_malicious = len(threats) \u0026gt; 0 if is_malicious: self.logger.warning(f\u0026#34;æ£€æµ‹åˆ°SQLæ³¨å…¥æ”»å‡»: {sql_query[:200]}...\u0026#34;) for threat in threats: self.logger.warning(f\u0026#34;å¨èƒè¯¦æƒ…: {threat}\u0026#34;) return is_malicious, threats def _analyze_sql_syntax(self, sql_query: str) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;åˆ†æSQLè¯­æ³•ç»“æ„\u0026#34;\u0026#34;\u0026#34; threats = [] try: parsed = parse(sql_query) for statement in parsed: # æ£€æŸ¥æ˜¯å¦åŒ…å«å¤šä¸ªè¯­å¥ï¼ˆå †å æŸ¥è¯¢ï¼‰ if len([token for token in statement.flatten() if token.ttype is tokens.Keyword and token.value.upper() in [\u0026#39;SELECT\u0026#39;, \u0026#39;INSERT\u0026#39;, \u0026#39;UPDATE\u0026#39;, \u0026#39;DELETE\u0026#39;, \u0026#39;DROP\u0026#39;]]) \u0026gt; 1: threats.append(\u0026#34;æ£€æµ‹åˆ°å †å æŸ¥è¯¢\u0026#34;) # æ£€æŸ¥UNIONæŸ¥è¯¢ union_count = len([token for token in statement.flatten() if token.ttype is tokens.Keyword and token.value.upper() == \u0026#39;UNION\u0026#39;]) if union_count \u0026gt; 0: threats.append(f\u0026#34;æ£€æµ‹åˆ°UNIONæŸ¥è¯¢ ({union_count}ä¸ª)\u0026#34;) # æ£€æŸ¥å­æŸ¥è¯¢åµŒå¥—æ·±åº¦ subquery_depth = self._count_subquery_depth(statement) if subquery_depth \u0026gt; 3: threats.append(f\u0026#34;å­æŸ¥è¯¢åµŒå¥—è¿‡æ·± (æ·±åº¦: {subquery_depth})\u0026#34;) except Exception as e: threats.append(f\u0026#34;SQLè¯­æ³•è§£æå¼‚å¸¸: {str(e)}\u0026#34;) return threats def _count_subquery_depth(self, statement, depth=0) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;è®¡ç®—å­æŸ¥è¯¢åµŒå¥—æ·±åº¦\u0026#34;\u0026#34;\u0026#34; max_depth = depth for token in statement.tokens: if hasattr(token, \u0026#39;tokens\u0026#39;): if any(t.ttype is tokens.Keyword and t.value.upper() == \u0026#39;SELECT\u0026#39; for t in token.flatten()): max_depth = max(max_depth, self._count_subquery_depth(token, depth + 1)) return max_depth def _check_parameters(self, parameters: List[Any]) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;æ£€æŸ¥å‚æ•°æ˜¯å¦åŒ…å«æ¶æ„å†…å®¹\u0026#34;\u0026#34;\u0026#34; threats = [] for i, param in enumerate(parameters): if isinstance(param, str): # æ£€æŸ¥å‚æ•°ä¸­çš„SQLå…³é”®å­— param_upper = param.upper() sql_keywords = [\u0026#39;SELECT\u0026#39;, \u0026#39;UNION\u0026#39;, \u0026#39;DROP\u0026#39;, \u0026#39;DELETE\u0026#39;, \u0026#39;INSERT\u0026#39;, \u0026#39;UPDATE\u0026#39;] for keyword in sql_keywords: if keyword in param_upper: threats.append(f\u0026#34;å‚æ•° {i} åŒ…å«SQLå…³é”®å­—: {keyword}\u0026#34;) # æ£€æŸ¥ç‰¹æ®Šå­—ç¬¦ if re.search(r\u0026#39;[\\\u0026#39;\u0026#34;;]\u0026#39;, param): threats.append(f\u0026#34;å‚æ•° {i} åŒ…å«ç‰¹æ®Šå­—ç¬¦\u0026#34;) # æ£€æŸ¥ç¼–ç ç»•è¿‡ if re.search(r\u0026#39;%[0-9a-fA-F]{2}\u0026#39;, param): threats.append(f\u0026#34;å‚æ•° {i} åŒ…å«URLç¼–ç \u0026#34;) return threats def _check_keyword_frequency(self, sql_query: str) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;æ£€æŸ¥å…³é”®å­—é¢‘ç‡å¼‚å¸¸\u0026#34;\u0026#34;\u0026#34; threats = [] # ç»Ÿè®¡å…³é”®å­—å‡ºç°æ¬¡æ•° keyword_counts = {} words = re.findall(r\u0026#39;\\b\\w+\\b\u0026#39;, sql_query.upper()) for word in words: if word in self.whitelist_keywords: keyword_counts[word] = keyword_counts.get(word, 0) + 1 # æ£€æŸ¥å¼‚å¸¸é¢‘ç‡ for keyword, count in keyword_counts.items(): if keyword == \u0026#39;UNION\u0026#39; and count \u0026gt; 1: threats.append(f\u0026#34;UNIONå…³é”®å­—å‡ºç°æ¬¡æ•°å¼‚å¸¸: {count}\u0026#34;) elif keyword in [\u0026#39;SELECT\u0026#39;, \u0026#39;INSERT\u0026#39;, \u0026#39;UPDATE\u0026#39;, \u0026#39;DELETE\u0026#39;] and count \u0026gt; 2: threats.append(f\u0026#34;{keyword}å…³é”®å­—å‡ºç°æ¬¡æ•°å¼‚å¸¸: {count}\u0026#34;) return threats def sanitize_input(self, user_input: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;æ¸…ç†ç”¨æˆ·è¾“å…¥\u0026#34;\u0026#34;\u0026#34; if not isinstance(user_input, str): return str(user_input) # ç§»é™¤å±é™©å­—ç¬¦ sanitized = re.sub(r\u0026#39;[\\\u0026#39;\u0026#34;;\\\\]\u0026#39;, \u0026#39;\u0026#39;, user_input) # ç§»é™¤SQLæ³¨é‡Š sanitized = re.sub(r\u0026#39;/\\*.*?\\*/\u0026#39;, \u0026#39;\u0026#39;, sanitized, flags=re.DOTALL) sanitized = re.sub(r\u0026#39;--.*$\u0026#39;, \u0026#39;\u0026#39;, sanitized, flags=re.MULTILINE) sanitized = re.sub(r\u0026#39;#.*$\u0026#39;, \u0026#39;\u0026#39;, sanitized, flags=re.MULTILINE) # é™åˆ¶é•¿åº¦ if len(sanitized) \u0026gt; 1000: sanitized = sanitized[:1000] return sanitized.strip() def create_safe_query(self, base_query: str, parameters: Dict[str, Any]) -\u0026gt; Tuple[str, List[Any]]: \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºå®‰å…¨çš„å‚æ•°åŒ–æŸ¥è¯¢\u0026#34;\u0026#34;\u0026#34; # éªŒè¯åŸºç¡€æŸ¥è¯¢ is_malicious, threats = self.detect_sql_injection(base_query) if is_malicious: raise ValueError(f\u0026#34;åŸºç¡€æŸ¥è¯¢åŒ…å«æ¶æ„å†…å®¹: {threats}\u0026#34;) # æ¸…ç†å‚æ•° safe_params = [] for key, value in parameters.items(): if isinstance(value, str): safe_value = self.sanitize_input(value) safe_params.append(safe_value) else: safe_params.append(value) return base_query, safe_params def validate_table_name(self, table_name: str, allowed_tables: List[str]) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;éªŒè¯è¡¨åæ˜¯å¦åœ¨å…è®¸åˆ—è¡¨ä¸­\u0026#34;\u0026#34;\u0026#34; # æ¸…ç†è¡¨å clean_table_name = re.sub(r\u0026#39;[^a-zA-Z0-9_]\u0026#39;, \u0026#39;\u0026#39;, table_name) # æ£€æŸ¥æ˜¯å¦åœ¨ç™½åå•ä¸­ return clean_table_name in allowed_tables def validate_column_name(self, column_name: str, allowed_columns: List[str]) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;éªŒè¯åˆ—åæ˜¯å¦åœ¨å…è®¸åˆ—è¡¨ä¸­\u0026#34;\u0026#34;\u0026#34; # æ¸…ç†åˆ—å clean_column_name = re.sub(r\u0026#39;[^a-zA-Z0-9_]\u0026#39;, \u0026#39;\u0026#39;, column_name) # æ£€æŸ¥æ˜¯å¦åœ¨ç™½åå•ä¸­ return clean_column_name in allowed_columns def create_prepared_statement(self, connection, query: str, parameters: List[Any]): \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºé¢„å¤„ç†è¯­å¥\u0026#34;\u0026#34;\u0026#34; try: if isinstance(connection, mysql.connector.MySQLConnection): cursor = connection.cursor(prepared=True) cursor.execute(query, parameters) return cursor elif hasattr(connection, \u0026#39;cursor\u0026#39;): # PostgreSQL cursor = connection.cursor() cursor.execute(query, parameters) return cursor else: raise ValueError(\u0026#34;ä¸æ”¯æŒçš„æ•°æ®åº“è¿æ¥ç±»å‹\u0026#34;) except Exception as e: self.logger.error(f\u0026#34;åˆ›å»ºé¢„å¤„ç†è¯­å¥å¤±è´¥: {e}\u0026#34;) raise def log_security_event(self, event_type: str, sql_query: str, client_ip: str, user_id: str, threats: List[str]): \u0026#34;\u0026#34;\u0026#34;è®°å½•å®‰å…¨äº‹ä»¶\u0026#34;\u0026#34;\u0026#34; event_data = { \u0026#39;timestamp\u0026#39;: datetime.now().isoformat(), \u0026#39;event_type\u0026#39;: event_type, \u0026#39;sql_query\u0026#39;: sql_query[:500], # é™åˆ¶é•¿åº¦ \u0026#39;client_ip\u0026#39;: client_ip, \u0026#39;user_id\u0026#39;: user_id, \u0026#39;threats\u0026#39;: threats, \u0026#39;query_hash\u0026#39;: hashlib.md5(sql_query.encode()).hexdigest() } self.logger.warning(f\u0026#34;å®‰å…¨äº‹ä»¶: {event_data}\u0026#34;) # è¿™é‡Œå¯ä»¥é›†æˆåˆ°SIEMç³»ç»Ÿæˆ–å‘é€å‘Šè­¦ def main(): # ç¤ºä¾‹ç”¨æ³• protector = SQLInjectionProtector() # æµ‹è¯•æ¶æ„æŸ¥è¯¢ malicious_queries = [ \u0026#34;SELECT * FROM users WHERE id = 1 UNION SELECT username, password FROM admin\u0026#34;, \u0026#34;SELECT * FROM products WHERE name = \u0026#39;test\u0026#39; OR 1=1--\u0026#34;, \u0026#34;SELECT * FROM users WHERE id = 1; DROP TABLE users;\u0026#34;, \u0026#34;SELECT * FROM users WHERE id = 1 AND SLEEP(5)\u0026#34;, \u0026#34;SELECT * FROM users WHERE id = extractvalue(1, concat(0x7e, (SELECT password FROM admin LIMIT 1), 0x7e))\u0026#34; ] for query in malicious_queries: is_malicious, threats = protector.detect_sql_injection(query) print(f\u0026#34;æŸ¥è¯¢: {query[:50]}...\u0026#34;) print(f\u0026#34;æ¶æ„: {is_malicious}\u0026#34;) print(f\u0026#34;å¨èƒ: {threats}\u0026#34;) print(\u0026#34;-\u0026#34; * 50) # æµ‹è¯•å®‰å…¨æŸ¥è¯¢ safe_query = \u0026#34;SELECT * FROM users WHERE id = %s AND status = %s\u0026#34; safe_params = [1, \u0026#39;active\u0026#39;] is_malicious, threats = protector.detect_sql_injection(safe_query, safe_params) print(f\u0026#34;å®‰å…¨æŸ¥è¯¢æ¶æ„æ£€æµ‹: {is_malicious}\u0026#34;) # åˆ›å»ºå®‰å…¨æŸ¥è¯¢ try: clean_query, clean_params = protector.create_safe_query( \u0026#34;SELECT * FROM users WHERE name = %s\u0026#34;, {\u0026#34;name\u0026#34;: \u0026#34;John\u0026#39;; DROP TABLE users; --\u0026#34;} ) print(f\u0026#34;æ¸…ç†åçš„æŸ¥è¯¢: {clean_query}\u0026#34;) print(f\u0026#34;æ¸…ç†åçš„å‚æ•°: {clean_params}\u0026#34;) except ValueError as e: print(f\u0026#34;æŸ¥è¯¢è¢«æ‹’ç»: {e}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() æ€»ç»“ æ•°æ®åº“å®‰å…¨æ˜¯ä¸€ä¸ªå¤šå±‚æ¬¡ã€å…¨æ–¹ä½çš„é˜²æŠ¤ä½“ç³»ï¼Œéœ€è¦ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œç»¼åˆè€ƒè™‘ï¼š\n","content":"æ•°æ®åº“å®‰å…¨ä¸æƒé™ç®¡ç†ï¼šä»åŸºç¡€é˜²æŠ¤åˆ°ä¼ä¸šçº§å®‰å…¨ç­–ç•¥çš„å®Œæ•´æŒ‡å— å¼•è¨€ æ•°æ®åº“å®‰å…¨æ˜¯ç°ä»£ä¼ä¸šä¿¡æ¯å®‰å…¨ä½“ç³»çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚éšç€æ•°æ®æ³„éœ²äº‹ä»¶é¢‘å‘å’Œç›‘ç®¡è¦æ±‚æ—¥è¶‹ä¸¥æ ¼ï¼Œå»ºç«‹å®Œå–„çš„æ•°æ®åº“å®‰å…¨é˜²æŠ¤ä½“ç³»å˜å¾—è‡³å…³é‡è¦ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨æ•°æ®åº“å®‰å…¨çš„å„ä¸ªå±‚é¢ï¼Œä»åŸºç¡€çš„è®¿é—®æ§åˆ¶åˆ°é«˜çº§çš„å®‰å…¨ç­–ç•¥å®æ–½ï¼Œä¸ºä¼ä¸šæä¾›å…¨é¢çš„æ•°æ®åº“å®‰å…¨è§£å†³æ–¹æ¡ˆã€‚\næ•°æ®åº“å®‰å…¨æ¶æ„æ¦‚è¿° å®‰å…¨é˜²æŠ¤å±‚æ¬¡æ¨¡å‹ graph TB subgraph \u0026amp;#34;æ•°æ®åº“å®‰å…¨æ¶æ„\u0026amp;#34; A[ç½‘ç»œå®‰å…¨å±‚] --\u0026amp;gt; B[ä¸»æœºå®‰å…¨å±‚] B --\u0026amp;gt; C[æ•°æ®åº“å®‰å…¨å±‚] C --\u0026amp;gt; D[åº”ç”¨å®‰å…¨å±‚] D --\u0026amp;gt; E[æ•°æ®å®‰å…¨å±‚] subgraph \u0026amp;#34;ç½‘ç»œå®‰å…¨\u0026amp;#34; F[é˜²ç«å¢™] G[VPN] H[ç½‘ç»œéš”ç¦»] end subgraph \u0026amp;#34;ä¸»æœºå®‰å…¨\u0026amp;#34; I[æ“ä½œç³»ç»ŸåŠ å›º] J[è®¿é—®æ§åˆ¶] K[æ—¥å¿—å®¡è®¡] end subgraph \u0026amp;#34;æ•°æ®åº“å®‰å…¨\u0026amp;#34; L[èº«ä»½è®¤è¯] M[æƒé™ç®¡ç†] N[æ•°æ®åŠ å¯†] O[å®¡è®¡ç›‘æ§] end subgraph \u0026amp;#34;åº”ç”¨å®‰å…¨\u0026amp;#34; P[SQLæ³¨å…¥é˜²æŠ¤] Q[è¿æ¥æ± å®‰å…¨] â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["æ•°æ®åº“å®‰å…¨","æƒé™ç®¡ç†","æ•°æ®åŠ å¯†","å®¡è®¡æ—¥å¿—","å®‰å…¨ç­–ç•¥"],"categories":["æ•°æ®åº“"],"author":"ææ˜","readingTime":27,"wordCount":5623,"section":"posts","type":"posts","draft":false,"featured":false,"series":null},{"title":"æ•°æ®åº“å¤‡ä»½ä¸æ¢å¤ç­–ç•¥ï¼šä»åŸºç¡€å¤‡ä»½åˆ°ç¾éš¾æ¢å¤çš„å®Œæ•´æ–¹æ¡ˆ","url":"https://www.dishuihengxin.com/posts/database-backup-recovery/","summary":"æ•°æ®åº“å¤‡ä»½ä¸æ¢å¤ç­–ç•¥ï¼šä»åŸºç¡€å¤‡ä»½åˆ°ç¾éš¾æ¢å¤çš„å®Œæ•´æ–¹æ¡ˆ æ•°æ®åº“å¤‡ä»½ä¸æ¢å¤æ˜¯æ•°æ®åº“ç®¡ç†ä¸­æœ€å…³é”®çš„ç¯èŠ‚ä¹‹ä¸€ï¼Œç›´æ¥å…³ç³»åˆ°æ•°æ®çš„å®‰å…¨æ€§å’Œä¸šåŠ¡çš„è¿ç»­æ€§ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨ç°ä»£æ•°æ®åº“å¤‡ä»½ä¸æ¢å¤çš„æœ€ä½³å®è·µï¼Œä»åŸºç¡€çš„å¤‡ä»½ç­–ç•¥åˆ°å¤æ‚çš„ç¾éš¾æ¢å¤æ–¹æ¡ˆã€‚\nå¤‡ä»½ç­–ç•¥è®¾è®¡ä¸è§„åˆ’ 1. å¤‡ä»½ç­–ç•¥æ¶æ„ graph TB A[å¤‡ä»½ç­–ç•¥è§„åˆ’] --\u0026gt; B[å¤‡ä»½ç±»å‹é€‰æ‹©] A --\u0026gt; C[å¤‡ä»½é¢‘ç‡è®¾è®¡] A --\u0026gt; D[å­˜å‚¨ç­–ç•¥è§„åˆ’] B --\u0026gt; E[å…¨é‡å¤‡ä»½] B --\u0026gt; F[å¢é‡å¤‡ä»½] B --\u0026gt; G[å·®å¼‚å¤‡ä»½] B --\u0026gt; H[æ—¥å¿—å¤‡ä»½] C --\u0026gt; I[å®æ—¶å¤‡ä»½] C --\u0026gt; J[å®šæ—¶å¤‡ä»½] C --\u0026gt; K[è§¦å‘å¤‡ä»½] D --\u0026gt; L[æœ¬åœ°å­˜å‚¨] D --\u0026gt; M[è¿œç¨‹å­˜å‚¨] D --\u0026gt; N[äº‘å­˜å‚¨] D --\u0026gt; O[å¤šåœ°å¤‡ä»½] E --\u0026gt; P[å®Œæ•´æ•°æ®å‰¯æœ¬] F --\u0026gt; Q[å˜æ›´æ•°æ®] G --\u0026gt; R[å·®å¼‚æ•°æ®] H --\u0026gt; S[äº‹åŠ¡æ—¥å¿—] P --\u0026gt; T[æ¢å¤åŸºå‡†ç‚¹] Q --\u0026gt; T R --\u0026gt; T S --\u0026gt; U[æ—¶é—´ç‚¹æ¢å¤] 2. å¤‡ä»½ç­–ç•¥é…ç½® # config/backup_strategy.yaml backup_strategy: # å…¨å±€é…ç½® global: retention_policy: daily_backups: 7 # ä¿ç•™7å¤©çš„æ—¥å¤‡ä»½ weekly_backups: 4 # ä¿ç•™4å‘¨çš„å‘¨å¤‡ä»½ monthly_backups: 12 # ä¿ç•™12ä¸ªæœˆçš„æœˆå¤‡ä»½ yearly_backups: 3 # ä¿ç•™3å¹´çš„å¹´å¤‡ä»½ compression: enabled: true algorithm: \u0026#34;gzip\u0026#34; # gzip, lz4, zstd level: 6 encryption: enabled: true algorithm: \u0026#34;AES-256\u0026#34; key_rotation_days: 90 verification: enabled: true checksum_algorithm: \u0026#34;SHA-256\u0026#34; test_restore: true test_frequency: \u0026#34;weekly\u0026#34; # MySQLå¤‡ä»½ç­–ç•¥ mysql: databases: - name: \u0026#34;production_db\u0026#34; backup_type: \u0026#34;full\u0026#34; schedule: \u0026#34;0 2 * * *\u0026#34; # æ¯å¤©å‡Œæ™¨2ç‚¹ storage_location: \u0026#34;/backup/mysql/full\u0026#34; - name: \u0026#34;production_db\u0026#34; backup_type: \u0026#34;incremental\u0026#34; schedule: \u0026#34;0 */6 * * *\u0026#34; # æ¯6å°æ—¶ storage_location: \u0026#34;/backup/mysql/incremental\u0026#34; - name: \u0026#34;production_db\u0026#34; backup_type: \u0026#34;binlog\u0026#34; schedule: \u0026#34;*/10 * * * *\u0026#34; # æ¯10åˆ†é’Ÿ storage_location: \u0026#34;/backup/mysql/binlog\u0026#34; options: single_transaction: true routines: true triggers: true events: true lock_tables: false master_data: 2 # PostgreSQLå¤‡ä»½ç­–ç•¥ postgresql: databases: - name: \u0026#34;production_db\u0026#34; backup_type: \u0026#34;full\u0026#34; schedule: \u0026#34;0 3 * * *\u0026#34; storage_location: \u0026#34;/backup/postgresql/full\u0026#34; - name: \u0026#34;production_db\u0026#34; backup_type: \u0026#34;wal\u0026#34; continuous: true storage_location: \u0026#34;/backup/postgresql/wal\u0026#34; options: format: \u0026#34;custom\u0026#34; compression_level: 6 verbose: true no_owner: true no_privileges: false # MongoDBå¤‡ä»½ç­–ç•¥ mongodb: databases: - name: \u0026#34;production_db\u0026#34; backup_type: \u0026#34;full\u0026#34; schedule: \u0026#34;0 4 * * *\u0026#34; storage_location: \u0026#34;/backup/mongodb/full\u0026#34; - name: \u0026#34;production_db\u0026#34; backup_type: \u0026#34;oplog\u0026#34; continuous: true storage_location: \u0026#34;/backup/mongodb/oplog\u0026#34; options: gzip: true repair: false oplog: true journal: true # å­˜å‚¨é…ç½® storage: local: path: \u0026#34;/backup\u0026#34; max_size: \u0026#34;1TB\u0026#34; cleanup_policy: \u0026#34;retention\u0026#34; remote: type: \u0026#34;nfs\u0026#34; server: \u0026#34;backup-server.company.com\u0026#34; path: \u0026#34;/shared/backups\u0026#34; mount_options: \u0026#34;rw,sync,hard,intr\u0026#34; cloud: provider: \u0026#34;aws_s3\u0026#34; bucket: \u0026#34;company-database-backups\u0026#34; region: \u0026#34;us-west-2\u0026#34; storage_class: \u0026#34;STANDARD_IA\u0026#34; lifecycle_policy: transition_to_glacier: 30 delete_after: 2555 # 7 years # ç›‘æ§å’Œå‘Šè­¦ monitoring: alerts: backup_failure: enabled: true notification_channels: [\u0026#34;email\u0026#34;, \u0026#34;slack\u0026#34;] escalation_time: 30 # minutes backup_size_anomaly: enabled: true threshold_percentage: 50 notification_channels: [\u0026#34;email\u0026#34;] storage_space_low: enabled: true threshold_percentage: 85 notification_channels: [\u0026#34;email\u0026#34;, \u0026#34;slack\u0026#34;] metrics: backup_duration: true backup_size: true success_rate: true storage_utilization: true MySQLå¤‡ä»½ä¸æ¢å¤å®ç° 1. è‡ªåŠ¨åŒ–å¤‡ä»½ç³»ç»Ÿ #!/bin/bash # scripts/mysql_backup_system.sh set -euo pipefail # é…ç½®æ–‡ä»¶è·¯å¾„ CONFIG_FILE=\u0026#34;/etc/mysql-backup/config.conf\u0026#34; LOG_FILE=\u0026#34;/var/log/mysql-backup.log\u0026#34; # é»˜è®¤é…ç½® MYSQL_HOST=\u0026#34;${MYSQL_HOST:-localhost}\u0026#34; MYSQL_PORT=\u0026#34;${MYSQL_PORT:-3306}\u0026#34; MYSQL_USER=\u0026#34;${MYSQL_USER:-backup_user}\u0026#34; MYSQL_PASSWORD=\u0026#34;${MYSQL_PASSWORD:-}\u0026#34; BACKUP_DIR=\u0026#34;${BACKUP_DIR:-/backup/mysql}\u0026#34; RETENTION_DAYS=\u0026#34;${RETENTION_DAYS:-7}\u0026#34; COMPRESSION=\u0026#34;${COMPRESSION:-true}\u0026#34; ENCRYPTION=\u0026#34;${ENCRYPTION:-true}\u0026#34; ENCRYPTION_KEY=\u0026#34;${ENCRYPTION_KEY:-}\u0026#34; # æ—¥å¿—å‡½æ•° log() { echo \u0026#34;[$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)] $1\u0026#34; | tee -a \u0026#34;$LOG_FILE\u0026#34; } error() { echo \u0026#34;[$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)] ERROR: $1\u0026#34; | tee -a \u0026#34;$LOG_FILE\u0026#34; \u0026gt;\u0026amp;2 } # åŠ è½½é…ç½®æ–‡ä»¶ load_config() { if [[ -f \u0026#34;$CONFIG_FILE\u0026#34; ]]; then source \u0026#34;$CONFIG_FILE\u0026#34; log \u0026#34;é…ç½®æ–‡ä»¶åŠ è½½å®Œæˆ: $CONFIG_FILE\u0026#34; else log \u0026#34;ä½¿ç”¨é»˜è®¤é…ç½®\u0026#34; fi } # æ£€æŸ¥ä¾èµ– check_dependencies() { local deps=(\u0026#34;mysqldump\u0026#34; \u0026#34;mysql\u0026#34; \u0026#34;gzip\u0026#34; \u0026#34;openssl\u0026#34;) for dep in \u0026#34;${deps[@]}\u0026#34;; do if ! command -v \u0026#34;$dep\u0026#34; \u0026amp;\u0026gt; /dev/null; then error \u0026#34;ä¾èµ–é¡¹æœªæ‰¾åˆ°: $dep\u0026#34; exit 1 fi done log \u0026#34;ä¾èµ–æ£€æŸ¥å®Œæˆ\u0026#34; } # åˆ›å»ºå¤‡ä»½ç›®å½• create_backup_directories() { local dirs=( \u0026#34;$BACKUP_DIR/full\u0026#34; \u0026#34;$BACKUP_DIR/incremental\u0026#34; \u0026#34;$BACKUP_DIR/binlog\u0026#34; \u0026#34;$BACKUP_DIR/temp\u0026#34; ) for dir in \u0026#34;${dirs[@]}\u0026#34;; do mkdir -p \u0026#34;$dir\u0026#34; done log \u0026#34;å¤‡ä»½ç›®å½•åˆ›å»ºå®Œæˆ\u0026#34; } # è·å–æ•°æ®åº“åˆ—è¡¨ get_databases() { mysql -h\u0026#34;$MYSQL_HOST\u0026#34; -P\u0026#34;$MYSQL_PORT\u0026#34; -u\u0026#34;$MYSQL_USER\u0026#34; -p\u0026#34;$MYSQL_PASSWORD\u0026#34; \\ -e \u0026#34;SHOW DATABASES;\u0026#34; | grep -Ev \u0026#39;^(Database|information_schema|performance_schema|mysql|sys)$\u0026#39; } # å…¨é‡å¤‡ä»½ full_backup() { local database=\u0026#34;$1\u0026#34; local timestamp=$(date \u0026#39;+%Y%m%d_%H%M%S\u0026#39;) local backup_file=\u0026#34;$BACKUP_DIR/full/${database}_full_${timestamp}.sql\u0026#34; log \u0026#34;å¼€å§‹å…¨é‡å¤‡ä»½: $database\u0026#34; # æ‰§è¡Œå¤‡ä»½ mysqldump \\ -h\u0026#34;$MYSQL_HOST\u0026#34; \\ -P\u0026#34;$MYSQL_PORT\u0026#34; \\ -u\u0026#34;$MYSQL_USER\u0026#34; \\ -p\u0026#34;$MYSQL_PASSWORD\u0026#34; \\ --single-transaction \\ --routines \\ --triggers \\ --events \\ --master-data=2 \\ --flush-logs \\ --hex-blob \\ --default-character-set=utf8mb4 \\ \u0026#34;$database\u0026#34; \u0026gt; \u0026#34;$backup_file\u0026#34; if [[ $? -eq 0 ]]; then log \u0026#34;å…¨é‡å¤‡ä»½å®Œæˆ: $backup_file\u0026#34; # å‹ç¼©å¤‡ä»½æ–‡ä»¶ if [[ \u0026#34;$COMPRESSION\u0026#34; == \u0026#34;true\u0026#34; ]]; then compress_backup \u0026#34;$backup_file\u0026#34; fi # åŠ å¯†å¤‡ä»½æ–‡ä»¶ if [[ \u0026#34;$ENCRYPTION\u0026#34; == \u0026#34;true\u0026#34; ]]; then encrypt_backup \u0026#34;$backup_file\u0026#34; fi # éªŒè¯å¤‡ä»½ verify_backup \u0026#34;$backup_file\u0026#34; # ç”Ÿæˆå¤‡ä»½å…ƒæ•°æ® generate_backup_metadata \u0026#34;$backup_file\u0026#34; \u0026#34;full\u0026#34; \u0026#34;$database\u0026#34; else error \u0026#34;å…¨é‡å¤‡ä»½å¤±è´¥: $database\u0026#34; return 1 fi } # å¢é‡å¤‡ä»½ï¼ˆåŸºäºbinlogï¼‰ incremental_backup() { local database=\u0026#34;$1\u0026#34; local timestamp=$(date \u0026#39;+%Y%m%d_%H%M%S\u0026#39;) local backup_dir=\u0026#34;$BACKUP_DIR/incremental/${database}_${timestamp}\u0026#34; log \u0026#34;å¼€å§‹å¢é‡å¤‡ä»½: $database\u0026#34; mkdir -p \u0026#34;$backup_dir\u0026#34; # è·å–å½“å‰binlogä½ç½® local binlog_info=$(mysql -h\u0026#34;$MYSQL_HOST\u0026#34; -P\u0026#34;$MYSQL_PORT\u0026#34; -u\u0026#34;$MYSQL_USER\u0026#34; -p\u0026#34;$MYSQL_PASSWORD\u0026#34; \\ -e \u0026#34;SHOW MASTER STATUS\\G\u0026#34; | grep -E \u0026#34;(File|Position)\u0026#34;) echo \u0026#34;$binlog_info\u0026#34; \u0026gt; \u0026#34;$backup_dir/binlog_position.txt\u0026#34; # åˆ·æ–°binlog mysql -h\u0026#34;$MYSQL_HOST\u0026#34; -P\u0026#34;$MYSQL_PORT\u0026#34; -u\u0026#34;$MYSQL_USER\u0026#34; -p\u0026#34;$MYSQL_PASSWORD\u0026#34; \\ -e \u0026#34;FLUSH LOGS;\u0026#34; # å¤åˆ¶binlogæ–‡ä»¶ local binlog_dir=$(mysql -h\u0026#34;$MYSQL_HOST\u0026#34; -P\u0026#34;$MYSQL_PORT\u0026#34; -u\u0026#34;$MYSQL_USER\u0026#34; -p\u0026#34;$MYSQL_PASSWORD\u0026#34; \\ -e \u0026#34;SHOW VARIABLES LIKE \u0026#39;log_bin_basename\u0026#39;;\u0026#34; | awk \u0026#39;NR==2 {print $2}\u0026#39; | xargs dirname) # è·å–æœ€æ–°çš„binlogæ–‡ä»¶ local latest_binlog=$(mysql -h\u0026#34;$MYSQL_HOST\u0026#34; -P\u0026#34;$MYSQL_PORT\u0026#34; -u\u0026#34;$MYSQL_USER\u0026#34; -p\u0026#34;$MYSQL_PASSWORD\u0026#34; \\ -e \u0026#34;SHOW BINARY LOGS;\u0026#34; | tail -n 1 | awk \u0026#39;{print $1}\u0026#39;) if [[ -n \u0026#34;$latest_binlog\u0026#34; ]]; then cp \u0026#34;$binlog_dir/$latest_binlog\u0026#34; \u0026#34;$backup_dir/\u0026#34; log \u0026#34;å¢é‡å¤‡ä»½å®Œæˆ: $backup_dir\u0026#34; # ç”Ÿæˆå¤‡ä»½å…ƒæ•°æ® generate_backup_metadata \u0026#34;$backup_dir\u0026#34; \u0026#34;incremental\u0026#34; \u0026#34;$database\u0026#34; else error \u0026#34;å¢é‡å¤‡ä»½å¤±è´¥: æ— æ³•è·å–binlogæ–‡ä»¶\u0026#34; return 1 fi } # å‹ç¼©å¤‡ä»½æ–‡ä»¶ compress_backup() { local backup_file=\u0026#34;$1\u0026#34; log \u0026#34;å‹ç¼©å¤‡ä»½æ–‡ä»¶: $backup_file\u0026#34; gzip \u0026#34;$backup_file\u0026#34; if [[ $? -eq 0 ]]; then log \u0026#34;å‹ç¼©å®Œæˆ: ${backup_file}.gz\u0026#34; else error \u0026#34;å‹ç¼©å¤±è´¥: $backup_file\u0026#34; return 1 fi } # åŠ å¯†å¤‡ä»½æ–‡ä»¶ encrypt_backup() { local backup_file=\u0026#34;$1\u0026#34; if [[ -z \u0026#34;$ENCRYPTION_KEY\u0026#34; ]]; then error \u0026#34;åŠ å¯†å¯†é’¥æœªè®¾ç½®\u0026#34; return 1 fi log \u0026#34;åŠ å¯†å¤‡ä»½æ–‡ä»¶: $backup_file\u0026#34; # å¦‚æœæ–‡ä»¶å·²å‹ç¼©ï¼Œå¤„ç†.gzæ–‡ä»¶ if [[ \u0026#34;$backup_file\u0026#34; == *.gz ]]; then openssl enc -aes-256-cbc -salt -in \u0026#34;$backup_file\u0026#34; -out \u0026#34;${backup_file}.enc\u0026#34; -k \u0026#34;$ENCRYPTION_KEY\u0026#34; rm \u0026#34;$backup_file\u0026#34; else openssl enc -aes-256-cbc -salt -in \u0026#34;$backup_file\u0026#34; -out \u0026#34;${backup_file}.enc\u0026#34; -k \u0026#34;$ENCRYPTION_KEY\u0026#34; rm \u0026#34;$backup_file\u0026#34; fi if [[ $? -eq 0 ]]; then log \u0026#34;åŠ å¯†å®Œæˆ: ${backup_file}.enc\u0026#34; else error \u0026#34;åŠ å¯†å¤±è´¥: $backup_file\u0026#34; return 1 fi } # éªŒè¯å¤‡ä»½æ–‡ä»¶ verify_backup() { local backup_file=\u0026#34;$1\u0026#34; log \u0026#34;éªŒè¯å¤‡ä»½æ–‡ä»¶: $backup_file\u0026#34; # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸ºç©º if [[ ! -f \u0026#34;$backup_file\u0026#34; ]] || [[ ! -s \u0026#34;$backup_file\u0026#34; ]]; then error \u0026#34;å¤‡ä»½æ–‡ä»¶æ— æ•ˆ: $backup_file\u0026#34; return 1 fi # ç”Ÿæˆæ ¡éªŒå’Œ local checksum=$(sha256sum \u0026#34;$backup_file\u0026#34; | awk \u0026#39;{print $1}\u0026#39;) echo \u0026#34;$checksum\u0026#34; \u0026gt; \u0026#34;${backup_file}.sha256\u0026#34; log \u0026#34;å¤‡ä»½éªŒè¯å®Œæˆï¼Œæ ¡éªŒå’Œ: $checksum\u0026#34; } # ç”Ÿæˆå¤‡ä»½å…ƒæ•°æ® generate_backup_metadata() { local backup_file=\u0026#34;$1\u0026#34; local backup_type=\u0026#34;$2\u0026#34; local database=\u0026#34;$3\u0026#34; local timestamp=$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) local metadata_file=\u0026#34;${backup_file}.metadata\u0026#34; cat \u0026gt; \u0026#34;$metadata_file\u0026#34; \u0026lt;\u0026lt; EOF { \u0026#34;backup_type\u0026#34;: \u0026#34;$backup_type\u0026#34;, \u0026#34;database\u0026#34;: \u0026#34;$database\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;$timestamp\u0026#34;, \u0026#34;backup_file\u0026#34;: \u0026#34;$backup_file\u0026#34;, \u0026#34;file_size\u0026#34;: $(stat -c%s \u0026#34;$backup_file\u0026#34; 2\u0026gt;/dev/null || echo \u0026#34;0\u0026#34;), \u0026#34;mysql_version\u0026#34;: \u0026#34;$(mysql -h\u0026#34;$MYSQL_HOST\u0026#34; -P\u0026#34;$MYSQL_PORT\u0026#34; -u\u0026#34;$MYSQL_USER\u0026#34; -p\u0026#34;$MYSQL_PASSWORD\u0026#34; -e \u0026#34;SELECT VERSION();\u0026#34; | tail -n 1)\u0026#34;, \u0026#34;compression\u0026#34;: \u0026#34;$COMPRESSION\u0026#34;, \u0026#34;encryption\u0026#34;: \u0026#34;$ENCRYPTION\u0026#34;, \u0026#34;checksum\u0026#34;: \u0026#34;$(cat \u0026#34;${backup_file}.sha256\u0026#34; 2\u0026gt;/dev/null || echo \u0026#34;N/A\u0026#34;)\u0026#34; } EOF log \u0026#34;å¤‡ä»½å…ƒæ•°æ®ç”Ÿæˆå®Œæˆ: $metadata_file\u0026#34; } # æ¸…ç†è¿‡æœŸå¤‡ä»½ cleanup_old_backups() { log \u0026#34;å¼€å§‹æ¸…ç†è¿‡æœŸå¤‡ä»½\u0026#34; # æ¸…ç†å…¨é‡å¤‡ä»½ find \u0026#34;$BACKUP_DIR/full\u0026#34; -name \u0026#34;*.sql*\u0026#34; -mtime +$RETENTION_DAYS -delete find \u0026#34;$BACKUP_DIR/full\u0026#34; -name \u0026#34;*.metadata\u0026#34; -mtime +$RETENTION_DAYS -delete find \u0026#34;$BACKUP_DIR/full\u0026#34; -name \u0026#34;*.sha256\u0026#34; -mtime +$RETENTION_DAYS -delete # æ¸…ç†å¢é‡å¤‡ä»½ find \u0026#34;$BACKUP_DIR/incremental\u0026#34; -type d -mtime +$RETENTION_DAYS -exec rm -rf {} + # æ¸…ç†binlogå¤‡ä»½ find \u0026#34;$BACKUP_DIR/binlog\u0026#34; -name \u0026#34;*.log*\u0026#34; -mtime +$RETENTION_DAYS -delete log \u0026#34;è¿‡æœŸå¤‡ä»½æ¸…ç†å®Œæˆ\u0026#34; } # å¤‡ä»½çŠ¶æ€æŠ¥å‘Š generate_backup_report() { local report_file=\u0026#34;$BACKUP_DIR/backup_report_$(date \u0026#39;+%Y%m%d\u0026#39;).txt\u0026#34; cat \u0026gt; \u0026#34;$report_file\u0026#34; \u0026lt;\u0026lt; EOF MySQLå¤‡ä»½çŠ¶æ€æŠ¥å‘Š ç”Ÿæˆæ—¶é—´: $(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) å¤‡ä»½ç›®å½•ç»Ÿè®¡: $(du -sh \u0026#34;$BACKUP_DIR\u0026#34;/* 2\u0026gt;/dev/null || echo \u0026#34;æ— å¤‡ä»½æ•°æ®\u0026#34;) æœ€è¿‘çš„å¤‡ä»½æ–‡ä»¶: å…¨é‡å¤‡ä»½: $(find \u0026#34;$BACKUP_DIR/full\u0026#34; -name \u0026#34;*.sql*\u0026#34; -mtime -1 -exec ls -lh {} \\; 2\u0026gt;/dev/null | head -5) å¢é‡å¤‡ä»½: $(find \u0026#34;$BACKUP_DIR/incremental\u0026#34; -type d -mtime -1 -exec ls -lhd {} \\; 2\u0026gt;/dev/null | head -5) å­˜å‚¨ç©ºé—´ä½¿ç”¨: $(df -h \u0026#34;$BACKUP_DIR\u0026#34; | tail -n 1) å¤‡ä»½éªŒè¯çŠ¶æ€: $(find \u0026#34;$BACKUP_DIR\u0026#34; -name \u0026#34;*.sha256\u0026#34; -mtime -1 | wc -l) ä¸ªæ–‡ä»¶å·²éªŒè¯ EOF log \u0026#34;å¤‡ä»½æŠ¥å‘Šç”Ÿæˆå®Œæˆ: $report_file\u0026#34; } # å‘é€å‘Šè­¦é€šçŸ¥ send_alert() { local message=\u0026#34;$1\u0026#34; local severity=\u0026#34;${2:-INFO}\u0026#34; # å‘é€é‚®ä»¶å‘Šè­¦ï¼ˆå¦‚æœé…ç½®äº†é‚®ä»¶ï¼‰ if [[ -n \u0026#34;${ALERT_EMAIL:-}\u0026#34; ]]; then echo \u0026#34;$message\u0026#34; | mail -s \u0026#34;MySQLå¤‡ä»½å‘Šè­¦ - $severity\u0026#34; \u0026#34;$ALERT_EMAIL\u0026#34; fi # å‘é€Slacké€šçŸ¥ï¼ˆå¦‚æœé…ç½®äº†Webhookï¼‰ if [[ -n \u0026#34;${SLACK_WEBHOOK:-}\u0026#34; ]]; then curl -X POST -H \u0026#39;Content-type: application/json\u0026#39; \\ --data \u0026#34;{\\\u0026#34;text\\\u0026#34;:\\\u0026#34;MySQLå¤‡ä»½å‘Šè­¦ - $severity: $message\\\u0026#34;}\u0026#34; \\ \u0026#34;$SLACK_WEBHOOK\u0026#34; fi log \u0026#34;å‘Šè­¦é€šçŸ¥å·²å‘é€: $message\u0026#34; } # ä¸»å¤‡ä»½å‡½æ•° main_backup() { local backup_type=\u0026#34;${1:-full}\u0026#34; local database=\u0026#34;${2:-all}\u0026#34; log \u0026#34;å¼€å§‹MySQLå¤‡ä»½ä»»åŠ¡ - ç±»å‹: $backup_type, æ•°æ®åº“: $database\u0026#34; # æ£€æŸ¥MySQLè¿æ¥ if ! mysql -h\u0026#34;$MYSQL_HOST\u0026#34; -P\u0026#34;$MYSQL_PORT\u0026#34; -u\u0026#34;$MYSQL_USER\u0026#34; -p\u0026#34;$MYSQL_PASSWORD\u0026#34; -e \u0026#34;SELECT 1;\u0026#34; \u0026amp;\u0026gt;/dev/null; then error \u0026#34;æ— æ³•è¿æ¥åˆ°MySQLæœåŠ¡å™¨\u0026#34; send_alert \u0026#34;MySQLå¤‡ä»½å¤±è´¥: æ— æ³•è¿æ¥åˆ°æ•°æ®åº“æœåŠ¡å™¨\u0026#34; \u0026#34;CRITICAL\u0026#34; exit 1 fi # è·å–æ•°æ®åº“åˆ—è¡¨ local databases if [[ \u0026#34;$database\u0026#34; == \u0026#34;all\u0026#34; ]]; then databases=$(get_databases) else databases=\u0026#34;$database\u0026#34; fi # æ‰§è¡Œå¤‡ä»½ local backup_count=0 local failed_count=0 for db in $databases; do case \u0026#34;$backup_type\u0026#34; in \u0026#34;full\u0026#34;) if full_backup \u0026#34;$db\u0026#34;; then ((backup_count++)) else ((failed_count++)) fi ;; \u0026#34;incremental\u0026#34;) if incremental_backup \u0026#34;$db\u0026#34;; then ((backup_count++)) else ((failed_count++)) fi ;; *) error \u0026#34;ä¸æ”¯æŒçš„å¤‡ä»½ç±»å‹: $backup_type\u0026#34; exit 1 ;; esac done # æ¸…ç†è¿‡æœŸå¤‡ä»½ cleanup_old_backups # ç”Ÿæˆå¤‡ä»½æŠ¥å‘Š generate_backup_report # å‘é€å®Œæˆé€šçŸ¥ if [[ $failed_count -eq 0 ]]; then log \u0026#34;å¤‡ä»½ä»»åŠ¡å®Œæˆ - æˆåŠŸ: $backup_count, å¤±è´¥: $failed_count\u0026#34; send_alert \u0026#34;MySQLå¤‡ä»½ä»»åŠ¡å®Œæˆ - æˆåŠŸå¤‡ä»½ $backup_count ä¸ªæ•°æ®åº“\u0026#34; \u0026#34;INFO\u0026#34; else error \u0026#34;å¤‡ä»½ä»»åŠ¡å®Œæˆä½†æœ‰å¤±è´¥ - æˆåŠŸ: $backup_count, å¤±è´¥: $failed_count\u0026#34; send_alert \u0026#34;MySQLå¤‡ä»½ä»»åŠ¡éƒ¨åˆ†å¤±è´¥ - æˆåŠŸ: $backup_count, å¤±è´¥: $failed_count\u0026#34; \u0026#34;WARNING\u0026#34; fi } # æ¢å¤å‡½æ•° restore_database() { local backup_file=\u0026#34;$1\u0026#34; local target_database=\u0026#34;$2\u0026#34; local target_host=\u0026#34;${3:-$MYSQL_HOST}\u0026#34; local target_port=\u0026#34;${4:-$MYSQL_PORT}\u0026#34; log \u0026#34;å¼€å§‹æ¢å¤æ•°æ®åº“: $target_database\u0026#34; # æ£€æŸ¥å¤‡ä»½æ–‡ä»¶ if [[ ! -f \u0026#34;$backup_file\u0026#34; ]]; then error \u0026#34;å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: $backup_file\u0026#34; return 1 fi # è§£å¯†å¤‡ä»½æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰ local restore_file=\u0026#34;$backup_file\u0026#34; if [[ \u0026#34;$backup_file\u0026#34; == *.enc ]]; then restore_file=\u0026#34;${backup_file%.enc}\u0026#34; log \u0026#34;è§£å¯†å¤‡ä»½æ–‡ä»¶: $backup_file\u0026#34; openssl enc -aes-256-cbc -d -in \u0026#34;$backup_file\u0026#34; -out \u0026#34;$restore_file\u0026#34; -k \u0026#34;$ENCRYPTION_KEY\u0026#34; fi # è§£å‹å¤‡ä»½æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰ if [[ \u0026#34;$restore_file\u0026#34; == *.gz ]]; then log \u0026#34;è§£å‹å¤‡ä»½æ–‡ä»¶: $restore_file\u0026#34; gunzip \u0026#34;$restore_file\u0026#34; restore_file=\u0026#34;${restore_file%.gz}\u0026#34; fi # åˆ›å»ºç›®æ ‡æ•°æ®åº“ mysql -h\u0026#34;$target_host\u0026#34; -P\u0026#34;$target_port\u0026#34; -u\u0026#34;$MYSQL_USER\u0026#34; -p\u0026#34;$MYSQL_PASSWORD\u0026#34; \\ -e \u0026#34;CREATE DATABASE IF NOT EXISTS \\`$target_database\\`;\u0026#34; # æ¢å¤æ•°æ® log \u0026#34;æ¢å¤æ•°æ®åˆ°æ•°æ®åº“: $target_database\u0026#34; mysql -h\u0026#34;$target_host\u0026#34; -P\u0026#34;$target_port\u0026#34; -u\u0026#34;$MYSQL_USER\u0026#34; -p\u0026#34;$MYSQL_PASSWORD\u0026#34; \\ \u0026#34;$target_database\u0026#34; \u0026lt; \u0026#34;$restore_file\u0026#34; if [[ $? -eq 0 ]]; then log \u0026#34;æ•°æ®åº“æ¢å¤å®Œæˆ: $target_database\u0026#34; # éªŒè¯æ¢å¤ç»“æœ local table_count=$(mysql -h\u0026#34;$target_host\u0026#34; -P\u0026#34;$target_port\u0026#34; -u\u0026#34;$MYSQL_USER\u0026#34; -p\u0026#34;$MYSQL_PASSWORD\u0026#34; \\ -e \u0026#34;SELECT COUNT(*) FROM information_schema.tables WHERE table_schema=\u0026#39;$target_database\u0026#39;;\u0026#34; | tail -n 1) log \u0026#34;æ¢å¤éªŒè¯ - è¡¨æ•°é‡: $table_count\u0026#34; else error \u0026#34;æ•°æ®åº“æ¢å¤å¤±è´¥: $target_database\u0026#34; return 1 fi # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ if [[ \u0026#34;$restore_file\u0026#34; != \u0026#34;$backup_file\u0026#34; ]]; then rm -f \u0026#34;$restore_file\u0026#34; fi } # æ—¶é—´ç‚¹æ¢å¤ point_in_time_recovery() { local database=\u0026#34;$1\u0026#34; local target_time=\u0026#34;$2\u0026#34; local backup_file=\u0026#34;$3\u0026#34; log \u0026#34;å¼€å§‹æ—¶é—´ç‚¹æ¢å¤: $database åˆ° $target_time\u0026#34; # æ¢å¤å…¨é‡å¤‡ä»½ restore_database \u0026#34;$backup_file\u0026#34; \u0026#34;${database}_pitr_temp\u0026#34; # åº”ç”¨binlogåˆ°æŒ‡å®šæ—¶é—´ç‚¹ local binlog_dir=\u0026#34;$BACKUP_DIR/binlog\u0026#34; local binlog_files=$(find \u0026#34;$binlog_dir\u0026#34; -name \u0026#34;*.log\u0026#34; -newer \u0026#34;$backup_file\u0026#34; | sort) for binlog_file in $binlog_files; do log \u0026#34;åº”ç”¨binlog: $binlog_file\u0026#34; mysqlbinlog --stop-datetime=\u0026#34;$target_time\u0026#34; \u0026#34;$binlog_file\u0026#34; | \\ mysql -h\u0026#34;$MYSQL_HOST\u0026#34; -P\u0026#34;$MYSQL_PORT\u0026#34; -u\u0026#34;$MYSQL_USER\u0026#34; -p\u0026#34;$MYSQL_PASSWORD\u0026#34; \\ \u0026#34;${database}_pitr_temp\u0026#34; done log \u0026#34;æ—¶é—´ç‚¹æ¢å¤å®Œæˆ: ${database}_pitr_temp\u0026#34; } # ä¸»å‡½æ•° main() { case \u0026#34;${1:-backup}\u0026#34; in \u0026#34;backup\u0026#34;) load_config check_dependencies create_backup_directories main_backup \u0026#34;${2:-full}\u0026#34; \u0026#34;${3:-all}\u0026#34; ;; \u0026#34;restore\u0026#34;) if [[ $# -lt 3 ]]; then echo \u0026#34;ç”¨æ³•: $0 restore \u0026lt;backup_file\u0026gt; \u0026lt;target_database\u0026gt; [target_host] [target_port]\u0026#34; exit 1 fi load_config restore_database \u0026#34;$2\u0026#34; \u0026#34;$3\u0026#34; \u0026#34;$4\u0026#34; \u0026#34;$5\u0026#34; ;; \u0026#34;pitr\u0026#34;) if [[ $# -lt 4 ]]; then echo \u0026#34;ç”¨æ³•: $0 pitr \u0026lt;database\u0026gt; \u0026lt;target_time\u0026gt; \u0026lt;backup_file\u0026gt;\u0026#34; exit 1 fi load_config point_in_time_recovery \u0026#34;$2\u0026#34; \u0026#34;$3\u0026#34; \u0026#34;$4\u0026#34; ;; \u0026#34;cleanup\u0026#34;) load_config cleanup_old_backups ;; \u0026#34;report\u0026#34;) load_config generate_backup_report ;; *) echo \u0026#34;ç”¨æ³•: $0 {backup|restore|pitr|cleanup|report}\u0026#34; echo \u0026#34; backup [full|incremental] [database|all] - æ‰§è¡Œå¤‡ä»½\u0026#34; echo \u0026#34; restore \u0026lt;backup_file\u0026gt; \u0026lt;target_database\u0026gt; - æ¢å¤æ•°æ®åº“\u0026#34; echo \u0026#34; pitr \u0026lt;database\u0026gt; \u0026lt;time\u0026gt; \u0026lt;backup_file\u0026gt; - æ—¶é—´ç‚¹æ¢å¤\u0026#34; echo \u0026#34; cleanup - æ¸…ç†è¿‡æœŸå¤‡ä»½\u0026#34; echo \u0026#34; report - ç”Ÿæˆå¤‡ä»½æŠ¥å‘Š\u0026#34; exit 1 ;; esac } main \u0026#34;$@\u0026#34; 2. MySQLæ¢å¤éªŒè¯è„šæœ¬ #!/usr/bin/env python3 # scripts/mysql_recovery_validator.py import mysql.connector import argparse import json import logging import hashlib import time from datetime import datetime from typing import Dict, List, Tuple, Any class MySQLRecoveryValidator: def __init__(self, config: Dict[str, Any]): self.config = config self.logger = self._setup_logging() # æ•°æ®åº“è¿æ¥é…ç½® self.source_conn = None self.target_conn = None def _setup_logging(self) -\u0026gt; logging.Logger: \u0026#34;\u0026#34;\u0026#34;è®¾ç½®æ—¥å¿—è®°å½•\u0026#34;\u0026#34;\u0026#34; logger = logging.getLogger(\u0026#39;MySQLRecoveryValidator\u0026#39;) logger.setLevel(logging.INFO) handler = logging.FileHandler(\u0026#39;/var/log/mysql_recovery_validation.log\u0026#39;) formatter = logging.Formatter( \u0026#39;%(asctime)s - %(name)s - %(levelname)s - %(message)s\u0026#39; ) handler.setFormatter(formatter) logger.addHandler(handler) # æ§åˆ¶å°è¾“å‡º console_handler = logging.StreamHandler() console_handler.setFormatter(formatter) logger.addHandler(console_handler) return logger def connect_databases(self): \u0026#34;\u0026#34;\u0026#34;è¿æ¥æºæ•°æ®åº“å’Œç›®æ ‡æ•°æ®åº“\u0026#34;\u0026#34;\u0026#34; try: # è¿æ¥æºæ•°æ®åº“ self.source_conn = mysql.connector.connect( **self.config[\u0026#39;source_database\u0026#39;] ) # è¿æ¥ç›®æ ‡æ•°æ®åº“ self.target_conn = mysql.connector.connect( **self.config[\u0026#39;target_database\u0026#39;] ) self.logger.info(\u0026#34;æ•°æ®åº“è¿æ¥å»ºç«‹æˆåŠŸ\u0026#34;) except Exception as e: self.logger.error(f\u0026#34;æ•°æ®åº“è¿æ¥å¤±è´¥: {e}\u0026#34;) raise def validate_schema_consistency(self, database_name: str) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;éªŒè¯æ¨¡å¼ä¸€è‡´æ€§\u0026#34;\u0026#34;\u0026#34; self.logger.info(f\u0026#34;å¼€å§‹éªŒè¯æ¨¡å¼ä¸€è‡´æ€§: {database_name}\u0026#34;) validation_result = { \u0026#39;database\u0026#39;: database_name, \u0026#39;schema_consistent\u0026#39;: True, \u0026#39;differences\u0026#39;: [], \u0026#39;table_count_match\u0026#39;: True, \u0026#39;index_count_match\u0026#39;: True, \u0026#39;constraint_count_match\u0026#39;: True } try: source_cursor = self.source_conn.cursor(dictionary=True) target_cursor = self.target_conn.cursor(dictionary=True) # éªŒè¯è¡¨ç»“æ„ table_diff = self._compare_tables(source_cursor, target_cursor, database_name) if table_diff: validation_result[\u0026#39;schema_consistent\u0026#39;] = False validation_result[\u0026#39;differences\u0026#39;].extend(table_diff) # éªŒè¯ç´¢å¼• index_diff = self._compare_indexes(source_cursor, target_cursor, database_name) if index_diff: validation_result[\u0026#39;schema_consistent\u0026#39;] = False validation_result[\u0026#39;differences\u0026#39;].extend(index_diff) # éªŒè¯çº¦æŸ constraint_diff = self._compare_constraints(source_cursor, target_cursor, database_name) if constraint_diff: validation_result[\u0026#39;schema_consistent\u0026#39;] = False validation_result[\u0026#39;differences\u0026#39;].extend(constraint_diff) source_cursor.close() target_cursor.close() except Exception as e: self.logger.error(f\u0026#34;æ¨¡å¼éªŒè¯å¤±è´¥: {e}\u0026#34;) validation_result[\u0026#39;schema_consistent\u0026#39;] = False validation_result[\u0026#39;error\u0026#39;] = str(e) return validation_result def _compare_tables(self, source_cursor, target_cursor, database_name: str) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;æ¯”è¾ƒè¡¨ç»“æ„\u0026#34;\u0026#34;\u0026#34; differences = [] # è·å–æºæ•°æ®åº“è¡¨ä¿¡æ¯ source_cursor.execute(f\u0026#34;\u0026#34;\u0026#34; SELECT table_name, table_type, engine, table_rows, data_length, index_length FROM information_schema.tables WHERE table_schema = \u0026#39;{database_name}\u0026#39; ORDER BY table_name \u0026#34;\u0026#34;\u0026#34;) source_tables = {row[\u0026#39;table_name\u0026#39;]: row for row in source_cursor.fetchall()} # è·å–ç›®æ ‡æ•°æ®åº“è¡¨ä¿¡æ¯ target_cursor.execute(f\u0026#34;\u0026#34;\u0026#34; SELECT table_name, table_type, engine, table_rows, data_length, index_length FROM information_schema.tables WHERE table_schema = \u0026#39;{database_name}\u0026#39; ORDER BY table_name \u0026#34;\u0026#34;\u0026#34;) target_tables = {row[\u0026#39;table_name\u0026#39;]: row for row in target_cursor.fetchall()} # æ¯”è¾ƒè¡¨ source_table_names = set(source_tables.keys()) target_table_names = set(target_tables.keys()) # æ£€æŸ¥ç¼ºå¤±çš„è¡¨ missing_tables = source_table_names - target_table_names if missing_tables: differences.append(f\u0026#34;ç›®æ ‡æ•°æ®åº“ç¼ºå¤±è¡¨: {\u0026#39;, \u0026#39;.join(missing_tables)}\u0026#34;) # æ£€æŸ¥å¤šä½™çš„è¡¨ extra_tables = target_table_names - source_table_names if extra_tables: differences.append(f\u0026#34;ç›®æ ‡æ•°æ®åº“å¤šä½™è¡¨: {\u0026#39;, \u0026#39;.join(extra_tables)}\u0026#34;) # æ¯”è¾ƒå…±åŒè¡¨çš„ç»“æ„ common_tables = source_table_names \u0026amp; target_table_names for table_name in common_tables: source_table = source_tables[table_name] target_table = target_tables[table_name] if source_table[\u0026#39;engine\u0026#39;] != target_table[\u0026#39;engine\u0026#39;]: differences.append(f\u0026#34;è¡¨ {table_name} å­˜å‚¨å¼•æ“ä¸åŒ¹é…: {source_table[\u0026#39;engine\u0026#39;]} vs {target_table[\u0026#39;engine\u0026#39;]}\u0026#34;) # æ¯”è¾ƒåˆ—ç»“æ„ column_diff = self._compare_columns(source_cursor, target_cursor, database_name, table_name) differences.extend(column_diff) return differences def _compare_columns(self, source_cursor, target_cursor, database_name: str, table_name: str) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;æ¯”è¾ƒåˆ—ç»“æ„\u0026#34;\u0026#34;\u0026#34; differences = [] # è·å–æºè¡¨åˆ—ä¿¡æ¯ source_cursor.execute(f\u0026#34;\u0026#34;\u0026#34; SELECT column_name, data_type, is_nullable, column_default, extra FROM information_schema.columns WHERE table_schema = \u0026#39;{database_name}\u0026#39; AND table_name = \u0026#39;{table_name}\u0026#39; ORDER BY ordinal_position \u0026#34;\u0026#34;\u0026#34;) source_columns = {row[\u0026#39;column_name\u0026#39;]: row for row in source_cursor.fetchall()} # è·å–ç›®æ ‡è¡¨åˆ—ä¿¡æ¯ target_cursor.execute(f\u0026#34;\u0026#34;\u0026#34; SELECT column_name, data_type, is_nullable, column_default, extra FROM information_schema.columns WHERE table_schema = \u0026#39;{database_name}\u0026#39; AND table_name = \u0026#39;{table_name}\u0026#39; ORDER BY ordinal_position \u0026#34;\u0026#34;\u0026#34;) target_columns = {row[\u0026#39;column_name\u0026#39;]: row for row in target_cursor.fetchall()} # æ¯”è¾ƒåˆ— source_column_names = set(source_columns.keys()) target_column_names = set(target_columns.keys()) # æ£€æŸ¥ç¼ºå¤±çš„åˆ— missing_columns = source_column_names - target_column_names if missing_columns: differences.append(f\u0026#34;è¡¨ {table_name} ç¼ºå¤±åˆ—: {\u0026#39;, \u0026#39;.join(missing_columns)}\u0026#34;) # æ£€æŸ¥å¤šä½™çš„åˆ— extra_columns = target_column_names - source_column_names if extra_columns: differences.append(f\u0026#34;è¡¨ {table_name} å¤šä½™åˆ—: {\u0026#39;, \u0026#39;.join(extra_columns)}\u0026#34;) # æ¯”è¾ƒå…±åŒåˆ—çš„å±æ€§ common_columns = source_column_names \u0026amp; target_column_names for column_name in common_columns: source_col = source_columns[column_name] target_col = target_columns[column_name] if source_col[\u0026#39;data_type\u0026#39;] != target_col[\u0026#39;data_type\u0026#39;]: differences.append(f\u0026#34;è¡¨ {table_name} åˆ— {column_name} æ•°æ®ç±»å‹ä¸åŒ¹é…: {source_col[\u0026#39;data_type\u0026#39;]} vs {target_col[\u0026#39;data_type\u0026#39;]}\u0026#34;) if source_col[\u0026#39;is_nullable\u0026#39;] != target_col[\u0026#39;is_nullable\u0026#39;]: differences.append(f\u0026#34;è¡¨ {table_name} åˆ— {column_name} å¯ç©ºå±æ€§ä¸åŒ¹é…\u0026#34;) return differences def _compare_indexes(self, source_cursor, target_cursor, database_name: str) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;æ¯”è¾ƒç´¢å¼•\u0026#34;\u0026#34;\u0026#34; differences = [] # è·å–æºæ•°æ®åº“ç´¢å¼•ä¿¡æ¯ source_cursor.execute(f\u0026#34;\u0026#34;\u0026#34; SELECT table_name, index_name, column_name, seq_in_index, non_unique FROM information_schema.statistics WHERE table_schema = \u0026#39;{database_name}\u0026#39; ORDER BY table_name, index_name, seq_in_index \u0026#34;\u0026#34;\u0026#34;) source_indexes = {} for row in source_cursor.fetchall(): key = f\u0026#34;{row[\u0026#39;table_name\u0026#39;]}.{row[\u0026#39;index_name\u0026#39;]}\u0026#34; if key not in source_indexes: source_indexes[key] = [] source_indexes[key].append(row) # è·å–ç›®æ ‡æ•°æ®åº“ç´¢å¼•ä¿¡æ¯ target_cursor.execute(f\u0026#34;\u0026#34;\u0026#34; SELECT table_name, index_name, column_name, seq_in_index, non_unique FROM information_schema.statistics WHERE table_schema = \u0026#39;{database_name}\u0026#39; ORDER BY table_name, index_name, seq_in_index \u0026#34;\u0026#34;\u0026#34;) target_indexes = {} for row in target_cursor.fetchall(): key = f\u0026#34;{row[\u0026#39;table_name\u0026#39;]}.{row[\u0026#39;index_name\u0026#39;]}\u0026#34; if key not in target_indexes: target_indexes[key] = [] target_indexes[key].append(row) # æ¯”è¾ƒç´¢å¼• source_index_names = set(source_indexes.keys()) target_index_names = set(target_indexes.keys()) missing_indexes = source_index_names - target_index_names if missing_indexes: differences.append(f\u0026#34;ç¼ºå¤±ç´¢å¼•: {\u0026#39;, \u0026#39;.join(missing_indexes)}\u0026#34;) extra_indexes = target_index_names - source_index_names if extra_indexes: differences.append(f\u0026#34;å¤šä½™ç´¢å¼•: {\u0026#39;, \u0026#39;.join(extra_indexes)}\u0026#34;) return differences def _compare_constraints(self, source_cursor, target_cursor, database_name: str) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;æ¯”è¾ƒçº¦æŸ\u0026#34;\u0026#34;\u0026#34; differences = [] # è·å–å¤–é”®çº¦æŸ source_cursor.execute(f\u0026#34;\u0026#34;\u0026#34; SELECT constraint_name, table_name, column_name, referenced_table_name, referenced_column_name FROM information_schema.key_column_usage WHERE table_schema = \u0026#39;{database_name}\u0026#39; AND referenced_table_name IS NOT NULL ORDER BY constraint_name \u0026#34;\u0026#34;\u0026#34;) source_constraints = set() for row in source_cursor.fetchall(): constraint_key = f\u0026#34;{row[\u0026#39;table_name\u0026#39;]}.{row[\u0026#39;constraint_name\u0026#39;]}\u0026#34; source_constraints.add(constraint_key) target_cursor.execute(f\u0026#34;\u0026#34;\u0026#34; SELECT constraint_name, table_name, column_name, referenced_table_name, referenced_column_name FROM information_schema.key_column_usage WHERE table_schema = \u0026#39;{database_name}\u0026#39; AND referenced_table_name IS NOT NULL ORDER BY constraint_name \u0026#34;\u0026#34;\u0026#34;) target_constraints = set() for row in target_cursor.fetchall(): constraint_key = f\u0026#34;{row[\u0026#39;table_name\u0026#39;]}.{row[\u0026#39;constraint_name\u0026#39;]}\u0026#34; target_constraints.add(constraint_key) # æ¯”è¾ƒçº¦æŸ missing_constraints = source_constraints - target_constraints if missing_constraints: differences.append(f\u0026#34;ç¼ºå¤±çº¦æŸ: {\u0026#39;, \u0026#39;.join(missing_constraints)}\u0026#34;) extra_constraints = target_constraints - source_constraints if extra_constraints: differences.append(f\u0026#34;å¤šä½™çº¦æŸ: {\u0026#39;, \u0026#39;.join(extra_constraints)}\u0026#34;) return differences def validate_data_consistency(self, database_name: str, sample_tables: List[str] = None) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;éªŒè¯æ•°æ®ä¸€è‡´æ€§\u0026#34;\u0026#34;\u0026#34; self.logger.info(f\u0026#34;å¼€å§‹éªŒè¯æ•°æ®ä¸€è‡´æ€§: {database_name}\u0026#34;) validation_result = { \u0026#39;database\u0026#39;: database_name, \u0026#39;data_consistent\u0026#39;: True, \u0026#39;table_results\u0026#39;: {}, \u0026#39;total_tables_checked\u0026#39;: 0, \u0026#39;inconsistent_tables\u0026#39;: [] } try: source_cursor = self.source_conn.cursor(dictionary=True) target_cursor = self.target_conn.cursor(dictionary=True) # è·å–è¦æ£€æŸ¥çš„è¡¨åˆ—è¡¨ if sample_tables: tables_to_check = sample_tables else: source_cursor.execute(f\u0026#34;\u0026#34;\u0026#34; SELECT table_name FROM information_schema.tables WHERE table_schema = \u0026#39;{database_name}\u0026#39; AND table_type = \u0026#39;BASE TABLE\u0026#39; ORDER BY table_name \u0026#34;\u0026#34;\u0026#34;) tables_to_check = [row[\u0026#39;table_name\u0026#39;] for row in source_cursor.fetchall()] validation_result[\u0026#39;total_tables_checked\u0026#39;] = len(tables_to_check) # æ£€æŸ¥æ¯ä¸ªè¡¨çš„æ•°æ®ä¸€è‡´æ€§ for table_name in tables_to_check: table_result = self._validate_table_data( source_cursor, target_cursor, database_name, table_name ) validation_result[\u0026#39;table_results\u0026#39;][table_name] = table_result if not table_result[\u0026#39;consistent\u0026#39;]: validation_result[\u0026#39;data_consistent\u0026#39;] = False validation_result[\u0026#39;inconsistent_tables\u0026#39;].append(table_name) source_cursor.close() target_cursor.close() except Exception as e: self.logger.error(f\u0026#34;æ•°æ®ä¸€è‡´æ€§éªŒè¯å¤±è´¥: {e}\u0026#34;) validation_result[\u0026#39;data_consistent\u0026#39;] = False validation_result[\u0026#39;error\u0026#39;] = str(e) return validation_result def _validate_table_data(self, source_cursor, target_cursor, database_name: str, table_name: str) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;éªŒè¯å•ä¸ªè¡¨çš„æ•°æ®ä¸€è‡´æ€§\u0026#34;\u0026#34;\u0026#34; table_result = { \u0026#39;table_name\u0026#39;: table_name, \u0026#39;consistent\u0026#39;: True, \u0026#39;row_count_match\u0026#39;: True, \u0026#39;checksum_match\u0026#39;: True, \u0026#39;source_row_count\u0026#39;: 0, \u0026#39;target_row_count\u0026#39;: 0, \u0026#39;source_checksum\u0026#39;: \u0026#39;\u0026#39;, \u0026#39;target_checksum\u0026#39;: \u0026#39;\u0026#39; } try: # æ¯”è¾ƒè¡Œæ•° source_cursor.execute(f\u0026#34;SELECT COUNT(*) as count FROM `{database_name}`.`{table_name}`\u0026#34;) source_count = source_cursor.fetchone()[\u0026#39;count\u0026#39;] table_result[\u0026#39;source_row_count\u0026#39;] = source_count target_cursor.execute(f\u0026#34;SELECT COUNT(*) as count FROM `{database_name}`.`{table_name}`\u0026#34;) target_count = target_cursor.fetchone()[\u0026#39;count\u0026#39;] table_result[\u0026#39;target_row_count\u0026#39;] = target_count if source_count != target_count: table_result[\u0026#39;consistent\u0026#39;] = False table_result[\u0026#39;row_count_match\u0026#39;] = False self.logger.warning(f\u0026#34;è¡¨ {table_name} è¡Œæ•°ä¸åŒ¹é…: {source_count} vs {target_count}\u0026#34;) # æ¯”è¾ƒæ•°æ®æ ¡éªŒå’Œï¼ˆä»…å¯¹å°è¡¨è¿›è¡Œï¼‰ if source_count \u0026lt;= 100000: # åªå¯¹10ä¸‡è¡Œä»¥ä¸‹çš„è¡¨è¿›è¡Œæ ¡éªŒå’Œæ¯”è¾ƒ source_checksum = self._calculate_table_checksum(source_cursor, database_name, table_name) target_checksum = self._calculate_table_checksum(target_cursor, database_name, table_name) table_result[\u0026#39;source_checksum\u0026#39;] = source_checksum table_result[\u0026#39;target_checksum\u0026#39;] = target_checksum if source_checksum != target_checksum: table_result[\u0026#39;consistent\u0026#39;] = False table_result[\u0026#39;checksum_match\u0026#39;] = False self.logger.warning(f\u0026#34;è¡¨ {table_name} æ ¡éªŒå’Œä¸åŒ¹é…\u0026#34;) except Exception as e: self.logger.error(f\u0026#34;è¡¨ {table_name} æ•°æ®éªŒè¯å¤±è´¥: {e}\u0026#34;) table_result[\u0026#39;consistent\u0026#39;] = False table_result[\u0026#39;error\u0026#39;] = str(e) return table_result def _calculate_table_checksum(self, cursor, database_name: str, table_name: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;è®¡ç®—è¡¨æ•°æ®æ ¡éªŒå’Œ\u0026#34;\u0026#34;\u0026#34; try: # è·å–è¡¨çš„æ‰€æœ‰åˆ— cursor.execute(f\u0026#34;\u0026#34;\u0026#34; SELECT column_name FROM information_schema.columns WHERE table_schema = \u0026#39;{database_name}\u0026#39; AND table_name = \u0026#39;{table_name}\u0026#39; ORDER BY ordinal_position \u0026#34;\u0026#34;\u0026#34;) columns = [row[\u0026#39;column_name\u0026#39;] for row in cursor.fetchall()] if not columns: return \u0026#34;\u0026#34; # æ„å»ºæŸ¥è¯¢è¯­å¥ column_list = \u0026#39;, \u0026#39;.join([f\u0026#39;COALESCE(`{col}`, \u0026#34;NULL\u0026#34;)\u0026#39; for col in columns]) query = f\u0026#34;\u0026#34;\u0026#34; SELECT MD5(GROUP_CONCAT( MD5(CONCAT({column_list})) ORDER BY {columns[0]} )) as checksum FROM `{database_name}`.`{table_name}` \u0026#34;\u0026#34;\u0026#34; cursor.execute(query) result = cursor.fetchone() return result[\u0026#39;checksum\u0026#39;] if result else \u0026#34;\u0026#34; except Exception as e: self.logger.error(f\u0026#34;è®¡ç®—è¡¨ {table_name} æ ¡éªŒå’Œå¤±è´¥: {e}\u0026#34;) return \u0026#34;\u0026#34; def validate_performance_metrics(self, database_name: str) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;éªŒè¯æ€§èƒ½æŒ‡æ ‡\u0026#34;\u0026#34;\u0026#34; self.logger.info(f\u0026#34;å¼€å§‹éªŒè¯æ€§èƒ½æŒ‡æ ‡: {database_name}\u0026#34;) validation_result = { \u0026#39;database\u0026#39;: database_name, \u0026#39;performance_acceptable\u0026#39;: True, \u0026#39;query_performance\u0026#39;: {}, \u0026#39;connection_test\u0026#39;: {}, \u0026#39;load_test\u0026#39;: {} } try: # æµ‹è¯•æŸ¥è¯¢æ€§èƒ½ query_result = self._test_query_performance(database_name) validation_result[\u0026#39;query_performance\u0026#39;] = query_result # æµ‹è¯•è¿æ¥æ€§èƒ½ connection_result = self._test_connection_performance() validation_result[\u0026#39;connection_test\u0026#39;] = connection_result # ç®€å•è´Ÿè½½æµ‹è¯• load_result = self._test_simple_load(database_name) validation_result[\u0026#39;load_test\u0026#39;] = load_result # åˆ¤æ–­æ•´ä½“æ€§èƒ½æ˜¯å¦å¯æ¥å— if (query_result.get(\u0026#39;average_response_time\u0026#39;, 0) \u0026gt; 1000 or # 1ç§’ connection_result.get(\u0026#39;average_connection_time\u0026#39;, 0) \u0026gt; 500 or # 0.5ç§’ load_result.get(\u0026#39;success_rate\u0026#39;, 0) \u0026lt; 0.95): # 95%æˆåŠŸç‡ validation_result[\u0026#39;performance_acceptable\u0026#39;] = False except Exception as e: self.logger.error(f\u0026#34;æ€§èƒ½éªŒè¯å¤±è´¥: {e}\u0026#34;) validation_result[\u0026#39;performance_acceptable\u0026#39;] = False validation_result[\u0026#39;error\u0026#39;] = str(e) return validation_result def _test_query_performance(self, database_name: str) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;æµ‹è¯•æŸ¥è¯¢æ€§èƒ½\u0026#34;\u0026#34;\u0026#34; result = { \u0026#39;queries_tested\u0026#39;: 0, \u0026#39;total_time\u0026#39;: 0, \u0026#39;average_response_time\u0026#39;: 0, \u0026#39;max_response_time\u0026#39;: 0, \u0026#39;min_response_time\u0026#39;: float(\u0026#39;inf\u0026#39;) } try: cursor = self.target_conn.cursor() # æµ‹è¯•æŸ¥è¯¢åˆ—è¡¨ test_queries = [ f\u0026#34;SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = \u0026#39;{database_name}\u0026#39;\u0026#34;, f\u0026#34;SELECT table_name, table_rows FROM information_schema.tables WHERE table_schema = \u0026#39;{database_name}\u0026#39; LIMIT 10\u0026#34;, \u0026#34;SELECT NOW()\u0026#34;, \u0026#34;SELECT VERSION()\u0026#34;, \u0026#34;SHOW STATUS LIKE \u0026#39;Threads_connected\u0026#39;\u0026#34; ] response_times = [] for query in test_queries: start_time = time.time() cursor.execute(query) cursor.fetchall() end_time = time.time() response_time = (end_time - start_time) * 1000 # è½¬æ¢ä¸ºæ¯«ç§’ response_times.append(response_time) result[\u0026#39;queries_tested\u0026#39;] += 1 if response_times: result[\u0026#39;total_time\u0026#39;] = sum(response_times) result[\u0026#39;average_response_time\u0026#39;] = sum(response_times) / len(response_times) result[\u0026#39;max_response_time\u0026#39;] = max(response_times) result[\u0026#39;min_response_time\u0026#39;] = min(response_times) cursor.close() except Exception as e: self.logger.error(f\u0026#34;æŸ¥è¯¢æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}\u0026#34;) result[\u0026#39;error\u0026#39;] = str(e) return result def _test_connection_performance(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;æµ‹è¯•è¿æ¥æ€§èƒ½\u0026#34;\u0026#34;\u0026#34; result = { \u0026#39;connections_tested\u0026#39;: 0, \u0026#39;successful_connections\u0026#39;: 0, \u0026#39;average_connection_time\u0026#39;: 0, \u0026#39;max_connection_time\u0026#39;: 0, \u0026#39;min_connection_time\u0026#39;: float(\u0026#39;inf\u0026#39;) } try: connection_times = [] # æµ‹è¯•10æ¬¡è¿æ¥ for i in range(10): start_time = time.time() try: test_conn = mysql.connector.connect( **self.config[\u0026#39;target_database\u0026#39;] ) test_conn.close() end_time = time.time() connection_time = (end_time - start_time) * 1000 # è½¬æ¢ä¸ºæ¯«ç§’ connection_times.append(connection_time) result[\u0026#39;successful_connections\u0026#39;] += 1 except Exception: pass result[\u0026#39;connections_tested\u0026#39;] += 1 if connection_times: result[\u0026#39;average_connection_time\u0026#39;] = sum(connection_times) / len(connection_times) result[\u0026#39;max_connection_time\u0026#39;] = max(connection_times) result[\u0026#39;min_connection_time\u0026#39;] = min(connection_times) except Exception as e: self.logger.error(f\u0026#34;è¿æ¥æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}\u0026#34;) result[\u0026#39;error\u0026#39;] = str(e) return result def _test_simple_load(self, database_name: str) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç®€å•è´Ÿè½½æµ‹è¯•\u0026#34;\u0026#34;\u0026#34; result = { \u0026#39;total_operations\u0026#39;: 0, \u0026#39;successful_operations\u0026#39;: 0, \u0026#39;failed_operations\u0026#39;: 0, \u0026#39;success_rate\u0026#39;: 0, \u0026#39;average_operation_time\u0026#39;: 0 } try: cursor = self.target_conn.cursor() operation_times = [] # æ‰§è¡Œ50æ¬¡ç®€å•æŸ¥è¯¢ for i in range(50): start_time = time.time() try: cursor.execute(f\u0026#34;SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = \u0026#39;{database_name}\u0026#39;\u0026#34;) cursor.fetchone() end_time = time.time() operation_time = (end_time - start_time) * 1000 operation_times.append(operation_time) result[\u0026#39;successful_operations\u0026#39;] += 1 except Exception: result[\u0026#39;failed_operations\u0026#39;] += 1 result[\u0026#39;total_operations\u0026#39;] += 1 if result[\u0026#39;total_operations\u0026#39;] \u0026gt; 0: result[\u0026#39;success_rate\u0026#39;] = result[\u0026#39;successful_operations\u0026#39;] / result[\u0026#39;total_operations\u0026#39;] if operation_times: result[\u0026#39;average_operation_time\u0026#39;] = sum(operation_times) / len(operation_times) cursor.close() except Exception as e: self.logger.error(f\u0026#34;è´Ÿè½½æµ‹è¯•å¤±è´¥: {e}\u0026#34;) result[\u0026#39;error\u0026#39;] = str(e) return result def generate_validation_report(self, validation_results: Dict[str, Any]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆéªŒè¯æŠ¥å‘Š\u0026#34;\u0026#34;\u0026#34; report_file = f\u0026#34;/var/reports/mysql_recovery_validation_{datetime.now().strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;)}.json\u0026#34; # æ·»åŠ æŠ¥å‘Šå…ƒæ•°æ® validation_results[\u0026#39;report_metadata\u0026#39;] = { \u0026#39;generated_at\u0026#39;: datetime.now().isoformat(), \u0026#39;validator_version\u0026#39;: \u0026#39;1.0.0\u0026#39;, \u0026#39;source_database\u0026#39;: self.config[\u0026#39;source_database\u0026#39;][\u0026#39;host\u0026#39;], \u0026#39;target_database\u0026#39;: self.config[\u0026#39;target_database\u0026#39;][\u0026#39;host\u0026#39;] } # å†™å…¥JSONæŠ¥å‘Š with open(report_file, \u0026#39;w\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: json.dump(validation_results, f, indent=2, ensure_ascii=False) self.logger.info(f\u0026#34;éªŒè¯æŠ¥å‘Šç”Ÿæˆå®Œæˆ: {report_file}\u0026#34;) return report_file def run_full_validation(self, database_name: str, sample_tables: List[str] = None) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è¿è¡Œå®Œæ•´éªŒè¯\u0026#34;\u0026#34;\u0026#34; self.logger.info(f\u0026#34;å¼€å§‹å®Œæ•´éªŒè¯: {database_name}\u0026#34;) validation_results = { \u0026#39;database\u0026#39;: database_name, \u0026#39;validation_start_time\u0026#39;: datetime.now().isoformat(), \u0026#39;overall_status\u0026#39;: \u0026#39;PASS\u0026#39;, \u0026#39;schema_validation\u0026#39;: {}, \u0026#39;data_validation\u0026#39;: {}, \u0026#39;performance_validation\u0026#39;: {} } try: # è¿æ¥æ•°æ®åº“ self.connect_databases() # æ¨¡å¼éªŒè¯ schema_result = self.validate_schema_consistency(database_name) validation_results[\u0026#39;schema_validation\u0026#39;] = schema_result if not schema_result[\u0026#39;schema_consistent\u0026#39;]: validation_results[\u0026#39;overall_status\u0026#39;] = \u0026#39;FAIL\u0026#39; # æ•°æ®éªŒè¯ data_result = self.validate_data_consistency(database_name, sample_tables) validation_results[\u0026#39;data_validation\u0026#39;] = data_result if not data_result[\u0026#39;data_consistent\u0026#39;]: validation_results[\u0026#39;overall_status\u0026#39;] = \u0026#39;FAIL\u0026#39; # æ€§èƒ½éªŒè¯ performance_result = self.validate_performance_metrics(database_name) validation_results[\u0026#39;performance_validation\u0026#39;] = performance_result if not performance_result[\u0026#39;performance_acceptable\u0026#39;]: if validation_results[\u0026#39;overall_status\u0026#39;] == \u0026#39;PASS\u0026#39;: validation_results[\u0026#39;overall_status\u0026#39;] = \u0026#39;WARNING\u0026#39; validation_results[\u0026#39;validation_end_time\u0026#39;] = datetime.now().isoformat() # ç”ŸæˆæŠ¥å‘Š report_file = self.generate_validation_report(validation_results) validation_results[\u0026#39;report_file\u0026#39;] = report_file except Exception as e: self.logger.error(f\u0026#34;éªŒè¯è¿‡ç¨‹å¤±è´¥: {e}\u0026#34;) validation_results[\u0026#39;overall_status\u0026#39;] = \u0026#39;ERROR\u0026#39; validation_results[\u0026#39;error\u0026#39;] = str(e) finally: # å…³é—­æ•°æ®åº“è¿æ¥ if self.source_conn: self.source_conn.close() if self.target_conn: self.target_conn.close() return validation_results def main(): parser = argparse.ArgumentParser(description=\u0026#39;MySQLæ¢å¤éªŒè¯å·¥å…·\u0026#39;) parser.add_argument(\u0026#39;--config\u0026#39;, required=True, help=\u0026#39;é…ç½®æ–‡ä»¶è·¯å¾„\u0026#39;) parser.add_argument(\u0026#39;--database\u0026#39;, required=True, help=\u0026#39;è¦éªŒè¯çš„æ•°æ®åº“åç§°\u0026#39;) parser.add_argument(\u0026#39;--sample-tables\u0026#39;, nargs=\u0026#39;*\u0026#39;, help=\u0026#39;è¦éªŒè¯çš„æ ·æœ¬è¡¨åˆ—è¡¨\u0026#39;) parser.add_argument(\u0026#39;--schema-only\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;ä»…éªŒè¯æ¨¡å¼\u0026#39;) parser.add_argument(\u0026#39;--data-only\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;ä»…éªŒè¯æ•°æ®\u0026#39;) parser.add_argument(\u0026#39;--performance-only\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;ä»…éªŒè¯æ€§èƒ½\u0026#39;) args = parser.parse_args() # åŠ è½½é…ç½® with open(args.config, \u0026#39;r\u0026#39;) as f: config = json.load(f) # åˆ›å»ºéªŒè¯å™¨ validator = MySQLRecoveryValidator(config) try: if args.schema_only: validator.connect_databases() result = validator.validate_schema_consistency(args.database) print(json.dumps(result, indent=2)) elif args.data_only: validator.connect_databases() result = validator.validate_data_consistency(args.database, args.sample_tables) print(json.dumps(result, indent=2)) elif args.performance_only: validator.connect_databases() result = validator.validate_performance_metrics(args.database) print(json.dumps(result, indent=2)) else: # å®Œæ•´éªŒè¯ result = validator.run_full_validation(args.database, args.sample_tables) print(f\u0026#34;éªŒè¯å®Œæˆï¼ŒçŠ¶æ€: {result[\u0026#39;overall_status\u0026#39;]}\u0026#34;) print(f\u0026#34;æŠ¥å‘Šæ–‡ä»¶: {result.get(\u0026#39;report_file\u0026#39;, \u0026#39;N/A\u0026#39;)}\u0026#34;) except Exception as e: print(f\u0026#34;éªŒè¯å¤±è´¥: {e}\u0026#34;) exit(1) if __name__ == \u0026#34;__main__\u0026#34;: main() PostgreSQLå¤‡ä»½ä¸æ¢å¤å®ç° 1. PostgreSQLå¤‡ä»½ç³»ç»Ÿ #!/bin/bash # scripts/postgresql_backup_system.sh set -euo pipefail # é…ç½®æ–‡ä»¶è·¯å¾„ CONFIG_FILE=\u0026#34;/etc/postgresql-backup/config.conf\u0026#34; LOG_FILE=\u0026#34;/var/log/postgresql-backup.log\u0026#34; # é»˜è®¤é…ç½® PGHOST=\u0026#34;${PGHOST:-localhost}\u0026#34; PGPORT=\u0026#34;${PGPORT:-5432}\u0026#34; PGUSER=\u0026#34;${PGUSER:-backup_user}\u0026#34; PGPASSWORD=\u0026#34;${PGPASSWORD:-}\u0026#34; BACKUP_DIR=\u0026#34;${BACKUP_DIR:-/backup/postgresql}\u0026#34; WAL_ARCHIVE_DIR=\u0026#34;${WAL_ARCHIVE_DIR:-/backup/postgresql/wal}\u0026#34; RETENTION_DAYS=\u0026#34;${RETENTION_DAYS:-7}\u0026#34; COMPRESSION=\u0026#34;${COMPRESSION:-true}\u0026#34; ENCRYPTION=\u0026#34;${ENCRYPTION:-true}\u0026#34; ENCRYPTION_KEY=\u0026#34;${ENCRYPTION_KEY:-}\u0026#34; # å¯¼å‡ºPostgreSQLç¯å¢ƒå˜é‡ export PGHOST PGPORT PGUSER PGPASSWORD # æ—¥å¿—å‡½æ•° log() { echo \u0026#34;[$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)] $1\u0026#34; | tee -a \u0026#34;$LOG_FILE\u0026#34; } error() { echo \u0026#34;[$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;)] ERROR: $1\u0026#34; | tee -a \u0026#34;$LOG_FILE\u0026#34; \u0026gt;\u0026amp;2 } # åŠ è½½é…ç½®æ–‡ä»¶ load_config() { if [[ -f \u0026#34;$CONFIG_FILE\u0026#34; ]]; then source \u0026#34;$CONFIG_FILE\u0026#34; log \u0026#34;é…ç½®æ–‡ä»¶åŠ è½½å®Œæˆ: $CONFIG_FILE\u0026#34; else log \u0026#34;ä½¿ç”¨é»˜è®¤é…ç½®\u0026#34; fi } # æ£€æŸ¥ä¾èµ– check_dependencies() { local deps=(\u0026#34;pg_dump\u0026#34; \u0026#34;pg_dumpall\u0026#34; \u0026#34;pg_basebackup\u0026#34; \u0026#34;psql\u0026#34; \u0026#34;gzip\u0026#34; \u0026#34;openssl\u0026#34;) for dep in \u0026#34;${deps[@]}\u0026#34;; do if ! command -v \u0026#34;$dep\u0026#34; \u0026amp;\u0026gt; /dev/null; then error \u0026#34;ä¾èµ–é¡¹æœªæ‰¾åˆ°: $dep\u0026#34; exit 1 fi done log \u0026#34;ä¾èµ–æ£€æŸ¥å®Œæˆ\u0026#34; } # åˆ›å»ºå¤‡ä»½ç›®å½• create_backup_directories() { local dirs=( \u0026#34;$BACKUP_DIR/full\u0026#34; \u0026#34;$BACKUP_DIR/incremental\u0026#34; \u0026#34;$BACKUP_DIR/wal\u0026#34; \u0026#34;$BACKUP_DIR/basebackup\u0026#34; \u0026#34;$BACKUP_DIR/temp\u0026#34; ) for dir in \u0026#34;${dirs[@]}\u0026#34;; do mkdir -p \u0026#34;$dir\u0026#34; done log \u0026#34;å¤‡ä»½ç›®å½•åˆ›å»ºå®Œæˆ\u0026#34; } # è·å–æ•°æ®åº“åˆ—è¡¨ get_databases() { psql -t -c \u0026#34;SELECT datname FROM pg_database WHERE datistemplate = false AND datname != \u0026#39;postgres\u0026#39;;\u0026#34; | grep -v \u0026#39;^$\u0026#39; } # é€»è¾‘å¤‡ä»½ï¼ˆpg_dumpï¼‰ logical_backup() { local database=\u0026#34;$1\u0026#34; local timestamp=$(date \u0026#39;+%Y%m%d_%H%M%S\u0026#39;) local backup_file=\u0026#34;$BACKUP_DIR/full/${database}_logical_${timestamp}.sql\u0026#34; log \u0026#34;å¼€å§‹é€»è¾‘å¤‡ä»½: $database\u0026#34; # æ‰§è¡Œå¤‡ä»½ pg_dump \\ --verbose \\ --format=custom \\ --compress=6 \\ --no-owner \\ --no-privileges \\ --create \\ --clean \\ --if-exists \\ --file=\u0026#34;$backup_file\u0026#34; \\ \u0026#34;$database\u0026#34; if [[ $? -eq 0 ]]; then log \u0026#34;é€»è¾‘å¤‡ä»½å®Œæˆ: $backup_file\u0026#34; # å‹ç¼©å¤‡ä»½æ–‡ä»¶ if [[ \u0026#34;$COMPRESSION\u0026#34; == \u0026#34;true\u0026#34; ]]; then compress_backup \u0026#34;$backup_file\u0026#34; fi # åŠ å¯†å¤‡ä»½æ–‡ä»¶ if [[ \u0026#34;$ENCRYPTION\u0026#34; == \u0026#34;true\u0026#34; ]]; then encrypt_backup \u0026#34;$backup_file\u0026#34; fi # éªŒè¯å¤‡ä»½ verify_backup \u0026#34;$backup_file\u0026#34; # ç”Ÿæˆå¤‡ä»½å…ƒæ•°æ® generate_backup_metadata \u0026#34;$backup_file\u0026#34; \u0026#34;logical\u0026#34; \u0026#34;$database\u0026#34; else error \u0026#34;é€»è¾‘å¤‡ä»½å¤±è´¥: $database\u0026#34; return 1 fi } # ç‰©ç†å¤‡ä»½ï¼ˆpg_basebackupï¼‰ physical_backup() { local timestamp=$(date \u0026#39;+%Y%m%d_%H%M%S\u0026#39;) local backup_dir=\u0026#34;$BACKUP_DIR/basebackup/basebackup_${timestamp}\u0026#34; log \u0026#34;å¼€å§‹ç‰©ç†å¤‡ä»½\u0026#34; mkdir -p \u0026#34;$backup_dir\u0026#34; # æ‰§è¡ŒåŸºç¡€å¤‡ä»½ pg_basebackup \\ --verbose \\ --progress \\ --format=tar \\ --gzip \\ --compress=6 \\ --checkpoint=fast \\ --wal-method=stream \\ --pgdata=\u0026#34;$backup_dir\u0026#34; if [[ $? -eq 0 ]]; then log \u0026#34;ç‰©ç†å¤‡ä»½å®Œæˆ: $backup_dir\u0026#34; # åŠ å¯†å¤‡ä»½æ–‡ä»¶ if [[ \u0026#34;$ENCRYPTION\u0026#34; == \u0026#34;true\u0026#34; ]]; then for tar_file in \u0026#34;$backup_dir\u0026#34;/*.tar.gz; do if [[ -f \u0026#34;$tar_file\u0026#34; ]]; then encrypt_backup \u0026#34;$tar_file\u0026#34; fi done fi # ç”Ÿæˆå¤‡ä»½å…ƒæ•°æ® generate_backup_metadata \u0026#34;$backup_dir\u0026#34; \u0026#34;physical\u0026#34; \u0026#34;cluster\u0026#34; else error \u0026#34;ç‰©ç†å¤‡ä»½å¤±è´¥\u0026#34; return 1 fi } # WALå½’æ¡£å¤‡ä»½ wal_archive_backup() { local wal_source_dir=\u0026#34;$1\u0026#34; local timestamp=$(date \u0026#39;+%Y%m%d_%H%M%S\u0026#39;) local archive_dir=\u0026#34;$WAL_ARCHIVE_DIR/${timestamp}\u0026#34; log \u0026#34;å¼€å§‹WALå½’æ¡£å¤‡ä»½\u0026#34; mkdir -p \u0026#34;$archive_dir\u0026#34; # å¤åˆ¶WALæ–‡ä»¶ if [[ -d \u0026#34;$wal_source_dir\u0026#34; ]]; then rsync -av --progress \u0026#34;$wal_source_dir\u0026#34;/ \u0026#34;$archive_dir\u0026#34;/ if [[ $? -eq 0 ]]; then log \u0026#34;WALå½’æ¡£å®Œæˆ: $archive_dir\u0026#34; # å‹ç¼©WALæ–‡ä»¶ if [[ \u0026#34;$COMPRESSION\u0026#34; == \u0026#34;true\u0026#34; ]]; then find \u0026#34;$archive_dir\u0026#34; -name \u0026#34;*.wal\u0026#34; -exec gzip {} \\; fi # ç”Ÿæˆå¤‡ä»½å…ƒæ•°æ® generate_backup_metadata \u0026#34;$archive_dir\u0026#34; \u0026#34;wal\u0026#34; \u0026#34;cluster\u0026#34; else error \u0026#34;WALå½’æ¡£å¤±è´¥\u0026#34; return 1 fi else error \u0026#34;WALæºç›®å½•ä¸å­˜åœ¨: $wal_source_dir\u0026#34; return 1 fi } # å‹ç¼©å¤‡ä»½æ–‡ä»¶ compress_backup() { local backup_file=\u0026#34;$1\u0026#34; log \u0026#34;å‹ç¼©å¤‡ä»½æ–‡ä»¶: $backup_file\u0026#34; gzip \u0026#34;$backup_file\u0026#34; if [[ $? -eq 0 ]]; then log \u0026#34;å‹ç¼©å®Œæˆ: ${backup_file}.gz\u0026#34; else error \u0026#34;å‹ç¼©å¤±è´¥: $backup_file\u0026#34; return 1 fi } # åŠ å¯†å¤‡ä»½æ–‡ä»¶ encrypt_backup() { local backup_file=\u0026#34;$1\u0026#34; if [[ -z \u0026#34;$ENCRYPTION_KEY\u0026#34; ]]; then error \u0026#34;åŠ å¯†å¯†é’¥æœªè®¾ç½®\u0026#34; return 1 fi log \u0026#34;åŠ å¯†å¤‡ä»½æ–‡ä»¶: $backup_file\u0026#34; openssl enc -aes-256-cbc -salt -in \u0026#34;$backup_file\u0026#34; -out \u0026#34;${backup_file}.enc\u0026#34; -k \u0026#34;$ENCRYPTION_KEY\u0026#34; if [[ $? -eq 0 ]]; then rm \u0026#34;$backup_file\u0026#34; log \u0026#34;åŠ å¯†å®Œæˆ: ${backup_file}.enc\u0026#34; else error \u0026#34;åŠ å¯†å¤±è´¥: $backup_file\u0026#34; return 1 fi } # éªŒè¯å¤‡ä»½æ–‡ä»¶ verify_backup() { local backup_file=\u0026#34;$1\u0026#34; log \u0026#34;éªŒè¯å¤‡ä»½æ–‡ä»¶: $backup_file\u0026#34; # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸ºç©º if [[ ! -f \u0026#34;$backup_file\u0026#34; ]] || [[ ! -s \u0026#34;$backup_file\u0026#34; ]]; then error \u0026#34;å¤‡ä»½æ–‡ä»¶æ— æ•ˆ: $backup_file\u0026#34; return 1 fi # ç”Ÿæˆæ ¡éªŒå’Œ local checksum=$(sha256sum \u0026#34;$backup_file\u0026#34; | awk \u0026#39;{print $1}\u0026#39;) echo \u0026#34;$checksum\u0026#34; \u0026gt; \u0026#34;${backup_file}.sha256\u0026#34; log \u0026#34;å¤‡ä»½éªŒè¯å®Œæˆï¼Œæ ¡éªŒå’Œ: $checksum\u0026#34; } # ç”Ÿæˆå¤‡ä»½å…ƒæ•°æ® generate_backup_metadata() { local backup_file=\u0026#34;$1\u0026#34; local backup_type=\u0026#34;$2\u0026#34; local database=\u0026#34;$3\u0026#34; local timestamp=$(date \u0026#39;+%Y-%m-%d %H:%M:%S\u0026#39;) local metadata_file=\u0026#34;${backup_file}.metadata\u0026#34; cat \u0026gt; \u0026#34;$metadata_file\u0026#34; \u0026lt;\u0026lt; EOF { \u0026#34;backup_type\u0026#34;: \u0026#34;$backup_type\u0026#34;, \u0026#34;database\u0026#34;: \u0026#34;$database\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;$timestamp\u0026#34;, \u0026#34;backup_file\u0026#34;: \u0026#34;$backup_file\u0026#34;, \u0026#34;file_size\u0026#34;: $(stat -c%s \u0026#34;$backup_file\u0026#34; 2\u0026gt;/dev/null || echo \u0026#34;0\u0026#34;), \u0026#34;postgresql_version\u0026#34;: \u0026#34;$(psql -t -c \u0026#34;SELECT version();\u0026#34; | head -n 1 | xargs)\u0026#34;, \u0026#34;compression\u0026#34;: \u0026#34;$COMPRESSION\u0026#34;, \u0026#34;encryption\u0026#34;: \u0026#34;$ENCRYPTION\u0026#34;, \u0026#34;checksum\u0026#34;: \u0026#34;$(cat \u0026#34;${backup_file}.sha256\u0026#34; 2\u0026gt;/dev/null || echo \u0026#34;N/A\u0026#34;)\u0026#34; } EOF log \u0026#34;å¤‡ä»½å…ƒæ•°æ®ç”Ÿæˆå®Œæˆ: $metadata_file\u0026#34; } # æ¢å¤æ•°æ®åº“ restore_database() { local backup_file=\u0026#34;$1\u0026#34; local target_database=\u0026#34;$2\u0026#34; local target_host=\u0026#34;${3:-$PGHOST}\u0026#34; local target_port=\u0026#34;${4:-$PGPORT}\u0026#34; log \u0026#34;å¼€å§‹æ¢å¤æ•°æ®åº“: $target_database\u0026#34; # æ£€æŸ¥å¤‡ä»½æ–‡ä»¶ if [[ ! -f \u0026#34;$backup_file\u0026#34; ]]; then error \u0026#34;å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: $backup_file\u0026#34; return 1 fi # è§£å¯†å¤‡ä»½æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰ local restore_file=\u0026#34;$backup_file\u0026#34; if [[ \u0026#34;$backup_file\u0026#34; == *.enc ]]; then restore_file=\u0026#34;${backup_file%.enc}\u0026#34; log \u0026#34;è§£å¯†å¤‡ä»½æ–‡ä»¶: $backup_file\u0026#34; openssl enc -aes-256-cbc -d -in \u0026#34;$backup_file\u0026#34; -out \u0026#34;$restore_file\u0026#34; -k \u0026#34;$ENCRYPTION_KEY\u0026#34; fi # è§£å‹å¤‡ä»½æ–‡ä»¶ï¼ˆå¦‚æœéœ€è¦ï¼‰ if [[ \u0026#34;$restore_file\u0026#34; == *.gz ]]; then log \u0026#34;è§£å‹å¤‡ä»½æ–‡ä»¶: $restore_file\u0026#34; gunzip \u0026#34;$restore_file\u0026#34; restore_file=\u0026#34;${restore_file%.gz}\u0026#34; fi # åˆ›å»ºç›®æ ‡æ•°æ®åº“ PGHOST=\u0026#34;$target_host\u0026#34; PGPORT=\u0026#34;$target_port\u0026#34; createdb \u0026#34;$target_database\u0026#34; 2\u0026gt;/dev/null || true # æ¢å¤æ•°æ® log \u0026#34;æ¢å¤æ•°æ®åˆ°æ•°æ®åº“: $target_database\u0026#34; PGHOST=\u0026#34;$target_host\u0026#34; PGPORT=\u0026#34;$target_port\u0026#34; pg_restore \\ --verbose \\ --clean \\ --if-exists \\ --create \\ --dbname=\u0026#34;$target_database\u0026#34; \\ \u0026#34;$restore_file\u0026#34; if [[ $? -eq 0 ]]; then log \u0026#34;æ•°æ®åº“æ¢å¤å®Œæˆ: $target_database\u0026#34; # éªŒè¯æ¢å¤ç»“æœ local table_count=$(PGHOST=\u0026#34;$target_host\u0026#34; PGPORT=\u0026#34;$target_port\u0026#34; psql -t -d \u0026#34;$target_database\u0026#34; \\ -c \u0026#34;SELECT count(*) FROM information_schema.tables WHERE table_schema = \u0026#39;public\u0026#39;;\u0026#34; | xargs) log \u0026#34;æ¢å¤éªŒè¯ - è¡¨æ•°é‡: $table_count\u0026#34; else error \u0026#34;æ•°æ®åº“æ¢å¤å¤±è´¥: $target_database\u0026#34; return 1 fi # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ if [[ \u0026#34;$restore_file\u0026#34; != \u0026#34;$backup_file\u0026#34; ]]; then rm -f \u0026#34;$restore_file\u0026#34; fi } # æ—¶é—´ç‚¹æ¢å¤ point_in_time_recovery() { local target_time=\u0026#34;$1\u0026#34; local base_backup_dir=\u0026#34;$2\u0026#34; local wal_archive_dir=\u0026#34;$3\u0026#34; local recovery_dir=\u0026#34;$4\u0026#34; log \u0026#34;å¼€å§‹æ—¶é—´ç‚¹æ¢å¤åˆ°: $target_time\u0026#34; # åˆ›å»ºæ¢å¤ç›®å½• mkdir -p \u0026#34;$recovery_dir\u0026#34; # è§£å‹åŸºç¡€å¤‡ä»½ if [[ -f \u0026#34;$base_backup_dir/base.tar.gz\u0026#34; ]]; then tar -xzf \u0026#34;$base_backup_dir/base.tar.gz\u0026#34; -C \u0026#34;$recovery_dir\u0026#34; else error \u0026#34;åŸºç¡€å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨\u0026#34; return 1 fi # åˆ›å»ºrecovery.confæ–‡ä»¶ cat \u0026gt; \u0026#34;$recovery_dir/recovery.conf\u0026#34; \u0026lt;\u0026lt; EOF restore_command = \u0026#39;cp $wal_archive_dir/%f %p\u0026#39; recovery_target_time = \u0026#39;$target_time\u0026#39; recovery_target_timeline = \u0026#39;latest\u0026#39; EOF log \u0026#34;æ—¶é—´ç‚¹æ¢å¤é…ç½®å®Œæˆ: $recovery_dir\u0026#34; log \u0026#34;è¯·æ‰‹åŠ¨å¯åŠ¨PostgreSQLå®ä¾‹è¿›è¡Œæ¢å¤\u0026#34; } # æ¸…ç†è¿‡æœŸå¤‡ä»½ cleanup_old_backups() { log \u0026#34;å¼€å§‹æ¸…ç†è¿‡æœŸå¤‡ä»½\u0026#34; # æ¸…ç†é€»è¾‘å¤‡ä»½ find \u0026#34;$BACKUP_DIR/full\u0026#34; -name \u0026#34;*.sql*\u0026#34; -mtime +$RETENTION_DAYS -delete find \u0026#34;$BACKUP_DIR/full\u0026#34; -name \u0026#34;*.metadata\u0026#34; -mtime +$RETENTION_DAYS -delete find \u0026#34;$BACKUP_DIR/full\u0026#34; -name \u0026#34;*.sha256\u0026#34; -mtime +$RETENTION_DAYS -delete # æ¸…ç†ç‰©ç†å¤‡ä»½ find \u0026#34;$BACKUP_DIR/basebackup\u0026#34; -type d -mtime +$RETENTION_DAYS -exec rm -rf {} + # æ¸…ç†WALå½’æ¡£ find \u0026#34;$WAL_ARCHIVE_DIR\u0026#34; -name \u0026#34;*.wal*\u0026#34; -mtime +$RETENTION_DAYS -delete log \u0026#34;è¿‡æœŸå¤‡ä»½æ¸…ç†å®Œæˆ\u0026#34; } # ä¸»å‡½æ•° main() { case \u0026#34;${1:-backup}\u0026#34; in \u0026#34;logical\u0026#34;) load_config check_dependencies create_backup_directories if [[ $# -lt 2 ]]; then echo \u0026#34;ç”¨æ³•: $0 logical \u0026lt;database\u0026gt;\u0026#34; exit 1 fi logical_backup \u0026#34;$2\u0026#34; ;; \u0026#34;physical\u0026#34;) load_config check_dependencies create_backup_directories physical_backup ;; \u0026#34;wal\u0026#34;) load_config check_dependencies create_backup_directories if [[ $# -lt 2 ]]; then echo \u0026#34;ç”¨æ³•: $0 wal \u0026lt;wal_source_dir\u0026gt;\u0026#34; exit 1 fi wal_archive_backup \u0026#34;$2\u0026#34; ;; \u0026#34;restore\u0026#34;) if [[ $# -lt 3 ]]; then echo \u0026#34;ç”¨æ³•: $0 restore \u0026lt;backup_file\u0026gt; \u0026lt;target_database\u0026gt; [target_host] [target_port]\u0026#34; exit 1 fi load_config restore_database \u0026#34;$2\u0026#34; \u0026#34;$3\u0026#34; \u0026#34;$4\u0026#34; \u0026#34;$5\u0026#34; ;; \u0026#34;pitr\u0026#34;) if [[ $# -lt 5 ]]; then echo \u0026#34;ç”¨æ³•: $0 pitr \u0026lt;target_time\u0026gt; \u0026lt;base_backup_dir\u0026gt; \u0026lt;wal_archive_dir\u0026gt; \u0026lt;recovery_dir\u0026gt;\u0026#34; exit 1 fi load_config point_in_time_recovery \u0026#34;$2\u0026#34; \u0026#34;$3\u0026#34; \u0026#34;$4\u0026#34; \u0026#34;$5\u0026#34; ;; \u0026#34;cleanup\u0026#34;) load_config cleanup_old_backups ;; *) echo \u0026#34;ç”¨æ³•: $0 {logical|physical|wal|restore|pitr|cleanup}\u0026#34; echo \u0026#34; logical \u0026lt;database\u0026gt; - é€»è¾‘å¤‡ä»½\u0026#34; echo \u0026#34; physical - ç‰©ç†å¤‡ä»½\u0026#34; echo \u0026#34; wal \u0026lt;wal_source_dir\u0026gt; - WALå½’æ¡£\u0026#34; echo \u0026#34; restore \u0026lt;backup_file\u0026gt; \u0026lt;target_database\u0026gt; - æ¢å¤æ•°æ®åº“\u0026#34; echo \u0026#34; pitr \u0026lt;time\u0026gt; \u0026lt;base_backup\u0026gt; \u0026lt;wal_dir\u0026gt; \u0026lt;recovery_dir\u0026gt; - æ—¶é—´ç‚¹æ¢å¤\u0026#34; echo \u0026#34; cleanup - æ¸…ç†è¿‡æœŸå¤‡ä»½\u0026#34; exit 1 ;; esac } main \u0026#34;$@\u0026#34; è·¨å¹³å°å¤‡ä»½ç®¡ç†ç³»ç»Ÿ 1. ç»Ÿä¸€å¤‡ä»½ç®¡ç†å™¨ #!/usr/bin/env python3 # scripts/unified_backup_manager.py import os import json import yaml import logging import subprocess import threading import time from datetime import datetime, timedelta from typing import Dict, List, Any, Optional from dataclasses import dataclass from enum import Enum import schedule import boto3 from azure.storage.blob import BlobServiceClient from google.cloud import storage as gcs class DatabaseType(Enum): MYSQL = \u0026#34;mysql\u0026#34; POSTGRESQL = \u0026#34;postgresql\u0026#34; MONGODB = \u0026#34;mongodb\u0026#34; REDIS = \u0026#34;redis\u0026#34; ELASTICSEARCH = \u0026#34;elasticsearch\u0026#34; class BackupType(Enum): FULL = \u0026#34;full\u0026#34; INCREMENTAL = \u0026#34;incremental\u0026#34; DIFFERENTIAL = \u0026#34;differential\u0026#34; LOG = \u0026#34;log\u0026#34; class BackupStatus(Enum): PENDING = \u0026#34;pending\u0026#34; RUNNING = \u0026#34;running\u0026#34; COMPLETED = \u0026#34;completed\u0026#34; FAILED = \u0026#34;failed\u0026#34; CANCELLED = \u0026#34;cancelled\u0026#34; @dataclass class BackupJob: id: str database_type: DatabaseType backup_type: BackupType database_name: str schedule: str retention_days: int compression: bool encryption: bool storage_locations: List[str] status: BackupStatus = BackupStatus.PENDING created_at: datetime = None started_at: datetime = None completed_at: datetime = None error_message: str = None backup_size: int = 0 backup_files: List[str] = None def __post_init__(self): if self.created_at is None: self.created_at = datetime.now() if self.backup_files is None: self.backup_files = [] class UnifiedBackupManager: def __init__(self, config_file: str): self.config = self._load_config(config_file) self.logger = self._setup_logging() self.jobs: Dict[str, BackupJob] = {} self.running_jobs: Dict[str, threading.Thread] = {} # åˆå§‹åŒ–äº‘å­˜å‚¨å®¢æˆ·ç«¯ self._init_cloud_storage() # å¯åŠ¨è°ƒåº¦å™¨ self.scheduler_thread = None self.stop_scheduler = False def _load_config(self, config_file: str) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;åŠ è½½é…ç½®æ–‡ä»¶\u0026#34;\u0026#34;\u0026#34; with open(config_file, \u0026#39;r\u0026#39;, encoding=\u0026#39;utf-8\u0026#39;) as f: if config_file.endswith(\u0026#39;.yaml\u0026#39;) or config_file.endswith(\u0026#39;.yml\u0026#39;): return yaml.safe_load(f) else: return json.load(f) def _setup_logging(self) -\u0026gt; logging.Logger: \u0026#34;\u0026#34;\u0026#34;è®¾ç½®æ—¥å¿—è®°å½•\u0026#34;\u0026#34;\u0026#34; logger = logging.getLogger(\u0026#39;UnifiedBackupManager\u0026#39;) logger.setLevel(logging.INFO) # æ–‡ä»¶å¤„ç†å™¨ file_handler = logging.FileHandler( self.config.get(\u0026#39;log_file\u0026#39;, \u0026#39;/var/log/unified_backup.log\u0026#39;) ) file_formatter = logging.Formatter( \u0026#39;%(asctime)s - %(name)s - %(levelname)s - %(message)s\u0026#39; ) file_handler.setFormatter(file_formatter) logger.addHandler(file_handler) # æ§åˆ¶å°å¤„ç†å™¨ console_handler = logging.StreamHandler() console_handler.setFormatter(file_formatter) logger.addHandler(console_handler) return logger def _init_cloud_storage(self): \u0026#34;\u0026#34;\u0026#34;åˆå§‹åŒ–äº‘å­˜å‚¨å®¢æˆ·ç«¯\u0026#34;\u0026#34;\u0026#34; self.cloud_clients = {} # AWS S3 if \u0026#39;aws\u0026#39; in self.config.get(\u0026#39;cloud_storage\u0026#39;, {}): aws_config = self.config[\u0026#39;cloud_storage\u0026#39;][\u0026#39;aws\u0026#39;] self.cloud_clients[\u0026#39;s3\u0026#39;] = boto3.client( \u0026#39;s3\u0026#39;, aws_access_key_id=aws_config.get(\u0026#39;access_key_id\u0026#39;), aws_secret_access_key=aws_config.get(\u0026#39;secret_access_key\u0026#39;), region_name=aws_config.get(\u0026#39;region\u0026#39;, \u0026#39;us-west-2\u0026#39;) ) # Azure Blob Storage if \u0026#39;azure\u0026#39; in self.config.get(\u0026#39;cloud_storage\u0026#39;, {}): azure_config = self.config[\u0026#39;cloud_storage\u0026#39;][\u0026#39;azure\u0026#39;] self.cloud_clients[\u0026#39;azure\u0026#39;] = BlobServiceClient( account_url=f\u0026#34;https://{azure_config[\u0026#39;account_name\u0026#39;]}.blob.core.windows.net\u0026#34;, credential=azure_config[\u0026#39;account_key\u0026#39;] ) # Google Cloud Storage if \u0026#39;gcp\u0026#39; in self.config.get(\u0026#39;cloud_storage\u0026#39;, {}): gcp_config = self.config[\u0026#39;cloud_storage\u0026#39;][\u0026#39;gcp\u0026#39;] os.environ[\u0026#39;GOOGLE_APPLICATION_CREDENTIALS\u0026#39;] = gcp_config[\u0026#39;credentials_file\u0026#39;] self.cloud_clients[\u0026#39;gcs\u0026#39;] = gcs.Client() def create_backup_job(self, job_config: Dict[str, Any]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºå¤‡ä»½ä»»åŠ¡\u0026#34;\u0026#34;\u0026#34; job_id = f\u0026#34;{job_config[\u0026#39;database_type\u0026#39;]}_{job_config[\u0026#39;database_name\u0026#39;]}_{int(time.time())}\u0026#34; job = BackupJob( id=job_id, database_type=DatabaseType(job_config[\u0026#39;database_type\u0026#39;]), backup_type=BackupType(job_config[\u0026#39;backup_type\u0026#39;]), database_name=job_config[\u0026#39;database_name\u0026#39;], schedule=job_config[\u0026#39;schedule\u0026#39;], retention_days=job_config.get(\u0026#39;retention_days\u0026#39;, 7), compression=job_config.get(\u0026#39;compression\u0026#39;, True), encryption=job_config.get(\u0026#39;encryption\u0026#39;, True), storage_locations=job_config.get(\u0026#39;storage_locations\u0026#39;, [\u0026#39;local\u0026#39;]) ) self.jobs[job_id] = job self.logger.info(f\u0026#34;å¤‡ä»½ä»»åŠ¡åˆ›å»ºæˆåŠŸ: {job_id}\u0026#34;) # æ³¨å†Œè°ƒåº¦ä»»åŠ¡ self._schedule_job(job) return job_id def _schedule_job(self, job: BackupJob): \u0026#34;\u0026#34;\u0026#34;æ³¨å†Œè°ƒåº¦ä»»åŠ¡\u0026#34;\u0026#34;\u0026#34; if job.schedule == \u0026#39;manual\u0026#39;: return # è§£æè°ƒåº¦è¡¨è¾¾å¼å¹¶æ³¨å†Œ if job.schedule.startswith(\u0026#39;cron:\u0026#39;): # ç®€åŒ–çš„cronæ”¯æŒ cron_expr = job.schedule[5:] # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„cronè§£æ schedule.every().day.at(\u0026#34;02:00\u0026#34;).do(self._execute_backup_job, job.id) elif job.schedule.startswith(\u0026#39;interval:\u0026#39;): # é—´éš”è°ƒåº¦ interval = job.schedule[9:] if interval.endswith(\u0026#39;h\u0026#39;): hours = int(interval[:-1]) schedule.every(hours).hours.do(self._execute_backup_job, job.id) elif interval.endswith(\u0026#39;d\u0026#39;): days = int(interval[:-1]) schedule.every(days).days.do(self._execute_backup_job, job.id) def execute_backup_job(self, job_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ‰§è¡Œå¤‡ä»½ä»»åŠ¡\u0026#34;\u0026#34;\u0026#34; if job_id not in self.jobs: self.logger.error(f\u0026#34;å¤‡ä»½ä»»åŠ¡ä¸å­˜åœ¨: {job_id}\u0026#34;) return False job = self.jobs[job_id] if job.status == BackupStatus.RUNNING: self.logger.warning(f\u0026#34;å¤‡ä»½ä»»åŠ¡æ­£åœ¨è¿è¡Œ: {job_id}\u0026#34;) return False # åœ¨æ–°çº¿ç¨‹ä¸­æ‰§è¡Œå¤‡ä»½ backup_thread = threading.Thread( target=self._execute_backup_job, args=(job_id,) ) backup_thread.start() self.running_jobs[job_id] = backup_thread return True def _execute_backup_job(self, job_id: str): \u0026#34;\u0026#34;\u0026#34;æ‰§è¡Œå¤‡ä»½ä»»åŠ¡çš„å†…éƒ¨æ–¹æ³•\u0026#34;\u0026#34;\u0026#34; job = self.jobs[job_id] job.status = BackupStatus.RUNNING job.started_at = datetime.now() self.logger.info(f\u0026#34;å¼€å§‹æ‰§è¡Œå¤‡ä»½ä»»åŠ¡: {job_id}\u0026#34;) try: # æ ¹æ®æ•°æ®åº“ç±»å‹æ‰§è¡Œç›¸åº”çš„å¤‡ä»½ if job.database_type == DatabaseType.MYSQL: backup_files = self._backup_mysql(job) elif job.database_type == DatabaseType.POSTGRESQL: backup_files = self._backup_postgresql(job) elif job.database_type == DatabaseType.MONGODB: backup_files = self._backup_mongodb(job) elif job.database_type == DatabaseType.REDIS: backup_files = self._backup_redis(job) elif job.database_type == DatabaseType.ELASTICSEARCH: backup_files = self._backup_elasticsearch(job) else: raise ValueError(f\u0026#34;ä¸æ”¯æŒçš„æ•°æ®åº“ç±»å‹: {job.database_type}\u0026#34;) job.backup_files = backup_files job.backup_size = sum(os.path.getsize(f) for f in backup_files if os.path.exists(f)) # ä¸Šä¼ åˆ°å­˜å‚¨ä½ç½® self._upload_backups(job) # æ¸…ç†è¿‡æœŸå¤‡ä»½ self._cleanup_old_backups(job) job.status = BackupStatus.COMPLETED job.completed_at = datetime.now() self.logger.info(f\u0026#34;å¤‡ä»½ä»»åŠ¡å®Œæˆ: {job_id}\u0026#34;) except Exception as e: job.status = BackupStatus.FAILED job.error_message = str(e) job.completed_at = datetime.now() self.logger.error(f\u0026#34;å¤‡ä»½ä»»åŠ¡å¤±è´¥: {job_id}, é”™è¯¯: {e}\u0026#34;) finally: # æ¸…ç†è¿è¡Œä¸­çš„ä»»åŠ¡è®°å½• if job_id in self.running_jobs: del self.running_jobs[job_id] def _backup_mysql(self, job: BackupJob) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;MySQLå¤‡ä»½\u0026#34;\u0026#34;\u0026#34; db_config = self.config[\u0026#39;databases\u0026#39;][\u0026#39;mysql\u0026#39;] backup_dir = os.path.join(self.config[\u0026#39;backup_base_dir\u0026#39;], \u0026#39;mysql\u0026#39;) os.makedirs(backup_dir, exist_ok=True) timestamp = datetime.now().strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;) backup_file = os.path.join(backup_dir, f\u0026#34;{job.database_name}_{timestamp}.sql\u0026#34;) # æ„å»ºmysqldumpå‘½ä»¤ cmd = [ \u0026#39;mysqldump\u0026#39;, f\u0026#34;--host={db_config[\u0026#39;host\u0026#39;]}\u0026#34;, f\u0026#34;--port={db_config[\u0026#39;port\u0026#39;]}\u0026#34;, f\u0026#34;--user={db_config[\u0026#39;user\u0026#39;]}\u0026#34;, f\u0026#34;--password={db_config[\u0026#39;password\u0026#39;]}\u0026#34;, \u0026#39;--single-transaction\u0026#39;, \u0026#39;--routines\u0026#39;, \u0026#39;--triggers\u0026#39;, \u0026#39;--events\u0026#39;, job.database_name ] # æ‰§è¡Œå¤‡ä»½ with open(backup_file, \u0026#39;w\u0026#39;) as f: result = subprocess.run(cmd, stdout=f, stderr=subprocess.PIPE, text=True) if result.returncode != 0: raise Exception(f\u0026#34;MySQLå¤‡ä»½å¤±è´¥: {result.stderr}\u0026#34;) backup_files = [backup_file] # å‹ç¼© if job.compression: compressed_file = f\u0026#34;{backup_file}.gz\u0026#34; subprocess.run([\u0026#39;gzip\u0026#39;, backup_file], check=True) backup_files = [compressed_file] # åŠ å¯† if job.encryption: encrypted_files = [] for file in backup_files: encrypted_file = f\u0026#34;{file}.enc\u0026#34; self._encrypt_file(file, encrypted_file) os.remove(file) encrypted_files.append(encrypted_file) backup_files = encrypted_files return backup_files def _backup_postgresql(self, job: BackupJob) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;PostgreSQLå¤‡ä»½\u0026#34;\u0026#34;\u0026#34; db_config = self.config[\u0026#39;databases\u0026#39;][\u0026#39;postgresql\u0026#39;] backup_dir = os.path.join(self.config[\u0026#39;backup_base_dir\u0026#39;], \u0026#39;postgresql\u0026#39;) os.makedirs(backup_dir, exist_ok=True) timestamp = datetime.now().strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;) backup_file = os.path.join(backup_dir, f\u0026#34;{job.database_name}_{timestamp}.dump\u0026#34;) # è®¾ç½®ç¯å¢ƒå˜é‡ env = os.environ.copy() env.update({ \u0026#39;PGHOST\u0026#39;: db_config[\u0026#39;host\u0026#39;], \u0026#39;PGPORT\u0026#39;: str(db_config[\u0026#39;port\u0026#39;]), \u0026#39;PGUSER\u0026#39;: db_config[\u0026#39;user\u0026#39;], \u0026#39;PGPASSWORD\u0026#39;: db_config[\u0026#39;password\u0026#39;] }) # æ„å»ºpg_dumpå‘½ä»¤ cmd = [ \u0026#39;pg_dump\u0026#39;, \u0026#39;--verbose\u0026#39;, \u0026#39;--format=custom\u0026#39;, \u0026#39;--compress=6\u0026#39;, \u0026#39;--no-owner\u0026#39;, \u0026#39;--no-privileges\u0026#39;, \u0026#39;--file\u0026#39;, backup_file, job.database_name ] # æ‰§è¡Œå¤‡ä»½ result = subprocess.run(cmd, env=env, stderr=subprocess.PIPE, text=True) if result.returncode != 0: raise Exception(f\u0026#34;PostgreSQLå¤‡ä»½å¤±è´¥: {result.stderr}\u0026#34;) backup_files = [backup_file] # åŠ å¯† if job.encryption: encrypted_file = f\u0026#34;{backup_file}.enc\u0026#34; self._encrypt_file(backup_file, encrypted_file) os.remove(backup_file) backup_files = [encrypted_file] return backup_files def _backup_mongodb(self, job: BackupJob) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;MongoDBå¤‡ä»½\u0026#34;\u0026#34;\u0026#34; db_config = self.config[\u0026#39;databases\u0026#39;][\u0026#39;mongodb\u0026#39;] backup_dir = os.path.join(self.config[\u0026#39;backup_base_dir\u0026#39;], \u0026#39;mongodb\u0026#39;) os.makedirs(backup_dir, exist_ok=True) timestamp = datetime.now().strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;) backup_path = os.path.join(backup_dir, f\u0026#34;{job.database_name}_{timestamp}\u0026#34;) # æ„å»ºmongodumpå‘½ä»¤ cmd = [ \u0026#39;mongodump\u0026#39;, \u0026#39;--host\u0026#39;, f\u0026#34;{db_config[\u0026#39;host\u0026#39;]}:{db_config[\u0026#39;port\u0026#39;]}\u0026#34;, \u0026#39;--db\u0026#39;, job.database_name, \u0026#39;--out\u0026#39;, backup_path ] if \u0026#39;username\u0026#39; in db_config: cmd.extend([\u0026#39;--username\u0026#39;, db_config[\u0026#39;username\u0026#39;]]) cmd.extend([\u0026#39;--password\u0026#39;, db_config[\u0026#39;password\u0026#39;]]) # æ‰§è¡Œå¤‡ä»½ result = subprocess.run(cmd, stderr=subprocess.PIPE, text=True) if result.returncode != 0: raise Exception(f\u0026#34;MongoDBå¤‡ä»½å¤±è´¥: {result.stderr}\u0026#34;) # å‹ç¼©å¤‡ä»½ç›®å½• archive_file = f\u0026#34;{backup_path}.tar.gz\u0026#34; subprocess.run([\u0026#39;tar\u0026#39;, \u0026#39;-czf\u0026#39;, archive_file, \u0026#39;-C\u0026#39;, backup_dir, os.path.basename(backup_path)], check=True) # åˆ é™¤åŸå§‹ç›®å½• subprocess.run([\u0026#39;rm\u0026#39;, \u0026#39;-rf\u0026#39;, backup_path], check=True) backup_files = [archive_file] # åŠ å¯† if job.encryption: encrypted_file = f\u0026#34;{archive_file}.enc\u0026#34; self._encrypt_file(archive_file, encrypted_file) os.remove(archive_file) backup_files = [encrypted_file] return backup_files def _backup_redis(self, job: BackupJob) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;Rediså¤‡ä»½\u0026#34;\u0026#34;\u0026#34; db_config = self.config[\u0026#39;databases\u0026#39;][\u0026#39;redis\u0026#39;] backup_dir = os.path.join(self.config[\u0026#39;backup_base_dir\u0026#39;], \u0026#39;redis\u0026#39;) os.makedirs(backup_dir, exist_ok=True) timestamp = datetime.now().strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;) backup_file = os.path.join(backup_dir, f\u0026#34;redis_{timestamp}.rdb\u0026#34;) # ä½¿ç”¨redis-cliæ‰§è¡ŒBGSAVE cmd = [ \u0026#39;redis-cli\u0026#39;, \u0026#39;-h\u0026#39;, db_config[\u0026#39;host\u0026#39;], \u0026#39;-p\u0026#39;, str(db_config[\u0026#39;port\u0026#39;]), \u0026#39;BGSAVE\u0026#39; ] if \u0026#39;password\u0026#39; in db_config: cmd.extend([\u0026#39;-a\u0026#39;, db_config[\u0026#39;password\u0026#39;]]) # æ‰§è¡Œåå°ä¿å­˜ result = subprocess.run(cmd, capture_output=True, text=True) if result.returncode != 0: raise Exception(f\u0026#34;Redis BGSAVEå¤±è´¥: {result.stderr}\u0026#34;) # ç­‰å¾…å¤‡ä»½å®Œæˆ time.sleep(5) # å¤åˆ¶RDBæ–‡ä»¶ rdb_source = db_config.get(\u0026#39;rdb_file\u0026#39;, \u0026#39;/var/lib/redis/dump.rdb\u0026#39;) subprocess.run([\u0026#39;cp\u0026#39;, rdb_source, backup_file], check=True) backup_files = [backup_file] # å‹ç¼©å’ŒåŠ å¯† if job.compression: compressed_file = f\u0026#34;{backup_file}.gz\u0026#34; subprocess.run([\u0026#39;gzip\u0026#39;, backup_file], check=True) backup_files = [compressed_file] if job.encryption: encrypted_files = [] for file in backup_files: encrypted_file = f\u0026#34;{file}.enc\u0026#34; self._encrypt_file(file, encrypted_file) os.remove(file) encrypted_files.append(encrypted_file) backup_files = encrypted_files return backup_files def _backup_elasticsearch(self, job: BackupJob) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;Elasticsearchå¤‡ä»½\u0026#34;\u0026#34;\u0026#34; # è¿™é‡Œå®ç°Elasticsearchå¿«ç…§å¤‡ä»½ # ç”±äºç¯‡å¹…é™åˆ¶ï¼Œè¿™é‡Œåªæ˜¯ä¸€ä¸ªç®€åŒ–çš„å®ç° backup_dir = os.path.join(self.config[\u0026#39;backup_base_dir\u0026#39;], \u0026#39;elasticsearch\u0026#39;) os.makedirs(backup_dir, exist_ok=True) timestamp = datetime.now().strftime(\u0026#39;%Y%m%d_%H%M%S\u0026#39;) snapshot_name = f\u0026#34;{job.database_name}_{timestamp}\u0026#34; # è¿™é‡Œåº”è¯¥è°ƒç”¨Elasticsearch APIåˆ›å»ºå¿«ç…§ # ç®€åŒ–å®ç°ï¼Œè¿”å›ç©ºåˆ—è¡¨ return [] def _encrypt_file(self, input_file: str, output_file: str): \u0026#34;\u0026#34;\u0026#34;åŠ å¯†æ–‡ä»¶\u0026#34;\u0026#34;\u0026#34; encryption_key = self.config.get(\u0026#39;encryption_key\u0026#39;, \u0026#39;default_key\u0026#39;) cmd = [ \u0026#39;openssl\u0026#39;, \u0026#39;enc\u0026#39;, \u0026#39;-aes-256-cbc\u0026#39;, \u0026#39;-salt\u0026#39;, \u0026#39;-in\u0026#39;, input_file, \u0026#39;-out\u0026#39;, output_file, \u0026#39;-k\u0026#39;, encryption_key ] result = subprocess.run(cmd, capture_output=True, text=True) if result.returncode != 0: raise Exception(f\u0026#34;æ–‡ä»¶åŠ å¯†å¤±è´¥: {result.stderr}\u0026#34;) def _upload_backups(self, job: BackupJob): \u0026#34;\u0026#34;\u0026#34;ä¸Šä¼ å¤‡ä»½æ–‡ä»¶åˆ°å­˜å‚¨ä½ç½®\u0026#34;\u0026#34;\u0026#34; for location in job.storage_locations: if location == \u0026#39;local\u0026#39;: continue # æœ¬åœ°å­˜å‚¨æ— éœ€ä¸Šä¼  elif location.startswith(\u0026#39;s3://\u0026#39;): self._upload_to_s3(job, location) elif location.startswith(\u0026#39;azure://\u0026#39;): self._upload_to_azure(job, location) elif location.startswith(\u0026#39;gcs://\u0026#39;): self._upload_to_gcs(job, location) def _upload_to_s3(self, job: BackupJob, s3_url: str): \u0026#34;\u0026#34;\u0026#34;ä¸Šä¼ åˆ°AWS S3\u0026#34;\u0026#34;\u0026#34; if \u0026#39;s3\u0026#39; not in self.cloud_clients: self.logger.warning(\u0026#34;S3å®¢æˆ·ç«¯æœªé…ç½®\u0026#34;) return bucket_name = s3_url.replace(\u0026#39;s3://\u0026#39;, \u0026#39;\u0026#39;).split(\u0026#39;/\u0026#39;)[0] s3_prefix = \u0026#39;/\u0026#39;.join(s3_url.replace(\u0026#39;s3://\u0026#39;, \u0026#39;\u0026#39;).split(\u0026#39;/\u0026#39;)[1:]) s3_client = self.cloud_clients[\u0026#39;s3\u0026#39;] for backup_file in job.backup_files: file_name = os.path.basename(backup_file) s3_key = f\u0026#34;{s3_prefix}/{job.database_type.value}/{file_name}\u0026#34; try: s3_client.upload_file(backup_file, bucket_name, s3_key) self.logger.info(f\u0026#34;æ–‡ä»¶ä¸Šä¼ åˆ°S3æˆåŠŸ: {s3_key}\u0026#34;) except Exception as e: self.logger.error(f\u0026#34;S3ä¸Šä¼ å¤±è´¥: {e}\u0026#34;) def _upload_to_azure(self, job: BackupJob, azure_url: str): \u0026#34;\u0026#34;\u0026#34;ä¸Šä¼ åˆ°Azure Blob Storage\u0026#34;\u0026#34;\u0026#34; # å®ç°Azureä¸Šä¼ é€»è¾‘ pass def _upload_to_gcs(self, job: BackupJob, gcs_url: str): \u0026#34;\u0026#34;\u0026#34;ä¸Šä¼ åˆ°Google Cloud Storage\u0026#34;\u0026#34;\u0026#34; # å®ç°GCSä¸Šä¼ é€»è¾‘ pass def _cleanup_old_backups(self, job: BackupJob): \u0026#34;\u0026#34;\u0026#34;æ¸…ç†è¿‡æœŸå¤‡ä»½\u0026#34;\u0026#34;\u0026#34; cutoff_date = datetime.now() - timedelta(days=job.retention_days) # æ¸…ç†æœ¬åœ°å¤‡ä»½ backup_dir = os.path.join( self.config[\u0026#39;backup_base_dir\u0026#39;], job.database_type.value ) if os.path.exists(backup_dir): for file in os.listdir(backup_dir): file_path = os.path.join(backup_dir, file) if os.path.isfile(file_path): file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path)) if file_mtime \u0026lt; cutoff_date: os.remove(file_path) self.logger.info(f\u0026#34;åˆ é™¤è¿‡æœŸå¤‡ä»½: {file_path}\u0026#34;) def get_job_status(self, job_id: str) -\u0026gt; Optional[BackupJob]: \u0026#34;\u0026#34;\u0026#34;è·å–ä»»åŠ¡çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; return self.jobs.get(job_id) def list_jobs(self) -\u0026gt; List[BackupJob]: \u0026#34;\u0026#34;\u0026#34;åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡\u0026#34;\u0026#34;\u0026#34; return list(self.jobs.values()) def cancel_job(self, job_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;å–æ¶ˆä»»åŠ¡\u0026#34;\u0026#34;\u0026#34; if job_id not in self.jobs: return False job = self.jobs[job_id] if job.status == BackupStatus.RUNNING: # å°è¯•åœæ­¢è¿è¡Œä¸­çš„çº¿ç¨‹ if job_id in self.running_jobs: # æ³¨æ„ï¼šPythonçº¿ç¨‹æ— æ³•å¼ºåˆ¶åœæ­¢ï¼Œè¿™é‡Œåªæ˜¯æ ‡è®° job.status = BackupStatus.CANCELLED return True return False def start_scheduler(self): \u0026#34;\u0026#34;\u0026#34;å¯åŠ¨è°ƒåº¦å™¨\u0026#34;\u0026#34;\u0026#34; def run_scheduler(): while not self.stop_scheduler: schedule.run_pending() time.sleep(60) # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡ self.scheduler_thread = threading.Thread(target=run_scheduler) self.scheduler_thread.start() self.logger.info(\u0026#34;å¤‡ä»½è°ƒåº¦å™¨å·²å¯åŠ¨\u0026#34;) def stop_scheduler(self): \u0026#34;\u0026#34;\u0026#34;åœæ­¢è°ƒåº¦å™¨\u0026#34;\u0026#34;\u0026#34; self.stop_scheduler = True if self.scheduler_thread: self.scheduler_thread.join() self.logger.info(\u0026#34;å¤‡ä»½è°ƒåº¦å™¨å·²åœæ­¢\u0026#34;) def main(): import argparse parser = argparse.ArgumentParser(description=\u0026#39;ç»Ÿä¸€å¤‡ä»½ç®¡ç†å™¨\u0026#39;) parser.add_argument(\u0026#39;--config\u0026#39;, required=True, help=\u0026#39;é…ç½®æ–‡ä»¶è·¯å¾„\u0026#39;) parser.add_argument(\u0026#39;--action\u0026#39;, choices=[\u0026#39;start\u0026#39;, \u0026#39;create\u0026#39;, \u0026#39;execute\u0026#39;, \u0026#39;status\u0026#39;, \u0026#39;list\u0026#39;], default=\u0026#39;start\u0026#39;, help=\u0026#39;æ‰§è¡Œçš„æ“ä½œ\u0026#39;) parser.add_argument(\u0026#39;--job-config\u0026#39;, help=\u0026#39;ä»»åŠ¡é…ç½®æ–‡ä»¶ï¼ˆç”¨äºcreateæ“ä½œï¼‰\u0026#39;) parser.add_argument(\u0026#39;--job-id\u0026#39;, help=\u0026#39;ä»»åŠ¡IDï¼ˆç”¨äºexecuteå’Œstatusæ“ä½œï¼‰\u0026#39;) args = parser.parse_args() # åˆ›å»ºå¤‡ä»½ç®¡ç†å™¨ manager = UnifiedBackupManager(args.config) if args.action == \u0026#39;start\u0026#39;: # å¯åŠ¨è°ƒåº¦å™¨ manager.start_scheduler() try: while True: time.sleep(1) except KeyboardInterrupt: manager.stop_scheduler() elif args.action == \u0026#39;create\u0026#39;: if not args.job_config: print(\u0026#34;é”™è¯¯ï¼šéœ€è¦æä¾›ä»»åŠ¡é…ç½®æ–‡ä»¶\u0026#34;) exit(1) with open(args.job_config, \u0026#39;r\u0026#39;) as f: job_config = json.load(f) job_id = manager.create_backup_job(job_config) print(f\u0026#34;ä»»åŠ¡åˆ›å»ºæˆåŠŸï¼ŒID: {job_id}\u0026#34;) elif args.action == \u0026#39;execute\u0026#39;: if not args.job_id: print(\u0026#34;é”™è¯¯ï¼šéœ€è¦æä¾›ä»»åŠ¡ID\u0026#34;) exit(1) success = manager.execute_backup_job(args.job_id) if success: print(f\u0026#34;ä»»åŠ¡ {args.job_id} å¼€å§‹æ‰§è¡Œ\u0026#34;) else: print(f\u0026#34;ä»»åŠ¡ {args.job_id} æ‰§è¡Œå¤±è´¥\u0026#34;) elif args.action == \u0026#39;status\u0026#39;: if not args.job_id: print(\u0026#34;é”™è¯¯ï¼šéœ€è¦æä¾›ä»»åŠ¡ID\u0026#34;) exit(1) job = manager.get_job_status(args.job_id) if job: print(f\u0026#34;ä»»åŠ¡çŠ¶æ€: {job.status.value}\u0026#34;) print(f\u0026#34;åˆ›å»ºæ—¶é—´: {job.created_at}\u0026#34;) if job.started_at: print(f\u0026#34;å¼€å§‹æ—¶é—´: {job.started_at}\u0026#34;) if job.completed_at: print(f\u0026#34;å®Œæˆæ—¶é—´: {job.completed_at}\u0026#34;) if job.error_message: print(f\u0026#34;é”™è¯¯ä¿¡æ¯: {job.error_message}\u0026#34;) else: print(f\u0026#34;ä»»åŠ¡ {args.job_id} ä¸å­˜åœ¨\u0026#34;) elif args.action == \u0026#39;list\u0026#39;: jobs = manager.list_jobs() print(f\u0026#34;å…±æœ‰ {len(jobs)} ä¸ªä»»åŠ¡:\u0026#34;) for job in jobs: print(f\u0026#34; {job.id}: {job.database_type.value}/{job.database_name} - {job.status.value}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: main() æ€»ç»“ æœ¬æ–‡æ·±å…¥æ¢è®¨äº†æ•°æ®åº“å¤‡ä»½ä¸æ¢å¤çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œæ¶µç›–äº†ä»¥ä¸‹æ ¸å¿ƒå†…å®¹ï¼š\n","content":"æ•°æ®åº“å¤‡ä»½ä¸æ¢å¤ç­–ç•¥ï¼šä»åŸºç¡€å¤‡ä»½åˆ°ç¾éš¾æ¢å¤çš„å®Œæ•´æ–¹æ¡ˆ æ•°æ®åº“å¤‡ä»½ä¸æ¢å¤æ˜¯æ•°æ®åº“ç®¡ç†ä¸­æœ€å…³é”®çš„ç¯èŠ‚ä¹‹ä¸€ï¼Œç›´æ¥å…³ç³»åˆ°æ•°æ®çš„å®‰å…¨æ€§å’Œä¸šåŠ¡çš„è¿ç»­æ€§ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨ç°ä»£æ•°æ®åº“å¤‡ä»½ä¸æ¢å¤çš„æœ€ä½³å®è·µï¼Œä»åŸºç¡€çš„å¤‡ä»½ç­–ç•¥åˆ°å¤æ‚çš„ç¾éš¾æ¢å¤æ–¹æ¡ˆã€‚\nå¤‡ä»½ç­–ç•¥è®¾è®¡ä¸è§„åˆ’ 1. å¤‡ä»½ç­–ç•¥æ¶æ„ graph TB A[å¤‡ä»½ç­–ç•¥è§„åˆ’] --\u0026amp;gt; B[å¤‡ä»½ç±»å‹é€‰æ‹©] A --\u0026amp;gt; C[å¤‡ä»½é¢‘ç‡è®¾è®¡] A --\u0026amp;gt; D[å­˜å‚¨ç­–ç•¥è§„åˆ’] B --\u0026amp;gt; E[å…¨é‡å¤‡ä»½] B --\u0026amp;gt; F[å¢é‡å¤‡ä»½] B --\u0026amp;gt; G[å·®å¼‚å¤‡ä»½] B --\u0026amp;gt; H[æ—¥å¿—å¤‡ä»½] C --\u0026amp;gt; I[å®æ—¶å¤‡ä»½] C --\u0026amp;gt; J[å®šæ—¶å¤‡ä»½] C --\u0026amp;gt; K[è§¦å‘å¤‡ä»½] D --\u0026amp;gt; L[æœ¬åœ°å­˜å‚¨] D --\u0026amp;gt; M[è¿œç¨‹å­˜å‚¨] D --\u0026amp;gt; N[äº‘å­˜å‚¨] D --\u0026amp;gt; O[å¤šåœ°å¤‡ä»½] E --\u0026amp;gt; P[å®Œæ•´æ•°æ®å‰¯æœ¬] F --\u0026amp;gt; Q[å˜æ›´æ•°æ®] G --\u0026amp;gt; R[å·®å¼‚æ•°æ®] H --\u0026amp;gt; S[äº‹åŠ¡æ—¥å¿—] P --\u0026amp;gt; T[æ¢å¤åŸºå‡†ç‚¹] Q --\u0026amp;gt; T R --\u0026amp;gt; â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["æ•°æ®åº“","å¤‡ä»½","æ¢å¤","ç¾éš¾æ¢å¤","MySQL","PostgreSQL","MongoDB"],"categories":["æ•°æ®åº“"],"author":"æ•°æ®åº“ä¸“å®¶","readingTime":27,"wordCount":5687,"section":"posts","type":"posts","draft":false,"featured":false,"series":null},{"title":"å¾®æœåŠ¡æ¶æ„åœ¨äº‘ç¯å¢ƒä¸­çš„è®¾è®¡æ¨¡å¼ï¼šæ„å»ºå¯æ‰©å±•ã€å¼¹æ€§çš„åˆ†å¸ƒå¼ç³»ç»Ÿ","url":"https://www.dishuihengxin.com/posts/microservices-cloud-architecture/","summary":"ç›®å½• å¾®æœåŠ¡æ¶æ„æ¦‚è¿° æœåŠ¡æ‹†åˆ†ä¸è®¾è®¡åŸåˆ™ æœåŠ¡é€šä¿¡æ¨¡å¼ æ•°æ®ç®¡ç†ç­–ç•¥ æœåŠ¡å‘ç°ä¸æ³¨å†Œ APIç½‘å…³è®¾è®¡ æœåŠ¡ç½‘æ ¼æ¶æ„ ç›‘æ§ä¸å¯è§‚æµ‹æ€§ éƒ¨ç½²ä¸è¿ç»´ æ€»ç»“ å¾®æœåŠ¡æ¶æ„æ¦‚è¿° å¾®æœåŠ¡æ¶æ„æ˜¯ä¸€ç§å°†å•ä¸€åº”ç”¨ç¨‹åºå¼€å‘ä¸ºä¸€å¥—å°å‹æœåŠ¡çš„æ–¹æ³•ï¼Œæ¯ä¸ªæœåŠ¡è¿è¡Œåœ¨è‡ªå·±çš„è¿›ç¨‹ä¸­ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§æœºåˆ¶ï¼ˆé€šå¸¸æ˜¯HTTP APIï¼‰è¿›è¡Œé€šä¿¡ã€‚\nå¾®æœåŠ¡æ¶æ„å›¾ graph TB subgraph \u0026#34;å®¢æˆ·ç«¯å±‚\u0026#34; Web[Webåº”ç”¨] Mobile[ç§»åŠ¨åº”ç”¨] API[ç¬¬ä¸‰æ–¹API] end subgraph \u0026#34;APIç½‘å…³å±‚\u0026#34; Gateway[APIç½‘å…³] LB[è´Ÿè½½å‡è¡¡å™¨] end subgraph \u0026#34;å¾®æœåŠ¡å±‚\u0026#34; UserService[ç”¨æˆ·æœåŠ¡] OrderService[è®¢å•æœåŠ¡] PaymentService[æ”¯ä»˜æœåŠ¡] ProductService[å•†å“æœåŠ¡] NotificationService[é€šçŸ¥æœåŠ¡] end subgraph \u0026#34;æ•°æ®å±‚\u0026#34; UserDB[(ç”¨æˆ·æ•°æ®åº“)] OrderDB[(è®¢å•æ•°æ®åº“)] PaymentDB[(æ”¯ä»˜æ•°æ®åº“)] ProductDB[(å•†å“æ•°æ®åº“)] Cache[(ç¼“å­˜å±‚)] end subgraph \u0026#34;åŸºç¡€è®¾æ–½å±‚\u0026#34; ServiceRegistry[æœåŠ¡æ³¨å†Œä¸­å¿ƒ] ConfigCenter[é…ç½®ä¸­å¿ƒ] MessageQueue[æ¶ˆæ¯é˜Ÿåˆ—] Monitoring[ç›‘æ§ç³»ç»Ÿ] end Web --\u0026gt; Gateway Mobile --\u0026gt; Gateway API --\u0026gt; Gateway Gateway --\u0026gt; LB LB --\u0026gt; UserService LB --\u0026gt; OrderService LB --\u0026gt; PaymentService LB --\u0026gt; ProductService LB --\u0026gt; NotificationService UserService --\u0026gt; UserDB OrderService --\u0026gt; OrderDB PaymentService --\u0026gt; PaymentDB ProductService --\u0026gt; ProductDB UserService --\u0026gt; Cache OrderService --\u0026gt; Cache ProductService --\u0026gt; Cache UserService --\u0026gt; ServiceRegistry OrderService --\u0026gt; ServiceRegistry PaymentService --\u0026gt; ServiceRegistry ProductService --\u0026gt; ServiceRegistry NotificationService --\u0026gt; ServiceRegistry OrderService --\u0026gt; MessageQueue PaymentService --\u0026gt; MessageQueue NotificationService --\u0026gt; MessageQueue å¾®æœåŠ¡æ¶æ„ä¼˜åŠ¿ æŠ€æœ¯å¤šæ ·æ€§ï¼šæ¯ä¸ªæœåŠ¡å¯ä»¥ä½¿ç”¨æœ€é€‚åˆçš„æŠ€æœ¯æ ˆ ç‹¬ç«‹éƒ¨ç½²ï¼šæœåŠ¡å¯ä»¥ç‹¬ç«‹å¼€å‘ã€æµ‹è¯•å’Œéƒ¨ç½² æ•…éšœéš”ç¦»ï¼šå•ä¸ªæœåŠ¡çš„æ•…éšœä¸ä¼šå½±å“æ•´ä¸ªç³»ç»Ÿ å›¢é˜Ÿè‡ªæ²»ï¼šå°å›¢é˜Ÿå¯ä»¥ç‹¬ç«‹è´Ÿè´£ç‰¹å®šæœåŠ¡ å¯æ‰©å±•æ€§ï¼šå¯ä»¥æ ¹æ®éœ€æ±‚ç‹¬ç«‹æ‰©å±•ç‰¹å®šæœåŠ¡ æœåŠ¡æ‹†åˆ†ä¸è®¾è®¡åŸåˆ™ åˆç†çš„æœåŠ¡æ‹†åˆ†æ˜¯å¾®æœåŠ¡æ¶æ„æˆåŠŸçš„å…³é”®ã€‚\n","content":"ç›®å½• å¾®æœåŠ¡æ¶æ„æ¦‚è¿° æœåŠ¡æ‹†åˆ†ä¸è®¾è®¡åŸåˆ™ æœåŠ¡é€šä¿¡æ¨¡å¼ æ•°æ®ç®¡ç†ç­–ç•¥ æœåŠ¡å‘ç°ä¸æ³¨å†Œ APIç½‘å…³è®¾è®¡ æœåŠ¡ç½‘æ ¼æ¶æ„ ç›‘æ§ä¸å¯è§‚æµ‹æ€§ éƒ¨ç½²ä¸è¿ç»´ æ€»ç»“ å¾®æœåŠ¡æ¶æ„æ¦‚è¿° å¾®æœåŠ¡æ¶æ„æ˜¯ä¸€ç§å°†å•ä¸€åº”ç”¨ç¨‹åºå¼€å‘ä¸ºä¸€å¥—å°å‹æœåŠ¡çš„æ–¹æ³•ï¼Œæ¯ä¸ªæœåŠ¡è¿è¡Œåœ¨è‡ªå·±çš„è¿›ç¨‹ä¸­ï¼Œå¹¶ä½¿ç”¨è½»é‡çº§æœºåˆ¶ï¼ˆé€šå¸¸æ˜¯HTTP APIï¼‰è¿›è¡Œé€šä¿¡ã€‚\nå¾®æœåŠ¡æ¶æ„å›¾ graph TB subgraph \u0026amp;#34;å®¢æˆ·ç«¯å±‚\u0026amp;#34; Web[Webåº”ç”¨] Mobile[ç§»åŠ¨åº”ç”¨] API[ç¬¬ä¸‰æ–¹API] end subgraph \u0026amp;#34;APIç½‘å…³å±‚\u0026amp;#34; Gateway[APIç½‘å…³] LB[è´Ÿè½½å‡è¡¡å™¨] end subgraph \u0026amp;#34;å¾®æœåŠ¡å±‚\u0026amp;#34; UserService[ç”¨æˆ·æœåŠ¡] OrderService[è®¢å•æœåŠ¡] PaymentService[æ”¯ä»˜æœåŠ¡] ProductService[å•†å“æœåŠ¡] NotificationService[é€šçŸ¥æœåŠ¡] end subgraph \u0026amp;#34;æ•°æ®å±‚\u0026amp;#34; UserDB[(ç”¨æˆ·æ•°æ®åº“)] OrderDB[(è®¢å•æ•°æ®åº“)] PaymentDB[(æ”¯ä»˜æ•°æ®åº“)] â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["å¾®æœåŠ¡","äº‘æ¶æ„","åˆ†å¸ƒå¼ç³»ç»Ÿ","å®¹å™¨åŒ–","æœåŠ¡ç½‘æ ¼","APIç½‘å…³"],"categories":["äº‘æ¶æ„"],"author":"åšä¸»","readingTime":39,"wordCount":8126,"section":"posts","type":"posts","draft":false,"featured":false,"series":["äº‘æ¶æ„è®¾è®¡ä¸å®è·µ"]},{"title":"éšç§æ”¿ç­–","url":"https://www.dishuihengxin.com/privacy/","summary":"éšç§æ”¿ç­– æœ€åæ›´æ–°æ—¶é—´ï¼š2025å¹´12æœˆ31æ—¥\næ¬¢è¿è®¿é—®æœ¬æŠ€æœ¯åšå®¢ã€‚æˆ‘ä»¬é‡è§†æ‚¨çš„éšç§ï¼Œå¹¶è‡´åŠ›äºä¿æŠ¤æ‚¨çš„ä¸ªäººä¿¡æ¯ã€‚æœ¬éšç§æ”¿ç­–è¯´æ˜äº†æˆ‘ä»¬å¦‚ä½•æ”¶é›†ã€ä½¿ç”¨å’Œä¿æŠ¤æ‚¨çš„ä¿¡æ¯ã€‚\n1. ä¿¡æ¯æ”¶é›† 1.1 è‡ªåŠ¨æ”¶é›†çš„ä¿¡æ¯ å½“æ‚¨è®¿é—®æœ¬ç½‘ç«™æ—¶ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šè‡ªåŠ¨æ”¶é›†ä»¥ä¸‹ä¿¡æ¯ï¼š\nè®¿é—®æ—¥å¿—ï¼šIPåœ°å€ã€æµè§ˆå™¨ç±»å‹ã€æ“ä½œç³»ç»Ÿã€è®¿é—®æ—¶é—´ã€è®¿é—®é¡µé¢ Cookieä¿¡æ¯ï¼šç”¨äºæ”¹å–„ç”¨æˆ·ä½“éªŒçš„æŠ€æœ¯æ€§Cookie åˆ†ææ•°æ®ï¼šé€šè¿‡Google Analyticsç­‰å·¥å…·æ”¶é›†çš„åŒ¿åç»Ÿè®¡æ•°æ® 1.2 ä¸»åŠ¨æä¾›çš„ä¿¡æ¯ å¦‚æœæ‚¨é€‰æ‹©ï¼š\nè®¢é˜…é‚®ä»¶åˆ—è¡¨ï¼šæˆ‘ä»¬ä¼šæ”¶é›†æ‚¨çš„ç”µå­é‚®ä»¶åœ°å€ å‘è¡¨è¯„è®ºï¼šæˆ‘ä»¬ä¼šæ”¶é›†æ‚¨æä¾›çš„å§“åã€é‚®ç®±å’Œè¯„è®ºå†…å®¹ è”ç³»æˆ‘ä»¬ï¼šæˆ‘ä»¬ä¼šæ”¶é›†æ‚¨åœ¨è”ç³»è¡¨å•ä¸­æä¾›çš„ä¿¡æ¯ 2. ä¿¡æ¯ä½¿ç”¨ æˆ‘ä»¬æ”¶é›†çš„ä¿¡æ¯ç”¨äºï¼š\næ”¹å–„ç½‘ç«™å†…å®¹å’Œç”¨æˆ·ä½“éªŒ å‘é€è®¢é˜…å†…å®¹å’Œæ›´æ–°é€šçŸ¥ï¼ˆä»…é™è®¢é˜…ç”¨æˆ·ï¼‰ å›å¤æ‚¨çš„å’¨è¯¢å’Œé—®é¢˜ è¿›è¡Œç½‘ç«™åˆ†æå’Œä¼˜åŒ– é˜²æ­¢æ¬ºè¯ˆå’Œæ»¥ç”¨è¡Œä¸º 3. Cookieä½¿ç”¨ æœ¬ç½‘ç«™ä½¿ç”¨Cookieæ¥ï¼š\nå¿…è¦Cookieï¼šç¡®ä¿ç½‘ç«™æ­£å¸¸è¿è¡Œ åŠŸèƒ½Cookieï¼šè®°ä½æ‚¨çš„åå¥½è®¾ç½®ï¼ˆå¦‚è¯­è¨€é€‰æ‹©ï¼‰ åˆ†æCookieï¼šäº†è§£ç½‘ç«™ä½¿ç”¨æƒ…å†µï¼Œæ”¹è¿›ç”¨æˆ·ä½“éªŒ æ‚¨å¯ä»¥é€šè¿‡æµè§ˆå™¨è®¾ç½®ç®¡ç†æˆ–ç¦ç”¨Cookieï¼Œä½†è¿™å¯èƒ½ä¼šå½±å“ç½‘ç«™çš„æŸäº›åŠŸèƒ½ã€‚\n4. ç¬¬ä¸‰æ–¹æœåŠ¡ æœ¬ç½‘ç«™å¯èƒ½ä½¿ç”¨ä»¥ä¸‹ç¬¬ä¸‰æ–¹æœåŠ¡ï¼š\n4.1 Google Analytics ç”¨é€”ï¼šç½‘ç«™æµé‡åˆ†æ æ”¶é›†ä¿¡æ¯ï¼šåŒ¿åè®¿é—®æ•°æ® éšç§æ”¿ç­–ï¼šGoogle Analyticséšç§æ”¿ç­– 4.2 è¯„è®ºç³»ç»Ÿ å¦‚æœå¯ç”¨è¯„è®ºåŠŸèƒ½ï¼Œå¯èƒ½ä½¿ç”¨ç¬¬ä¸‰æ–¹è¯„è®ºç³»ç»Ÿï¼ˆå¦‚Disqusã€Utterancesç­‰ï¼‰ã€‚è¿™äº›æœåŠ¡æœ‰å„è‡ªçš„éšç§æ”¿ç­–ã€‚\n4.3 CDNæœåŠ¡ æœ¬ç½‘ç«™ä½¿ç”¨CDNï¼ˆå†…å®¹åˆ†å‘ç½‘ç»œï¼‰æ¥åŠ é€Ÿå†…å®¹ä¼ é€’ï¼Œè¿™äº›æœåŠ¡æä¾›å•†å¯èƒ½ä¼šæ”¶é›†è®¿é—®æ—¥å¿—ã€‚\n5. æ•°æ®å®‰å…¨ æˆ‘ä»¬é‡‡å–åˆç†çš„æŠ€æœ¯å’Œç»„ç»‡æªæ–½æ¥ä¿æŠ¤æ‚¨çš„ä¸ªäººä¿¡æ¯ï¼š\nä½¿ç”¨HTTPSåŠ å¯†ä¼ è¾“ å®šæœŸæ›´æ–°å®‰å…¨æªæ–½ é™åˆ¶å¯¹ä¸ªäººä¿¡æ¯çš„è®¿é—® ä¸ä¼šå‡ºå”®ã€äº¤æ˜“æˆ–å‡ºç§Ÿæ‚¨çš„ä¸ªäººä¿¡æ¯ 6. æ•°æ®ä¿ç•™ æˆ‘ä»¬ä¼šæ ¹æ®ä»¥ä¸‹åŸåˆ™ä¿ç•™æ‚¨çš„ä¿¡æ¯ï¼š\nè®¿é—®æ—¥å¿—ï¼šé€šå¸¸ä¿ç•™30-90å¤© è®¢é˜…ä¿¡æ¯ï¼šç›´åˆ°æ‚¨å–æ¶ˆè®¢é˜… è¯„è®ºå†…å®¹ï¼šæ°¸ä¹…ä¿ç•™ï¼ˆé™¤éæ‚¨è¦æ±‚åˆ é™¤ï¼‰ 7. æ‚¨çš„æƒåˆ© æ ¹æ®é€‚ç”¨çš„æ•°æ®ä¿æŠ¤æ³•å¾‹ï¼Œæ‚¨äº«æœ‰ä»¥ä¸‹æƒåˆ©ï¼š\nè®¿é—®æƒï¼šæŸ¥çœ‹æˆ‘ä»¬æŒæœ‰çš„å…³äºæ‚¨çš„ä¿¡æ¯ æ›´æ­£æƒï¼šæ›´æ­£ä¸å‡†ç¡®çš„ä¸ªäººä¿¡æ¯ åˆ é™¤æƒï¼šè¦æ±‚åˆ é™¤æ‚¨çš„ä¸ªäººä¿¡æ¯ åå¯¹æƒï¼šåå¯¹å¤„ç†æ‚¨çš„ä¸ªäººä¿¡æ¯ å¯ç§»æ¤æƒï¼šä»¥ç»“æ„åŒ–æ ¼å¼æ¥æ”¶æ‚¨çš„æ•°æ® å¦‚éœ€è¡Œä½¿è¿™äº›æƒåˆ©ï¼Œè¯·é€šè¿‡é¡µé¢åº•éƒ¨çš„è”ç³»æ–¹å¼ä¸æˆ‘ä»¬è”ç³»ã€‚\n","content":"éšç§æ”¿ç­– æœ€åæ›´æ–°æ—¶é—´ï¼š2025å¹´12æœˆ31æ—¥\næ¬¢è¿è®¿é—®æœ¬æŠ€æœ¯åšå®¢ã€‚æˆ‘ä»¬é‡è§†æ‚¨çš„éšç§ï¼Œå¹¶è‡´åŠ›äºä¿æŠ¤æ‚¨çš„ä¸ªäººä¿¡æ¯ã€‚æœ¬éšç§æ”¿ç­–è¯´æ˜äº†æˆ‘ä»¬å¦‚ä½•æ”¶é›†ã€ä½¿ç”¨å’Œä¿æŠ¤æ‚¨çš„ä¿¡æ¯ã€‚\n1. ä¿¡æ¯æ”¶é›† 1.1 è‡ªåŠ¨æ”¶é›†çš„ä¿¡æ¯ å½“æ‚¨è®¿é—®æœ¬ç½‘ç«™æ—¶ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šè‡ªåŠ¨æ”¶é›†ä»¥ä¸‹ä¿¡æ¯ï¼š\nè®¿é—®æ—¥å¿—ï¼šIPåœ°å€ã€æµè§ˆå™¨ç±»å‹ã€æ“ä½œç³»ç»Ÿã€è®¿é—®æ—¶é—´ã€è®¿é—®é¡µé¢ Cookieä¿¡æ¯ï¼šç”¨äºæ”¹å–„ç”¨æˆ·ä½“éªŒçš„æŠ€æœ¯æ€§Cookie åˆ†ææ•°æ®ï¼šé€šè¿‡Google Analyticsç­‰å·¥å…·æ”¶é›†çš„åŒ¿åç»Ÿè®¡æ•°æ® 1.2 ä¸»åŠ¨æä¾›çš„ä¿¡æ¯ å¦‚æœæ‚¨é€‰æ‹©ï¼š\nè®¢é˜…é‚®ä»¶åˆ—è¡¨ï¼šæˆ‘ä»¬ä¼šæ”¶é›†æ‚¨çš„ç”µå­é‚®ä»¶åœ°å€ å‘è¡¨è¯„è®ºï¼šæˆ‘ä»¬ä¼šæ”¶é›†æ‚¨æä¾›çš„å§“åã€é‚®ç®±å’Œè¯„è®ºå†…å®¹ è”ç³»æˆ‘ä»¬ï¼šæˆ‘ä»¬ä¼šæ”¶é›†æ‚¨åœ¨è”ç³»è¡¨å•ä¸­æä¾›çš„ä¿¡æ¯ 2. ä¿¡æ¯ä½¿ç”¨ æˆ‘ä»¬æ”¶é›†çš„ä¿¡æ¯ç”¨äºï¼š\næ”¹å–„ç½‘ç«™å†…å®¹å’Œç”¨æˆ·ä½“éªŒ å‘é€è®¢é˜…å†…å®¹å’Œæ›´æ–°é€šçŸ¥ï¼ˆä»…é™è®¢é˜…ç”¨æˆ·ï¼‰ å›å¤æ‚¨çš„å’¨è¯¢å’Œé—®é¢˜ è¿›è¡Œç½‘ç«™åˆ†æå’Œä¼˜åŒ– é˜²æ­¢æ¬ºè¯ˆå’Œæ»¥ç”¨è¡Œä¸º 3. Cookieä½¿ç”¨ æœ¬ç½‘ç«™ä½¿ç”¨Cookieæ¥ï¼š\nå¿…è¦Cookieï¼šç¡®ä¿ç½‘ç«™æ­£å¸¸è¿è¡Œ åŠŸèƒ½Cookieï¼šè®°ä½æ‚¨çš„åå¥½è®¾ç½®ï¼ˆå¦‚è¯­è¨€é€‰æ‹©ï¼‰ åˆ†æCookieï¼šäº†è§£ç½‘ç«™ä½¿ç”¨æƒ…å†µï¼Œæ”¹è¿›ç”¨æˆ·ä½“éªŒ æ‚¨å¯ä»¥é€šè¿‡æµè§ˆå™¨è®¾ç½®ç®¡ç†æˆ–ç¦ â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":null,"categories":null,"author":"åšä¸»","readingTime":1,"wordCount":92,"section":"","type":"page","draft":false,"featured":false,"series":null},{"title":"äº‘å®‰å…¨æ¶æ„è®¾è®¡ï¼šé›¶ä¿¡ä»»æ¨¡å‹çš„å®æ–½","url":"https://www.dishuihengxin.com/posts/zero-trust-cloud-security-architecture/","summary":"äº‘å®‰å…¨æ¶æ„è®¾è®¡ï¼šé›¶ä¿¡ä»»æ¨¡å‹çš„å®æ–½ å¼•è¨€ åœ¨ä¼ ç»Ÿçš„ç½‘ç»œå®‰å…¨æ¨¡å‹ä¸­ï¼Œä¼ä¸šé€šå¸¸é‡‡ç”¨\u0026quot;åŸå ¡å’ŒæŠ¤åŸæ²³\u0026quot;çš„é˜²æŠ¤ç­–ç•¥ï¼Œå³åœ¨ç½‘ç»œè¾¹ç•Œå»ºç«‹å¼ºå¤§çš„é˜²æŠ¤æªæ–½ï¼Œè€Œå¯¹å†…éƒ¨ç½‘ç»œç›¸å¯¹ä¿¡ä»»ã€‚ç„¶è€Œï¼Œéšç€äº‘è®¡ç®—ã€ç§»åŠ¨åŠå…¬ã€ç‰©è”ç½‘ç­‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œä¼ ç»Ÿçš„è¾¹ç•Œé˜²æŠ¤æ¨¡å¼å·²ç»æ— æ³•æ»¡è¶³ç°ä»£ä¼ä¸šçš„å®‰å…¨éœ€æ±‚ã€‚é›¶ä¿¡ä»»å®‰å…¨æ¨¡å‹åº”è¿è€Œç”Ÿï¼Œå®ƒåŸºäº\u0026quot;æ°¸ä¸ä¿¡ä»»ï¼Œå§‹ç»ˆéªŒè¯\u0026quot;çš„æ ¸å¿ƒç†å¿µï¼Œä¸ºäº‘ç¯å¢ƒæä¾›äº†æ›´åŠ å®‰å…¨ã€çµæ´»çš„é˜²æŠ¤æ–¹æ¡ˆã€‚\nç›®å½• é›¶ä¿¡ä»»å®‰å…¨æ¨¡å‹æ¦‚è¿° é›¶ä¿¡ä»»æ¶æ„æ ¸å¿ƒç»„ä»¶ èº«ä»½ä¸è®¿é—®ç®¡ç† ç½‘ç»œå¾®åˆ†æ®µ æ•°æ®ä¿æŠ¤ä¸åŠ å¯† è®¾å¤‡ä¿¡ä»»ä¸ç®¡ç† åº”ç”¨å®‰å…¨ ç›‘æ§ä¸åˆ†æ å®æ–½ç­–ç•¥ä¸æœ€ä½³å®è·µ é›¶ä¿¡ä»»å®‰å…¨æ¨¡å‹æ¦‚è¿° é›¶ä¿¡ä»»å®‰å…¨æ¨¡å‹æ˜¯ä¸€ç§ç½‘ç»œå®‰å…¨èŒƒå¼ï¼Œå®ƒå‡è®¾ç½‘ç»œå†…å¤–éƒ½å­˜åœ¨å¨èƒï¼Œå› æ­¤ä¸ä¼šè‡ªåŠ¨ä¿¡ä»»ä»»ä½•ç”¨æˆ·ã€è®¾å¤‡æˆ–åº”ç”¨ç¨‹åºã€‚è¯¥æ¨¡å‹è¦æ±‚å¯¹æ¯ä¸ªè®¿é—®è¯·æ±‚è¿›è¡Œä¸¥æ ¼çš„èº«ä»½éªŒè¯å’Œæˆæƒï¼Œæ— è®ºè¯·æ±‚æ¥è‡ªç½‘ç»œå†…éƒ¨è¿˜æ˜¯å¤–éƒ¨ã€‚\né›¶ä¿¡ä»»æ¶æ„å›¾ graph TB subgraph \u0026#34;é›¶ä¿¡ä»»äº‘å®‰å…¨æ¶æ„\u0026#34; subgraph \u0026#34;ç”¨æˆ·å±‚\u0026#34; U1[å†…éƒ¨ç”¨æˆ·] U2[å¤–éƒ¨ç”¨æˆ·] U3[åˆä½œä¼™ä¼´] U4[ç§»åŠ¨è®¾å¤‡] end subgraph \u0026#34;èº«ä»½ä¸è®¿é—®ç®¡ç†å±‚\u0026#34; IAM[èº«ä»½è®¤è¯ä¸­å¿ƒ] MFA[å¤šå› å­è®¤è¯] PAM[ç‰¹æƒè®¿é—®ç®¡ç†] RBAC[åŸºäºè§’è‰²çš„è®¿é—®æ§åˆ¶] end subgraph \u0026#34;ç­–ç•¥å¼•æ“å±‚\u0026#34; PE[ç­–ç•¥å¼•æ“] RA[é£é™©è¯„ä¼°] AD[å¼‚å¸¸æ£€æµ‹] ML[æœºå™¨å­¦ä¹ åˆ†æ] end subgraph \u0026#34;ç½‘ç»œå®‰å…¨å±‚\u0026#34; SDP[è½¯ä»¶å®šä¹‰è¾¹ç•Œ] MSG[å¾®åˆ†æ®µç½‘å…³] FW[ä¸‹ä¸€ä»£é˜²ç«å¢™] IPS[å…¥ä¾µé˜²æŠ¤ç³»ç»Ÿ] end subgraph \u0026#34;æ•°æ®ä¿æŠ¤å±‚\u0026#34; DLP[æ•°æ®é˜²æ³„æ¼] ENC[ç«¯åˆ°ç«¯åŠ å¯†] KMS[å¯†é’¥ç®¡ç†æœåŠ¡] CASB[äº‘è®¿é—®å®‰å…¨ä»£ç†] end subgraph \u0026#34;åº”ç”¨ä¸æœåŠ¡å±‚\u0026#34; API[APIç½‘å…³] MS[å¾®æœåŠ¡] DB[(æ•°æ®åº“)] FS[æ–‡ä»¶å­˜å‚¨] end subgraph \u0026#34;ç›‘æ§ä¸åˆ†æå±‚\u0026#34; SIEM[å®‰å…¨ä¿¡æ¯ä¸äº‹ä»¶ç®¡ç†] SOAR[å®‰å…¨ç¼–æ’ä¸è‡ªåŠ¨å“åº”] UBA[ç”¨æˆ·è¡Œä¸ºåˆ†æ] TI[å¨èƒæƒ…æŠ¥] end end U1 --\u0026gt; IAM U2 --\u0026gt; IAM U3 --\u0026gt; IAM U4 --\u0026gt; IAM IAM --\u0026gt; PE MFA --\u0026gt; PE PAM --\u0026gt; PE RBAC --\u0026gt; PE PE --\u0026gt; SDP RA --\u0026gt; MSG AD --\u0026gt; FW ML --\u0026gt; IPS SDP --\u0026gt; API MSG --\u0026gt; MS FW --\u0026gt; DB IPS --\u0026gt; FS DLP --\u0026gt; SIEM ENC --\u0026gt; SOAR KMS --\u0026gt; UBA CASB --\u0026gt; TI style IAM fill:#e1f5fe style PE fill:#f3e5f5 style SDP fill:#e8f5e8 style DLP fill:#fff3e0 style SIEM fill:#fce4ec é›¶ä¿¡ä»»æ ¸å¿ƒåŸåˆ™ æ°¸ä¸ä¿¡ä»»ï¼Œå§‹ç»ˆéªŒè¯ - å¯¹æ‰€æœ‰è®¿é—®è¯·æ±‚è¿›è¡ŒéªŒè¯ æœ€å°æƒé™åŸåˆ™ - ä»…æˆäºˆå®Œæˆä»»åŠ¡æ‰€éœ€çš„æœ€å°æƒé™ å‡è®¾è¿è§„ - å‡è®¾ç½‘ç»œå·²è¢«å…¥ä¾µï¼Œè®¾è®¡ç›¸åº”çš„é˜²æŠ¤æªæ–½ æ˜¾å¼éªŒè¯ - åŸºäºæ‰€æœ‰å¯ç”¨æ•°æ®ç‚¹è¿›è¡Œè®¿é—®å†³ç­– æŒç»­ç›‘æ§ - å®æ—¶ç›‘æ§å’Œåˆ†ææ‰€æœ‰æ´»åŠ¨ é›¶ä¿¡ä»»æ¶æ„æ ¸å¿ƒç»„ä»¶ é›¶ä¿¡ä»»æ¶æ„åˆ†æå™¨ #!/usr/bin/env python3 \u0026#34;\u0026#34;\u0026#34; é›¶ä¿¡ä»»æ¶æ„åˆ†æå™¨ è¯„ä¼°å’Œè®¾è®¡é›¶ä¿¡ä»»å®‰å…¨æ¶æ„ \u0026#34;\u0026#34;\u0026#34; import json import logging from typing import Dict, List, Any, Optional from dataclasses import dataclass, asdict from enum import Enum import hashlib import time from datetime import datetime, timedelta # é…ç½®æ—¥å¿— logging.basicConfig(level=logging.INFO) logger = logging.getLogger(__name__) class TrustLevel(Enum): \u0026#34;\u0026#34;\u0026#34;ä¿¡ä»»çº§åˆ«æšä¸¾\u0026#34;\u0026#34;\u0026#34; UNTRUSTED = 0 LOW = 1 MEDIUM = 2 HIGH = 3 VERIFIED = 4 class RiskLevel(Enum): \u0026#34;\u0026#34;\u0026#34;é£é™©çº§åˆ«æšä¸¾\u0026#34;\u0026#34;\u0026#34; LOW = 1 MEDIUM = 2 HIGH = 3 CRITICAL = 4 @dataclass class User: \u0026#34;\u0026#34;\u0026#34;ç”¨æˆ·å®ä½“\u0026#34;\u0026#34;\u0026#34; user_id: str username: str email: str roles: List[str] department: str trust_score: float last_login: datetime failed_attempts: int mfa_enabled: bool device_registered: bool @dataclass class Device: \u0026#34;\u0026#34;\u0026#34;è®¾å¤‡å®ä½“\u0026#34;\u0026#34;\u0026#34; device_id: str device_type: str os_version: str security_patch_level: str compliance_status: bool trust_score: float last_seen: datetime location: str owner: str @dataclass class AccessRequest: \u0026#34;\u0026#34;\u0026#34;è®¿é—®è¯·æ±‚\u0026#34;\u0026#34;\u0026#34; request_id: str user_id: str device_id: str resource: str action: str timestamp: datetime source_ip: str location: str user_agent: str @dataclass class PolicyRule: \u0026#34;\u0026#34;\u0026#34;ç­–ç•¥è§„åˆ™\u0026#34;\u0026#34;\u0026#34; rule_id: str name: str description: str conditions: Dict[str, Any] actions: List[str] priority: int enabled: bool class ZeroTrustArchitectureAnalyzer: \u0026#34;\u0026#34;\u0026#34;é›¶ä¿¡ä»»æ¶æ„åˆ†æå™¨\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.users: Dict[str, User] = {} self.devices: Dict[str, Device] = {} self.policies: Dict[str, PolicyRule] = {} self.access_logs: List[AccessRequest] = [] self.risk_scores: Dict[str, float] = {} def register_user(self, user: User) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ³¨å†Œç”¨æˆ·\u0026#34;\u0026#34;\u0026#34; try: # éªŒè¯ç”¨æˆ·ä¿¡æ¯ if not user.user_id or not user.email: logger.error(\u0026#34;ç”¨æˆ·IDå’Œé‚®ç®±ä¸èƒ½ä¸ºç©º\u0026#34;) return False # è®¡ç®—åˆå§‹ä¿¡ä»»åˆ†æ•° trust_score = self._calculate_user_trust_score(user) user.trust_score = trust_score self.users[user.user_id] = user logger.info(f\u0026#34;ç”¨æˆ· {user.username} æ³¨å†ŒæˆåŠŸï¼Œä¿¡ä»»åˆ†æ•°: {trust_score}\u0026#34;) return True except Exception as e: logger.error(f\u0026#34;ç”¨æˆ·æ³¨å†Œå¤±è´¥: {e}\u0026#34;) return False def register_device(self, device: Device) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ³¨å†Œè®¾å¤‡\u0026#34;\u0026#34;\u0026#34; try: # éªŒè¯è®¾å¤‡ä¿¡æ¯ if not device.device_id or not device.device_type: logger.error(\u0026#34;è®¾å¤‡IDå’Œç±»å‹ä¸èƒ½ä¸ºç©º\u0026#34;) return False # è®¡ç®—è®¾å¤‡ä¿¡ä»»åˆ†æ•° trust_score = self._calculate_device_trust_score(device) device.trust_score = trust_score self.devices[device.device_id] = device logger.info(f\u0026#34;è®¾å¤‡ {device.device_id} æ³¨å†ŒæˆåŠŸï¼Œä¿¡ä»»åˆ†æ•°: {trust_score}\u0026#34;) return True except Exception as e: logger.error(f\u0026#34;è®¾å¤‡æ³¨å†Œå¤±è´¥: {e}\u0026#34;) return False def create_policy(self, policy: PolicyRule) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºç­–ç•¥è§„åˆ™\u0026#34;\u0026#34;\u0026#34; try: if not policy.rule_id or not policy.name: logger.error(\u0026#34;ç­–ç•¥IDå’Œåç§°ä¸èƒ½ä¸ºç©º\u0026#34;) return False self.policies[policy.rule_id] = policy logger.info(f\u0026#34;ç­–ç•¥ {policy.name} åˆ›å»ºæˆåŠŸ\u0026#34;) return True except Exception as e: logger.error(f\u0026#34;ç­–ç•¥åˆ›å»ºå¤±è´¥: {e}\u0026#34;) return False def evaluate_access_request(self, request: AccessRequest) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è¯„ä¼°è®¿é—®è¯·æ±‚\u0026#34;\u0026#34;\u0026#34; try: # è·å–ç”¨æˆ·å’Œè®¾å¤‡ä¿¡æ¯ user = self.users.get(request.user_id) device = self.devices.get(request.device_id) if not user: return { \u0026#39;allowed\u0026#39;: False, \u0026#39;reason\u0026#39;: \u0026#39;ç”¨æˆ·æœªæ³¨å†Œ\u0026#39;, \u0026#39;risk_level\u0026#39;: RiskLevel.CRITICAL.name, \u0026#39;trust_score\u0026#39;: 0 } if not device: return { \u0026#39;allowed\u0026#39;: False, \u0026#39;reason\u0026#39;: \u0026#39;è®¾å¤‡æœªæ³¨å†Œ\u0026#39;, \u0026#39;risk_level\u0026#39;: RiskLevel.HIGH.name, \u0026#39;trust_score\u0026#39;: 0 } # è®¡ç®—ç»¼åˆé£é™©åˆ†æ•° risk_score = self._calculate_risk_score(request, user, device) # åº”ç”¨ç­–ç•¥è§„åˆ™ policy_result = self._apply_policies(request, user, device, risk_score) # è®°å½•è®¿é—®è¯·æ±‚ self.access_logs.append(request) # æ›´æ–°ç”¨æˆ·å’Œè®¾å¤‡ä¿¡ä»»åˆ†æ•° self._update_trust_scores(request, user, device, policy_result[\u0026#39;allowed\u0026#39;]) return { \u0026#39;allowed\u0026#39;: policy_result[\u0026#39;allowed\u0026#39;], \u0026#39;reason\u0026#39;: policy_result[\u0026#39;reason\u0026#39;], \u0026#39;risk_level\u0026#39;: self._get_risk_level(risk_score).name, \u0026#39;trust_score\u0026#39;: (user.trust_score + device.trust_score) / 2, \u0026#39;required_actions\u0026#39;: policy_result.get(\u0026#39;required_actions\u0026#39;, []), \u0026#39;session_timeout\u0026#39;: policy_result.get(\u0026#39;session_timeout\u0026#39;, 3600) } except Exception as e: logger.error(f\u0026#34;è®¿é—®è¯·æ±‚è¯„ä¼°å¤±è´¥: {e}\u0026#34;) return { \u0026#39;allowed\u0026#39;: False, \u0026#39;reason\u0026#39;: f\u0026#39;è¯„ä¼°å¤±è´¥: {e}\u0026#39;, \u0026#39;risk_level\u0026#39;: RiskLevel.CRITICAL.name, \u0026#39;trust_score\u0026#39;: 0 } def _calculate_user_trust_score(self, user: User) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;è®¡ç®—ç”¨æˆ·ä¿¡ä»»åˆ†æ•°\u0026#34;\u0026#34;\u0026#34; score = 50.0 # åŸºç¡€åˆ†æ•° # MFAå¯ç”¨åŠ åˆ† if user.mfa_enabled: score += 20 # è®¾å¤‡æ³¨å†ŒåŠ åˆ† if user.device_registered: score += 15 # è§’è‰²æƒé‡ if \u0026#39;admin\u0026#39; in user.roles: score += 10 elif \u0026#39;user\u0026#39; in user.roles: score += 5 # å¤±è´¥å°è¯•æ‰£åˆ† score -= user.failed_attempts * 5 # æœ€è¿‘ç™»å½•æ—¶é—´ if user.last_login: days_since_login = (datetime.now() - user.last_login).days if days_since_login \u0026gt; 30: score -= 10 elif days_since_login \u0026gt; 7: score -= 5 return max(0, min(100, score)) def _calculate_device_trust_score(self, device: Device) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;è®¡ç®—è®¾å¤‡ä¿¡ä»»åˆ†æ•°\u0026#34;\u0026#34;\u0026#34; score = 50.0 # åŸºç¡€åˆ†æ•° # åˆè§„çŠ¶æ€ if device.compliance_status: score += 25 else: score -= 20 # è®¾å¤‡ç±»å‹ if device.device_type in [\u0026#39;laptop\u0026#39;, \u0026#39;desktop\u0026#39;]: score += 10 elif device.device_type in [\u0026#39;mobile\u0026#39;, \u0026#39;tablet\u0026#39;]: score += 5 # å®‰å…¨è¡¥ä¸çº§åˆ« if device.security_patch_level == \u0026#39;latest\u0026#39;: score += 15 elif device.security_patch_level == \u0026#39;recent\u0026#39;: score += 10 elif device.security_patch_level == \u0026#39;outdated\u0026#39;: score -= 15 # æœ€è¿‘æ´»åŠ¨ if device.last_seen: hours_since_seen = (datetime.now() - device.last_seen).total_seconds() / 3600 if hours_since_seen \u0026gt; 168: # ä¸€å‘¨ score -= 10 elif hours_since_seen \u0026gt; 24: # ä¸€å¤© score -= 5 return max(0, min(100, score)) def _calculate_risk_score(self, request: AccessRequest, user: User, device: Device) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;è®¡ç®—è®¿é—®è¯·æ±‚é£é™©åˆ†æ•°\u0026#34;\u0026#34;\u0026#34; risk_score = 0.0 # ç”¨æˆ·é£é™©å› ç´  if user.failed_attempts \u0026gt; 3: risk_score += 30 if not user.mfa_enabled: risk_score += 25 # è®¾å¤‡é£é™©å› ç´  if not device.compliance_status: risk_score += 35 if device.security_patch_level == \u0026#39;outdated\u0026#39;: risk_score += 20 # è®¿é—®æ¨¡å¼é£é™© risk_score += self._analyze_access_pattern(request, user) # åœ°ç†ä½ç½®é£é™© risk_score += self._analyze_location_risk(request, user, device) # æ—¶é—´é£é™© risk_score += self._analyze_time_risk(request) return min(100, risk_score) def _analyze_access_pattern(self, request: AccessRequest, user: User) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;åˆ†æè®¿é—®æ¨¡å¼é£é™©\u0026#34;\u0026#34;\u0026#34; risk = 0.0 # è·å–ç”¨æˆ·å†å²è®¿é—®è®°å½• user_logs = [log for log in self.access_logs if log.user_id == user.user_id] if len(user_logs) \u0026lt; 5: # æ–°ç”¨æˆ·æˆ–è®¿é—®è®°å½•å°‘ risk += 10 else: # åˆ†æè®¿é—®é¢‘ç‡ recent_logs = [log for log in user_logs if (datetime.now() - log.timestamp).days \u0026lt;= 7] if len(recent_logs) \u0026gt; 50: # å¼‚å¸¸é«˜é¢‘è®¿é—® risk += 15 # åˆ†æè®¿é—®èµ„æºç±»å‹ accessed_resources = set(log.resource for log in recent_logs) if request.resource not in accessed_resources: # è®¿é—®æ–°èµ„æº risk += 5 return risk def _analyze_location_risk(self, request: AccessRequest, user: User, device: Device) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;åˆ†æåœ°ç†ä½ç½®é£é™©\u0026#34;\u0026#34;\u0026#34; risk = 0.0 # ç®€åŒ–çš„åœ°ç†ä½ç½®é£é™©åˆ†æ # å®é™…å®ç°ä¸­åº”è¯¥ä½¿ç”¨æ›´ç²¾ç¡®çš„åœ°ç†ä½ç½®æœåŠ¡ # æ£€æŸ¥IPåœ°å€å˜åŒ– recent_logs = [log for log in self.access_logs if log.user_id == user.user_id and (datetime.now() - log.timestamp).hours \u0026lt;= 24] if recent_logs: recent_ips = set(log.source_ip for log in recent_logs) if request.source_ip not in recent_ips: risk += 10 # æ£€æŸ¥è®¾å¤‡ä½ç½®å˜åŒ– if device.location != request.location: risk += 15 return risk def _analyze_time_risk(self, request: AccessRequest) -\u0026gt; float: \u0026#34;\u0026#34;\u0026#34;åˆ†ææ—¶é—´é£é™©\u0026#34;\u0026#34;\u0026#34; risk = 0.0 # æ£€æŸ¥è®¿é—®æ—¶é—´ hour = request.timestamp.hour # éå·¥ä½œæ—¶é—´è®¿é—® if hour \u0026lt; 6 or hour \u0026gt; 22: risk += 10 # å‘¨æœ«è®¿é—® if request.timestamp.weekday() \u0026gt;= 5: risk += 5 return risk def _apply_policies(self, request: AccessRequest, user: User, device: Device, risk_score: float) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;åº”ç”¨ç­–ç•¥è§„åˆ™\u0026#34;\u0026#34;\u0026#34; result = { \u0026#39;allowed\u0026#39;: False, \u0026#39;reason\u0026#39;: \u0026#39;é»˜è®¤æ‹’ç»\u0026#39;, \u0026#39;required_actions\u0026#39;: [], \u0026#39;session_timeout\u0026#39;: 3600 } # æŒ‰ä¼˜å…ˆçº§æ’åºç­–ç•¥ sorted_policies = sorted(self.policies.values(), key=lambda p: p.priority, reverse=True) for policy in sorted_policies: if not policy.enabled: continue if self._match_policy_conditions(policy, request, user, device, risk_score): # æ‰§è¡Œç­–ç•¥åŠ¨ä½œ for action in policy.actions: if action == \u0026#39;allow\u0026#39;: result[\u0026#39;allowed\u0026#39;] = True result[\u0026#39;reason\u0026#39;] = f\u0026#39;ç­–ç•¥ {policy.name} å…è®¸è®¿é—®\u0026#39; elif action == \u0026#39;deny\u0026#39;: result[\u0026#39;allowed\u0026#39;] = False result[\u0026#39;reason\u0026#39;] = f\u0026#39;ç­–ç•¥ {policy.name} æ‹’ç»è®¿é—®\u0026#39; return result # æ‹’ç»ç­–ç•¥ç«‹å³è¿”å› elif action == \u0026#39;require_mfa\u0026#39;: result[\u0026#39;required_actions\u0026#39;].append(\u0026#39;mfa\u0026#39;) elif action == \u0026#39;require_device_verification\u0026#39;: result[\u0026#39;required_actions\u0026#39;].append(\u0026#39;device_verification\u0026#39;) elif action.startswith(\u0026#39;session_timeout:\u0026#39;): timeout = int(action.split(\u0026#39;:\u0026#39;)[1]) result[\u0026#39;session_timeout\u0026#39;] = timeout # å¦‚æœåŒ¹é…åˆ°ç­–ç•¥ï¼Œåœæ­¢å¤„ç†åç»­ç­–ç•¥ break return result def _match_policy_conditions(self, policy: PolicyRule, request: AccessRequest, user: User, device: Device, risk_score: float) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;åŒ¹é…ç­–ç•¥æ¡ä»¶\u0026#34;\u0026#34;\u0026#34; conditions = policy.conditions # æ£€æŸ¥ç”¨æˆ·æ¡ä»¶ if \u0026#39;user_roles\u0026#39; in conditions: required_roles = conditions[\u0026#39;user_roles\u0026#39;] if not any(role in user.roles for role in required_roles): return False if \u0026#39;user_trust_score_min\u0026#39; in conditions: if user.trust_score \u0026lt; conditions[\u0026#39;user_trust_score_min\u0026#39;]: return False # æ£€æŸ¥è®¾å¤‡æ¡ä»¶ if \u0026#39;device_compliance\u0026#39; in conditions: if device.compliance_status != conditions[\u0026#39;device_compliance\u0026#39;]: return False if \u0026#39;device_trust_score_min\u0026#39; in conditions: if device.trust_score \u0026lt; conditions[\u0026#39;device_trust_score_min\u0026#39;]: return False # æ£€æŸ¥é£é™©æ¡ä»¶ if \u0026#39;max_risk_score\u0026#39; in conditions: if risk_score \u0026gt; conditions[\u0026#39;max_risk_score\u0026#39;]: return False # æ£€æŸ¥èµ„æºæ¡ä»¶ if \u0026#39;resources\u0026#39; in conditions: allowed_resources = conditions[\u0026#39;resources\u0026#39;] if request.resource not in allowed_resources: return False # æ£€æŸ¥æ—¶é—´æ¡ä»¶ if \u0026#39;allowed_hours\u0026#39; in conditions: allowed_hours = conditions[\u0026#39;allowed_hours\u0026#39;] current_hour = request.timestamp.hour if current_hour not in allowed_hours: return False return True def _get_risk_level(self, risk_score: float) -\u0026gt; RiskLevel: \u0026#34;\u0026#34;\u0026#34;è·å–é£é™©çº§åˆ«\u0026#34;\u0026#34;\u0026#34; if risk_score \u0026gt;= 75: return RiskLevel.CRITICAL elif risk_score \u0026gt;= 50: return RiskLevel.HIGH elif risk_score \u0026gt;= 25: return RiskLevel.MEDIUM else: return RiskLevel.LOW def _update_trust_scores(self, request: AccessRequest, user: User, device: Device, allowed: bool): \u0026#34;\u0026#34;\u0026#34;æ›´æ–°ä¿¡ä»»åˆ†æ•°\u0026#34;\u0026#34;\u0026#34; if allowed: # æˆåŠŸè®¿é—®ï¼Œè½»å¾®æå‡ä¿¡ä»»åˆ†æ•° user.trust_score = min(100, user.trust_score + 1) device.trust_score = min(100, device.trust_score + 1) user.failed_attempts = 0 else: # è®¿é—®è¢«æ‹’ç»ï¼Œé™ä½ä¿¡ä»»åˆ†æ•° user.trust_score = max(0, user.trust_score - 5) device.trust_score = max(0, device.trust_score - 3) user.failed_attempts += 1 def generate_security_report(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆå®‰å…¨æŠ¥å‘Š\u0026#34;\u0026#34;\u0026#34; total_requests = len(self.access_logs) if total_requests == 0: return {\u0026#39;message\u0026#39;: \u0026#39;æš‚æ— è®¿é—®è®°å½•\u0026#39;} # ç»Ÿè®¡è®¿é—®æˆåŠŸç‡ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…éœ€è¦è®°å½•è®¿é—®ç»“æœï¼‰ recent_logs = [log for log in self.access_logs if (datetime.now() - log.timestamp).days \u0026lt;= 7] # ç”¨æˆ·é£é™©åˆ†æ high_risk_users = [user for user in self.users.values() if user.trust_score \u0026lt; 30] # è®¾å¤‡é£é™©åˆ†æ non_compliant_devices = [device for device in self.devices.values() if not device.compliance_status] # è®¿é—®æ¨¡å¼åˆ†æ resource_access_count = {} for log in recent_logs: resource_access_count[log.resource] = resource_access_count.get(log.resource, 0) + 1 most_accessed_resources = sorted(resource_access_count.items(), key=lambda x: x[1], reverse=True)[:10] return { \u0026#39;report_time\u0026#39;: datetime.now().isoformat(), \u0026#39;total_users\u0026#39;: len(self.users), \u0026#39;total_devices\u0026#39;: len(self.devices), \u0026#39;total_policies\u0026#39;: len(self.policies), \u0026#39;recent_access_requests\u0026#39;: len(recent_logs), \u0026#39;high_risk_users\u0026#39;: len(high_risk_users), \u0026#39;non_compliant_devices\u0026#39;: len(non_compliant_devices), \u0026#39;most_accessed_resources\u0026#39;: most_accessed_resources, \u0026#39;average_user_trust_score\u0026#39;: sum(u.trust_score for u in self.users.values()) / len(self.users) if self.users else 0, \u0026#39;average_device_trust_score\u0026#39;: sum(d.trust_score for d in self.devices.values()) / len(self.devices) if self.devices else 0 } def main(): \u0026#34;\u0026#34;\u0026#34;ä¸»å‡½æ•° - æ¼”ç¤ºé›¶ä¿¡ä»»æ¶æ„åˆ†æå™¨\u0026#34;\u0026#34;\u0026#34; analyzer = ZeroTrustArchitectureAnalyzer() # æ³¨å†Œç”¨æˆ· user1 = User( user_id=\u0026#34;user001\u0026#34;, username=\u0026#34;alice\u0026#34;, email=\u0026#34;alice@company.com\u0026#34;, roles=[\u0026#34;user\u0026#34;, \u0026#34;developer\u0026#34;], department=\u0026#34;engineering\u0026#34;, trust_score=0, last_login=datetime.now() - timedelta(days=1), failed_attempts=0, mfa_enabled=True, device_registered=True ) analyzer.register_user(user1) # æ³¨å†Œè®¾å¤‡ device1 = Device( device_id=\u0026#34;device001\u0026#34;, device_type=\u0026#34;laptop\u0026#34;, os_version=\u0026#34;Windows 11\u0026#34;, security_patch_level=\u0026#34;latest\u0026#34;, compliance_status=True, trust_score=0, last_seen=datetime.now(), location=\u0026#34;Beijing\u0026#34;, owner=\u0026#34;alice\u0026#34; ) analyzer.register_device(device1) # åˆ›å»ºç­–ç•¥ policy1 = PolicyRule( rule_id=\u0026#34;policy001\u0026#34;, name=\u0026#34;å¼€å‘è€…è®¿é—®ç­–ç•¥\u0026#34;, description=\u0026#34;å…è®¸å¼€å‘è€…è®¿é—®å¼€å‘ç¯å¢ƒ\u0026#34;, conditions={ \u0026#34;user_roles\u0026#34;: [\u0026#34;developer\u0026#34;], \u0026#34;device_compliance\u0026#34;: True, \u0026#34;max_risk_score\u0026#34;: 30, \u0026#34;resources\u0026#34;: [\u0026#34;dev-server\u0026#34;, \u0026#34;test-db\u0026#34;], \u0026#34;allowed_hours\u0026#34;: list(range(8, 18)) # 8:00-18:00 }, actions=[\u0026#34;allow\u0026#34;, \u0026#34;session_timeout:7200\u0026#34;], priority=10, enabled=True ) analyzer.create_policy(policy1) # æ¨¡æ‹Ÿè®¿é—®è¯·æ±‚ request1 = AccessRequest( request_id=\u0026#34;req001\u0026#34;, user_id=\u0026#34;user001\u0026#34;, device_id=\u0026#34;device001\u0026#34;, resource=\u0026#34;dev-server\u0026#34;, action=\u0026#34;read\u0026#34;, timestamp=datetime.now(), source_ip=\u0026#34;192.168.1.100\u0026#34;, location=\u0026#34;Beijing\u0026#34;, user_agent=\u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64)\u0026#34; ) # è¯„ä¼°è®¿é—®è¯·æ±‚ result = analyzer.evaluate_access_request(request1) print(\u0026#34;è®¿é—®è¯·æ±‚è¯„ä¼°ç»“æœ:\u0026#34;) print(json.dumps(result, indent=2, ensure_ascii=False)) # ç”Ÿæˆå®‰å…¨æŠ¥å‘Š report = analyzer.generate_security_report() print(\u0026#34;\\nå®‰å…¨æŠ¥å‘Š:\u0026#34;) print(json.dumps(report, indent=2, ensure_ascii=False)) if __name__ == \u0026#34;__main__\u0026#34;: main() æœ€ä½³å®è·µä¸å»ºè®® æ¶æ„è®¾è®¡åŸåˆ™ æœ€å°æƒé™åŸåˆ™\n","content":"äº‘å®‰å…¨æ¶æ„è®¾è®¡ï¼šé›¶ä¿¡ä»»æ¨¡å‹çš„å®æ–½ å¼•è¨€ åœ¨ä¼ ç»Ÿçš„ç½‘ç»œå®‰å…¨æ¨¡å‹ä¸­ï¼Œä¼ä¸šé€šå¸¸é‡‡ç”¨\u0026amp;quot;åŸå ¡å’ŒæŠ¤åŸæ²³\u0026amp;quot;çš„é˜²æŠ¤ç­–ç•¥ï¼Œå³åœ¨ç½‘ç»œè¾¹ç•Œå»ºç«‹å¼ºå¤§çš„é˜²æŠ¤æªæ–½ï¼Œè€Œå¯¹å†…éƒ¨ç½‘ç»œç›¸å¯¹ä¿¡ä»»ã€‚ç„¶è€Œï¼Œéšç€äº‘è®¡ç®—ã€ç§»åŠ¨åŠå…¬ã€ç‰©è”ç½‘ç­‰æŠ€æœ¯çš„å¿«é€Ÿå‘å±•ï¼Œä¼ ç»Ÿçš„è¾¹ç•Œé˜²æŠ¤æ¨¡å¼å·²ç»æ— æ³•æ»¡è¶³ç°ä»£ä¼ä¸šçš„å®‰å…¨éœ€æ±‚ã€‚é›¶ä¿¡ä»»å®‰å…¨æ¨¡å‹åº”è¿è€Œç”Ÿï¼Œå®ƒåŸºäº\u0026amp;quot;æ°¸ä¸ä¿¡ä»»ï¼Œå§‹ç»ˆéªŒè¯\u0026amp;quot;çš„æ ¸å¿ƒç†å¿µï¼Œä¸ºäº‘ç¯å¢ƒæä¾›äº†æ›´åŠ å®‰å…¨ã€çµæ´»çš„é˜²æŠ¤æ–¹æ¡ˆã€‚\nç›®å½• é›¶ä¿¡ä»»å®‰å…¨æ¨¡å‹æ¦‚è¿° é›¶ä¿¡ä»»æ¶æ„æ ¸å¿ƒç»„ä»¶ èº«ä»½ä¸è®¿é—®ç®¡ç† ç½‘ç»œå¾®åˆ†æ®µ æ•°æ®ä¿æŠ¤ä¸åŠ å¯† è®¾å¤‡ä¿¡ä»»ä¸ç®¡ç† åº”ç”¨å®‰å…¨ ç›‘æ§ä¸åˆ†æ å®æ–½ç­–ç•¥ä¸æœ€ä½³å®è·µ é›¶ä¿¡ä»»å®‰å…¨æ¨¡å‹æ¦‚è¿° é›¶ä¿¡ä»»å®‰å…¨æ¨¡å‹æ˜¯ä¸€ç§ç½‘ç»œå®‰å…¨èŒƒå¼ï¼Œå®ƒå‡è®¾ç½‘ç»œå†…å¤–éƒ½å­˜åœ¨å¨èƒï¼Œå› æ­¤ä¸ä¼šè‡ªåŠ¨ä¿¡ä»»ä»»ä½•ç”¨æˆ·ã€è®¾å¤‡æˆ–åº”ç”¨ç¨‹åºã€‚è¯¥æ¨¡å‹è¦æ±‚å¯¹æ¯ä¸ªè®¿é—®è¯·æ±‚è¿›è¡Œä¸¥æ ¼çš„èº«ä»½éªŒè¯å’Œæˆæƒï¼Œæ— è®ºè¯·æ±‚æ¥è‡ªç½‘ç»œå†…éƒ¨è¿˜æ˜¯å¤–éƒ¨ã€‚\né›¶ä¿¡ä»»æ¶æ„å›¾ graph TB subgraph \u0026amp;#34;é›¶ä¿¡ä»»äº‘å®‰å…¨æ¶æ„\u0026amp;#34; subgraph \u0026amp;#34;ç”¨æˆ·å±‚\u0026amp;#34; U1[å†…éƒ¨ç”¨æˆ·] U2[å¤–éƒ¨ç”¨æˆ·] U3[åˆä½œä¼™ä¼´] U4[ç§»åŠ¨è®¾å¤‡] end subgraph \u0026amp;#34; â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["äº‘å®‰å…¨","é›¶ä¿¡ä»»","æ¶æ„è®¾è®¡","ç½‘ç»œå®‰å…¨","èº«ä»½è®¤è¯"],"categories":["äº‘æ¶æ„"],"author":"äº‘æ¶æ„ä¸“å®¶","readingTime":28,"wordCount":5762,"section":"posts","type":"posts","draft":false,"featured":false,"series":null},{"title":"äº‘å®‰å…¨æ¶æ„è®¾è®¡ä¸é›¶ä¿¡ä»»å®æ–½ï¼šæ„å»ºç°ä»£ä¼ä¸šå®‰å…¨é˜²æŠ¤ä½“ç³»","url":"https://www.dishuihengxin.com/posts/cloud-security-zero-trust-architecture/","summary":"äº‘å®‰å…¨æ¶æ„è®¾è®¡ä¸é›¶ä¿¡ä»»å®æ–½ï¼šæ„å»ºç°ä»£ä¼ä¸šå®‰å…¨é˜²æŠ¤ä½“ç³» ç›®å½• äº‘å®‰å…¨æ¶æ„æ¦‚è¿° é›¶ä¿¡ä»»å®‰å…¨æ¨¡å‹ èº«ä»½ä¸è®¿é—®ç®¡ç†(IAM) ç½‘ç»œå®‰å…¨æ¶æ„ æ•°æ®ä¿æŠ¤ä¸åŠ å¯† å¨èƒæ£€æµ‹ä¸å“åº” åˆè§„æ€§ä¸æ²»ç† å®‰å…¨è¿è¥ä¸­å¿ƒ(SOC) æœ€ä½³å®è·µä¸æ€»ç»“ 1. äº‘å®‰å…¨æ¶æ„æ¦‚è¿° 1.1 äº‘å®‰å…¨æŒ‘æˆ˜ä¸æœºé‡ ç°ä»£ä¼ä¸šåœ¨äº‘åŒ–è½¬å‹è¿‡ç¨‹ä¸­é¢ä¸´ç€å‰æ‰€æœªæœ‰çš„å®‰å…¨æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„è¾¹ç•Œå®‰å…¨æ¨¡å‹å·²æ— æ³•é€‚åº”äº‘ç¯å¢ƒçš„åŠ¨æ€æ€§å’Œå¤æ‚æ€§ï¼Œéœ€è¦æ„å»ºå…¨æ–°çš„å®‰å…¨æ¶æ„æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚\ngraph TB subgraph \u0026#34;ä¼ ç»Ÿå®‰å…¨æ¨¡å‹\u0026#34; A[ç½‘ç»œè¾¹ç•Œ] --\u0026gt; B[é˜²ç«å¢™] B --\u0026gt; C[å†…ç½‘ä¿¡ä»»] C --\u0026gt; D[é™æ€è®¿é—®æ§åˆ¶] end subgraph \u0026#34;äº‘å®‰å…¨æŒ‘æˆ˜\u0026#34; E[è¾¹ç•Œæ¨¡ç³Š] --\u0026gt; F[åŠ¨æ€ç¯å¢ƒ] F --\u0026gt; G[å¤šäº‘å¤æ‚æ€§] G --\u0026gt; H[åˆè§„è¦æ±‚] end subgraph \u0026#34;é›¶ä¿¡ä»»æ¨¡å‹\u0026#34; I[æ°¸ä¸ä¿¡ä»»] --\u0026gt; J[æŒç»­éªŒè¯] J --\u0026gt; K[æœ€å°æƒé™] K --\u0026gt; L[åŠ¨æ€è®¿é—®æ§åˆ¶] end A -.-\u0026gt; E E --\u0026gt; I 1.2 äº‘å®‰å…¨æ¶æ„è®¾è®¡å™¨ è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªç»¼åˆçš„äº‘å®‰å…¨æ¶æ„è®¾è®¡å™¨ï¼š\nfrom typing import Dict, List, Any, Optional, Union from dataclasses import dataclass, field from enum import Enum import json import hashlib import uuid from datetime import datetime, timedelta import logging class SecurityDomain(Enum): \u0026#34;\u0026#34;\u0026#34;å®‰å…¨åŸŸ\u0026#34;\u0026#34;\u0026#34; IDENTITY = \u0026#34;identity\u0026#34; NETWORK = \u0026#34;network\u0026#34; DATA = \u0026#34;data\u0026#34; APPLICATION = \u0026#34;application\u0026#34; INFRASTRUCTURE = \u0026#34;infrastructure\u0026#34; COMPLIANCE = \u0026#34;compliance\u0026#34; class ThreatLevel(Enum): \u0026#34;\u0026#34;\u0026#34;å¨èƒçº§åˆ«\u0026#34;\u0026#34;\u0026#34; LOW = \u0026#34;low\u0026#34; MEDIUM = \u0026#34;medium\u0026#34; HIGH = \u0026#34;high\u0026#34; CRITICAL = \u0026#34;critical\u0026#34; class SecurityControl(Enum): \u0026#34;\u0026#34;\u0026#34;å®‰å…¨æ§åˆ¶ç±»å‹\u0026#34;\u0026#34;\u0026#34; PREVENTIVE = \u0026#34;preventive\u0026#34; DETECTIVE = \u0026#34;detective\u0026#34; CORRECTIVE = \u0026#34;corrective\u0026#34; DETERRENT = \u0026#34;deterrent\u0026#34; @dataclass class SecurityRequirement: \u0026#34;\u0026#34;\u0026#34;å®‰å…¨éœ€æ±‚\u0026#34;\u0026#34;\u0026#34; id: str name: str description: str domain: SecurityDomain threat_level: ThreatLevel control_type: SecurityControl compliance_frameworks: List[str] = field(default_factory=list) implementation_priority: int = 1 # 1-5, 1ä¸ºæœ€é«˜ä¼˜å…ˆçº§ @dataclass class SecurityComponent: \u0026#34;\u0026#34;\u0026#34;å®‰å…¨ç»„ä»¶\u0026#34;\u0026#34;\u0026#34; id: str name: str type: str domain: SecurityDomain capabilities: List[str] dependencies: List[str] = field(default_factory=list) configuration: Dict[str, Any] = field(default_factory=dict) cost_estimate: float = 0.0 @dataclass class SecurityPolicy: \u0026#34;\u0026#34;\u0026#34;å®‰å…¨ç­–ç•¥\u0026#34;\u0026#34;\u0026#34; id: str name: str description: str rules: List[Dict[str, Any]] enforcement_mode: str # \u0026#34;enforce\u0026#34;, \u0026#34;monitor\u0026#34;, \u0026#34;disabled\u0026#34; applicable_resources: List[str] exceptions: List[str] = field(default_factory=list) class CloudSecurityArchitect: \u0026#34;\u0026#34;\u0026#34;äº‘å®‰å…¨æ¶æ„å¸ˆ\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.security_requirements = [] self.security_components = [] self.security_policies = [] self.threat_model = {} self.compliance_frameworks = self._load_compliance_frameworks() def _load_compliance_frameworks(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;åŠ è½½åˆè§„æ¡†æ¶\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;SOC2\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;SOC 2 Type II\u0026#34;, \u0026#34;domains\u0026#34;: [\u0026#34;security\u0026#34;, \u0026#34;availability\u0026#34;, \u0026#34;processing_integrity\u0026#34;, \u0026#34;confidentiality\u0026#34;, \u0026#34;privacy\u0026#34;], \u0026#34;controls\u0026#34;: [ \u0026#34;access_controls\u0026#34;, \u0026#34;system_monitoring\u0026#34;, \u0026#34;change_management\u0026#34;, \u0026#34;data_protection\u0026#34;, \u0026#34;incident_response\u0026#34; ] }, \u0026#34;ISO27001\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;ISO/IEC 27001\u0026#34;, \u0026#34;domains\u0026#34;: [\u0026#34;information_security_policies\u0026#34;, \u0026#34;organization_security\u0026#34;, \u0026#34;human_resource_security\u0026#34;, \u0026#34;asset_management\u0026#34;, \u0026#34;access_control\u0026#34;, \u0026#34;cryptography\u0026#34;], \u0026#34;controls\u0026#34;: [ \u0026#34;security_policy\u0026#34;, \u0026#34;risk_management\u0026#34;, \u0026#34;supplier_relationships\u0026#34;, \u0026#34;incident_management\u0026#34;, \u0026#34;business_continuity\u0026#34; ] }, \u0026#34;GDPR\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;General Data Protection Regulation\u0026#34;, \u0026#34;domains\u0026#34;: [\u0026#34;data_protection\u0026#34;, \u0026#34;privacy_rights\u0026#34;, \u0026#34;consent_management\u0026#34;, \u0026#34;data_breach_notification\u0026#34;, \u0026#34;privacy_by_design\u0026#34;], \u0026#34;controls\u0026#34;: [ \u0026#34;data_minimization\u0026#34;, \u0026#34;purpose_limitation\u0026#34;, \u0026#34;storage_limitation\u0026#34;, \u0026#34;accuracy\u0026#34;, \u0026#34;integrity_confidentiality\u0026#34; ] }, \u0026#34;HIPAA\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Health Insurance Portability and Accountability Act\u0026#34;, \u0026#34;domains\u0026#34;: [\u0026#34;administrative_safeguards\u0026#34;, \u0026#34;physical_safeguards\u0026#34;, \u0026#34;technical_safeguards\u0026#34;], \u0026#34;controls\u0026#34;: [ \u0026#34;access_management\u0026#34;, \u0026#34;audit_controls\u0026#34;, \u0026#34;integrity\u0026#34;, \u0026#34;person_authentication\u0026#34;, \u0026#34;transmission_security\u0026#34; ] } } def analyze_security_requirements(self, business_context: Dict[str, Any]) -\u0026gt; List[SecurityRequirement]: \u0026#34;\u0026#34;\u0026#34;åˆ†æå®‰å…¨éœ€æ±‚\u0026#34;\u0026#34;\u0026#34; requirements = [] # åŸºäºä¸šåŠ¡ä¸Šä¸‹æ–‡ç”Ÿæˆå®‰å…¨éœ€æ±‚ industry = business_context.get(\u0026#34;industry\u0026#34;, \u0026#34;general\u0026#34;) data_sensitivity = business_context.get(\u0026#34;data_sensitivity\u0026#34;, \u0026#34;medium\u0026#34;) compliance_requirements = business_context.get(\u0026#34;compliance\u0026#34;, []) # èº«ä»½ä¸è®¿é—®ç®¡ç†éœ€æ±‚ requirements.extend(self._generate_iam_requirements( data_sensitivity, compliance_requirements )) # ç½‘ç»œå®‰å…¨éœ€æ±‚ requirements.extend(self._generate_network_requirements( business_context.get(\u0026#34;network_architecture\u0026#34;, \u0026#34;hybrid\u0026#34;) )) # æ•°æ®ä¿æŠ¤éœ€æ±‚ requirements.extend(self._generate_data_protection_requirements( data_sensitivity, compliance_requirements )) # åº”ç”¨å®‰å…¨éœ€æ±‚ requirements.extend(self._generate_application_security_requirements( business_context.get(\u0026#34;application_types\u0026#34;, [\u0026#34;web\u0026#34;, \u0026#34;api\u0026#34;]) )) # åŸºç¡€è®¾æ–½å®‰å…¨éœ€æ±‚ requirements.extend(self._generate_infrastructure_requirements( business_context.get(\u0026#34;cloud_providers\u0026#34;, [\u0026#34;aws\u0026#34;]) )) self.security_requirements = requirements return requirements def _generate_iam_requirements(self, data_sensitivity: str, compliance: List[str]) -\u0026gt; List[SecurityRequirement]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆIAMå®‰å…¨éœ€æ±‚\u0026#34;\u0026#34;\u0026#34; requirements = [] # å¤šå› ç´ è®¤è¯ mfa_threat_level = ThreatLevel.HIGH if data_sensitivity == \u0026#34;high\u0026#34; else ThreatLevel.MEDIUM requirements.append(SecurityRequirement( id=\u0026#34;iam_001\u0026#34;, name=\u0026#34;å¤šå› ç´ è®¤è¯(MFA)\u0026#34;, description=\u0026#34;æ‰€æœ‰ç”¨æˆ·è®¿é—®å¿…é¡»å¯ç”¨å¤šå› ç´ è®¤è¯\u0026#34;, domain=SecurityDomain.IDENTITY, threat_level=mfa_threat_level, control_type=SecurityControl.PREVENTIVE, compliance_frameworks=compliance, implementation_priority=1 )) # ç‰¹æƒè®¿é—®ç®¡ç† requirements.append(SecurityRequirement( id=\u0026#34;iam_002\u0026#34;, name=\u0026#34;ç‰¹æƒè®¿é—®ç®¡ç†(PAM)\u0026#34;, description=\u0026#34;ç®¡ç†å’Œç›‘æ§ç‰¹æƒè´¦æˆ·è®¿é—®\u0026#34;, domain=SecurityDomain.IDENTITY, threat_level=ThreatLevel.CRITICAL, control_type=SecurityControl.PREVENTIVE, compliance_frameworks=compliance, implementation_priority=1 )) # å•ç‚¹ç™»å½• requirements.append(SecurityRequirement( id=\u0026#34;iam_003\u0026#34;, name=\u0026#34;å•ç‚¹ç™»å½•(SSO)\u0026#34;, description=\u0026#34;ç»Ÿä¸€èº«ä»½è®¤è¯å’Œè®¿é—®ç®¡ç†\u0026#34;, domain=SecurityDomain.IDENTITY, threat_level=ThreatLevel.MEDIUM, control_type=SecurityControl.PREVENTIVE, compliance_frameworks=compliance, implementation_priority=2 )) return requirements def _generate_network_requirements(self, architecture: str) -\u0026gt; List[SecurityRequirement]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆç½‘ç»œå®‰å…¨éœ€æ±‚\u0026#34;\u0026#34;\u0026#34; requirements = [] # ç½‘ç»œåˆ†æ®µ requirements.append(SecurityRequirement( id=\u0026#34;net_001\u0026#34;, name=\u0026#34;ç½‘ç»œå¾®åˆ†æ®µ\u0026#34;, description=\u0026#34;å®æ–½ç»†ç²’åº¦çš„ç½‘ç»œåˆ†æ®µå’Œè®¿é—®æ§åˆ¶\u0026#34;, domain=SecurityDomain.NETWORK, threat_level=ThreatLevel.HIGH, control_type=SecurityControl.PREVENTIVE, implementation_priority=1 )) # DDoSé˜²æŠ¤ requirements.append(SecurityRequirement( id=\u0026#34;net_002\u0026#34;, name=\u0026#34;DDoSé˜²æŠ¤\u0026#34;, description=\u0026#34;éƒ¨ç½²åˆ†å¸ƒå¼æ‹’ç»æœåŠ¡æ”»å‡»é˜²æŠ¤\u0026#34;, domain=SecurityDomain.NETWORK, threat_level=ThreatLevel.HIGH, control_type=SecurityControl.PREVENTIVE, implementation_priority=2 )) # Webåº”ç”¨é˜²ç«å¢™ requirements.append(SecurityRequirement( id=\u0026#34;net_003\u0026#34;, name=\u0026#34;Webåº”ç”¨é˜²ç«å¢™(WAF)\u0026#34;, description=\u0026#34;ä¿æŠ¤Webåº”ç”¨å…å—å¸¸è§æ”»å‡»\u0026#34;, domain=SecurityDomain.NETWORK, threat_level=ThreatLevel.HIGH, control_type=SecurityControl.PREVENTIVE, implementation_priority=2 )) return requirements def _generate_data_protection_requirements(self, sensitivity: str, compliance: List[str]) -\u0026gt; List[SecurityRequirement]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆæ•°æ®ä¿æŠ¤éœ€æ±‚\u0026#34;\u0026#34;\u0026#34; requirements = [] # æ•°æ®åŠ å¯† encryption_threat_level = ThreatLevel.CRITICAL if sensitivity == \u0026#34;high\u0026#34; else ThreatLevel.HIGH requirements.append(SecurityRequirement( id=\u0026#34;data_001\u0026#34;, name=\u0026#34;æ•°æ®åŠ å¯†\u0026#34;, description=\u0026#34;é™æ€å’Œä¼ è¾“ä¸­çš„æ•°æ®åŠ å¯†\u0026#34;, domain=SecurityDomain.DATA, threat_level=encryption_threat_level, control_type=SecurityControl.PREVENTIVE, compliance_frameworks=compliance, implementation_priority=1 )) # æ•°æ®ä¸¢å¤±é˜²æŠ¤ requirements.append(SecurityRequirement( id=\u0026#34;data_002\u0026#34;, name=\u0026#34;æ•°æ®ä¸¢å¤±é˜²æŠ¤(DLP)\u0026#34;, description=\u0026#34;é˜²æ­¢æ•æ„Ÿæ•°æ®æ³„éœ²\u0026#34;, domain=SecurityDomain.DATA, threat_level=ThreatLevel.HIGH, control_type=SecurityControl.DETECTIVE, compliance_frameworks=compliance, implementation_priority=2 )) # æ•°æ®å¤‡ä»½ä¸æ¢å¤ requirements.append(SecurityRequirement( id=\u0026#34;data_003\u0026#34;, name=\u0026#34;æ•°æ®å¤‡ä»½ä¸æ¢å¤\u0026#34;, description=\u0026#34;ç¡®ä¿æ•°æ®çš„å¯ç”¨æ€§å’Œå®Œæ•´æ€§\u0026#34;, domain=SecurityDomain.DATA, threat_level=ThreatLevel.MEDIUM, control_type=SecurityControl.CORRECTIVE, implementation_priority=2 )) return requirements def _generate_application_security_requirements(self, app_types: List[str]) -\u0026gt; List[SecurityRequirement]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆåº”ç”¨å®‰å…¨éœ€æ±‚\u0026#34;\u0026#34;\u0026#34; requirements = [] # åº”ç”¨å®‰å…¨æ‰«æ requirements.append(SecurityRequirement( id=\u0026#34;app_001\u0026#34;, name=\u0026#34;åº”ç”¨å®‰å…¨æ‰«æ\u0026#34;, description=\u0026#34;é™æ€å’ŒåŠ¨æ€åº”ç”¨å®‰å…¨æµ‹è¯•\u0026#34;, domain=SecurityDomain.APPLICATION, threat_level=ThreatLevel.HIGH, control_type=SecurityControl.DETECTIVE, implementation_priority=2 )) # APIå®‰å…¨ if \u0026#34;api\u0026#34; in app_types: requirements.append(SecurityRequirement( id=\u0026#34;app_002\u0026#34;, name=\u0026#34;APIå®‰å…¨ç½‘å…³\u0026#34;, description=\u0026#34;APIè®¿é—®æ§åˆ¶ã€é™æµå’Œç›‘æ§\u0026#34;, domain=SecurityDomain.APPLICATION, threat_level=ThreatLevel.HIGH, control_type=SecurityControl.PREVENTIVE, implementation_priority=1 )) return requirements def _generate_infrastructure_requirements(self, providers: List[str]) -\u0026gt; List[SecurityRequirement]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆåŸºç¡€è®¾æ–½å®‰å…¨éœ€æ±‚\u0026#34;\u0026#34;\u0026#34; requirements = [] # å®¹å™¨å®‰å…¨ requirements.append(SecurityRequirement( id=\u0026#34;infra_001\u0026#34;, name=\u0026#34;å®¹å™¨å®‰å…¨\u0026#34;, description=\u0026#34;å®¹å™¨é•œåƒæ‰«æå’Œè¿è¡Œæ—¶ä¿æŠ¤\u0026#34;, domain=SecurityDomain.INFRASTRUCTURE, threat_level=ThreatLevel.HIGH, control_type=SecurityControl.PREVENTIVE, implementation_priority=2 )) # äº‘é…ç½®ç®¡ç† requirements.append(SecurityRequirement( id=\u0026#34;infra_002\u0026#34;, name=\u0026#34;äº‘é…ç½®å®‰å…¨\u0026#34;, description=\u0026#34;äº‘èµ„æºé…ç½®çš„å®‰å…¨åŸºçº¿ç®¡ç†\u0026#34;, domain=SecurityDomain.INFRASTRUCTURE, threat_level=ThreatLevel.MEDIUM, control_type=SecurityControl.DETECTIVE, implementation_priority=2 )) return requirements def design_security_architecture(self, requirements: List[SecurityRequirement]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è®¾è®¡å®‰å…¨æ¶æ„\u0026#34;\u0026#34;\u0026#34; architecture = { \u0026#34;identity_layer\u0026#34;: self._design_identity_layer(requirements), \u0026#34;network_layer\u0026#34;: self._design_network_layer(requirements), \u0026#34;data_layer\u0026#34;: self._design_data_layer(requirements), \u0026#34;application_layer\u0026#34;: self._design_application_layer(requirements), \u0026#34;infrastructure_layer\u0026#34;: self._design_infrastructure_layer(requirements), \u0026#34;management_layer\u0026#34;: self._design_management_layer(requirements) } return architecture def _design_identity_layer(self, requirements: List[SecurityRequirement]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è®¾è®¡èº«ä»½å±‚\u0026#34;\u0026#34;\u0026#34; identity_reqs = [req for req in requirements if req.domain == SecurityDomain.IDENTITY] components = [] # èº«ä»½æä¾›å•† components.append(SecurityComponent( id=\u0026#34;idp_001\u0026#34;, name=\u0026#34;Identity Provider\u0026#34;, type=\u0026#34;identity_provider\u0026#34;, domain=SecurityDomain.IDENTITY, capabilities=[\u0026#34;authentication\u0026#34;, \u0026#34;authorization\u0026#34;, \u0026#34;sso\u0026#34;, \u0026#34;mfa\u0026#34;], configuration={ \u0026#34;protocols\u0026#34;: [\u0026#34;SAML\u0026#34;, \u0026#34;OIDC\u0026#34;, \u0026#34;OAuth2\u0026#34;], \u0026#34;mfa_methods\u0026#34;: [\u0026#34;totp\u0026#34;, \u0026#34;sms\u0026#34;, \u0026#34;push\u0026#34;, \u0026#34;biometric\u0026#34;], \u0026#34;session_timeout\u0026#34;: 8, # hours \u0026#34;password_policy\u0026#34;: { \u0026#34;min_length\u0026#34;: 12, \u0026#34;complexity\u0026#34;: True, \u0026#34;history\u0026#34;: 12, \u0026#34;max_age\u0026#34;: 90 } }, cost_estimate=5000.0 )) # ç‰¹æƒè®¿é—®ç®¡ç† components.append(SecurityComponent( id=\u0026#34;pam_001\u0026#34;, name=\u0026#34;Privileged Access Management\u0026#34;, type=\u0026#34;pam_solution\u0026#34;, domain=SecurityDomain.IDENTITY, capabilities=[\u0026#34;privileged_session_management\u0026#34;, \u0026#34;password_vaulting\u0026#34;, \u0026#34;just_in_time_access\u0026#34;, \u0026#34;session_recording\u0026#34;], dependencies=[\u0026#34;idp_001\u0026#34;], configuration={ \u0026#34;session_recording\u0026#34;: True, \u0026#34;approval_workflow\u0026#34;: True, \u0026#34;emergency_access\u0026#34;: True, \u0026#34;rotation_schedule\u0026#34;: \u0026#34;weekly\u0026#34; }, cost_estimate=15000.0 )) return { \u0026#34;components\u0026#34;: components, \u0026#34;policies\u0026#34;: self._generate_identity_policies(), \u0026#34;integration_points\u0026#34;: [\u0026#34;directory_services\u0026#34;, \u0026#34;cloud_providers\u0026#34;, \u0026#34;applications\u0026#34;] } def _design_network_layer(self, requirements: List[SecurityRequirement]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è®¾è®¡ç½‘ç»œå±‚\u0026#34;\u0026#34;\u0026#34; network_reqs = [req for req in requirements if req.domain == SecurityDomain.NETWORK] components = [] # ç½‘ç»œé˜²ç«å¢™ components.append(SecurityComponent( id=\u0026#34;fw_001\u0026#34;, name=\u0026#34;Next-Generation Firewall\u0026#34;, type=\u0026#34;ngfw\u0026#34;, domain=SecurityDomain.NETWORK, capabilities=[\u0026#34;stateful_inspection\u0026#34;, \u0026#34;application_control\u0026#34;, \u0026#34;intrusion_prevention\u0026#34;, \u0026#34;ssl_inspection\u0026#34;], configuration={ \u0026#34;high_availability\u0026#34;: True, \u0026#34;throughput\u0026#34;: \u0026#34;10Gbps\u0026#34;, \u0026#34;ssl_inspection\u0026#34;: True, \u0026#34;geo_blocking\u0026#34;: True }, cost_estimate=25000.0 )) # Webåº”ç”¨é˜²ç«å¢™ components.append(SecurityComponent( id=\u0026#34;waf_001\u0026#34;, name=\u0026#34;Web Application Firewall\u0026#34;, type=\u0026#34;waf\u0026#34;, domain=SecurityDomain.NETWORK, capabilities=[\u0026#34;owasp_protection\u0026#34;, \u0026#34;bot_mitigation\u0026#34;, \u0026#34;rate_limiting\u0026#34;, \u0026#34;geo_filtering\u0026#34;], configuration={ \u0026#34;rule_sets\u0026#34;: [\u0026#34;OWASP_CRS\u0026#34;, \u0026#34;custom_rules\u0026#34;], \u0026#34;learning_mode\u0026#34;: True, \u0026#34;api_protection\u0026#34;: True, \u0026#34;ddos_protection\u0026#34;: True }, cost_estimate=8000.0 )) # ç½‘ç»œåˆ†æ®µ components.append(SecurityComponent( id=\u0026#34;seg_001\u0026#34;, name=\u0026#34;Network Segmentation\u0026#34;, type=\u0026#34;micro_segmentation\u0026#34;, domain=SecurityDomain.NETWORK, capabilities=[\u0026#34;zero_trust_networking\u0026#34;, \u0026#34;application_aware_segmentation\u0026#34;, \u0026#34;dynamic_policy_enforcement\u0026#34;], dependencies=[\u0026#34;fw_001\u0026#34;], configuration={ \u0026#34;enforcement_mode\u0026#34;: \u0026#34;transparent\u0026#34;, \u0026#34;learning_period\u0026#34;: 30, # days \u0026#34;auto_policy_generation\u0026#34;: True }, cost_estimate=12000.0 )) return { \u0026#34;components\u0026#34;: components, \u0026#34;network_topology\u0026#34;: self._design_network_topology(), \u0026#34;security_zones\u0026#34;: self._define_security_zones() } def _design_data_layer(self, requirements: List[SecurityRequirement]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è®¾è®¡æ•°æ®å±‚\u0026#34;\u0026#34;\u0026#34; data_reqs = [req for req in requirements if req.domain == SecurityDomain.DATA] components = [] # å¯†é’¥ç®¡ç†æœåŠ¡ components.append(SecurityComponent( id=\u0026#34;kms_001\u0026#34;, name=\u0026#34;Key Management Service\u0026#34;, type=\u0026#34;kms\u0026#34;, domain=SecurityDomain.DATA, capabilities=[\u0026#34;key_generation\u0026#34;, \u0026#34;key_rotation\u0026#34;, \u0026#34;hsm_integration\u0026#34;, \u0026#34;envelope_encryption\u0026#34;], configuration={ \u0026#34;hsm_backed\u0026#34;: True, \u0026#34;auto_rotation\u0026#34;: True, \u0026#34;rotation_period\u0026#34;: 365, # days \u0026#34;key_usage_logging\u0026#34;: True }, cost_estimate=3000.0 )) # æ•°æ®ä¸¢å¤±é˜²æŠ¤ components.append(SecurityComponent( id=\u0026#34;dlp_001\u0026#34;, name=\u0026#34;Data Loss Prevention\u0026#34;, type=\u0026#34;dlp_solution\u0026#34;, domain=SecurityDomain.DATA, capabilities=[\u0026#34;content_inspection\u0026#34;, \u0026#34;policy_enforcement\u0026#34;, \u0026#34;incident_response\u0026#34;, \u0026#34;data_classification\u0026#34;], dependencies=[\u0026#34;kms_001\u0026#34;], configuration={ \u0026#34;scan_modes\u0026#34;: [\u0026#34;real_time\u0026#34;, \u0026#34;scheduled\u0026#34;], \u0026#34;data_types\u0026#34;: [\u0026#34;pii\u0026#34;, \u0026#34;phi\u0026#34;, \u0026#34;pci\u0026#34;, \u0026#34;custom\u0026#34;], \u0026#34;actions\u0026#34;: [\u0026#34;block\u0026#34;, \u0026#34;quarantine\u0026#34;, \u0026#34;alert\u0026#34;, \u0026#34;encrypt\u0026#34;] }, cost_estimate=10000.0 )) return { \u0026#34;components\u0026#34;: components, \u0026#34;encryption_strategy\u0026#34;: self._design_encryption_strategy(), \u0026#34;data_classification\u0026#34;: self._define_data_classification() } def _design_application_layer(self, requirements: List[SecurityRequirement]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è®¾è®¡åº”ç”¨å±‚\u0026#34;\u0026#34;\u0026#34; app_reqs = [req for req in requirements if req.domain == SecurityDomain.APPLICATION] components = [] # åº”ç”¨å®‰å…¨æ‰«æ components.append(SecurityComponent( id=\u0026#34;ast_001\u0026#34;, name=\u0026#34;Application Security Testing\u0026#34;, type=\u0026#34;ast_platform\u0026#34;, domain=SecurityDomain.APPLICATION, capabilities=[\u0026#34;sast\u0026#34;, \u0026#34;dast\u0026#34;, \u0026#34;iast\u0026#34;, \u0026#34;sca\u0026#34;, \u0026#34;container_scanning\u0026#34;], configuration={ \u0026#34;integration\u0026#34;: [\u0026#34;ci_cd\u0026#34;, \u0026#34;ide\u0026#34;, \u0026#34;scm\u0026#34;], \u0026#34;scan_frequency\u0026#34;: \u0026#34;on_commit\u0026#34;, \u0026#34;vulnerability_management\u0026#34;: True, \u0026#34;compliance_reporting\u0026#34;: True }, cost_estimate=15000.0 )) # APIå®‰å…¨ç½‘å…³ components.append(SecurityComponent( id=\u0026#34;api_gw_001\u0026#34;, name=\u0026#34;API Security Gateway\u0026#34;, type=\u0026#34;api_gateway\u0026#34;, domain=SecurityDomain.APPLICATION, capabilities=[\u0026#34;api_authentication\u0026#34;, \u0026#34;rate_limiting\u0026#34;, \u0026#34;threat_protection\u0026#34;, \u0026#34;api_analytics\u0026#34;], dependencies=[\u0026#34;idp_001\u0026#34;], configuration={ \u0026#34;authentication_methods\u0026#34;: [\u0026#34;oauth2\u0026#34;, \u0026#34;api_key\u0026#34;, \u0026#34;jwt\u0026#34;], \u0026#34;rate_limiting\u0026#34;: True, \u0026#34;threat_protection\u0026#34;: True, \u0026#34;analytics\u0026#34;: True }, cost_estimate=8000.0 )) return { \u0026#34;components\u0026#34;: components, \u0026#34;secure_development\u0026#34;: self._design_secure_development_lifecycle(), \u0026#34;runtime_protection\u0026#34;: self._design_runtime_protection() } def _design_infrastructure_layer(self, requirements: List[SecurityRequirement]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è®¾è®¡åŸºç¡€è®¾æ–½å±‚\u0026#34;\u0026#34;\u0026#34; infra_reqs = [req for req in requirements if req.domain == SecurityDomain.INFRASTRUCTURE] components = [] # äº‘å®‰å…¨æ€åŠ¿ç®¡ç† components.append(SecurityComponent( id=\u0026#34;cspm_001\u0026#34;, name=\u0026#34;Cloud Security Posture Management\u0026#34;, type=\u0026#34;cspm_platform\u0026#34;, domain=SecurityDomain.INFRASTRUCTURE, capabilities=[\u0026#34;configuration_assessment\u0026#34;, \u0026#34;compliance_monitoring\u0026#34;, \u0026#34;threat_detection\u0026#34;, \u0026#34;remediation_automation\u0026#34;], configuration={ \u0026#34;cloud_providers\u0026#34;: [\u0026#34;aws\u0026#34;, \u0026#34;azure\u0026#34;, \u0026#34;gcp\u0026#34;], \u0026#34;compliance_frameworks\u0026#34;: [\u0026#34;cis\u0026#34;, \u0026#34;nist\u0026#34;, \u0026#34;pci_dss\u0026#34;], \u0026#34;auto_remediation\u0026#34;: True, \u0026#34;real_time_monitoring\u0026#34;: True }, cost_estimate=12000.0 )) # å®¹å™¨å®‰å…¨ components.append(SecurityComponent( id=\u0026#34;container_sec_001\u0026#34;, name=\u0026#34;Container Security Platform\u0026#34;, type=\u0026#34;container_security\u0026#34;, domain=SecurityDomain.INFRASTRUCTURE, capabilities=[\u0026#34;image_scanning\u0026#34;, \u0026#34;runtime_protection\u0026#34;, \u0026#34;compliance_checking\u0026#34;, \u0026#34;network_policy_enforcement\u0026#34;], configuration={ \u0026#34;registry_integration\u0026#34;: True, \u0026#34;ci_cd_integration\u0026#34;: True, \u0026#34;runtime_monitoring\u0026#34;: True, \u0026#34;policy_enforcement\u0026#34;: True }, cost_estimate=10000.0 )) return { \u0026#34;components\u0026#34;: components, \u0026#34;infrastructure_hardening\u0026#34;: self._design_infrastructure_hardening(), \u0026#34;monitoring_strategy\u0026#34;: self._design_infrastructure_monitoring() } def _design_management_layer(self, requirements: List[SecurityRequirement]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è®¾è®¡ç®¡ç†å±‚\u0026#34;\u0026#34;\u0026#34; components = [] # å®‰å…¨ä¿¡æ¯ä¸äº‹ä»¶ç®¡ç† components.append(SecurityComponent( id=\u0026#34;siem_001\u0026#34;, name=\u0026#34;Security Information and Event Management\u0026#34;, type=\u0026#34;siem_platform\u0026#34;, domain=SecurityDomain.INFRASTRUCTURE, capabilities=[\u0026#34;log_aggregation\u0026#34;, \u0026#34;correlation_analysis\u0026#34;, \u0026#34;threat_detection\u0026#34;, \u0026#34;incident_response\u0026#34;, \u0026#34;compliance_reporting\u0026#34;], configuration={ \u0026#34;data_sources\u0026#34;: [\u0026#34;network\u0026#34;, \u0026#34;endpoint\u0026#34;, \u0026#34;cloud\u0026#34;, \u0026#34;application\u0026#34;], \u0026#34;retention_period\u0026#34;: 365, # days \u0026#34;real_time_analysis\u0026#34;: True, \u0026#34;machine_learning\u0026#34;: True }, cost_estimate=25000.0 )) # å®‰å…¨ç¼–æ’ä¸è‡ªåŠ¨åŒ– components.append(SecurityComponent( id=\u0026#34;soar_001\u0026#34;, name=\u0026#34;Security Orchestration and Automated Response\u0026#34;, type=\u0026#34;soar_platform\u0026#34;, domain=SecurityDomain.INFRASTRUCTURE, capabilities=[\u0026#34;workflow_automation\u0026#34;, \u0026#34;playbook_execution\u0026#34;, \u0026#34;threat_intelligence\u0026#34;, \u0026#34;case_management\u0026#34;], dependencies=[\u0026#34;siem_001\u0026#34;], configuration={ \u0026#34;integration_apis\u0026#34;: 50, \u0026#34;custom_playbooks\u0026#34;: True, \u0026#34;threat_intel_feeds\u0026#34;: True, \u0026#34;automated_response\u0026#34;: True }, cost_estimate=20000.0 )) return { \u0026#34;components\u0026#34;: components, \u0026#34;governance_framework\u0026#34;: self._design_governance_framework(), \u0026#34;metrics_and_kpis\u0026#34;: self._define_security_metrics() } def _generate_identity_policies(self) -\u0026gt; List[SecurityPolicy]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆèº«ä»½ç­–ç•¥\u0026#34;\u0026#34;\u0026#34; policies = [] # å¯†ç ç­–ç•¥ policies.append(SecurityPolicy( id=\u0026#34;pwd_policy_001\u0026#34;, name=\u0026#34;Password Policy\u0026#34;, description=\u0026#34;ä¼ä¸šå¯†ç å¤æ‚åº¦å’Œç”Ÿå‘½å‘¨æœŸç­–ç•¥\u0026#34;, rules=[ {\u0026#34;type\u0026#34;: \u0026#34;min_length\u0026#34;, \u0026#34;value\u0026#34;: 12}, {\u0026#34;type\u0026#34;: \u0026#34;complexity\u0026#34;, \u0026#34;value\u0026#34;: True}, {\u0026#34;type\u0026#34;: \u0026#34;history\u0026#34;, \u0026#34;value\u0026#34;: 12}, {\u0026#34;type\u0026#34;: \u0026#34;max_age\u0026#34;, \u0026#34;value\u0026#34;: 90}, {\u0026#34;type\u0026#34;: \u0026#34;lockout_threshold\u0026#34;, \u0026#34;value\u0026#34;: 5} ], enforcement_mode=\u0026#34;enforce\u0026#34;, applicable_resources=[\u0026#34;all_users\u0026#34;] )) # MFAç­–ç•¥ policies.append(SecurityPolicy( id=\u0026#34;mfa_policy_001\u0026#34;, name=\u0026#34;Multi-Factor Authentication Policy\u0026#34;, description=\u0026#34;å¤šå› ç´ è®¤è¯å¼ºåˆ¶ç­–ç•¥\u0026#34;, rules=[ {\u0026#34;type\u0026#34;: \u0026#34;required_for\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;admin_users\u0026#34;, \u0026#34;privileged_access\u0026#34;]}, {\u0026#34;type\u0026#34;: \u0026#34;methods\u0026#34;, \u0026#34;value\u0026#34;: [\u0026#34;totp\u0026#34;, \u0026#34;push\u0026#34;, \u0026#34;biometric\u0026#34;]}, {\u0026#34;type\u0026#34;: \u0026#34;backup_methods\u0026#34;, \u0026#34;value\u0026#34;: 2} ], enforcement_mode=\u0026#34;enforce\u0026#34;, applicable_resources=[\u0026#34;all_privileged_users\u0026#34;] )) return policies def _design_network_topology(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è®¾è®¡ç½‘ç»œæ‹“æ‰‘\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;architecture\u0026#34;: \u0026#34;hub_and_spoke\u0026#34;, \u0026#34;zones\u0026#34;: { \u0026#34;dmz\u0026#34;: { \u0026#34;purpose\u0026#34;: \u0026#34;external_facing_services\u0026#34;, \u0026#34;security_level\u0026#34;: \u0026#34;high\u0026#34;, \u0026#34;allowed_traffic\u0026#34;: [\u0026#34;https\u0026#34;, \u0026#34;dns\u0026#34;] }, \u0026#34;application\u0026#34;: { \u0026#34;purpose\u0026#34;: \u0026#34;application_services\u0026#34;, \u0026#34;security_level\u0026#34;: \u0026#34;medium\u0026#34;, \u0026#34;allowed_traffic\u0026#34;: [\u0026#34;application_specific\u0026#34;] }, \u0026#34;data\u0026#34;: { \u0026#34;purpose\u0026#34;: \u0026#34;database_services\u0026#34;, \u0026#34;security_level\u0026#34;: \u0026#34;high\u0026#34;, \u0026#34;allowed_traffic\u0026#34;: [\u0026#34;database_protocols\u0026#34;] }, \u0026#34;management\u0026#34;: { \u0026#34;purpose\u0026#34;: \u0026#34;administrative_access\u0026#34;, \u0026#34;security_level\u0026#34;: \u0026#34;critical\u0026#34;, \u0026#34;allowed_traffic\u0026#34;: [\u0026#34;ssh\u0026#34;, \u0026#34;rdp\u0026#34;, \u0026#34;management_protocols\u0026#34;] } }, \u0026#34;connectivity\u0026#34;: { \u0026#34;inter_zone_rules\u0026#34;: \u0026#34;default_deny\u0026#34;, \u0026#34;inspection\u0026#34;: \u0026#34;deep_packet_inspection\u0026#34;, \u0026#34;logging\u0026#34;: \u0026#34;all_traffic\u0026#34; } } def _define_security_zones(self) -\u0026gt; List[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;å®šä¹‰å®‰å…¨åŒºåŸŸ\u0026#34;\u0026#34;\u0026#34; return [ { \u0026#34;name\u0026#34;: \u0026#34;Internet\u0026#34;, \u0026#34;trust_level\u0026#34;: 0, \u0026#34;description\u0026#34;: \u0026#34;ä¸å—ä¿¡ä»»çš„å¤–éƒ¨ç½‘ç»œ\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;DMZ\u0026#34;, \u0026#34;trust_level\u0026#34;: 25, \u0026#34;description\u0026#34;: \u0026#34;é¢å‘å¤–éƒ¨çš„æœåŠ¡åŒºåŸŸ\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;Application Zone\u0026#34;, \u0026#34;trust_level\u0026#34;: 50, \u0026#34;description\u0026#34;: \u0026#34;åº”ç”¨æœåŠ¡åŒºåŸŸ\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;Data Zone\u0026#34;, \u0026#34;trust_level\u0026#34;: 75, \u0026#34;description\u0026#34;: \u0026#34;æ•°æ®æœåŠ¡åŒºåŸŸ\u0026#34; }, { \u0026#34;name\u0026#34;: \u0026#34;Management Zone\u0026#34;, \u0026#34;trust_level\u0026#34;: 90, \u0026#34;description\u0026#34;: \u0026#34;ç®¡ç†å’Œç›‘æ§åŒºåŸŸ\u0026#34; } ] def _design_encryption_strategy(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è®¾è®¡åŠ å¯†ç­–ç•¥\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;data_at_rest\u0026#34;: { \u0026#34;algorithm\u0026#34;: \u0026#34;AES-256\u0026#34;, \u0026#34;key_management\u0026#34;: \u0026#34;hsm_backed\u0026#34;, \u0026#34;scope\u0026#34;: \u0026#34;all_sensitive_data\u0026#34; }, \u0026#34;data_in_transit\u0026#34;: { \u0026#34;protocol\u0026#34;: \u0026#34;TLS 1.3\u0026#34;, \u0026#34;certificate_management\u0026#34;: \u0026#34;automated\u0026#34;, \u0026#34;perfect_forward_secrecy\u0026#34;: True }, \u0026#34;data_in_use\u0026#34;: { \u0026#34;technologies\u0026#34;: [\u0026#34;homomorphic_encryption\u0026#34;, \u0026#34;secure_enclaves\u0026#34;], \u0026#34;use_cases\u0026#34;: [\u0026#34;analytics\u0026#34;, \u0026#34;machine_learning\u0026#34;] }, \u0026#34;key_management\u0026#34;: { \u0026#34;rotation_frequency\u0026#34;: \u0026#34;annual\u0026#34;, \u0026#34;backup_strategy\u0026#34;: \u0026#34;geographic_distribution\u0026#34;, \u0026#34;access_control\u0026#34;: \u0026#34;role_based\u0026#34; } } def _define_data_classification(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;å®šä¹‰æ•°æ®åˆ†ç±»\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;public\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;å¯å…¬å¼€è®¿é—®çš„æ•°æ®\u0026#34;, \u0026#34;protection_level\u0026#34;: \u0026#34;basic\u0026#34;, \u0026#34;retention_period\u0026#34;: \u0026#34;indefinite\u0026#34; }, \u0026#34;internal\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;å†…éƒ¨ä½¿ç”¨æ•°æ®\u0026#34;, \u0026#34;protection_level\u0026#34;: \u0026#34;standard\u0026#34;, \u0026#34;retention_period\u0026#34;: \u0026#34;7_years\u0026#34; }, \u0026#34;confidential\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;æœºå¯†æ•°æ®\u0026#34;, \u0026#34;protection_level\u0026#34;: \u0026#34;enhanced\u0026#34;, \u0026#34;retention_period\u0026#34;: \u0026#34;as_required\u0026#34; }, \u0026#34;restricted\u0026#34;: { \u0026#34;description\u0026#34;: \u0026#34;å—é™è®¿é—®æ•°æ®\u0026#34;, \u0026#34;protection_level\u0026#34;: \u0026#34;maximum\u0026#34;, \u0026#34;retention_period\u0026#34;: \u0026#34;as_required\u0026#34; } } def _design_secure_development_lifecycle(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è®¾è®¡å®‰å…¨å¼€å‘ç”Ÿå‘½å‘¨æœŸ\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;phases\u0026#34;: { \u0026#34;planning\u0026#34;: { \u0026#34;activities\u0026#34;: [\u0026#34;threat_modeling\u0026#34;, \u0026#34;security_requirements\u0026#34;], \u0026#34;deliverables\u0026#34;: [\u0026#34;security_plan\u0026#34;, \u0026#34;threat_model\u0026#34;] }, \u0026#34;design\u0026#34;: { \u0026#34;activities\u0026#34;: [\u0026#34;security_architecture_review\u0026#34;, \u0026#34;secure_design_patterns\u0026#34;], \u0026#34;deliverables\u0026#34;: [\u0026#34;security_architecture\u0026#34;, \u0026#34;design_review_report\u0026#34;] }, \u0026#34;implementation\u0026#34;: { \u0026#34;activities\u0026#34;: [\u0026#34;secure_coding\u0026#34;, \u0026#34;code_review\u0026#34;, \u0026#34;static_analysis\u0026#34;], \u0026#34;deliverables\u0026#34;: [\u0026#34;secure_code\u0026#34;, \u0026#34;code_review_report\u0026#34;] }, \u0026#34;testing\u0026#34;: { \u0026#34;activities\u0026#34;: [\u0026#34;security_testing\u0026#34;, \u0026#34;penetration_testing\u0026#34;, \u0026#34;vulnerability_assessment\u0026#34;], \u0026#34;deliverables\u0026#34;: [\u0026#34;security_test_report\u0026#34;, \u0026#34;penetration_test_report\u0026#34;] }, \u0026#34;deployment\u0026#34;: { \u0026#34;activities\u0026#34;: [\u0026#34;security_configuration\u0026#34;, \u0026#34;deployment_verification\u0026#34;], \u0026#34;deliverables\u0026#34;: [\u0026#34;deployment_checklist\u0026#34;, \u0026#34;security_configuration\u0026#34;] }, \u0026#34;maintenance\u0026#34;: { \u0026#34;activities\u0026#34;: [\u0026#34;security_monitoring\u0026#34;, \u0026#34;vulnerability_management\u0026#34;, \u0026#34;incident_response\u0026#34;], \u0026#34;deliverables\u0026#34;: [\u0026#34;monitoring_reports\u0026#34;, \u0026#34;incident_reports\u0026#34;] } }, \u0026#34;tools\u0026#34;: { \u0026#34;sast\u0026#34;: \u0026#34;static_application_security_testing\u0026#34;, \u0026#34;dast\u0026#34;: \u0026#34;dynamic_application_security_testing\u0026#34;, \u0026#34;iast\u0026#34;: \u0026#34;interactive_application_security_testing\u0026#34;, \u0026#34;sca\u0026#34;: \u0026#34;software_composition_analysis\u0026#34; } } def _design_runtime_protection(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è®¾è®¡è¿è¡Œæ—¶ä¿æŠ¤\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;application_protection\u0026#34;: { \u0026#34;rasp\u0026#34;: \u0026#34;runtime_application_self_protection\u0026#34;, \u0026#34;waf\u0026#34;: \u0026#34;web_application_firewall\u0026#34;, \u0026#34;api_gateway\u0026#34;: \u0026#34;api_security_gateway\u0026#34; }, \u0026#34;infrastructure_protection\u0026#34;: { \u0026#34;container_security\u0026#34;: \u0026#34;runtime_container_protection\u0026#34;, \u0026#34;host_security\u0026#34;: \u0026#34;endpoint_detection_response\u0026#34;, \u0026#34;network_security\u0026#34;: \u0026#34;network_detection_response\u0026#34; }, \u0026#34;monitoring\u0026#34;: { \u0026#34;application_monitoring\u0026#34;: \u0026#34;apm_with_security\u0026#34;, \u0026#34;infrastructure_monitoring\u0026#34;: \u0026#34;infrastructure_security_monitoring\u0026#34;, \u0026#34;user_behavior\u0026#34;: \u0026#34;user_behavior_analytics\u0026#34; } } def _design_infrastructure_hardening(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è®¾è®¡åŸºç¡€è®¾æ–½åŠ å›º\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;operating_system\u0026#34;: { \u0026#34;baseline\u0026#34;: \u0026#34;cis_benchmarks\u0026#34;, \u0026#34;patch_management\u0026#34;: \u0026#34;automated\u0026#34;, \u0026#34;configuration_management\u0026#34;: \u0026#34;infrastructure_as_code\u0026#34; }, \u0026#34;container_platform\u0026#34;: { \u0026#34;image_security\u0026#34;: \u0026#34;vulnerability_scanning\u0026#34;, \u0026#34;runtime_security\u0026#34;: \u0026#34;behavior_monitoring\u0026#34;, \u0026#34;network_policies\u0026#34;: \u0026#34;zero_trust_networking\u0026#34; }, \u0026#34;cloud_services\u0026#34;: { \u0026#34;configuration_management\u0026#34;: \u0026#34;cloud_security_posture_management\u0026#34;, \u0026#34;access_control\u0026#34;: \u0026#34;least_privilege_access\u0026#34;, \u0026#34;monitoring\u0026#34;: \u0026#34;cloud_workload_protection\u0026#34; } } def _design_infrastructure_monitoring(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è®¾è®¡åŸºç¡€è®¾æ–½ç›‘æ§\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;metrics\u0026#34;: { \u0026#34;security_events\u0026#34;: \u0026#34;real_time_collection\u0026#34;, \u0026#34;performance_metrics\u0026#34;: \u0026#34;continuous_monitoring\u0026#34;, \u0026#34;compliance_status\u0026#34;: \u0026#34;automated_assessment\u0026#34; }, \u0026#34;alerting\u0026#34;: { \u0026#34;threat_detection\u0026#34;: \u0026#34;machine_learning_based\u0026#34;, \u0026#34;anomaly_detection\u0026#34;: \u0026#34;behavioral_analysis\u0026#34;, \u0026#34;incident_response\u0026#34;: \u0026#34;automated_workflows\u0026#34; }, \u0026#34;reporting\u0026#34;: { \u0026#34;security_dashboards\u0026#34;: \u0026#34;real_time_visibility\u0026#34;, \u0026#34;compliance_reports\u0026#34;: \u0026#34;automated_generation\u0026#34;, \u0026#34;executive_summaries\u0026#34;: \u0026#34;periodic_reporting\u0026#34; } } def _design_governance_framework(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è®¾è®¡æ²»ç†æ¡†æ¶\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;policies\u0026#34;: { \u0026#34;security_policy\u0026#34;: \u0026#34;enterprise_security_policy\u0026#34;, \u0026#34;data_governance\u0026#34;: \u0026#34;data_protection_policy\u0026#34;, \u0026#34;incident_response\u0026#34;: \u0026#34;incident_response_policy\u0026#34; }, \u0026#34;procedures\u0026#34;: { \u0026#34;access_management\u0026#34;: \u0026#34;identity_lifecycle_management\u0026#34;, \u0026#34;change_management\u0026#34;: \u0026#34;security_change_control\u0026#34;, \u0026#34;vendor_management\u0026#34;: \u0026#34;third_party_risk_management\u0026#34; }, \u0026#34;roles_responsibilities\u0026#34;: { \u0026#34;ciso\u0026#34;: \u0026#34;chief_information_security_officer\u0026#34;, \u0026#34;security_team\u0026#34;: \u0026#34;security_operations_team\u0026#34;, \u0026#34;business_units\u0026#34;: \u0026#34;security_champions\u0026#34; } } def _define_security_metrics(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;å®šä¹‰å®‰å…¨æŒ‡æ ‡\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;operational_metrics\u0026#34;: { \u0026#34;mean_time_to_detect\u0026#34;: \u0026#34;mttr_detection\u0026#34;, \u0026#34;mean_time_to_respond\u0026#34;: \u0026#34;mttr_response\u0026#34;, \u0026#34;security_incident_volume\u0026#34;: \u0026#34;incident_count\u0026#34; }, \u0026#34;risk_metrics\u0026#34;: { \u0026#34;vulnerability_exposure\u0026#34;: \u0026#34;vulnerability_metrics\u0026#34;, \u0026#34;threat_landscape\u0026#34;: \u0026#34;threat_intelligence\u0026#34;, \u0026#34;compliance_posture\u0026#34;: \u0026#34;compliance_score\u0026#34; }, \u0026#34;business_metrics\u0026#34;: { \u0026#34;security_investment_roi\u0026#34;: \u0026#34;return_on_investment\u0026#34;, \u0026#34;business_impact\u0026#34;: \u0026#34;availability_metrics\u0026#34;, \u0026#34;customer_trust\u0026#34;: \u0026#34;security_perception\u0026#34; } } def generate_implementation_roadmap(self, architecture: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆå®æ–½è·¯çº¿å›¾\u0026#34;\u0026#34;\u0026#34; roadmap = { \u0026#34;phase_1\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Foundation Phase\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;3-6 months\u0026#34;, \u0026#34;objectives\u0026#34;: [ \u0026#34;Establish identity and access management\u0026#34;, \u0026#34;Implement basic network security\u0026#34;, \u0026#34;Deploy endpoint protection\u0026#34; ], \u0026#34;deliverables\u0026#34;: [ \u0026#34;Identity provider deployment\u0026#34;, \u0026#34;Network segmentation\u0026#34;, \u0026#34;Endpoint security rollout\u0026#34; ], \u0026#34;success_criteria\u0026#34;: [ \u0026#34;100% user enrollment in IAM\u0026#34;, \u0026#34;Network zones implemented\u0026#34;, \u0026#34;All endpoints protected\u0026#34; ] }, \u0026#34;phase_2\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Detection and Response Phase\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;6-9 months\u0026#34;, \u0026#34;objectives\u0026#34;: [ \u0026#34;Deploy SIEM and monitoring\u0026#34;, \u0026#34;Establish SOC capabilities\u0026#34;, \u0026#34;Implement threat detection\u0026#34; ], \u0026#34;deliverables\u0026#34;: [ \u0026#34;SIEM platform deployment\u0026#34;, \u0026#34;SOC establishment\u0026#34;, \u0026#34;Threat detection rules\u0026#34; ], \u0026#34;success_criteria\u0026#34;: [ \u0026#34;24/7 monitoring capability\u0026#34;, \u0026#34;Incident response procedures\u0026#34;, \u0026#34;Threat detection accuracy \u0026gt;95%\u0026#34; ] }, \u0026#34;phase_3\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Advanced Protection Phase\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;9-12 months\u0026#34;, \u0026#34;objectives\u0026#34;: [ \u0026#34;Implement zero trust architecture\u0026#34;, \u0026#34;Deploy advanced threat protection\u0026#34;, \u0026#34;Establish security automation\u0026#34; ], \u0026#34;deliverables\u0026#34;: [ \u0026#34;Zero trust implementation\u0026#34;, \u0026#34;Advanced threat protection\u0026#34;, \u0026#34;Security automation platform\u0026#34; ], \u0026#34;success_criteria\u0026#34;: [ \u0026#34;Zero trust policies enforced\u0026#34;, \u0026#34;Advanced threats blocked\u0026#34;, \u0026#34;Automated response workflows\u0026#34; ] }, \u0026#34;phase_4\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Optimization Phase\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;12+ months\u0026#34;, \u0026#34;objectives\u0026#34;: [ \u0026#34;Continuous improvement\u0026#34;, \u0026#34;Advanced analytics\u0026#34;, \u0026#34;Threat intelligence integration\u0026#34; ], \u0026#34;deliverables\u0026#34;: [ \u0026#34;Security metrics dashboard\u0026#34;, \u0026#34;Threat intelligence platform\u0026#34;, \u0026#34;Security optimization reports\u0026#34; ], \u0026#34;success_criteria\u0026#34;: [ \u0026#34;Proactive threat hunting\u0026#34;, \u0026#34;Predictive security analytics\u0026#34;, \u0026#34;Continuous security improvement\u0026#34; ] } } return roadmap # ä½¿ç”¨ç¤ºä¾‹ def security_architecture_example(): \u0026#34;\u0026#34;\u0026#34;å®‰å…¨æ¶æ„è®¾è®¡ç¤ºä¾‹\u0026#34;\u0026#34;\u0026#34; architect = CloudSecurityArchitect() # ä¸šåŠ¡ä¸Šä¸‹æ–‡ business_context = { \u0026#34;industry\u0026#34;: \u0026#34;financial_services\u0026#34;, \u0026#34;data_sensitivity\u0026#34;: \u0026#34;high\u0026#34;, \u0026#34;compliance\u0026#34;: [\u0026#34;SOC2\u0026#34;, \u0026#34;PCI_DSS\u0026#34;, \u0026#34;GDPR\u0026#34;], \u0026#34;network_architecture\u0026#34;: \u0026#34;hybrid\u0026#34;, \u0026#34;application_types\u0026#34;: [\u0026#34;web\u0026#34;, \u0026#34;api\u0026#34;, \u0026#34;mobile\u0026#34;], \u0026#34;cloud_providers\u0026#34;: [\u0026#34;aws\u0026#34;, \u0026#34;azure\u0026#34;] } print(\u0026#34;=== å®‰å…¨éœ€æ±‚åˆ†æ ===\u0026#34;) requirements = architect.analyze_security_requirements(business_context) print(f\u0026#34;è¯†åˆ«å®‰å…¨éœ€æ±‚: {len(requirements)}ä¸ª\u0026#34;) for req in requirements[:5]: # æ˜¾ç¤ºå‰5ä¸ªéœ€æ±‚ print(f\u0026#34;- {req.name}: {req.description}\u0026#34;) print(\u0026#34;\\n=== å®‰å…¨æ¶æ„è®¾è®¡ ===\u0026#34;) architecture = architect.design_security_architecture(requirements) for layer, details in architecture.items(): print(f\u0026#34;\\n{layer.upper()}:\u0026#34;) if \u0026#39;components\u0026#39; in details: print(f\u0026#34; ç»„ä»¶æ•°é‡: {len(details[\u0026#39;components\u0026#39;])}\u0026#34;) for component in details[\u0026#39;components\u0026#39;]: print(f\u0026#34; - {component.name}: {\u0026#39;, \u0026#39;.join(component.capabilities)}\u0026#34;) print(\u0026#34;\\n=== å®æ–½è·¯çº¿å›¾ ===\u0026#34;) roadmap = architect.generate_implementation_roadmap(architecture) for phase_id, phase in roadmap.items(): print(f\u0026#34;\\n{phase[\u0026#39;name\u0026#39;]} ({phase[\u0026#39;duration\u0026#39;]}):\u0026#34;) print(f\u0026#34; ç›®æ ‡: {\u0026#39;, \u0026#39;.join(phase[\u0026#39;objectives\u0026#39;])}\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: security_architecture_example() 2. é›¶ä¿¡ä»»å®‰å…¨æ¨¡å‹ 2.1 é›¶ä¿¡ä»»æ ¸å¿ƒåŸåˆ™ é›¶ä¿¡ä»»å®‰å…¨æ¨¡å‹åŸºäº\u0026quot;æ°¸ä¸ä¿¡ä»»ï¼Œå§‹ç»ˆéªŒè¯\u0026quot;çš„æ ¸å¿ƒç†å¿µï¼Œé‡æ–°å®šä¹‰äº†ä¼ä¸šå®‰å…¨è¾¹ç•Œã€‚\n","content":"äº‘å®‰å…¨æ¶æ„è®¾è®¡ä¸é›¶ä¿¡ä»»å®æ–½ï¼šæ„å»ºç°ä»£ä¼ä¸šå®‰å…¨é˜²æŠ¤ä½“ç³» ç›®å½• äº‘å®‰å…¨æ¶æ„æ¦‚è¿° é›¶ä¿¡ä»»å®‰å…¨æ¨¡å‹ èº«ä»½ä¸è®¿é—®ç®¡ç†(IAM) ç½‘ç»œå®‰å…¨æ¶æ„ æ•°æ®ä¿æŠ¤ä¸åŠ å¯† å¨èƒæ£€æµ‹ä¸å“åº” åˆè§„æ€§ä¸æ²»ç† å®‰å…¨è¿è¥ä¸­å¿ƒ(SOC) æœ€ä½³å®è·µä¸æ€»ç»“ 1. äº‘å®‰å…¨æ¶æ„æ¦‚è¿° 1.1 äº‘å®‰å…¨æŒ‘æˆ˜ä¸æœºé‡ ç°ä»£ä¼ä¸šåœ¨äº‘åŒ–è½¬å‹è¿‡ç¨‹ä¸­é¢ä¸´ç€å‰æ‰€æœªæœ‰çš„å®‰å…¨æŒ‘æˆ˜ã€‚ä¼ ç»Ÿçš„è¾¹ç•Œå®‰å…¨æ¨¡å‹å·²æ— æ³•é€‚åº”äº‘ç¯å¢ƒçš„åŠ¨æ€æ€§å’Œå¤æ‚æ€§ï¼Œéœ€è¦æ„å»ºå…¨æ–°çš„å®‰å…¨æ¶æ„æ¥åº”å¯¹è¿™äº›æŒ‘æˆ˜ã€‚\ngraph TB subgraph \u0026amp;#34;ä¼ ç»Ÿå®‰å…¨æ¨¡å‹\u0026amp;#34; A[ç½‘ç»œè¾¹ç•Œ] --\u0026amp;gt; B[é˜²ç«å¢™] B --\u0026amp;gt; C[å†…ç½‘ä¿¡ä»»] C --\u0026amp;gt; D[é™æ€è®¿é—®æ§åˆ¶] end subgraph \u0026amp;#34;äº‘å®‰å…¨æŒ‘æˆ˜\u0026amp;#34; E[è¾¹ç•Œæ¨¡ç³Š] --\u0026amp;gt; F[åŠ¨æ€ç¯å¢ƒ] F --\u0026amp;gt; G[å¤šäº‘å¤æ‚æ€§] G --\u0026amp;gt; H[åˆè§„è¦æ±‚] end subgraph \u0026amp;#34;é›¶ä¿¡ä»»æ¨¡å‹\u0026amp;#34; I[æ°¸ä¸ä¿¡ä»»] --\u0026amp;gt; J[æŒç»­éªŒè¯] J --\u0026amp;gt; K[æœ€å°æƒé™] K --\u0026amp;gt; L[åŠ¨æ€è®¿é—®æ§åˆ¶] end A -.-\u0026amp;gt; E E --\u0026amp;gt; I â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["äº‘å®‰å…¨","é›¶ä¿¡ä»»","ç½‘ç»œå®‰å…¨","èº«ä»½è®¤è¯","æ•°æ®ä¿æŠ¤","åˆè§„æ²»ç†"],"categories":["äº‘æ¶æ„"],"author":"äº‘æ¶æ„ä¸“å®¶","readingTime":48,"wordCount":10200,"section":"posts","type":"posts","draft":false,"featured":false,"series":null},{"title":"äº‘æˆæœ¬ä¼˜åŒ–ç­–ç•¥ä¸FinOpså®è·µï¼šæ„å»ºä¼ä¸šäº‘è´¢åŠ¡ç®¡ç†ä½“ç³»","url":"https://www.dishuihengxin.com/posts/cloud-cost-optimization-finops/","summary":"äº‘æˆæœ¬ä¼˜åŒ–ç­–ç•¥ä¸FinOpså®è·µï¼šæ„å»ºä¼ä¸šäº‘è´¢åŠ¡ç®¡ç†ä½“ç³» ç›®å½• FinOpsæ¦‚è¿°ä¸ä»·å€¼ äº‘æˆæœ¬åˆ†æä¸å¯è§†åŒ– é¢„ç®—ç®¡ç†ä¸æˆæœ¬æ§åˆ¶ èµ„æºä¼˜åŒ–ç­–ç•¥ è‡ªåŠ¨åŒ–æˆæœ¬æ²»ç† å¤šäº‘æˆæœ¬ç®¡ç† FinOpsç»„ç»‡ä¸æµç¨‹ æœ€ä½³å®è·µä¸æ¡ˆä¾‹ 1. FinOpsæ¦‚è¿°ä¸ä»·å€¼ 1.1 FinOpsæ ¸å¿ƒç†å¿µ FinOpsï¼ˆFinancial Operationsï¼‰æ˜¯ä¸€ç§äº‘è´¢åŠ¡ç®¡ç†å®è·µï¼Œæ—¨åœ¨é€šè¿‡è·¨èŒèƒ½åä½œæ¥ä¼˜åŒ–äº‘æˆæœ¬ï¼Œå®ç°ä¸šåŠ¡ä»·å€¼æœ€å¤§åŒ–ã€‚\ngraph TB subgraph \u0026#34;FinOpsæ ¸å¿ƒåŸåˆ™\u0026#34; A[å›¢é˜Ÿåä½œ] --\u0026gt; B[ä¸šåŠ¡ä»·å€¼é©±åŠ¨] B --\u0026gt; C[æ•°æ®é©±åŠ¨å†³ç­–] C --\u0026gt; D[æŒç»­ä¼˜åŒ–] end subgraph \u0026#34;FinOpsç”Ÿå‘½å‘¨æœŸ\u0026#34; E[é€šçŸ¥Inform] --\u0026gt; F[ä¼˜åŒ–Optimize] F --\u0026gt; G[è¿è¥Operate] G --\u0026gt; E end subgraph \u0026#34;å…³é”®åˆ©ç›Šç›¸å…³è€…\u0026#34; H[è´¢åŠ¡å›¢é˜Ÿ] I[å·¥ç¨‹å›¢é˜Ÿ] J[ä¸šåŠ¡å›¢é˜Ÿ] K[é«˜ç®¡å›¢é˜Ÿ] end 1.2 äº‘æˆæœ¬ç®¡ç†å¹³å° from dataclasses import dataclass from typing import Dict, List, Optional, Tuple from datetime import datetime, timedelta from enum import Enum import json class CostCategory(Enum): COMPUTE = \u0026#34;compute\u0026#34; STORAGE = \u0026#34;storage\u0026#34; NETWORK = \u0026#34;network\u0026#34; DATABASE = \u0026#34;database\u0026#34; SECURITY = \u0026#34;security\u0026#34; ANALYTICS = \u0026#34;analytics\u0026#34; OTHER = \u0026#34;other\u0026#34; class OptimizationAction(Enum): RIGHTSIZING = \u0026#34;rightsizing\u0026#34; RESERVED_INSTANCES = \u0026#34;reserved_instances\u0026#34; SPOT_INSTANCES = \u0026#34;spot_instances\u0026#34; STORAGE_OPTIMIZATION = \u0026#34;storage_optimization\u0026#34; NETWORK_OPTIMIZATION = \u0026#34;network_optimization\u0026#34; SCHEDULING = \u0026#34;scheduling\u0026#34; @dataclass class CostData: resource_id: str service: str category: CostCategory cost: float usage: Dict[str, float] tags: Dict[str, str] timestamp: datetime region: str account_id: str @dataclass class OptimizationRecommendation: resource_id: str action: OptimizationAction current_cost: float optimized_cost: float savings: float confidence: float implementation_effort: str description: str risk_level: str class CloudCostOptimizer: def __init__(self): self.cost_data: List[CostData] = [] self.recommendations: List[OptimizationRecommendation] = [] self.budgets: Dict[str, Dict] = {} self.alerts: List[Dict] = [] def collect_cost_data(self, provider: str, time_range: int = 30) -\u0026gt; List[CostData]: \u0026#34;\u0026#34;\u0026#34;æ”¶é›†äº‘æˆæœ¬æ•°æ®\u0026#34;\u0026#34;\u0026#34; # æ¨¡æ‹Ÿå¤šäº‘æˆæœ¬æ•°æ®æ”¶é›† sample_data = [] services = [\u0026#34;EC2\u0026#34;, \u0026#34;RDS\u0026#34;, \u0026#34;S3\u0026#34;, \u0026#34;Lambda\u0026#34;, \u0026#34;ELB\u0026#34;, \u0026#34;CloudFront\u0026#34;] categories = [CostCategory.COMPUTE, CostCategory.DATABASE, CostCategory.STORAGE, CostCategory.COMPUTE, CostCategory.NETWORK, CostCategory.NETWORK] for i in range(100): service = services[i % len(services)] category = categories[i % len(categories)] cost_data = CostData( resource_id=f\u0026#34;{provider}-{service}-{i:03d}\u0026#34;, service=service, category=category, cost=round(50 + (i * 10) % 500, 2), usage={ \u0026#34;cpu_hours\u0026#34;: round(24 * 30 * (0.3 + (i % 7) * 0.1), 2), \u0026#34;memory_gb_hours\u0026#34;: round(8 * 24 * 30 * (0.4 + (i % 5) * 0.1), 2), \u0026#34;storage_gb\u0026#34;: round(100 + (i * 50) % 1000, 2) }, tags={ \u0026#34;Environment\u0026#34;: [\u0026#34;prod\u0026#34;, \u0026#34;staging\u0026#34;, \u0026#34;dev\u0026#34;][i % 3], \u0026#34;Team\u0026#34;: [\u0026#34;backend\u0026#34;, \u0026#34;frontend\u0026#34;, \u0026#34;data\u0026#34;][i % 3], \u0026#34;Project\u0026#34;: f\u0026#34;project-{(i % 5) + 1}\u0026#34; }, timestamp=datetime.now() - timedelta(days=i % 30), region=[\u0026#34;us-east-1\u0026#34;, \u0026#34;us-west-2\u0026#34;, \u0026#34;eu-west-1\u0026#34;][i % 3], account_id=f\u0026#34;account-{(i % 3) + 1}\u0026#34; ) sample_data.append(cost_data) self.cost_data.extend(sample_data) return sample_data def analyze_cost_trends(self) -\u0026gt; Dict[str, any]: \u0026#34;\u0026#34;\u0026#34;åˆ†ææˆæœ¬è¶‹åŠ¿\u0026#34;\u0026#34;\u0026#34; if not self.cost_data: return {} # æŒ‰æœåŠ¡åˆ†ç»„åˆ†æ service_costs = {} category_costs = {} daily_costs = {} for data in self.cost_data: # æœåŠ¡æˆæœ¬ if data.service not in service_costs: service_costs[data.service] = 0 service_costs[data.service] += data.cost # ç±»åˆ«æˆæœ¬ category = data.category.value if category not in category_costs: category_costs[category] = 0 category_costs[category] += data.cost # æ—¥æˆæœ¬ date_key = data.timestamp.strftime(\u0026#34;%Y-%m-%d\u0026#34;) if date_key not in daily_costs: daily_costs[date_key] = 0 daily_costs[date_key] += data.cost # è®¡ç®—å¢é•¿ç‡ sorted_dates = sorted(daily_costs.keys()) growth_rate = 0 if len(sorted_dates) \u0026gt;= 2: recent_cost = daily_costs[sorted_dates[-1]] previous_cost = daily_costs[sorted_dates[-2]] if previous_cost \u0026gt; 0: growth_rate = ((recent_cost - previous_cost) / previous_cost) * 100 return { \u0026#34;total_cost\u0026#34;: sum(data.cost for data in self.cost_data), \u0026#34;service_breakdown\u0026#34;: service_costs, \u0026#34;category_breakdown\u0026#34;: category_costs, \u0026#34;daily_trends\u0026#34;: daily_costs, \u0026#34;growth_rate\u0026#34;: round(growth_rate, 2), \u0026#34;top_cost_services\u0026#34;: sorted(service_costs.items(), key=lambda x: x[1], reverse=True)[:5] } def generate_optimization_recommendations(self) -\u0026gt; List[OptimizationRecommendation]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆä¼˜åŒ–å»ºè®®\u0026#34;\u0026#34;\u0026#34; recommendations = [] # åˆ†ææ¯ä¸ªèµ„æºçš„ä¼˜åŒ–æœºä¼š for data in self.cost_data: # è®¡ç®—èµ„æºåˆ©ç”¨ç‡ cpu_utilization = data.usage.get(\u0026#34;cpu_hours\u0026#34;, 0) / (24 * 30) if data.usage.get(\u0026#34;cpu_hours\u0026#34;) else 0 # å³è°ƒå¤§å°å»ºè®® if cpu_utilization \u0026lt; 0.3 and data.cost \u0026gt; 100: savings = data.cost * 0.4 recommendations.append(OptimizationRecommendation( resource_id=data.resource_id, action=OptimizationAction.RIGHTSIZING, current_cost=data.cost, optimized_cost=data.cost - savings, savings=savings, confidence=0.85, implementation_effort=\u0026#34;Low\u0026#34;, description=f\u0026#34;Resource {data.resource_id} shows low utilization ({cpu_utilization:.1%}). Consider downsizing.\u0026#34;, risk_level=\u0026#34;Low\u0026#34; )) # é¢„ç•™å®ä¾‹å»ºè®® if data.service in [\u0026#34;EC2\u0026#34;, \u0026#34;RDS\u0026#34;] and data.cost \u0026gt; 200: savings = data.cost * 0.3 recommendations.append(OptimizationRecommendation( resource_id=data.resource_id, action=OptimizationAction.RESERVED_INSTANCES, current_cost=data.cost, optimized_cost=data.cost - savings, savings=savings, confidence=0.9, implementation_effort=\u0026#34;Medium\u0026#34;, description=f\u0026#34;Consider purchasing reserved instances for {data.service}\u0026#34;, risk_level=\u0026#34;Low\u0026#34; )) # å­˜å‚¨ä¼˜åŒ–å»ºè®® if data.category == CostCategory.STORAGE and data.cost \u0026gt; 50: savings = data.cost * 0.25 recommendations.append(OptimizationRecommendation( resource_id=data.resource_id, action=OptimizationAction.STORAGE_OPTIMIZATION, current_cost=data.cost, optimized_cost=data.cost - savings, savings=savings, confidence=0.75, implementation_effort=\u0026#34;Low\u0026#34;, description=\u0026#34;Optimize storage class and lifecycle policies\u0026#34;, risk_level=\u0026#34;Low\u0026#34; )) # æŒ‰èŠ‚çœé‡‘é¢æ’åº recommendations.sort(key=lambda x: x.savings, reverse=True) self.recommendations = recommendations[:20] # å–å‰20ä¸ªå»ºè®® return self.recommendations def create_budget_alerts(self, budget_config: Dict[str, any]) -\u0026gt; List[Dict]: \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºé¢„ç®—å‘Šè­¦\u0026#34;\u0026#34;\u0026#34; alerts = [] current_spend = sum(data.cost for data in self.cost_data) budget_amount = budget_config.get(\u0026#34;amount\u0026#34;, 10000) alert_thresholds = budget_config.get(\u0026#34;thresholds\u0026#34;, [50, 80, 100]) for threshold in alert_thresholds: threshold_amount = budget_amount * (threshold / 100) if current_spend \u0026gt;= threshold_amount: alert = { \u0026#34;type\u0026#34;: \u0026#34;budget_alert\u0026#34;, \u0026#34;severity\u0026#34;: \u0026#34;high\u0026#34; if threshold \u0026gt;= 100 else \u0026#34;medium\u0026#34; if threshold \u0026gt;= 80 else \u0026#34;low\u0026#34;, \u0026#34;message\u0026#34;: f\u0026#34;Current spend ${current_spend:.2f} has exceeded {threshold}% of budget ${budget_amount:.2f}\u0026#34;, \u0026#34;threshold\u0026#34;: threshold, \u0026#34;current_spend\u0026#34;: current_spend, \u0026#34;budget_amount\u0026#34;: budget_amount, \u0026#34;timestamp\u0026#34;: datetime.now().isoformat() } alerts.append(alert) self.alerts.extend(alerts) return alerts def generate_cost_report(self) -\u0026gt; Dict[str, any]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆæˆæœ¬æŠ¥å‘Š\u0026#34;\u0026#34;\u0026#34; trends = self.analyze_cost_trends() recommendations = self.generate_optimization_recommendations() total_potential_savings = sum(rec.savings for rec in recommendations) return { \u0026#34;report_date\u0026#34;: datetime.now().isoformat(), \u0026#34;summary\u0026#34;: { \u0026#34;total_cost\u0026#34;: trends.get(\u0026#34;total_cost\u0026#34;, 0), \u0026#34;growth_rate\u0026#34;: trends.get(\u0026#34;growth_rate\u0026#34;, 0), \u0026#34;potential_savings\u0026#34;: total_potential_savings, \u0026#34;optimization_opportunities\u0026#34;: len(recommendations) }, \u0026#34;cost_breakdown\u0026#34;: { \u0026#34;by_service\u0026#34;: trends.get(\u0026#34;service_breakdown\u0026#34;, {}), \u0026#34;by_category\u0026#34;: trends.get(\u0026#34;category_breakdown\u0026#34;, {}), \u0026#34;top_services\u0026#34;: trends.get(\u0026#34;top_cost_services\u0026#34;, []) }, \u0026#34;optimization_recommendations\u0026#34;: [ { \u0026#34;resource_id\u0026#34;: rec.resource_id, \u0026#34;action\u0026#34;: rec.action.value, \u0026#34;savings\u0026#34;: rec.savings, \u0026#34;confidence\u0026#34;: rec.confidence, \u0026#34;description\u0026#34;: rec.description } for rec in recommendations[:10] ], \u0026#34;trends\u0026#34;: trends.get(\u0026#34;daily_trends\u0026#34;, {}), \u0026#34;alerts\u0026#34;: self.alerts } # ä½¿ç”¨ç¤ºä¾‹ def cost_optimization_example(): optimizer = CloudCostOptimizer() # æ”¶é›†æˆæœ¬æ•°æ® print(\u0026#34;æ”¶é›†AWSæˆæœ¬æ•°æ®...\u0026#34;) aws_data = optimizer.collect_cost_data(\u0026#34;AWS\u0026#34;) print(f\u0026#34;æ”¶é›†åˆ° {len(aws_data)} æ¡AWSæˆæœ¬è®°å½•\u0026#34;) print(\u0026#34;\\næ”¶é›†Azureæˆæœ¬æ•°æ®...\u0026#34;) azure_data = optimizer.collect_cost_data(\u0026#34;Azure\u0026#34;) print(f\u0026#34;æ”¶é›†åˆ° {len(azure_data)} æ¡Azureæˆæœ¬è®°å½•\u0026#34;) # åˆ†ææˆæœ¬è¶‹åŠ¿ print(\u0026#34;\\nåˆ†ææˆæœ¬è¶‹åŠ¿...\u0026#34;) trends = optimizer.analyze_cost_trends() print(f\u0026#34;æ€»æˆæœ¬: ${trends[\u0026#39;total_cost\u0026#39;]:.2f}\u0026#34;) print(f\u0026#34;æˆæœ¬å¢é•¿ç‡: {trends[\u0026#39;growth_rate\u0026#39;]:.2f}%\u0026#34;) print(f\u0026#34;å‰5å¤§æœåŠ¡æˆæœ¬: {trends[\u0026#39;top_cost_services\u0026#39;]}\u0026#34;) # ç”Ÿæˆä¼˜åŒ–å»ºè®® print(\u0026#34;\\nç”Ÿæˆä¼˜åŒ–å»ºè®®...\u0026#34;) recommendations = optimizer.generate_optimization_recommendations() print(f\u0026#34;ç”Ÿæˆ {len(recommendations)} æ¡ä¼˜åŒ–å»ºè®®\u0026#34;) total_savings = sum(rec.savings for rec in recommendations) print(f\u0026#34;æ½œåœ¨èŠ‚çœ: ${total_savings:.2f}\u0026#34;) # åˆ›å»ºé¢„ç®—å‘Šè­¦ print(\u0026#34;\\nåˆ›å»ºé¢„ç®—å‘Šè­¦...\u0026#34;) budget_config = {\u0026#34;amount\u0026#34;: 15000, \u0026#34;thresholds\u0026#34;: [50, 80, 100]} alerts = optimizer.create_budget_alerts(budget_config) print(f\u0026#34;ç”Ÿæˆ {len(alerts)} æ¡é¢„ç®—å‘Šè­¦\u0026#34;) # ç”ŸæˆæŠ¥å‘Š print(\u0026#34;\\nç”Ÿæˆæˆæœ¬æŠ¥å‘Š...\u0026#34;) report = optimizer.generate_cost_report() print(\u0026#34;æˆæœ¬æŠ¥å‘Šç”Ÿæˆå®Œæˆ\u0026#34;) return optimizer, report if __name__ == \u0026#34;__main__\u0026#34;: optimizer, report = cost_optimization_example() 2. äº‘æˆæœ¬åˆ†æä¸å¯è§†åŒ– 2.1 æˆæœ¬æ•°æ®æ”¶é›†æ¶æ„ graph TB subgraph \u0026#34;æ•°æ®æº\u0026#34; A[AWS Cost Explorer] B[Azure Cost Management] C[GCP Billing] D[Kubernetes Metrics] end subgraph \u0026#34;æ•°æ®å¤„ç†\u0026#34; E[æ•°æ®æ”¶é›†å™¨] --\u0026gt; F[æ•°æ®æ ‡å‡†åŒ–] F --\u0026gt; G[æ•°æ®å­˜å‚¨] G --\u0026gt; H[æ•°æ®åˆ†æ] end subgraph \u0026#34;å¯è§†åŒ–å±‚\u0026#34; I[æˆæœ¬ä»ªè¡¨æ¿] J[è¶‹åŠ¿åˆ†æ] K[é¢„ç®—ç›‘æ§] L[ä¼˜åŒ–å»ºè®®] end A --\u0026gt; E B --\u0026gt; E C --\u0026gt; E D --\u0026gt; E H --\u0026gt; I H --\u0026gt; J H --\u0026gt; K H --\u0026gt; L 2.2 æˆæœ¬å¯è§†åŒ–ç³»ç»Ÿ class CostVisualizationEngine: def __init__(self): self.dashboards = {} self.widgets = {} self.data_sources = {} def create_cost_dashboard(self, dashboard_config: Dict) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºæˆæœ¬ä»ªè¡¨æ¿\u0026#34;\u0026#34;\u0026#34; dashboard = { \u0026#34;id\u0026#34;: dashboard_config[\u0026#34;id\u0026#34;], \u0026#34;name\u0026#34;: dashboard_config[\u0026#34;name\u0026#34;], \u0026#34;widgets\u0026#34;: [], \u0026#34;filters\u0026#34;: dashboard_config.get(\u0026#34;filters\u0026#34;, {}), \u0026#34;refresh_interval\u0026#34;: dashboard_config.get(\u0026#34;refresh_interval\u0026#34;, 300) } # æ·»åŠ æ ‡å‡†å°éƒ¨ä»¶ widgets = [ { \u0026#34;type\u0026#34;: \u0026#34;cost_summary\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;æˆæœ¬æ¦‚è§ˆ\u0026#34;, \u0026#34;data_source\u0026#34;: \u0026#34;cost_aggregator\u0026#34;, \u0026#34;config\u0026#34;: {\u0026#34;time_range\u0026#34;: \u0026#34;30d\u0026#34;} }, { \u0026#34;type\u0026#34;: \u0026#34;trend_chart\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;æˆæœ¬è¶‹åŠ¿\u0026#34;, \u0026#34;data_source\u0026#34;: \u0026#34;cost_trends\u0026#34;, \u0026#34;config\u0026#34;: {\u0026#34;chart_type\u0026#34;: \u0026#34;line\u0026#34;, \u0026#34;time_range\u0026#34;: \u0026#34;90d\u0026#34;} }, { \u0026#34;type\u0026#34;: \u0026#34;service_breakdown\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;æœåŠ¡æˆæœ¬åˆ†è§£\u0026#34;, \u0026#34;data_source\u0026#34;: \u0026#34;service_costs\u0026#34;, \u0026#34;config\u0026#34;: {\u0026#34;chart_type\u0026#34;: \u0026#34;pie\u0026#34;, \u0026#34;top_n\u0026#34;: 10} }, { \u0026#34;type\u0026#34;: \u0026#34;optimization_recommendations\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;ä¼˜åŒ–å»ºè®®\u0026#34;, \u0026#34;data_source\u0026#34;: \u0026#34;recommendations\u0026#34;, \u0026#34;config\u0026#34;: {\u0026#34;max_items\u0026#34;: 5} } ] dashboard[\u0026#34;widgets\u0026#34;] = widgets self.dashboards[dashboard[\u0026#34;id\u0026#34;]] = dashboard return dashboard def generate_cost_insights(self, cost_data: List[CostData]) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆæˆæœ¬æ´å¯Ÿ\u0026#34;\u0026#34;\u0026#34; insights = { \u0026#34;anomalies\u0026#34;: [], \u0026#34;trends\u0026#34;: {}, \u0026#34;recommendations\u0026#34;: [], \u0026#34;forecasts\u0026#34;: {} } # æ£€æµ‹æˆæœ¬å¼‚å¸¸ daily_costs = {} for data in cost_data: date = data.timestamp.strftime(\u0026#34;%Y-%m-%d\u0026#34;) if date not in daily_costs: daily_costs[date] = 0 daily_costs[date] += data.cost # ç®€å•å¼‚å¸¸æ£€æµ‹ï¼ˆåŸºäºæ ‡å‡†å·®ï¼‰ costs = list(daily_costs.values()) if len(costs) \u0026gt; 7: mean_cost = sum(costs) / len(costs) std_dev = (sum((x - mean_cost) ** 2 for x in costs) / len(costs)) ** 0.5 for date, cost in daily_costs.items(): if abs(cost - mean_cost) \u0026gt; 2 * std_dev: insights[\u0026#34;anomalies\u0026#34;].append({ \u0026#34;date\u0026#34;: date, \u0026#34;cost\u0026#34;: cost, \u0026#34;expected_cost\u0026#34;: mean_cost, \u0026#34;deviation\u0026#34;: abs(cost - mean_cost), \u0026#34;type\u0026#34;: \u0026#34;cost_spike\u0026#34; if cost \u0026gt; mean_cost else \u0026#34;cost_drop\u0026#34; }) return insights 3. é¢„ç®—ç®¡ç†ä¸æˆæœ¬æ§åˆ¶ 3.1 é¢„ç®—ç®¡ç†ç³»ç»Ÿ class BudgetManager: def __init__(self): self.budgets = {} self.alerts = [] self.policies = {} def create_budget(self, budget_config: Dict) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºé¢„ç®—\u0026#34;\u0026#34;\u0026#34; budget = { \u0026#34;id\u0026#34;: budget_config[\u0026#34;id\u0026#34;], \u0026#34;name\u0026#34;: budget_config[\u0026#34;name\u0026#34;], \u0026#34;amount\u0026#34;: budget_config[\u0026#34;amount\u0026#34;], \u0026#34;period\u0026#34;: budget_config.get(\u0026#34;period\u0026#34;, \u0026#34;monthly\u0026#34;), \u0026#34;scope\u0026#34;: budget_config.get(\u0026#34;scope\u0026#34;, {}), \u0026#34;alert_thresholds\u0026#34;: budget_config.get(\u0026#34;alert_thresholds\u0026#34;, [50, 80, 100]), \u0026#34;actions\u0026#34;: budget_config.get(\u0026#34;actions\u0026#34;, []), \u0026#34;created_at\u0026#34;: datetime.now(), \u0026#34;status\u0026#34;: \u0026#34;active\u0026#34; } self.budgets[budget[\u0026#34;id\u0026#34;]] = budget return budget def monitor_budget_compliance(self, budget_id: str, current_spend: float) -\u0026gt; List[Dict]: \u0026#34;\u0026#34;\u0026#34;ç›‘æ§é¢„ç®—åˆè§„æ€§\u0026#34;\u0026#34;\u0026#34; if budget_id not in self.budgets: return [] budget = self.budgets[budget_id] alerts = [] for threshold in budget[\u0026#34;alert_thresholds\u0026#34;]: threshold_amount = budget[\u0026#34;amount\u0026#34;] * (threshold / 100) if current_spend \u0026gt;= threshold_amount: alert = { \u0026#34;budget_id\u0026#34;: budget_id, \u0026#34;threshold\u0026#34;: threshold, \u0026#34;current_spend\u0026#34;: current_spend, \u0026#34;budget_amount\u0026#34;: budget[\u0026#34;amount\u0026#34;], \u0026#34;severity\u0026#34;: self._get_alert_severity(threshold), \u0026#34;message\u0026#34;: f\u0026#34;Budget \u0026#39;{budget[\u0026#39;name\u0026#39;]}\u0026#39; has exceeded {threshold}% threshold\u0026#34;, \u0026#34;timestamp\u0026#34;: datetime.now(), \u0026#34;actions_required\u0026#34;: self._get_required_actions(budget, threshold) } alerts.append(alert) self.alerts.extend(alerts) return alerts def _get_alert_severity(self, threshold: int) -\u0026gt; str: if threshold \u0026gt;= 100: return \u0026#34;critical\u0026#34; elif threshold \u0026gt;= 80: return \u0026#34;high\u0026#34; elif threshold \u0026gt;= 50: return \u0026#34;medium\u0026#34; else: return \u0026#34;low\u0026#34; def _get_required_actions(self, budget: Dict, threshold: int) -\u0026gt; List[str]: actions = [] if threshold \u0026gt;= 100: actions.extend([\u0026#34;immediate_review\u0026#34;, \u0026#34;cost_freeze\u0026#34;, \u0026#34;executive_notification\u0026#34;]) elif threshold \u0026gt;= 80: actions.extend([\u0026#34;cost_review\u0026#34;, \u0026#34;optimization_plan\u0026#34;, \u0026#34;manager_notification\u0026#34;]) elif threshold \u0026gt;= 50: actions.extend([\u0026#34;cost_analysis\u0026#34;, \u0026#34;team_notification\u0026#34;]) return actions 4. èµ„æºä¼˜åŒ–ç­–ç•¥ 4.1 è‡ªåŠ¨åŒ–èµ„æºä¼˜åŒ– class ResourceOptimizer: def __init__(self): self.optimization_rules = {} self.schedules = {} def setup_rightsizing_automation(self, config: Dict): \u0026#34;\u0026#34;\u0026#34;è®¾ç½®è‡ªåŠ¨å³è°ƒå¤§å°\u0026#34;\u0026#34;\u0026#34; rule = { \u0026#34;name\u0026#34;: \u0026#34;auto_rightsizing\u0026#34;, \u0026#34;conditions\u0026#34;: { \u0026#34;cpu_utilization_threshold\u0026#34;: config.get(\u0026#34;cpu_threshold\u0026#34;, 30), \u0026#34;memory_utilization_threshold\u0026#34;: config.get(\u0026#34;memory_threshold\u0026#34;, 30), \u0026#34;observation_period\u0026#34;: config.get(\u0026#34;observation_days\u0026#34;, 7) }, \u0026#34;actions\u0026#34;: { \u0026#34;downsize_percentage\u0026#34;: config.get(\u0026#34;downsize_percentage\u0026#34;, 50), \u0026#34;approval_required\u0026#34;: config.get(\u0026#34;approval_required\u0026#34;, True), \u0026#34;dry_run\u0026#34;: config.get(\u0026#34;dry_run\u0026#34;, True) } } self.optimization_rules[\u0026#34;rightsizing\u0026#34;] = rule return rule def setup_scheduling_automation(self, config: Dict): \u0026#34;\u0026#34;\u0026#34;è®¾ç½®è‡ªåŠ¨è°ƒåº¦\u0026#34;\u0026#34;\u0026#34; schedule = { \u0026#34;name\u0026#34;: config[\u0026#34;name\u0026#34;], \u0026#34;resources\u0026#34;: config[\u0026#34;resources\u0026#34;], \u0026#34;schedule\u0026#34;: { \u0026#34;start_time\u0026#34;: config[\u0026#34;start_time\u0026#34;], \u0026#34;stop_time\u0026#34;: config[\u0026#34;stop_time\u0026#34;], \u0026#34;days\u0026#34;: config.get(\u0026#34;days\u0026#34;, [\u0026#34;monday\u0026#34;, \u0026#34;tuesday\u0026#34;, \u0026#34;wednesday\u0026#34;, \u0026#34;thursday\u0026#34;, \u0026#34;friday\u0026#34;]), \u0026#34;timezone\u0026#34;: config.get(\u0026#34;timezone\u0026#34;, \u0026#34;UTC\u0026#34;) }, \u0026#34;actions\u0026#34;: { \u0026#34;start_action\u0026#34;: config.get(\u0026#34;start_action\u0026#34;, \u0026#34;start_instances\u0026#34;), \u0026#34;stop_action\u0026#34;: config.get(\u0026#34;stop_action\u0026#34;, \u0026#34;stop_instances\u0026#34;) } } self.schedules[config[\u0026#34;name\u0026#34;]] = schedule return schedule 5. è‡ªåŠ¨åŒ–æˆæœ¬æ²»ç† 5.1 æˆæœ¬æ²»ç†ç­–ç•¥ graph TB subgraph \u0026#34;æ²»ç†ç­–ç•¥\u0026#34; A[æ ‡ç­¾ç­–ç•¥] --\u0026gt; B[é¢„ç®—æ§åˆ¶] B --\u0026gt; C[èµ„æºé…é¢] C --\u0026gt; D[è‡ªåŠ¨åŒ–æ“ä½œ] end subgraph \u0026#34;æ‰§è¡Œå±‚\u0026#34; E[ç­–ç•¥å¼•æ“] --\u0026gt; F[ç›‘æ§ç³»ç»Ÿ] F --\u0026gt; G[å‘Šè­¦ç³»ç»Ÿ] G --\u0026gt; H[è‡ªåŠ¨ä¿®å¤] end subgraph \u0026#34;åé¦ˆå¾ªç¯\u0026#34; I[æˆæœ¬åˆ†æ] --\u0026gt; J[ç­–ç•¥è°ƒæ•´] J --\u0026gt; K[æ•ˆæœè¯„ä¼°] K --\u0026gt; I end 6. å¤šäº‘æˆæœ¬ç®¡ç† 6.1 å¤šäº‘æˆæœ¬ç»Ÿä¸€ç®¡ç† class MultiCloudCostManager: def __init__(self): self.providers = {} self.cost_normalizer = CostNormalizer() def add_cloud_provider(self, provider_config: Dict): \u0026#34;\u0026#34;\u0026#34;æ·»åŠ äº‘æä¾›å•†\u0026#34;\u0026#34;\u0026#34; provider = { \u0026#34;name\u0026#34;: provider_config[\u0026#34;name\u0026#34;], \u0026#34;type\u0026#34;: provider_config[\u0026#34;type\u0026#34;], \u0026#34;credentials\u0026#34;: provider_config[\u0026#34;credentials\u0026#34;], \u0026#34;cost_api\u0026#34;: self._initialize_cost_api(provider_config), \u0026#34;currency\u0026#34;: provider_config.get(\u0026#34;currency\u0026#34;, \u0026#34;USD\u0026#34;) } self.providers[provider[\u0026#34;name\u0026#34;]] = provider return provider def get_unified_cost_view(self, time_range: str) -\u0026gt; Dict: \u0026#34;\u0026#34;\u0026#34;è·å–ç»Ÿä¸€æˆæœ¬è§†å›¾\u0026#34;\u0026#34;\u0026#34; unified_costs = { \u0026#34;total_cost\u0026#34;: 0, \u0026#34;provider_breakdown\u0026#34;: {}, \u0026#34;service_breakdown\u0026#34;: {}, \u0026#34;currency\u0026#34;: \u0026#34;USD\u0026#34; } for provider_name, provider in self.providers.items(): provider_costs = self._fetch_provider_costs(provider, time_range) normalized_costs = self.cost_normalizer.normalize(provider_costs, provider[\u0026#34;currency\u0026#34;]) unified_costs[\u0026#34;provider_breakdown\u0026#34;][provider_name] = normalized_costs unified_costs[\u0026#34;total_cost\u0026#34;] += normalized_costs[\u0026#34;total\u0026#34;] # åˆå¹¶æœåŠ¡æˆæœ¬ for service, cost in normalized_costs.get(\u0026#34;services\u0026#34;, {}).items(): service_key = f\u0026#34;{provider_name}_{service}\u0026#34; unified_costs[\u0026#34;service_breakdown\u0026#34;][service_key] = cost return unified_costs 7. FinOpsç»„ç»‡ä¸æµç¨‹ 7.1 FinOpså›¢é˜Ÿç»“æ„ graph TB subgraph \u0026#34;FinOpså›¢é˜Ÿ\u0026#34; A[FinOpsè´Ÿè´£äºº] --\u0026gt; B[æˆæœ¬åˆ†æå¸ˆ] A --\u0026gt; C[äº‘å·¥ç¨‹å¸ˆ] A --\u0026gt; D[è´¢åŠ¡åˆ†æå¸ˆ] end subgraph \u0026#34;åä½œå›¢é˜Ÿ\u0026#34; E[å¼€å‘å›¢é˜Ÿ] --\u0026gt; F[è¿ç»´å›¢é˜Ÿ] F --\u0026gt; G[äº§å“å›¢é˜Ÿ] G --\u0026gt; H[è´¢åŠ¡å›¢é˜Ÿ] end subgraph \u0026#34;æ²»ç†æµç¨‹\u0026#34; I[æˆæœ¬å®¡æŸ¥] --\u0026gt; J[ä¼˜åŒ–è®¡åˆ’] J --\u0026gt; K[å®æ–½è·Ÿè¸ª] K --\u0026gt; L[æ•ˆæœè¯„ä¼°] end 8. æœ€ä½³å®è·µä¸æ¡ˆä¾‹ 8.1 å®æ–½æœ€ä½³å®è·µ 8.1.1 ç»„ç»‡æœ€ä½³å®è·µ å»ºç«‹FinOpsæ–‡åŒ–\n","content":"äº‘æˆæœ¬ä¼˜åŒ–ç­–ç•¥ä¸FinOpså®è·µï¼šæ„å»ºä¼ä¸šäº‘è´¢åŠ¡ç®¡ç†ä½“ç³» ç›®å½• FinOpsæ¦‚è¿°ä¸ä»·å€¼ äº‘æˆæœ¬åˆ†æä¸å¯è§†åŒ– é¢„ç®—ç®¡ç†ä¸æˆæœ¬æ§åˆ¶ èµ„æºä¼˜åŒ–ç­–ç•¥ è‡ªåŠ¨åŒ–æˆæœ¬æ²»ç† å¤šäº‘æˆæœ¬ç®¡ç† FinOpsç»„ç»‡ä¸æµç¨‹ æœ€ä½³å®è·µä¸æ¡ˆä¾‹ 1. FinOpsæ¦‚è¿°ä¸ä»·å€¼ 1.1 FinOpsæ ¸å¿ƒç†å¿µ FinOpsï¼ˆFinancial Operationsï¼‰æ˜¯ä¸€ç§äº‘è´¢åŠ¡ç®¡ç†å®è·µï¼Œæ—¨åœ¨é€šè¿‡è·¨èŒèƒ½åä½œæ¥ä¼˜åŒ–äº‘æˆæœ¬ï¼Œå®ç°ä¸šåŠ¡ä»·å€¼æœ€å¤§åŒ–ã€‚\ngraph TB subgraph \u0026amp;#34;FinOpsæ ¸å¿ƒåŸåˆ™\u0026amp;#34; A[å›¢é˜Ÿåä½œ] --\u0026amp;gt; B[ä¸šåŠ¡ä»·å€¼é©±åŠ¨] B --\u0026amp;gt; C[æ•°æ®é©±åŠ¨å†³ç­–] C --\u0026amp;gt; D[æŒç»­ä¼˜åŒ–] end subgraph \u0026amp;#34;FinOpsç”Ÿå‘½å‘¨æœŸ\u0026amp;#34; E[é€šçŸ¥Inform] --\u0026amp;gt; F[ä¼˜åŒ–Optimize] F --\u0026amp;gt; G[è¿è¥Operate] G --\u0026amp;gt; E end subgraph \u0026amp;#34;å…³é”®åˆ©ç›Šç›¸å…³è€…\u0026amp;#34; H[è´¢åŠ¡å›¢é˜Ÿ] I[å·¥ç¨‹å›¢é˜Ÿ] J[ä¸šåŠ¡å›¢é˜Ÿ] K[é«˜ç®¡å›¢é˜Ÿ] end 1.2 äº‘æˆæœ¬ç®¡ç†å¹³å° from dataclasses â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["äº‘æˆæœ¬","FinOps","æˆæœ¬ä¼˜åŒ–","äº‘æ²»ç†","è´¢åŠ¡ç®¡ç†","èµ„æºç®¡ç†"],"categories":["äº‘æ¶æ„"],"author":"äº‘æ¶æ„ä¸“å®¶","readingTime":8,"wordCount":1630,"section":"posts","type":"posts","draft":false,"featured":false,"series":null},{"title":"äº‘åŸºç¡€è®¾æ–½æ¶æ„è®¾è®¡ä¸å®è·µï¼šæ„å»ºå¯æ‰©å±•ã€é«˜å¯ç”¨çš„ç°ä»£äº‘å¹³å°","url":"https://www.dishuihengxin.com/posts/cloud-infrastructure-architecture/","summary":"ç›®å½• äº‘åŸºç¡€è®¾æ–½æ¶æ„æ¦‚è¿° ç½‘ç»œæ¶æ„è®¾è®¡ è®¡ç®—èµ„æºæ¶æ„ å­˜å‚¨æ¶æ„è®¾è®¡ å®‰å…¨æ¶æ„å®æ–½ ç›‘æ§ä¸å¯è§‚æµ‹æ€§ åŸºç¡€è®¾æ–½å³ä»£ç  æˆæœ¬ä¼˜åŒ–ç­–ç•¥ æ€»ç»“ äº‘åŸºç¡€è®¾æ–½æ¶æ„æ¦‚è¿° ç°ä»£äº‘åŸºç¡€è®¾æ–½æ¶æ„éœ€è¦æ»¡è¶³é«˜å¯ç”¨æ€§ã€å¯æ‰©å±•æ€§ã€å®‰å…¨æ€§å’Œæˆæœ¬æ•ˆç›Šç­‰å¤šé‡è¦æ±‚ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨å¦‚ä½•è®¾è®¡å’Œå®æ–½ä¸€ä¸ªå®Œæ•´çš„äº‘åŸºç¡€è®¾æ–½æ¶æ„ã€‚\næ¶æ„è®¾è®¡åŸåˆ™ # äº‘åŸºç¡€è®¾æ–½æ¶æ„è®¾è®¡åŸåˆ™ architecture_principles: scalability: horizontal_scaling: true auto_scaling: true load_balancing: true availability: multi_az_deployment: true disaster_recovery: true fault_tolerance: true security: defense_in_depth: true zero_trust_model: true encryption_everywhere: true cost_optimization: resource_rightsizing: true reserved_instances: true spot_instances: true operational_excellence: infrastructure_as_code: true automated_deployment: true monitoring_alerting: true æ¶æ„å±‚æ¬¡æ¨¡å‹ graph TB subgraph \u0026#34;åº”ç”¨å±‚\u0026#34; A[Webåº”ç”¨] --\u0026gt; B[APIæœåŠ¡] B --\u0026gt; C[å¾®æœåŠ¡] end subgraph \u0026#34;å¹³å°å±‚\u0026#34; D[å®¹å™¨ç¼–æ’] --\u0026gt; E[æœåŠ¡ç½‘æ ¼] E --\u0026gt; F[APIç½‘å…³] end subgraph \u0026#34;åŸºç¡€è®¾æ–½å±‚\u0026#34; G[è®¡ç®—èµ„æº] --\u0026gt; H[å­˜å‚¨ç³»ç»Ÿ] H --\u0026gt; I[ç½‘ç»œæ¶æ„] end subgraph \u0026#34;å®‰å…¨å±‚\u0026#34; J[èº«ä»½è®¤è¯] --\u0026gt; K[è®¿é—®æ§åˆ¶] K --\u0026gt; L[æ•°æ®åŠ å¯†] end A --\u0026gt; D D --\u0026gt; G G --\u0026gt; J ç½‘ç»œæ¶æ„è®¾è®¡ ç½‘ç»œæ¶æ„æ˜¯äº‘åŸºç¡€è®¾æ–½çš„æ ¸å¿ƒç»„ä»¶ï¼Œéœ€è¦æä¾›å®‰å…¨ã€é«˜æ€§èƒ½å’Œå¯æ‰©å±•çš„è¿æ¥èƒ½åŠ›ã€‚\n","content":"ç›®å½• äº‘åŸºç¡€è®¾æ–½æ¶æ„æ¦‚è¿° ç½‘ç»œæ¶æ„è®¾è®¡ è®¡ç®—èµ„æºæ¶æ„ å­˜å‚¨æ¶æ„è®¾è®¡ å®‰å…¨æ¶æ„å®æ–½ ç›‘æ§ä¸å¯è§‚æµ‹æ€§ åŸºç¡€è®¾æ–½å³ä»£ç  æˆæœ¬ä¼˜åŒ–ç­–ç•¥ æ€»ç»“ äº‘åŸºç¡€è®¾æ–½æ¶æ„æ¦‚è¿° ç°ä»£äº‘åŸºç¡€è®¾æ–½æ¶æ„éœ€è¦æ»¡è¶³é«˜å¯ç”¨æ€§ã€å¯æ‰©å±•æ€§ã€å®‰å…¨æ€§å’Œæˆæœ¬æ•ˆç›Šç­‰å¤šé‡è¦æ±‚ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨å¦‚ä½•è®¾è®¡å’Œå®æ–½ä¸€ä¸ªå®Œæ•´çš„äº‘åŸºç¡€è®¾æ–½æ¶æ„ã€‚\næ¶æ„è®¾è®¡åŸåˆ™ # äº‘åŸºç¡€è®¾æ–½æ¶æ„è®¾è®¡åŸåˆ™ architecture_principles: scalability: horizontal_scaling: true auto_scaling: true load_balancing: true availability: multi_az_deployment: true disaster_recovery: true fault_tolerance: true security: defense_in_depth: true zero_trust_model: true encryption_everywhere: true cost_optimization: resource_rightsizing: true â€¦","date":"2025-12-31","lastmod":"2023-03-15","tags":["äº‘åŸºç¡€è®¾æ–½","æ¶æ„è®¾è®¡","é«˜å¯ç”¨","å¯æ‰©å±•æ€§","è‡ªåŠ¨åŒ–","è¿ç»´","æŠ€æœ¯"],"categories":["æŠ€æœ¯åˆ†äº«"],"author":"DeepSeek","readingTime":24,"wordCount":4939,"section":"posts","type":"posts","draft":false,"featured":false,"series":null},{"title":"äº‘æ•°æ®æ¶æ„ï¼šå¤§æ•°æ®ä¸AIå¹³å°è®¾è®¡","url":"https://www.dishuihengxin.com/posts/cloud-data-architecture-big-data-ai-platform/","summary":"äº‘æ•°æ®æ¶æ„ï¼šå¤§æ•°æ®ä¸AIå¹³å°è®¾è®¡ ç›®å½• å¼•è¨€ æ•°æ®æ¶æ„æ¦‚è¿° æ•°æ®æ¹–æ¶æ„è®¾è®¡ æ•°æ®ä»“åº“ä¸æ•°æ®é›†å¸‚ å®æ—¶æ•°æ®å¤„ç† æœºå™¨å­¦ä¹ å¹³å° æ•°æ®æ²»ç†ä¸è´¨é‡ å®‰å…¨ä¸åˆè§„ æ€§èƒ½ä¼˜åŒ– æœ€ä½³å®è·µä¸å»ºè®® æ€»ç»“ å¼•è¨€ åœ¨æ•°å­—åŒ–è½¬å‹çš„æµªæ½®ä¸­ï¼Œæ•°æ®å·²æˆä¸ºä¼ä¸šæœ€å®è´µçš„èµ„äº§ã€‚äº‘æ•°æ®æ¶æ„ä¸ºç»„ç»‡æä¾›äº†å¤„ç†æµ·é‡æ•°æ®ã€æ„å»ºæ™ºèƒ½åº”ç”¨çš„å¼ºå¤§å¹³å°ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨å¦‚ä½•åœ¨äº‘ç¯å¢ƒä¸­è®¾è®¡å’Œå®æ–½å¤§æ•°æ®ä¸AIå¹³å°ï¼Œæ¶µç›–ä»æ•°æ®é‡‡é›†ã€å­˜å‚¨ã€å¤„ç†åˆ°åˆ†æçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸã€‚\nç°ä»£äº‘æ•°æ®æ¶æ„éœ€è¦æ”¯æŒå¤šæ ·åŒ–çš„æ•°æ®æºã€å®æ—¶å’Œæ‰¹å¤„ç†å·¥ä½œè´Ÿè½½ã€æœºå™¨å­¦ä¹ å·¥ä½œæµï¼ŒåŒæ—¶ç¡®ä¿æ•°æ®å®‰å…¨ã€æ²»ç†å’Œåˆè§„æ€§ã€‚é€šè¿‡åˆç†çš„æ¶æ„è®¾è®¡ï¼Œä¼ä¸šå¯ä»¥æ„å»ºæ•æ·ã€å¯æ‰©å±•ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„æ•°æ®å¹³å°ã€‚\næ•°æ®æ¶æ„æ¦‚è¿° ç°ä»£æ•°æ®æ¶æ„æ¨¡å¼ graph TB subgraph \u0026#34;æ•°æ®æºå±‚\u0026#34; A[ä¸šåŠ¡ç³»ç»Ÿ] --\u0026gt; D[æ•°æ®é‡‡é›†å±‚] B[IoTè®¾å¤‡] --\u0026gt; D C[å¤–éƒ¨API] --\u0026gt; D E[æ—¥å¿—æ–‡ä»¶] --\u0026gt; D F[æµæ•°æ®] --\u0026gt; D end subgraph \u0026#34;æ•°æ®é‡‡é›†å±‚\u0026#34; D --\u0026gt; G[æ‰¹é‡ETL] D --\u0026gt; H[æµå¤„ç†] D --\u0026gt; I[CDCå˜æ›´æ•è·] end subgraph \u0026#34;æ•°æ®å­˜å‚¨å±‚\u0026#34; G --\u0026gt; J[æ•°æ®æ¹–] H --\u0026gt; J I --\u0026gt; J J --\u0026gt; K[æ•°æ®ä»“åº“] J --\u0026gt; L[ç‰¹å¾å­˜å‚¨] end subgraph \u0026#34;æ•°æ®å¤„ç†å±‚\u0026#34; K --\u0026gt; M[æ‰¹å¤„ç†å¼•æ“] J --\u0026gt; N[æµå¤„ç†å¼•æ“] L --\u0026gt; O[MLè®­ç»ƒ] end subgraph \u0026#34;æ•°æ®æœåŠ¡å±‚\u0026#34; M --\u0026gt; P[æ•°æ®API] N --\u0026gt; P O --\u0026gt; Q[æ¨¡å‹æœåŠ¡] P --\u0026gt; R[BIå·¥å…·] Q --\u0026gt; S[AIåº”ç”¨] end subgraph \u0026#34;æ•°æ®æ²»ç†å±‚\u0026#34; T[å…ƒæ•°æ®ç®¡ç†] -.-\u0026gt; J T -.-\u0026gt; K T -.-\u0026gt; L U[æ•°æ®è´¨é‡] -.-\u0026gt; M U -.-\u0026gt; N V[æ•°æ®å®‰å…¨] -.-\u0026gt; P V -.-\u0026gt; Q end æ•°æ®æ¶æ„åˆ†æå™¨ import json import time import uuid from typing import Dict, Any, List, Optional, Union, Tuple from dataclasses import dataclass, asdict from datetime import datetime, timedelta from enum import Enum import boto3 import pandas as pd import numpy as np from abc import ABC, abstractmethod import logging # é…ç½®æ—¥å¿— logging.basicConfig(level=logging.INFO) logger = logging.getLogger(\u0026#34;data-architecture\u0026#34;) class DataSourceType(Enum): \u0026#34;\u0026#34;\u0026#34;æ•°æ®æºç±»å‹\u0026#34;\u0026#34;\u0026#34; RELATIONAL_DB = \u0026#34;relational_db\u0026#34; NOSQL_DB = \u0026#34;nosql_db\u0026#34; FILE_SYSTEM = \u0026#34;file_system\u0026#34; STREAMING = \u0026#34;streaming\u0026#34; API = \u0026#34;api\u0026#34; IOT = \u0026#34;iot\u0026#34; LOG = \u0026#34;log\u0026#34; class DataFormat(Enum): \u0026#34;\u0026#34;\u0026#34;æ•°æ®æ ¼å¼\u0026#34;\u0026#34;\u0026#34; JSON = \u0026#34;json\u0026#34; CSV = \u0026#34;csv\u0026#34; PARQUET = \u0026#34;parquet\u0026#34; AVRO = \u0026#34;avro\u0026#34; ORC = \u0026#34;orc\u0026#34; XML = \u0026#34;xml\u0026#34; BINARY = \u0026#34;binary\u0026#34; class ProcessingType(Enum): \u0026#34;\u0026#34;\u0026#34;å¤„ç†ç±»å‹\u0026#34;\u0026#34;\u0026#34; BATCH = \u0026#34;batch\u0026#34; STREAMING = \u0026#34;streaming\u0026#34; MICRO_BATCH = \u0026#34;micro_batch\u0026#34; REAL_TIME = \u0026#34;real_time\u0026#34; @dataclass class DataSource: \u0026#34;\u0026#34;\u0026#34;æ•°æ®æºå®šä¹‰\u0026#34;\u0026#34;\u0026#34; id: str name: str type: DataSourceType format: DataFormat location: str schema: Dict[str, Any] volume_gb_per_day: float velocity_records_per_second: int variety_score: int # 1-10, æ•°æ®å¤šæ ·æ€§è¯„åˆ† created_at: datetime updated_at: datetime @dataclass class DataPipeline: \u0026#34;\u0026#34;\u0026#34;æ•°æ®ç®¡é“å®šä¹‰\u0026#34;\u0026#34;\u0026#34; id: str name: str source_ids: List[str] processing_type: ProcessingType transformations: List[Dict[str, Any]] destination: str schedule: Optional[str] sla_minutes: int created_at: datetime updated_at: datetime @dataclass class DataAsset: \u0026#34;\u0026#34;\u0026#34;æ•°æ®èµ„äº§å®šä¹‰\u0026#34;\u0026#34;\u0026#34; id: str name: str description: str owner: str tags: List[str] schema: Dict[str, Any] quality_score: float usage_frequency: str business_value: str created_at: datetime updated_at: datetime class DataArchitectureAnalyzer: \u0026#34;\u0026#34;\u0026#34;æ•°æ®æ¶æ„åˆ†æå™¨\u0026#34;\u0026#34;\u0026#34; def __init__(self, aws_region: str = \u0026#34;us-east-1\u0026#34;): self.aws_region = aws_region self.data_sources: Dict[str, DataSource] = {} self.data_pipelines: Dict[str, DataPipeline] = {} self.data_assets: Dict[str, DataAsset] = {} # AWSæœåŠ¡å®¢æˆ·ç«¯ self.s3 = boto3.client(\u0026#39;s3\u0026#39;, region_name=aws_region) self.glue = boto3.client(\u0026#39;glue\u0026#39;, region_name=aws_region) self.athena = boto3.client(\u0026#39;athena\u0026#39;, region_name=aws_region) self.redshift = boto3.client(\u0026#39;redshift\u0026#39;, region_name=aws_region) self.kinesis = boto3.client(\u0026#39;kinesis\u0026#39;, region_name=aws_region) def add_data_source(self, data_source: DataSource): \u0026#34;\u0026#34;\u0026#34;æ·»åŠ æ•°æ®æº\u0026#34;\u0026#34;\u0026#34; self.data_sources[data_source.id] = data_source logger.info(f\u0026#34;Added data source: {data_source.name}\u0026#34;) def add_data_pipeline(self, pipeline: DataPipeline): \u0026#34;\u0026#34;\u0026#34;æ·»åŠ æ•°æ®ç®¡é“\u0026#34;\u0026#34;\u0026#34; self.data_pipelines[pipeline.id] = pipeline logger.info(f\u0026#34;Added data pipeline: {pipeline.name}\u0026#34;) def add_data_asset(self, asset: DataAsset): \u0026#34;\u0026#34;\u0026#34;æ·»åŠ æ•°æ®èµ„äº§\u0026#34;\u0026#34;\u0026#34; self.data_assets[asset.id] = asset logger.info(f\u0026#34;Added data asset: {asset.name}\u0026#34;) def analyze_data_volume(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;åˆ†ææ•°æ®é‡\u0026#34;\u0026#34;\u0026#34; total_volume = sum(source.volume_gb_per_day for source in self.data_sources.values()) volume_by_type = {} for source in self.data_sources.values(): source_type = source.type.value if source_type not in volume_by_type: volume_by_type[source_type] = 0 volume_by_type[source_type] += source.volume_gb_per_day return { \u0026#34;total_volume_gb_per_day\u0026#34;: total_volume, \u0026#34;volume_by_type\u0026#34;: volume_by_type, \u0026#34;projected_monthly_volume_gb\u0026#34;: total_volume * 30, \u0026#34;projected_yearly_volume_tb\u0026#34;: (total_volume * 365) / 1024 } def analyze_data_velocity(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;åˆ†ææ•°æ®é€Ÿåº¦\u0026#34;\u0026#34;\u0026#34; total_velocity = sum(source.velocity_records_per_second for source in self.data_sources.values()) velocity_by_type = {} for source in self.data_sources.values(): source_type = source.type.value if source_type not in velocity_by_type: velocity_by_type[source_type] = 0 velocity_by_type[source_type] += source.velocity_records_per_second # åˆ†ç±»é€Ÿåº¦çº§åˆ« speed_categories = { \u0026#34;low\u0026#34;: 0, # \u0026lt; 100 records/sec \u0026#34;medium\u0026#34;: 0, # 100-1000 records/sec \u0026#34;high\u0026#34;: 0, # 1000-10000 records/sec \u0026#34;very_high\u0026#34;: 0 # \u0026gt; 10000 records/sec } for source in self.data_sources.values(): velocity = source.velocity_records_per_second if velocity \u0026lt; 100: speed_categories[\u0026#34;low\u0026#34;] += 1 elif velocity \u0026lt; 1000: speed_categories[\u0026#34;medium\u0026#34;] += 1 elif velocity \u0026lt; 10000: speed_categories[\u0026#34;high\u0026#34;] += 1 else: speed_categories[\u0026#34;very_high\u0026#34;] += 1 return { \u0026#34;total_velocity_records_per_second\u0026#34;: total_velocity, \u0026#34;velocity_by_type\u0026#34;: velocity_by_type, \u0026#34;speed_distribution\u0026#34;: speed_categories, \u0026#34;peak_daily_records\u0026#34;: total_velocity * 86400 } def analyze_data_variety(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;åˆ†ææ•°æ®å¤šæ ·æ€§\u0026#34;\u0026#34;\u0026#34; format_distribution = {} type_distribution = {} variety_scores = [] for source in self.data_sources.values(): # æ ¼å¼åˆ†å¸ƒ format_name = source.format.value if format_name not in format_distribution: format_distribution[format_name] = 0 format_distribution[format_name] += 1 # ç±»å‹åˆ†å¸ƒ type_name = source.type.value if type_name not in type_distribution: type_distribution[type_name] = 0 type_distribution[type_name] += 1 # å¤šæ ·æ€§è¯„åˆ† variety_scores.append(source.variety_score) avg_variety_score = np.mean(variety_scores) if variety_scores else 0 return { \u0026#34;format_distribution\u0026#34;: format_distribution, \u0026#34;type_distribution\u0026#34;: type_distribution, \u0026#34;average_variety_score\u0026#34;: avg_variety_score, \u0026#34;total_data_sources\u0026#34;: len(self.data_sources), \u0026#34;unique_formats\u0026#34;: len(format_distribution), \u0026#34;unique_types\u0026#34;: len(type_distribution) } def analyze_pipeline_complexity(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;åˆ†æç®¡é“å¤æ‚æ€§\u0026#34;\u0026#34;\u0026#34; processing_type_distribution = {} transformation_complexity = [] sla_distribution = {\u0026#34;under_1h\u0026#34;: 0, \u0026#34;1h_to_4h\u0026#34;: 0, \u0026#34;4h_to_24h\u0026#34;: 0, \u0026#34;over_24h\u0026#34;: 0} for pipeline in self.data_pipelines.values(): # å¤„ç†ç±»å‹åˆ†å¸ƒ proc_type = pipeline.processing_type.value if proc_type not in processing_type_distribution: processing_type_distribution[proc_type] = 0 processing_type_distribution[proc_type] += 1 # è½¬æ¢å¤æ‚æ€§ transformation_complexity.append(len(pipeline.transformations)) # SLAåˆ†å¸ƒ sla_hours = pipeline.sla_minutes / 60 if sla_hours \u0026lt; 1: sla_distribution[\u0026#34;under_1h\u0026#34;] += 1 elif sla_hours \u0026lt; 4: sla_distribution[\u0026#34;1h_to_4h\u0026#34;] += 1 elif sla_hours \u0026lt; 24: sla_distribution[\u0026#34;4h_to_24h\u0026#34;] += 1 else: sla_distribution[\u0026#34;over_24h\u0026#34;] += 1 avg_transformations = np.mean(transformation_complexity) if transformation_complexity else 0 return { \u0026#34;total_pipelines\u0026#34;: len(self.data_pipelines), \u0026#34;processing_type_distribution\u0026#34;: processing_type_distribution, \u0026#34;average_transformations_per_pipeline\u0026#34;: avg_transformations, \u0026#34;sla_distribution\u0026#34;: sla_distribution, \u0026#34;complex_pipelines\u0026#34;: len([p for p in self.data_pipelines.values() if len(p.transformations) \u0026gt; 5]) } def generate_architecture_recommendations(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆæ¶æ„å»ºè®®\u0026#34;\u0026#34;\u0026#34; volume_analysis = self.analyze_data_volume() velocity_analysis = self.analyze_data_velocity() variety_analysis = self.analyze_data_variety() pipeline_analysis = self.analyze_pipeline_complexity() recommendations = { \u0026#34;storage_recommendations\u0026#34;: [], \u0026#34;processing_recommendations\u0026#34;: [], \u0026#34;architecture_patterns\u0026#34;: [], \u0026#34;technology_stack\u0026#34;: [], \u0026#34;cost_optimization\u0026#34;: [], \u0026#34;performance_optimization\u0026#34;: [] } # å­˜å‚¨å»ºè®® total_volume = volume_analysis[\u0026#34;total_volume_gb_per_day\u0026#34;] if total_volume \u0026gt; 1000: # \u0026gt; 1TB/day recommendations[\u0026#34;storage_recommendations\u0026#34;].append({ \u0026#34;type\u0026#34;: \u0026#34;data_lake\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;å¤§æ•°æ®é‡éœ€è¦å¯æ‰©å±•çš„æ•°æ®æ¹–æ¶æ„\u0026#34;, \u0026#34;technology\u0026#34;: [\u0026#34;Amazon S3\u0026#34;, \u0026#34;Azure Data Lake\u0026#34;, \u0026#34;Google Cloud Storage\u0026#34;] }) if variety_analysis[\u0026#34;unique_formats\u0026#34;] \u0026gt; 3: recommendations[\u0026#34;storage_recommendations\u0026#34;].append({ \u0026#34;type\u0026#34;: \u0026#34;multi_format_support\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;å¤šç§æ•°æ®æ ¼å¼éœ€è¦çµæ´»çš„å­˜å‚¨æ–¹æ¡ˆ\u0026#34;, \u0026#34;technology\u0026#34;: [\u0026#34;Apache Parquet\u0026#34;, \u0026#34;Delta Lake\u0026#34;, \u0026#34;Apache Iceberg\u0026#34;] }) # å¤„ç†å»ºè®® total_velocity = velocity_analysis[\u0026#34;total_velocity_records_per_second\u0026#34;] if total_velocity \u0026gt; 1000: recommendations[\u0026#34;processing_recommendations\u0026#34;].append({ \u0026#34;type\u0026#34;: \u0026#34;stream_processing\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;é«˜é€Ÿæ•°æ®æµéœ€è¦å®æ—¶å¤„ç†èƒ½åŠ›\u0026#34;, \u0026#34;technology\u0026#34;: [\u0026#34;Apache Kafka\u0026#34;, \u0026#34;Amazon Kinesis\u0026#34;, \u0026#34;Apache Flink\u0026#34;] }) if pipeline_analysis[\u0026#34;complex_pipelines\u0026#34;] \u0026gt; 0: recommendations[\u0026#34;processing_recommendations\u0026#34;].append({ \u0026#34;type\u0026#34;: \u0026#34;workflow_orchestration\u0026#34;, \u0026#34;reason\u0026#34;: \u0026#34;å¤æ‚ç®¡é“éœ€è¦å·¥ä½œæµç¼–æ’\u0026#34;, \u0026#34;technology\u0026#34;: [\u0026#34;Apache Airflow\u0026#34;, \u0026#34;AWS Step Functions\u0026#34;, \u0026#34;Azure Data Factory\u0026#34;] }) # æ¶æ„æ¨¡å¼å»ºè®® if (total_volume \u0026gt; 100 and total_velocity \u0026gt; 100 and variety_analysis[\u0026#34;average_variety_score\u0026#34;] \u0026gt; 5): recommendations[\u0026#34;architecture_patterns\u0026#34;].append({ \u0026#34;pattern\u0026#34;: \u0026#34;lambda_architecture\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;æ‰¹å¤„ç†å’Œæµå¤„ç†æ··åˆæ¶æ„\u0026#34;, \u0026#34;use_case\u0026#34;: \u0026#34;å¤„ç†å¤§é‡ã€é«˜é€Ÿã€å¤šæ ·åŒ–æ•°æ®\u0026#34; }) if variety_analysis[\u0026#34;unique_types\u0026#34;] \u0026gt; 5: recommendations[\u0026#34;architecture_patterns\u0026#34;].append({ \u0026#34;pattern\u0026#34;: \u0026#34;data_mesh\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;å»ä¸­å¿ƒåŒ–æ•°æ®æ¶æ„\u0026#34;, \u0026#34;use_case\u0026#34;: \u0026#34;å¤šåŸŸæ•°æ®ç®¡ç†å’Œæ²»ç†\u0026#34; }) return recommendations def generate_cost_estimation(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆæˆæœ¬ä¼°ç®—\u0026#34;\u0026#34;\u0026#34; volume_analysis = self.analyze_data_volume() velocity_analysis = self.analyze_data_velocity() # å­˜å‚¨æˆæœ¬ä¼°ç®—ï¼ˆåŸºäºAWS S3å®šä»·ï¼‰ monthly_volume_gb = volume_analysis[\u0026#34;projected_monthly_volume_gb\u0026#34;] storage_cost_per_gb = 0.023 # AWS S3æ ‡å‡†å­˜å‚¨ monthly_storage_cost = monthly_volume_gb * storage_cost_per_gb # è®¡ç®—æˆæœ¬ä¼°ç®—ï¼ˆåŸºäºæ•°æ®å¤„ç†é‡ï¼‰ daily_records = velocity_analysis[\u0026#34;peak_daily_records\u0026#34;] processing_cost_per_million_records = 1.0 # å‡è®¾æˆæœ¬ monthly_processing_cost = (daily_records * 30 / 1000000) * processing_cost_per_million_records # ä¼ è¾“æˆæœ¬ä¼°ç®— monthly_transfer_cost = monthly_volume_gb * 0.09 # æ•°æ®ä¼ è¾“æˆæœ¬ total_monthly_cost = monthly_storage_cost + monthly_processing_cost + monthly_transfer_cost return { \u0026#34;monthly_costs\u0026#34;: { \u0026#34;storage\u0026#34;: monthly_storage_cost, \u0026#34;processing\u0026#34;: monthly_processing_cost, \u0026#34;transfer\u0026#34;: monthly_transfer_cost, \u0026#34;total\u0026#34;: total_monthly_cost }, \u0026#34;yearly_projection\u0026#34;: total_monthly_cost * 12, \u0026#34;cost_breakdown\u0026#34;: { \u0026#34;storage_percentage\u0026#34;: (monthly_storage_cost / total_monthly_cost) * 100, \u0026#34;processing_percentage\u0026#34;: (monthly_processing_cost / total_monthly_cost) * 100, \u0026#34;transfer_percentage\u0026#34;: (monthly_transfer_cost / total_monthly_cost) * 100 } } def generate_architecture_report(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆæ¶æ„æŠ¥å‘Š\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;timestamp\u0026#34;: datetime.utcnow().isoformat(), \u0026#34;summary\u0026#34;: { \u0026#34;total_data_sources\u0026#34;: len(self.data_sources), \u0026#34;total_pipelines\u0026#34;: len(self.data_pipelines), \u0026#34;total_assets\u0026#34;: len(self.data_assets) }, \u0026#34;volume_analysis\u0026#34;: self.analyze_data_volume(), \u0026#34;velocity_analysis\u0026#34;: self.analyze_data_velocity(), \u0026#34;variety_analysis\u0026#34;: self.analyze_data_variety(), \u0026#34;pipeline_analysis\u0026#34;: self.analyze_pipeline_complexity(), \u0026#34;recommendations\u0026#34;: self.generate_architecture_recommendations(), \u0026#34;cost_estimation\u0026#34;: self.generate_cost_estimation() } # ä½¿ç”¨ç¤ºä¾‹ def data_architecture_example(): \u0026#34;\u0026#34;\u0026#34;æ•°æ®æ¶æ„ç¤ºä¾‹\u0026#34;\u0026#34;\u0026#34; analyzer = DataArchitectureAnalyzer() # æ·»åŠ æ•°æ®æº sources = [ DataSource( id=\u0026#34;src-001\u0026#34;, name=\u0026#34;ç”¨æˆ·è¡Œä¸ºæ•°æ®åº“\u0026#34;, type=DataSourceType.RELATIONAL_DB, format=DataFormat.JSON, location=\u0026#34;mysql://prod-db/user_events\u0026#34;, schema={\u0026#34;user_id\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;event_type\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;timestamp\u0026#34;: \u0026#34;datetime\u0026#34;}, volume_gb_per_day=50.0, velocity_records_per_second=500, variety_score=6, created_at=datetime.utcnow(), updated_at=datetime.utcnow() ), DataSource( id=\u0026#34;src-002\u0026#34;, name=\u0026#34;IoTä¼ æ„Ÿå™¨æ•°æ®\u0026#34;, type=DataSourceType.STREAMING, format=DataFormat.AVRO, location=\u0026#34;kafka://iot-cluster/sensor-data\u0026#34;, schema={\u0026#34;device_id\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;temperature\u0026#34;: \u0026#34;float\u0026#34;, \u0026#34;humidity\u0026#34;: \u0026#34;float\u0026#34;}, volume_gb_per_day=200.0, velocity_records_per_second=2000, variety_score=8, created_at=datetime.utcnow(), updated_at=datetime.utcnow() ), DataSource( id=\u0026#34;src-003\u0026#34;, name=\u0026#34;åº”ç”¨æ—¥å¿—\u0026#34;, type=DataSourceType.LOG, format=DataFormat.JSON, location=\u0026#34;s3://logs-bucket/app-logs/\u0026#34;, schema={\u0026#34;timestamp\u0026#34;: \u0026#34;datetime\u0026#34;, \u0026#34;level\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;string\u0026#34;}, volume_gb_per_day=100.0, velocity_records_per_second=1000, variety_score=4, created_at=datetime.utcnow(), updated_at=datetime.utcnow() ) ] for source in sources: analyzer.add_data_source(source) # æ·»åŠ æ•°æ®ç®¡é“ pipelines = [ DataPipeline( id=\u0026#34;pipe-001\u0026#34;, name=\u0026#34;ç”¨æˆ·è¡Œä¸ºETL\u0026#34;, source_ids=[\u0026#34;src-001\u0026#34;], processing_type=ProcessingType.BATCH, transformations=[ {\u0026#34;type\u0026#34;: \u0026#34;filter\u0026#34;, \u0026#34;condition\u0026#34;: \u0026#34;event_type != \u0026#39;test\u0026#39;\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;aggregate\u0026#34;, \u0026#34;group_by\u0026#34;: \u0026#34;user_id\u0026#34;, \u0026#34;metrics\u0026#34;: [\u0026#34;count\u0026#34;, \u0026#34;avg\u0026#34;]}, {\u0026#34;type\u0026#34;: \u0026#34;join\u0026#34;, \u0026#34;table\u0026#34;: \u0026#34;user_profiles\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;user_id\u0026#34;} ], destination=\u0026#34;s3://data-lake/user-behavior/\u0026#34;, schedule=\u0026#34;0 2 * * *\u0026#34;, # æ¯å¤©å‡Œæ™¨2ç‚¹ sla_minutes=120, created_at=datetime.utcnow(), updated_at=datetime.utcnow() ), DataPipeline( id=\u0026#34;pipe-002\u0026#34;, name=\u0026#34;IoTå®æ—¶å¤„ç†\u0026#34;, source_ids=[\u0026#34;src-002\u0026#34;], processing_type=ProcessingType.STREAMING, transformations=[ {\u0026#34;type\u0026#34;: \u0026#34;validate\u0026#34;, \u0026#34;schema\u0026#34;: \u0026#34;iot_schema\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;enrich\u0026#34;, \u0026#34;lookup\u0026#34;: \u0026#34;device_metadata\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;window\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;tumbling\u0026#34;, \u0026#34;duration\u0026#34;: \u0026#34;5m\u0026#34;}, {\u0026#34;type\u0026#34;: \u0026#34;aggregate\u0026#34;, \u0026#34;metrics\u0026#34;: [\u0026#34;avg\u0026#34;, \u0026#34;max\u0026#34;, \u0026#34;min\u0026#34;]} ], destination=\u0026#34;kinesis://processed-iot-stream\u0026#34;, schedule=None, sla_minutes=5, created_at=datetime.utcnow(), updated_at=datetime.utcnow() ) ] for pipeline in pipelines: analyzer.add_data_pipeline(pipeline) # æ·»åŠ æ•°æ®èµ„äº§ assets = [ DataAsset( id=\u0026#34;asset-001\u0026#34;, name=\u0026#34;ç”¨æˆ·è¡Œä¸ºåˆ†æè¡¨\u0026#34;, description=\u0026#34;ç”¨æˆ·è¡Œä¸ºæ•°æ®çš„èšåˆåˆ†æç»“æœ\u0026#34;, owner=\u0026#34;æ•°æ®å›¢é˜Ÿ\u0026#34;, tags=[\u0026#34;ç”¨æˆ·åˆ†æ\u0026#34;, \u0026#34;è¡Œä¸ºæ•°æ®\u0026#34;, \u0026#34;è¥é”€\u0026#34;], schema={\u0026#34;user_id\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;total_events\u0026#34;: \u0026#34;int\u0026#34;, \u0026#34;avg_session_duration\u0026#34;: \u0026#34;float\u0026#34;}, quality_score=8.5, usage_frequency=\u0026#34;daily\u0026#34;, business_value=\u0026#34;high\u0026#34;, created_at=datetime.utcnow(), updated_at=datetime.utcnow() ), DataAsset( id=\u0026#34;asset-002\u0026#34;, name=\u0026#34;IoTè®¾å¤‡ç›‘æ§ä»ªè¡¨æ¿\u0026#34;, description=\u0026#34;å®æ—¶IoTè®¾å¤‡çŠ¶æ€ç›‘æ§æ•°æ®\u0026#34;, owner=\u0026#34;è¿ç»´å›¢é˜Ÿ\u0026#34;, tags=[\u0026#34;IoT\u0026#34;, \u0026#34;ç›‘æ§\u0026#34;, \u0026#34;å®æ—¶\u0026#34;], schema={\u0026#34;device_id\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;string\u0026#34;, \u0026#34;last_update\u0026#34;: \u0026#34;datetime\u0026#34;}, quality_score=9.2, usage_frequency=\u0026#34;real-time\u0026#34;, business_value=\u0026#34;critical\u0026#34;, created_at=datetime.utcnow(), updated_at=datetime.utcnow() ) ] for asset in assets: analyzer.add_data_asset(asset) # ç”Ÿæˆæ¶æ„æŠ¥å‘Š report = analyzer.generate_architecture_report() print(\u0026#34;æ•°æ®æ¶æ„åˆ†ææŠ¥å‘Š\u0026#34;) print(\u0026#34;=\u0026#34; * 50) print(f\u0026#34;æ•°æ®æºæ€»æ•°: {report[\u0026#39;summary\u0026#39;][\u0026#39;total_data_sources\u0026#39;]}\u0026#34;) print(f\u0026#34;æ•°æ®ç®¡é“æ€»æ•°: {report[\u0026#39;summary\u0026#39;][\u0026#39;total_pipelines\u0026#39;]}\u0026#34;) print(f\u0026#34;æ•°æ®èµ„äº§æ€»æ•°: {report[\u0026#39;summary\u0026#39;][\u0026#39;total_assets\u0026#39;]}\u0026#34;) print(\u0026#34;\\næ•°æ®é‡åˆ†æ:\u0026#34;) volume = report[\u0026#39;volume_analysis\u0026#39;] print(f\u0026#34; æ¯æ—¥æ•°æ®é‡: {volume[\u0026#39;total_volume_gb_per_day\u0026#39;]:.2f} GB\u0026#34;) print(f\u0026#34; å¹´åº¦é¢„è®¡æ•°æ®é‡: {volume[\u0026#39;projected_yearly_volume_tb\u0026#39;]:.2f} TB\u0026#34;) print(\u0026#34;\\næ•°æ®é€Ÿåº¦åˆ†æ:\u0026#34;) velocity = report[\u0026#39;velocity_analysis\u0026#39;] print(f\u0026#34; æ€»è®°å½•é€Ÿåº¦: {velocity[\u0026#39;total_velocity_records_per_second\u0026#39;]:,} records/sec\u0026#34;) print(f\u0026#34; æ¯æ—¥å³°å€¼è®°å½•: {velocity[\u0026#39;peak_daily_records\u0026#39;]:,} records\u0026#34;) print(\u0026#34;\\næ¶æ„å»ºè®®:\u0026#34;) recommendations = report[\u0026#39;recommendations\u0026#39;] for category, items in recommendations.items(): if items: print(f\u0026#34; {category}:\u0026#34;) for item in items: if isinstance(item, dict): print(f\u0026#34; - {item.get(\u0026#39;type\u0026#39;, item.get(\u0026#39;pattern\u0026#39;, \u0026#39;N/A\u0026#39;))}: {item.get(\u0026#39;reason\u0026#39;, item.get(\u0026#39;description\u0026#39;, \u0026#39;N/A\u0026#39;))}\u0026#34;) print(\u0026#34;\\næˆæœ¬ä¼°ç®—:\u0026#34;) costs = report[\u0026#39;cost_estimation\u0026#39;] print(f\u0026#34; æœˆåº¦æ€»æˆæœ¬: ${costs[\u0026#39;monthly_costs\u0026#39;][\u0026#39;total\u0026#39;]:.2f}\u0026#34;) print(f\u0026#34; å¹´åº¦é¢„è®¡æˆæœ¬: ${costs[\u0026#39;yearly_projection\u0026#39;]:.2f}\u0026#34;) print(f\u0026#34; å­˜å‚¨æˆæœ¬å æ¯”: {costs[\u0026#39;cost_breakdown\u0026#39;][\u0026#39;storage_percentage\u0026#39;]:.1f}%\u0026#34;) print(f\u0026#34; å¤„ç†æˆæœ¬å æ¯”: {costs[\u0026#39;cost_breakdown\u0026#39;][\u0026#39;processing_percentage\u0026#39;]:.1f}%\u0026#34;) if __name__ == \u0026#34;__main__\u0026#34;: data_architecture_example() æ•°æ®æ¹–æ¶æ„è®¾è®¡ åˆ†å±‚æ•°æ®æ¹–æ¶æ„ æ•°æ®æ¹–é‡‡ç”¨åˆ†å±‚æ¶æ„ï¼Œç¡®ä¿æ•°æ®çš„æœ‰åºç®¡ç†å’Œé«˜æ•ˆè®¿é—®ï¼š\n","content":"äº‘æ•°æ®æ¶æ„ï¼šå¤§æ•°æ®ä¸AIå¹³å°è®¾è®¡ ç›®å½• å¼•è¨€ æ•°æ®æ¶æ„æ¦‚è¿° æ•°æ®æ¹–æ¶æ„è®¾è®¡ æ•°æ®ä»“åº“ä¸æ•°æ®é›†å¸‚ å®æ—¶æ•°æ®å¤„ç† æœºå™¨å­¦ä¹ å¹³å° æ•°æ®æ²»ç†ä¸è´¨é‡ å®‰å…¨ä¸åˆè§„ æ€§èƒ½ä¼˜åŒ– æœ€ä½³å®è·µä¸å»ºè®® æ€»ç»“ å¼•è¨€ åœ¨æ•°å­—åŒ–è½¬å‹çš„æµªæ½®ä¸­ï¼Œæ•°æ®å·²æˆä¸ºä¼ä¸šæœ€å®è´µçš„èµ„äº§ã€‚äº‘æ•°æ®æ¶æ„ä¸ºç»„ç»‡æä¾›äº†å¤„ç†æµ·é‡æ•°æ®ã€æ„å»ºæ™ºèƒ½åº”ç”¨çš„å¼ºå¤§å¹³å°ã€‚æœ¬æ–‡å°†æ·±å…¥æ¢è®¨å¦‚ä½•åœ¨äº‘ç¯å¢ƒä¸­è®¾è®¡å’Œå®æ–½å¤§æ•°æ®ä¸AIå¹³å°ï¼Œæ¶µç›–ä»æ•°æ®é‡‡é›†ã€å­˜å‚¨ã€å¤„ç†åˆ°åˆ†æçš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸã€‚\nç°ä»£äº‘æ•°æ®æ¶æ„éœ€è¦æ”¯æŒå¤šæ ·åŒ–çš„æ•°æ®æºã€å®æ—¶å’Œæ‰¹å¤„ç†å·¥ä½œè´Ÿè½½ã€æœºå™¨å­¦ä¹ å·¥ä½œæµï¼ŒåŒæ—¶ç¡®ä¿æ•°æ®å®‰å…¨ã€æ²»ç†å’Œåˆè§„æ€§ã€‚é€šè¿‡åˆç†çš„æ¶æ„è®¾è®¡ï¼Œä¼ä¸šå¯ä»¥æ„å»ºæ•æ·ã€å¯æ‰©å±•ä¸”æˆæœ¬æ•ˆç›Šé«˜çš„æ•°æ®å¹³å°ã€‚\næ•°æ®æ¶æ„æ¦‚è¿° ç°ä»£æ•°æ®æ¶æ„æ¨¡å¼ graph TB subgraph \u0026amp;#34;æ•°æ®æºå±‚\u0026amp;#34; A[ä¸šåŠ¡ç³»ç»Ÿ] --\u0026amp;gt; D[æ•°æ®é‡‡é›†å±‚] B[IoTè®¾å¤‡] --\u0026amp;gt; D C[å¤–éƒ¨API] --\u0026amp;gt; D E[æ—¥å¿—æ–‡ä»¶] --\u0026amp;gt; D F[æµæ•°æ®] --\u0026amp;gt; D end subgraph \u0026amp;#34;æ•°æ®é‡‡é›†å±‚\u0026amp;#34; D --\u0026amp;gt; G[æ‰¹é‡ETL] D --\u0026amp;gt; H[æµå¤„ç†] D â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["äº‘æ¶æ„","å¤§æ•°æ®","äººå·¥æ™ºèƒ½","æ•°æ®å¹³å°","æœºå™¨å­¦ä¹ ","æ•°æ®æ¹–","æ•°æ®ä»“åº“"],"categories":["äº‘è®¡ç®—","æ•°æ®æ¶æ„"],"author":"äº‘æ¶æ„ä¸“å®¶","readingTime":38,"wordCount":7981,"section":"posts","type":"posts","draft":false,"featured":false,"series":null},{"title":"äº‘åŸç”Ÿå¾®æœåŠ¡æ¶æ„è®¾è®¡ä¸å®è·µ","url":"https://www.dishuihengxin.com/posts/cloud-native-microservices-architecture/","summary":"äº‘åŸç”Ÿå¾®æœåŠ¡æ¶æ„è®¾è®¡ä¸å®è·µ å¼•è¨€ éšç€æ•°å­—åŒ–è½¬å‹çš„æ·±å…¥æ¨è¿›ï¼Œä¼ ç»Ÿçš„å•ä½“åº”ç”¨æ¶æ„å·²ç»æ— æ³•æ»¡è¶³ç°ä»£ä¼ä¸šå¯¹æ•æ·æ€§ã€å¯æ‰©å±•æ€§å’Œé«˜å¯ç”¨æ€§çš„éœ€æ±‚ã€‚äº‘åŸç”Ÿå¾®æœåŠ¡æ¶æ„ä½œä¸ºç°ä»£åº”ç”¨å¼€å‘çš„ä¸»æµæ¨¡å¼ï¼Œé€šè¿‡å°†å¤æ‚çš„å•ä½“åº”ç”¨æ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹çš„å¾®æœåŠ¡ï¼Œå®ç°äº†æ›´å¥½çš„å¯ç»´æŠ¤æ€§ã€å¯æ‰©å±•æ€§å’ŒæŠ€æœ¯å¤šæ ·æ€§ã€‚\næœ¬æ–‡å°†æ·±å…¥æ¢è®¨äº‘åŸç”Ÿå¾®æœåŠ¡æ¶æ„çš„è®¾è®¡åŸåˆ™ã€å®ç°æ–¹æ¡ˆå’Œæœ€ä½³å®è·µï¼Œå¸®åŠ©ä¼ä¸šæ„å»ºé«˜æ•ˆã€å¯é çš„å¾®æœåŠ¡ç³»ç»Ÿã€‚\nç›®å½• å¾®æœåŠ¡æ¶æ„æ¦‚è¿° æœåŠ¡æ‹†åˆ†ç­–ç•¥ å®¹å™¨åŒ–ä¸ç¼–æ’ æœåŠ¡ç½‘æ ¼æ¶æ„ APIç½‘å…³è®¾è®¡ æ•°æ®ç®¡ç†ç­–ç•¥ ç›‘æ§ä¸å¯è§‚æµ‹æ€§ æœ€ä½³å®è·µä¸å»ºè®® æ€»ç»“ å¾®æœåŠ¡æ¶æ„æ¦‚è¿° æ¶æ„æ¼”è¿›è·¯å¾„ graph TD A[å•ä½“åº”ç”¨] --\u0026gt; B[åˆ†å±‚æ¶æ„] B --\u0026gt; C[SOAæ¶æ„] C --\u0026gt; D[å¾®æœåŠ¡æ¶æ„] D --\u0026gt; E[äº‘åŸç”Ÿå¾®æœåŠ¡] A1[éƒ¨ç½²ç®€å•\u0026lt;br/\u0026gt;å¼€å‘æ•ˆç‡é«˜\u0026lt;br/\u0026gt;æŠ€æœ¯æ ˆç»Ÿä¸€] --\u0026gt; A B1[æ¨¡å—åŒ–\u0026lt;br/\u0026gt;èŒè´£åˆ†ç¦»\u0026lt;br/\u0026gt;å¯ç»´æŠ¤æ€§æå‡] --\u0026gt; B C1[æœåŠ¡é‡ç”¨\u0026lt;br/\u0026gt;æ¾è€¦åˆ\u0026lt;br/\u0026gt;æ ‡å‡†åŒ–æ¥å£] --\u0026gt; C D1[ç‹¬ç«‹éƒ¨ç½²\u0026lt;br/\u0026gt;æŠ€æœ¯å¤šæ ·æ€§\u0026lt;br/\u0026gt;å›¢é˜Ÿè‡ªæ²»] --\u0026gt; D E1[å®¹å™¨åŒ–\u0026lt;br/\u0026gt;è‡ªåŠ¨åŒ–è¿ç»´\u0026lt;br/\u0026gt;å¼¹æ€§ä¼¸ç¼©] --\u0026gt; E å¾®æœåŠ¡æ¶æ„åˆ†æå™¨ from dataclasses import dataclass from typing import List, Dict, Optional, Tuple from enum import Enum import json import yaml class ServiceType(Enum): BUSINESS = \u0026#34;business\u0026#34; INFRASTRUCTURE = \u0026#34;infrastructure\u0026#34; GATEWAY = \u0026#34;gateway\u0026#34; DATA = \u0026#34;data\u0026#34; class CommunicationPattern(Enum): SYNCHRONOUS = \u0026#34;synchronous\u0026#34; ASYNCHRONOUS = \u0026#34;asynchronous\u0026#34; EVENT_DRIVEN = \u0026#34;event_driven\u0026#34; @dataclass class ServiceDefinition: name: str type: ServiceType responsibilities: List[str] dependencies: List[str] data_stores: List[str] communication_patterns: List[CommunicationPattern] scalability_requirements: Dict[str, any] @dataclass class ArchitectureRecommendation: service_count: int complexity_score: float recommended_patterns: List[str] technology_stack: Dict[str, str] deployment_strategy: str estimated_cost: Dict[str, float] class MicroservicesArchitectureAnalyzer: def __init__(self): self.services = [] self.domain_boundaries = {} def analyze_domain(self, business_capabilities: List[str], data_entities: List[str], user_journeys: List[str]) -\u0026gt; Dict[str, any]: \u0026#34;\u0026#34;\u0026#34;åˆ†æä¸šåŠ¡åŸŸï¼Œè¯†åˆ«æœåŠ¡è¾¹ç•Œ\u0026#34;\u0026#34;\u0026#34; # åŸºäºä¸šåŠ¡èƒ½åŠ›åˆ†æ capability_groups = self._group_capabilities(business_capabilities) # åŸºäºæ•°æ®å®ä½“åˆ†æ data_clusters = self._cluster_data_entities(data_entities) # åŸºäºç”¨æˆ·æ—…ç¨‹åˆ†æ journey_flows = self._analyze_user_journeys(user_journeys) # ç”ŸæˆæœåŠ¡å»ºè®® service_recommendations = self._generate_service_recommendations( capability_groups, data_clusters, journey_flows ) return { \u0026#34;capability_groups\u0026#34;: capability_groups, \u0026#34;data_clusters\u0026#34;: data_clusters, \u0026#34;journey_flows\u0026#34;: journey_flows, \u0026#34;service_recommendations\u0026#34;: service_recommendations, \u0026#34;complexity_analysis\u0026#34;: self._calculate_complexity(service_recommendations) } def _group_capabilities(self, capabilities: List[str]) -\u0026gt; Dict[str, List[str]]: \u0026#34;\u0026#34;\u0026#34;åŸºäºä¸šåŠ¡èƒ½åŠ›åˆ†ç»„\u0026#34;\u0026#34;\u0026#34; groups = { \u0026#34;user_management\u0026#34;: [], \u0026#34;order_processing\u0026#34;: [], \u0026#34;payment_handling\u0026#34;: [], \u0026#34;inventory_management\u0026#34;: [], \u0026#34;notification_services\u0026#34;: [], \u0026#34;analytics_reporting\u0026#34;: [] } capability_mapping = { \u0026#34;user\u0026#34;: \u0026#34;user_management\u0026#34;, \u0026#34;auth\u0026#34;: \u0026#34;user_management\u0026#34;, \u0026#34;order\u0026#34;: \u0026#34;order_processing\u0026#34;, \u0026#34;cart\u0026#34;: \u0026#34;order_processing\u0026#34;, \u0026#34;payment\u0026#34;: \u0026#34;payment_handling\u0026#34;, \u0026#34;billing\u0026#34;: \u0026#34;payment_handling\u0026#34;, \u0026#34;inventory\u0026#34;: \u0026#34;inventory_management\u0026#34;, \u0026#34;product\u0026#34;: \u0026#34;inventory_management\u0026#34;, \u0026#34;notification\u0026#34;: \u0026#34;notification_services\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;notification_services\u0026#34;, \u0026#34;analytics\u0026#34;: \u0026#34;analytics_reporting\u0026#34;, \u0026#34;report\u0026#34;: \u0026#34;analytics_reporting\u0026#34; } for capability in capabilities: for keyword, group in capability_mapping.items(): if keyword in capability.lower(): groups[group].append(capability) break return {k: v for k, v in groups.items() if v} def _cluster_data_entities(self, entities: List[str]) -\u0026gt; Dict[str, List[str]]: \u0026#34;\u0026#34;\u0026#34;åŸºäºæ•°æ®å®ä½“èšç±»\u0026#34;\u0026#34;\u0026#34; clusters = {} # ç®€åŒ–çš„èšç±»é€»è¾‘ entity_relationships = { \u0026#34;user\u0026#34;: [\u0026#34;profile\u0026#34;, \u0026#34;preference\u0026#34;, \u0026#34;authentication\u0026#34;], \u0026#34;order\u0026#34;: [\u0026#34;item\u0026#34;, \u0026#34;shipping\u0026#34;, \u0026#34;status\u0026#34;], \u0026#34;product\u0026#34;: [\u0026#34;catalog\u0026#34;, \u0026#34;inventory\u0026#34;, \u0026#34;pricing\u0026#34;], \u0026#34;payment\u0026#34;: [\u0026#34;transaction\u0026#34;, \u0026#34;billing\u0026#34;, \u0026#34;refund\u0026#34;] } for entity in entities: for cluster_key, related_entities in entity_relationships.items(): if any(related in entity.lower() for related in related_entities): if cluster_key not in clusters: clusters[cluster_key] = [] clusters[cluster_key].append(entity) break return clusters def _analyze_user_journeys(self, journeys: List[str]) -\u0026gt; Dict[str, List[str]]: \u0026#34;\u0026#34;\u0026#34;åˆ†æç”¨æˆ·æ—…ç¨‹\u0026#34;\u0026#34;\u0026#34; flows = {} for journey in journeys: # ç®€åŒ–çš„æ—…ç¨‹åˆ†æ if \u0026#34;registration\u0026#34; in journey.lower(): flows[\u0026#34;user_onboarding\u0026#34;] = [\u0026#34;authentication\u0026#34;, \u0026#34;profile_creation\u0026#34;, \u0026#34;verification\u0026#34;] elif \u0026#34;purchase\u0026#34; in journey.lower(): flows[\u0026#34;purchase_flow\u0026#34;] = [\u0026#34;product_search\u0026#34;, \u0026#34;cart_management\u0026#34;, \u0026#34;payment\u0026#34;, \u0026#34;order_fulfillment\u0026#34;] elif \u0026#34;support\u0026#34; in journey.lower(): flows[\u0026#34;customer_support\u0026#34;] = [\u0026#34;ticket_creation\u0026#34;, \u0026#34;communication\u0026#34;, \u0026#34;resolution\u0026#34;] return flows def _generate_service_recommendations(self, capability_groups: Dict, data_clusters: Dict, journey_flows: Dict) -\u0026gt; List[ServiceDefinition]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆæœåŠ¡å»ºè®®\u0026#34;\u0026#34;\u0026#34; services = [] # åŸºäºèƒ½åŠ›ç»„ç”ŸæˆæœåŠ¡ for group_name, capabilities in capability_groups.items(): service = ServiceDefinition( name=f\u0026#34;{group_name}_service\u0026#34;, type=ServiceType.BUSINESS, responsibilities=capabilities, dependencies=[], data_stores=[group_name + \u0026#34;_db\u0026#34;], communication_patterns=[CommunicationPattern.SYNCHRONOUS], scalability_requirements={\u0026#34;min_instances\u0026#34;: 2, \u0026#34;max_instances\u0026#34;: 10} ) services.append(service) # æ·»åŠ åŸºç¡€è®¾æ–½æœåŠ¡ infrastructure_services = [ ServiceDefinition( name=\u0026#34;api_gateway\u0026#34;, type=ServiceType.GATEWAY, responsibilities=[\u0026#34;request_routing\u0026#34;, \u0026#34;authentication\u0026#34;, \u0026#34;rate_limiting\u0026#34;], dependencies=[], data_stores=[\u0026#34;gateway_cache\u0026#34;], communication_patterns=[CommunicationPattern.SYNCHRONOUS], scalability_requirements={\u0026#34;min_instances\u0026#34;: 3, \u0026#34;max_instances\u0026#34;: 20} ), ServiceDefinition( name=\u0026#34;event_bus\u0026#34;, type=ServiceType.INFRASTRUCTURE, responsibilities=[\u0026#34;event_routing\u0026#34;, \u0026#34;message_persistence\u0026#34;, \u0026#34;delivery_guarantee\u0026#34;], dependencies=[], data_stores=[\u0026#34;event_store\u0026#34;], communication_patterns=[CommunicationPattern.ASYNCHRONOUS], scalability_requirements={\u0026#34;min_instances\u0026#34;: 3, \u0026#34;max_instances\u0026#34;: 15} ) ] services.extend(infrastructure_services) return services def _calculate_complexity(self, services: List[ServiceDefinition]) -\u0026gt; Dict[str, any]: \u0026#34;\u0026#34;\u0026#34;è®¡ç®—æ¶æ„å¤æ‚åº¦\u0026#34;\u0026#34;\u0026#34; service_count = len(services) # è®¡ç®—å¤æ‚åº¦åˆ†æ•° complexity_factors = { \u0026#34;service_count\u0026#34;: service_count * 0.3, \u0026#34;communication_complexity\u0026#34;: sum(len(s.dependencies) for s in services) * 0.4, \u0026#34;data_complexity\u0026#34;: sum(len(s.data_stores) for s in services) * 0.3 } total_complexity = sum(complexity_factors.values()) return { \u0026#34;total_score\u0026#34;: total_complexity, \u0026#34;factors\u0026#34;: complexity_factors, \u0026#34;recommendation\u0026#34;: self._get_complexity_recommendation(total_complexity) } def _get_complexity_recommendation(self, score: float) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;åŸºäºå¤æ‚åº¦åˆ†æ•°æä¾›å»ºè®®\u0026#34;\u0026#34;\u0026#34; if score \u0026lt; 10: return \u0026#34;ä½å¤æ‚åº¦ï¼šé€‚åˆå°å›¢é˜Ÿï¼Œå»ºè®®ä»ç®€å•çš„æœåŠ¡æ‹†åˆ†å¼€å§‹\u0026#34; elif score \u0026lt; 25: return \u0026#34;ä¸­ç­‰å¤æ‚åº¦ï¼šéœ€è¦å®Œå–„çš„DevOpsæµç¨‹å’Œç›‘æ§ä½“ç³»\u0026#34; else: return \u0026#34;é«˜å¤æ‚åº¦ï¼šéœ€è¦ä¸“ä¸šçš„å¾®æœåŠ¡æ²»ç†å¹³å°å’Œç»éªŒä¸°å¯Œçš„å›¢é˜Ÿ\u0026#34; def generate_architecture_blueprint(self, services: List[ServiceDefinition]) -\u0026gt; Dict[str, any]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆæ¶æ„è“å›¾\u0026#34;\u0026#34;\u0026#34; blueprint = { \u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;services\u0026#34;: {}, \u0026#34;communication_matrix\u0026#34;: {}, \u0026#34;deployment_topology\u0026#34;: {}, \u0026#34;monitoring_strategy\u0026#34;: {} } # æœåŠ¡å®šä¹‰ for service in services: blueprint[\u0026#34;services\u0026#34;][service.name] = { \u0026#34;type\u0026#34;: service.type.value, \u0026#34;responsibilities\u0026#34;: service.responsibilities, \u0026#34;dependencies\u0026#34;: service.dependencies, \u0026#34;data_stores\u0026#34;: service.data_stores, \u0026#34;scaling\u0026#34;: service.scalability_requirements } # é€šä¿¡çŸ©é˜µ blueprint[\u0026#34;communication_matrix\u0026#34;] = self._build_communication_matrix(services) # éƒ¨ç½²æ‹“æ‰‘ blueprint[\u0026#34;deployment_topology\u0026#34;] = self._design_deployment_topology(services) # ç›‘æ§ç­–ç•¥ blueprint[\u0026#34;monitoring_strategy\u0026#34;] = self._design_monitoring_strategy(services) return blueprint def _build_communication_matrix(self, services: List[ServiceDefinition]) -\u0026gt; Dict[str, Dict[str, str]]: \u0026#34;\u0026#34;\u0026#34;æ„å»ºæœåŠ¡é€šä¿¡çŸ©é˜µ\u0026#34;\u0026#34;\u0026#34; matrix = {} for service in services: matrix[service.name] = {} for dep in service.dependencies: # ç®€åŒ–çš„é€šä¿¡æ¨¡å¼æ¨æ–­ if service.type == ServiceType.GATEWAY: matrix[service.name][dep] = \u0026#34;http_sync\u0026#34; elif \u0026#34;event\u0026#34; in dep: matrix[service.name][dep] = \u0026#34;event_async\u0026#34; else: matrix[service.name][dep] = \u0026#34;http_sync\u0026#34; return matrix def _design_deployment_topology(self, services: List[ServiceDefinition]) -\u0026gt; Dict[str, any]: \u0026#34;\u0026#34;\u0026#34;è®¾è®¡éƒ¨ç½²æ‹“æ‰‘\u0026#34;\u0026#34;\u0026#34; topology = { \u0026#34;clusters\u0026#34;: { \u0026#34;production\u0026#34;: { \u0026#34;nodes\u0026#34;: 6, \u0026#34;node_type\u0026#34;: \u0026#34;m5.large\u0026#34;, \u0026#34;availability_zones\u0026#34;: 3 }, \u0026#34;staging\u0026#34;: { \u0026#34;nodes\u0026#34;: 3, \u0026#34;node_type\u0026#34;: \u0026#34;m5.medium\u0026#34;, \u0026#34;availability_zones\u0026#34;: 2 } }, \u0026#34;namespaces\u0026#34;: { \u0026#34;gateway\u0026#34;: [\u0026#34;api_gateway\u0026#34;], \u0026#34;business\u0026#34;: [s.name for s in services if s.type == ServiceType.BUSINESS], \u0026#34;infrastructure\u0026#34;: [s.name for s in services if s.type == ServiceType.INFRASTRUCTURE], \u0026#34;data\u0026#34;: [s.name for s in services if s.type == ServiceType.DATA] }, \u0026#34;ingress\u0026#34;: { \u0026#34;load_balancer\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;ssl_termination\u0026#34;: True, \u0026#34;rate_limiting\u0026#34;: True } } return topology def _design_monitoring_strategy(self, services: List[ServiceDefinition]) -\u0026gt; Dict[str, any]: \u0026#34;\u0026#34;\u0026#34;è®¾è®¡ç›‘æ§ç­–ç•¥\u0026#34;\u0026#34;\u0026#34; strategy = { \u0026#34;metrics\u0026#34;: { \u0026#34;application\u0026#34;: [\u0026#34;request_rate\u0026#34;, \u0026#34;error_rate\u0026#34;, \u0026#34;response_time\u0026#34;], \u0026#34;infrastructure\u0026#34;: [\u0026#34;cpu_usage\u0026#34;, \u0026#34;memory_usage\u0026#34;, \u0026#34;disk_io\u0026#34;], \u0026#34;business\u0026#34;: [\u0026#34;transaction_volume\u0026#34;, \u0026#34;user_activity\u0026#34;, \u0026#34;revenue_metrics\u0026#34;] }, \u0026#34;logging\u0026#34;: { \u0026#34;centralized\u0026#34;: True, \u0026#34;structured\u0026#34;: True, \u0026#34;retention_days\u0026#34;: 30 }, \u0026#34;tracing\u0026#34;: { \u0026#34;distributed\u0026#34;: True, \u0026#34;sampling_rate\u0026#34;: 0.1, \u0026#34;trace_retention_hours\u0026#34;: 72 }, \u0026#34;alerting\u0026#34;: { \u0026#34;sla_violations\u0026#34;: True, \u0026#34;error_rate_threshold\u0026#34;: 0.05, \u0026#34;response_time_threshold\u0026#34;: \u0026#34;2s\u0026#34; } } return strategy # ä½¿ç”¨ç¤ºä¾‹ def microservices_analysis_example(): analyzer = MicroservicesArchitectureAnalyzer() # ä¸šåŠ¡èƒ½åŠ› business_capabilities = [ \u0026#34;User Registration\u0026#34;, \u0026#34;User Authentication\u0026#34;, \u0026#34;User Profile Management\u0026#34;, \u0026#34;Product Catalog\u0026#34;, \u0026#34;Inventory Management\u0026#34;, \u0026#34;Order Processing\u0026#34;, \u0026#34;Payment Processing\u0026#34;, \u0026#34;Shipping Management\u0026#34;, \u0026#34;Notification Service\u0026#34;, \u0026#34;Analytics and Reporting\u0026#34;, \u0026#34;Customer Support\u0026#34; ] # æ•°æ®å®ä½“ data_entities = [ \u0026#34;User\u0026#34;, \u0026#34;UserProfile\u0026#34;, \u0026#34;Product\u0026#34;, \u0026#34;Category\u0026#34;, \u0026#34;Inventory\u0026#34;, \u0026#34;Order\u0026#34;, \u0026#34;OrderItem\u0026#34;, \u0026#34;Payment\u0026#34;, \u0026#34;Transaction\u0026#34;, \u0026#34;Shipment\u0026#34;, \u0026#34;Notification\u0026#34;, \u0026#34;Report\u0026#34;, \u0026#34;SupportTicket\u0026#34; ] # ç”¨æˆ·æ—…ç¨‹ user_journeys = [ \u0026#34;User Registration and Onboarding\u0026#34;, \u0026#34;Product Discovery and Purchase\u0026#34;, \u0026#34;Order Tracking and Support\u0026#34;, \u0026#34;Account Management\u0026#34; ] # åˆ†æåŸŸ analysis_result = analyzer.analyze_domain( business_capabilities, data_entities, user_journeys ) print(\u0026#34;=== å¾®æœåŠ¡æ¶æ„åˆ†æç»“æœ ===\u0026#34;) print(f\u0026#34;è¯†åˆ«çš„èƒ½åŠ›ç»„: {list(analysis_result[\u0026#39;capability_groups\u0026#39;].keys())}\u0026#34;) print(f\u0026#34;æ•°æ®èšç±»: {list(analysis_result[\u0026#39;data_clusters\u0026#39;].keys())}\u0026#34;) print(f\u0026#34;ç”¨æˆ·æ—…ç¨‹: {list(analysis_result[\u0026#39;journey_flows\u0026#39;].keys())}\u0026#34;) print(f\u0026#34;å¤æ‚åº¦åˆ†æ: {analysis_result[\u0026#39;complexity_analysis\u0026#39;][\u0026#39;recommendation\u0026#39;]}\u0026#34;) # ç”Ÿæˆæ¶æ„è“å›¾ services = analysis_result[\u0026#39;service_recommendations\u0026#39;] blueprint = analyzer.generate_architecture_blueprint(services) print(f\u0026#34;\\n=== æ¶æ„è“å›¾ ===\u0026#34;) print(f\u0026#34;æœåŠ¡æ•°é‡: {len(blueprint[\u0026#39;services\u0026#39;])}\u0026#34;) print(f\u0026#34;éƒ¨ç½²é›†ç¾¤: {list(blueprint[\u0026#39;deployment_topology\u0026#39;][\u0026#39;clusters\u0026#39;].keys())}\u0026#34;) print(f\u0026#34;ç›‘æ§æŒ‡æ ‡: {blueprint[\u0026#39;monitoring_strategy\u0026#39;][\u0026#39;metrics\u0026#39;][\u0026#39;application\u0026#39;]}\u0026#34;) return analysis_result, blueprint if __name__ == \u0026#34;__main__\u0026#34;: microservices_analysis_example() æœåŠ¡æ‹†åˆ†ç­–ç•¥ é¢†åŸŸé©±åŠ¨è®¾è®¡ï¼ˆDDDï¼‰ from abc import ABC, abstractmethod from typing import List, Dict, Any, Optional from dataclasses import dataclass, field from enum import Enum import uuid class BoundedContextType(Enum): CORE = \u0026#34;core\u0026#34; SUPPORTING = \u0026#34;supporting\u0026#34; GENERIC = \u0026#34;generic\u0026#34; @dataclass class DomainEvent: event_id: str = field(default_factory=lambda: str(uuid.uuid4())) event_type: str = \u0026#34;\u0026#34; aggregate_id: str = \u0026#34;\u0026#34; version: int = 1 timestamp: str = \u0026#34;\u0026#34; data: Dict[str, Any] = field(default_factory=dict) class AggregateRoot(ABC): def __init__(self, aggregate_id: str): self.aggregate_id = aggregate_id self.version = 0 self.uncommitted_events: List[DomainEvent] = [] def mark_events_as_committed(self): self.uncommitted_events.clear() def get_uncommitted_events(self) -\u0026gt; List[DomainEvent]: return self.uncommitted_events.copy() def apply_event(self, event: DomainEvent): self.uncommitted_events.append(event) self.version += 1 @dataclass class BoundedContext: name: str type: BoundedContextType domain_services: List[str] aggregates: List[str] value_objects: List[str] repositories: List[str] domain_events: List[str] integration_events: List[str] class ServiceDecompositionStrategy: def __init__(self): self.bounded_contexts = {} self.context_map = {} def define_bounded_context(self, context: BoundedContext): \u0026#34;\u0026#34;\u0026#34;å®šä¹‰é™ç•Œä¸Šä¸‹æ–‡\u0026#34;\u0026#34;\u0026#34; self.bounded_contexts[context.name] = context def analyze_context_relationships(self) -\u0026gt; Dict[str, Dict[str, str]]: \u0026#34;\u0026#34;\u0026#34;åˆ†æä¸Šä¸‹æ–‡å…³ç³»\u0026#34;\u0026#34;\u0026#34; relationships = {} # å®šä¹‰ä¸Šä¸‹æ–‡é—´çš„å…³ç³»æ¨¡å¼ relationship_patterns = { (\u0026#34;user_management\u0026#34;, \u0026#34;order_processing\u0026#34;): \u0026#34;customer_supplier\u0026#34;, (\u0026#34;order_processing\u0026#34;, \u0026#34;payment_processing\u0026#34;): \u0026#34;partnership\u0026#34;, (\u0026#34;order_processing\u0026#34;, \u0026#34;inventory_management\u0026#34;): \u0026#34;shared_kernel\u0026#34;, (\u0026#34;notification_service\u0026#34;, \u0026#34;order_processing\u0026#34;): \u0026#34;conformist\u0026#34;, (\u0026#34;analytics\u0026#34;, \u0026#34;order_processing\u0026#34;): \u0026#34;anticorruption_layer\u0026#34; } for (upstream, downstream), pattern in relationship_patterns.items(): if upstream not in relationships: relationships[upstream] = {} relationships[upstream][downstream] = pattern return relationships def generate_service_boundaries(self) -\u0026gt; Dict[str, Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆæœåŠ¡è¾¹ç•Œ\u0026#34;\u0026#34;\u0026#34; service_boundaries = {} for context_name, context in self.bounded_contexts.items(): # åŸºäºèšåˆæ ¹ç¡®å®šæœåŠ¡è¾¹ç•Œ if context.type == BoundedContextType.CORE: # æ ¸å¿ƒåŸŸï¼šæ¯ä¸ªèšåˆä¸€ä¸ªæœåŠ¡ for aggregate in context.aggregates: service_name = f\u0026#34;{context_name}_{aggregate}_service\u0026#34; service_boundaries[service_name] = { \u0026#34;bounded_context\u0026#34;: context_name, \u0026#34;primary_aggregate\u0026#34;: aggregate, \u0026#34;responsibilities\u0026#34;: [aggregate], \u0026#34;data_ownership\u0026#34;: [aggregate.lower() + \u0026#34;_data\u0026#34;], \u0026#34;api_contracts\u0026#34;: self._generate_api_contract(aggregate), \u0026#34;integration_patterns\u0026#34;: [\u0026#34;event_sourcing\u0026#34;, \u0026#34;cqrs\u0026#34;] } else: # æ”¯æ’‘åŸŸå’Œé€šç”¨åŸŸï¼šå¯ä»¥åˆå¹¶å¤šä¸ªèšåˆ service_name = f\u0026#34;{context_name}_service\u0026#34; service_boundaries[service_name] = { \u0026#34;bounded_context\u0026#34;: context_name, \u0026#34;aggregates\u0026#34;: context.aggregates, \u0026#34;responsibilities\u0026#34;: context.aggregates, \u0026#34;data_ownership\u0026#34;: [agg.lower() + \u0026#34;_data\u0026#34; for agg in context.aggregates], \u0026#34;api_contracts\u0026#34;: self._generate_composite_api_contract(context.aggregates), \u0026#34;integration_patterns\u0026#34;: [\u0026#34;crud\u0026#34;, \u0026#34;shared_database\u0026#34;] } return service_boundaries def _generate_api_contract(self, aggregate: str) -\u0026gt; Dict[str, List[str]]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆAPIå¥‘çº¦\u0026#34;\u0026#34;\u0026#34; base_operations = [\u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;delete\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;] return { \u0026#34;commands\u0026#34;: [f\u0026#34;{op}_{aggregate}\u0026#34; for op in base_operations[:3]], \u0026#34;queries\u0026#34;: [f\u0026#34;{op}_{aggregate}\u0026#34; for op in base_operations[3:]], \u0026#34;events\u0026#34;: [f\u0026#34;{aggregate}_created\u0026#34;, f\u0026#34;{aggregate}_updated\u0026#34;, f\u0026#34;{aggregate}_deleted\u0026#34;] } def _generate_composite_api_contract(self, aggregates: List[str]) -\u0026gt; Dict[str, List[str]]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆå¤åˆAPIå¥‘çº¦\u0026#34;\u0026#34;\u0026#34; commands = [] queries = [] events = [] for aggregate in aggregates: contract = self._generate_api_contract(aggregate) commands.extend(contract[\u0026#34;commands\u0026#34;]) queries.extend(contract[\u0026#34;queries\u0026#34;]) events.extend(contract[\u0026#34;events\u0026#34;]) return { \u0026#34;commands\u0026#34;: commands, \u0026#34;queries\u0026#34;: queries, \u0026#34;events\u0026#34;: events } def validate_service_design(self, service_boundaries: Dict[str, Dict[str, Any]]) -\u0026gt; Dict[str, List[str]]: \u0026#34;\u0026#34;\u0026#34;éªŒè¯æœåŠ¡è®¾è®¡\u0026#34;\u0026#34;\u0026#34; validation_results = { \u0026#34;warnings\u0026#34;: [], \u0026#34;recommendations\u0026#34;: [], \u0026#34;violations\u0026#34;: [] } # æ£€æŸ¥æœåŠ¡æ•°é‡ service_count = len(service_boundaries) if service_count \u0026gt; 20: validation_results[\u0026#34;warnings\u0026#34;].append(f\u0026#34;æœåŠ¡æ•°é‡è¿‡å¤š({service_count})ï¼Œå¯èƒ½å¯¼è‡´ç®¡ç†å¤æ‚åº¦è¿‡é«˜\u0026#34;) elif service_count \u0026lt; 3: validation_results[\u0026#34;warnings\u0026#34;].append(f\u0026#34;æœåŠ¡æ•°é‡è¿‡å°‘({service_count})ï¼Œå¯èƒ½æœªå……åˆ†åˆ©ç”¨å¾®æœåŠ¡ä¼˜åŠ¿\u0026#34;) # æ£€æŸ¥æ•°æ®æ‰€æœ‰æƒ data_ownership = {} for service_name, service_info in service_boundaries.items(): for data in service_info.get(\u0026#34;data_ownership\u0026#34;, []): if data in data_ownership: validation_results[\u0026#34;violations\u0026#34;].append( f\u0026#34;æ•°æ®æ‰€æœ‰æƒå†²çª: {data} è¢« {service_name} å’Œ {data_ownership[data]} åŒæ—¶æ‹¥æœ‰\u0026#34; ) data_ownership[data] = service_name # æ£€æŸ¥APIè®¾è®¡ for service_name, service_info in service_boundaries.items(): api_contract = service_info.get(\u0026#34;api_contracts\u0026#34;, {}) if not api_contract.get(\u0026#34;events\u0026#34;): validation_results[\u0026#34;recommendations\u0026#34;].append( f\u0026#34;{service_name} ç¼ºå°‘é¢†åŸŸäº‹ä»¶ï¼Œå»ºè®®æ·»åŠ äº‹ä»¶å‘å¸ƒæœºåˆ¶\u0026#34; ) return validation_results # ä½¿ç”¨ç¤ºä¾‹ def service_decomposition_example(): strategy = ServiceDecompositionStrategy() # å®šä¹‰é™ç•Œä¸Šä¸‹æ–‡ contexts = [ BoundedContext( name=\u0026#34;user_management\u0026#34;, type=BoundedContextType.CORE, domain_services=[\u0026#34;UserService\u0026#34;, \u0026#34;AuthenticationService\u0026#34;], aggregates=[\u0026#34;User\u0026#34;, \u0026#34;UserProfile\u0026#34;], value_objects=[\u0026#34;Email\u0026#34;, \u0026#34;Password\u0026#34;], repositories=[\u0026#34;UserRepository\u0026#34;], domain_events=[\u0026#34;UserRegistered\u0026#34;, \u0026#34;UserProfileUpdated\u0026#34;], integration_events=[\u0026#34;UserCreated\u0026#34;, \u0026#34;UserActivated\u0026#34;] ), BoundedContext( name=\u0026#34;order_processing\u0026#34;, type=BoundedContextType.CORE, domain_services=[\u0026#34;OrderService\u0026#34;, \u0026#34;OrderValidationService\u0026#34;], aggregates=[\u0026#34;Order\u0026#34;, \u0026#34;OrderItem\u0026#34;], value_objects=[\u0026#34;Money\u0026#34;, \u0026#34;Quantity\u0026#34;], repositories=[\u0026#34;OrderRepository\u0026#34;], domain_events=[\u0026#34;OrderPlaced\u0026#34;, \u0026#34;OrderShipped\u0026#34;], integration_events=[\u0026#34;OrderCreated\u0026#34;, \u0026#34;OrderCompleted\u0026#34;] ), BoundedContext( name=\u0026#34;payment_processing\u0026#34;, type=BoundedContextType.SUPPORTING, domain_services=[\u0026#34;PaymentService\u0026#34;], aggregates=[\u0026#34;Payment\u0026#34;, \u0026#34;Transaction\u0026#34;], value_objects=[\u0026#34;CreditCard\u0026#34;, \u0026#34;Amount\u0026#34;], repositories=[\u0026#34;PaymentRepository\u0026#34;], domain_events=[\u0026#34;PaymentProcessed\u0026#34;, \u0026#34;PaymentFailed\u0026#34;], integration_events=[\u0026#34;PaymentCompleted\u0026#34;, \u0026#34;RefundIssued\u0026#34;] ) ] # æ³¨å†Œä¸Šä¸‹æ–‡ for context in contexts: strategy.define_bounded_context(context) # åˆ†æå…³ç³» relationships = strategy.analyze_context_relationships() print(\u0026#34;=== ä¸Šä¸‹æ–‡å…³ç³»åˆ†æ ===\u0026#34;) for upstream, downstreams in relationships.items(): for downstream, pattern in downstreams.items(): print(f\u0026#34;{upstream} -\u0026gt; {downstream}: {pattern}\u0026#34;) # ç”ŸæˆæœåŠ¡è¾¹ç•Œ service_boundaries = strategy.generate_service_boundaries() print(f\u0026#34;\\n=== æœåŠ¡è¾¹ç•Œè®¾è®¡ ===\u0026#34;) for service_name, service_info in service_boundaries.items(): print(f\u0026#34;\\næœåŠ¡: {service_name}\u0026#34;) print(f\u0026#34; é™ç•Œä¸Šä¸‹æ–‡: {service_info[\u0026#39;bounded_context\u0026#39;]}\u0026#34;) print(f\u0026#34; èŒè´£: {service_info[\u0026#39;responsibilities\u0026#39;]}\u0026#34;) print(f\u0026#34; æ•°æ®æ‰€æœ‰æƒ: {service_info[\u0026#39;data_ownership\u0026#39;]}\u0026#34;) # éªŒè¯è®¾è®¡ validation = strategy.validate_service_design(service_boundaries) print(f\u0026#34;\\n=== è®¾è®¡éªŒè¯ ===\u0026#34;) if validation[\u0026#34;violations\u0026#34;]: print(\u0026#34;è¿è§„é¡¹:\u0026#34;) for violation in validation[\u0026#34;violations\u0026#34;]: print(f\u0026#34; - {violation}\u0026#34;) if validation[\u0026#34;warnings\u0026#34;]: print(\u0026#34;è­¦å‘Šé¡¹:\u0026#34;) for warning in validation[\u0026#34;warnings\u0026#34;]: print(f\u0026#34; - {warning}\u0026#34;) if validation[\u0026#34;recommendations\u0026#34;]: print(\u0026#34;å»ºè®®é¡¹:\u0026#34;) for recommendation in validation[\u0026#34;recommendations\u0026#34;]: print(f\u0026#34; - {recommendation}\u0026#34;) return service_boundaries, validation if __name__ == \u0026#34;__main__\u0026#34;: service_decomposition_example() å®¹å™¨åŒ–ä¸ç¼–æ’ Kuberneteséƒ¨ç½²é…ç½® import yaml from typing import Dict, List, Any, Optional from dataclasses import dataclass from enum import Enum class DeploymentStrategy(Enum): ROLLING_UPDATE = \u0026#34;RollingUpdate\u0026#34; RECREATE = \u0026#34;Recreate\u0026#34; BLUE_GREEN = \u0026#34;BlueGreen\u0026#34; CANARY = \u0026#34;Canary\u0026#34; class ServiceType(Enum): CLUSTER_IP = \u0026#34;ClusterIP\u0026#34; NODE_PORT = \u0026#34;NodePort\u0026#34; LOAD_BALANCER = \u0026#34;LoadBalancer\u0026#34; @dataclass class ResourceRequirements: cpu_request: str = \u0026#34;100m\u0026#34; cpu_limit: str = \u0026#34;500m\u0026#34; memory_request: str = \u0026#34;128Mi\u0026#34; memory_limit: str = \u0026#34;512Mi\u0026#34; @dataclass class HealthCheck: path: str = \u0026#34;/health\u0026#34; port: int = 8080 initial_delay: int = 30 period: int = 10 timeout: int = 5 failure_threshold: int = 3 class KubernetesManifestGenerator: def __init__(self): self.namespace = \u0026#34;microservices\u0026#34; self.app_version = \u0026#34;v1.0.0\u0026#34; def generate_namespace(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆå‘½åç©ºé—´é…ç½®\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Namespace\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: self.namespace, \u0026#34;labels\u0026#34;: { \u0026#34;name\u0026#34;: self.namespace, \u0026#34;environment\u0026#34;: \u0026#34;production\u0026#34; } } } def generate_deployment(self, service_name: str, image: str, replicas: int = 3, resources: ResourceRequirements = None, health_check: HealthCheck = None, env_vars: Dict[str, str] = None, strategy: DeploymentStrategy = DeploymentStrategy.ROLLING_UPDATE) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆDeploymenté…ç½®\u0026#34;\u0026#34;\u0026#34; if resources is None: resources = ResourceRequirements() if health_check is None: health_check = HealthCheck() if env_vars is None: env_vars = {} deployment = { \u0026#34;apiVersion\u0026#34;: \u0026#34;apps/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Deployment\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: service_name, \u0026#34;namespace\u0026#34;: self.namespace, \u0026#34;labels\u0026#34;: { \u0026#34;app\u0026#34;: service_name, \u0026#34;version\u0026#34;: self.app_version } }, \u0026#34;spec\u0026#34;: { \u0026#34;replicas\u0026#34;: replicas, \u0026#34;strategy\u0026#34;: { \u0026#34;type\u0026#34;: strategy.value }, \u0026#34;selector\u0026#34;: { \u0026#34;matchLabels\u0026#34;: { \u0026#34;app\u0026#34;: service_name } }, \u0026#34;template\u0026#34;: { \u0026#34;metadata\u0026#34;: { \u0026#34;labels\u0026#34;: { \u0026#34;app\u0026#34;: service_name, \u0026#34;version\u0026#34;: self.app_version } }, \u0026#34;spec\u0026#34;: { \u0026#34;containers\u0026#34;: [{ \u0026#34;name\u0026#34;: service_name, \u0026#34;image\u0026#34;: image, \u0026#34;ports\u0026#34;: [{ \u0026#34;containerPort\u0026#34;: health_check.port, \u0026#34;name\u0026#34;: \u0026#34;http\u0026#34; }], \u0026#34;env\u0026#34;: [{\u0026#34;name\u0026#34;: k, \u0026#34;value\u0026#34;: v} for k, v in env_vars.items()], \u0026#34;resources\u0026#34;: { \u0026#34;requests\u0026#34;: { \u0026#34;cpu\u0026#34;: resources.cpu_request, \u0026#34;memory\u0026#34;: resources.memory_request }, \u0026#34;limits\u0026#34;: { \u0026#34;cpu\u0026#34;: resources.cpu_limit, \u0026#34;memory\u0026#34;: resources.memory_limit } }, \u0026#34;livenessProbe\u0026#34;: { \u0026#34;httpGet\u0026#34;: { \u0026#34;path\u0026#34;: health_check.path, \u0026#34;port\u0026#34;: health_check.port }, \u0026#34;initialDelaySeconds\u0026#34;: health_check.initial_delay, \u0026#34;periodSeconds\u0026#34;: health_check.period, \u0026#34;timeoutSeconds\u0026#34;: health_check.timeout, \u0026#34;failureThreshold\u0026#34;: health_check.failure_threshold }, \u0026#34;readinessProbe\u0026#34;: { \u0026#34;httpGet\u0026#34;: { \u0026#34;path\u0026#34;: health_check.path, \u0026#34;port\u0026#34;: health_check.port }, \u0026#34;initialDelaySeconds\u0026#34;: 10, \u0026#34;periodSeconds\u0026#34;: 5, \u0026#34;timeoutSeconds\u0026#34;: health_check.timeout, \u0026#34;failureThreshold\u0026#34;: 2 } }] } } } } # æ·»åŠ æ»šåŠ¨æ›´æ–°ç­–ç•¥é…ç½® if strategy == DeploymentStrategy.ROLLING_UPDATE: deployment[\u0026#34;spec\u0026#34;][\u0026#34;strategy\u0026#34;][\u0026#34;rollingUpdate\u0026#34;] = { \u0026#34;maxUnavailable\u0026#34;: \u0026#34;25%\u0026#34;, \u0026#34;maxSurge\u0026#34;: \u0026#34;25%\u0026#34; } return deployment def generate_service(self, service_name: str, port: int = 80, target_port: int = 8080, service_type: ServiceType = ServiceType.CLUSTER_IP) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆServiceé…ç½®\u0026#34;\u0026#34;\u0026#34; service = { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Service\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: service_name, \u0026#34;namespace\u0026#34;: self.namespace, \u0026#34;labels\u0026#34;: { \u0026#34;app\u0026#34;: service_name } }, \u0026#34;spec\u0026#34;: { \u0026#34;type\u0026#34;: service_type.value, \u0026#34;ports\u0026#34;: [{ \u0026#34;port\u0026#34;: port, \u0026#34;targetPort\u0026#34;: target_port, \u0026#34;protocol\u0026#34;: \u0026#34;TCP\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;http\u0026#34; }], \u0026#34;selector\u0026#34;: { \u0026#34;app\u0026#34;: service_name } } } return service def generate_configmap(self, name: str, data: Dict[str, str]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆConfigMapé…ç½®\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;ConfigMap\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: name, \u0026#34;namespace\u0026#34;: self.namespace }, \u0026#34;data\u0026#34;: data } def generate_secret(self, name: str, data: Dict[str, str]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆSecreté…ç½®\u0026#34;\u0026#34;\u0026#34; import base64 # å¯¹æ•°æ®è¿›è¡Œbase64ç¼–ç  encoded_data = {k: base64.b64encode(v.encode()).decode() for k, v in data.items()} return { \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Secret\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: name, \u0026#34;namespace\u0026#34;: self.namespace }, \u0026#34;type\u0026#34;: \u0026#34;Opaque\u0026#34;, \u0026#34;data\u0026#34;: encoded_data } def generate_hpa(self, service_name: str, min_replicas: int = 2, max_replicas: int = 10, cpu_threshold: int = 70) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆHorizontalPodAutoscaleré…ç½®\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;apiVersion\u0026#34;: \u0026#34;autoscaling/v2\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;HorizontalPodAutoscaler\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: f\u0026#34;{service_name}-hpa\u0026#34;, \u0026#34;namespace\u0026#34;: self.namespace }, \u0026#34;spec\u0026#34;: { \u0026#34;scaleTargetRef\u0026#34;: { \u0026#34;apiVersion\u0026#34;: \u0026#34;apps/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Deployment\u0026#34;, \u0026#34;name\u0026#34;: service_name }, \u0026#34;minReplicas\u0026#34;: min_replicas, \u0026#34;maxReplicas\u0026#34;: max_replicas, \u0026#34;metrics\u0026#34;: [{ \u0026#34;type\u0026#34;: \u0026#34;Resource\u0026#34;, \u0026#34;resource\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;cpu\u0026#34;, \u0026#34;target\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;Utilization\u0026#34;, \u0026#34;averageUtilization\u0026#34;: cpu_threshold } } }] } } def generate_ingress(self, name: str, rules: List[Dict[str, Any]], tls_enabled: bool = True) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆIngressé…ç½®\u0026#34;\u0026#34;\u0026#34; ingress = { \u0026#34;apiVersion\u0026#34;: \u0026#34;networking.k8s.io/v1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Ingress\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: name, \u0026#34;namespace\u0026#34;: self.namespace, \u0026#34;annotations\u0026#34;: { \u0026#34;nginx.ingress.kubernetes.io/rewrite-target\u0026#34;: \u0026#34;/\u0026#34;, \u0026#34;nginx.ingress.kubernetes.io/ssl-redirect\u0026#34;: \u0026#34;true\u0026#34; if tls_enabled else \u0026#34;false\u0026#34; } }, \u0026#34;spec\u0026#34;: { \u0026#34;ingressClassName\u0026#34;: \u0026#34;nginx\u0026#34;, \u0026#34;rules\u0026#34;: rules } } if tls_enabled: ingress[\u0026#34;spec\u0026#34;][\u0026#34;tls\u0026#34;] = [{ \u0026#34;hosts\u0026#34;: [rule[\u0026#34;host\u0026#34;] for rule in rules if \u0026#34;host\u0026#34; in rule], \u0026#34;secretName\u0026#34;: f\u0026#34;{name}-tls\u0026#34; }] return ingress def generate_complete_manifest(self, service_configs: List[Dict[str, Any]]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆå®Œæ•´çš„Kubernetesæ¸…å•\u0026#34;\u0026#34;\u0026#34; manifests = [] # æ·»åŠ å‘½åç©ºé—´ manifests.append(self.generate_namespace()) # ä¸ºæ¯ä¸ªæœåŠ¡ç”Ÿæˆé…ç½® for config in service_configs: service_name = config[\u0026#34;name\u0026#34;] image = config[\u0026#34;image\u0026#34;] # Deployment deployment = self.generate_deployment( service_name=service_name, image=image, replicas=config.get(\u0026#34;replicas\u0026#34;, 3), resources=config.get(\u0026#34;resources\u0026#34;), health_check=config.get(\u0026#34;health_check\u0026#34;), env_vars=config.get(\u0026#34;env_vars\u0026#34;, {}) ) manifests.append(deployment) # Service service = self.generate_service( service_name=service_name, port=config.get(\u0026#34;port\u0026#34;, 80), target_port=config.get(\u0026#34;target_port\u0026#34;, 8080) ) manifests.append(service) # HPA if config.get(\u0026#34;auto_scaling\u0026#34;, True): hpa = self.generate_hpa( service_name=service_name, min_replicas=config.get(\u0026#34;min_replicas\u0026#34;, 2), max_replicas=config.get(\u0026#34;max_replicas\u0026#34;, 10) ) manifests.append(hpa) # ConfigMap if config.get(\u0026#34;config_data\u0026#34;): configmap = self.generate_configmap( name=f\u0026#34;{service_name}-config\u0026#34;, data=config[\u0026#34;config_data\u0026#34;] ) manifests.append(configmap) # Secret if config.get(\u0026#34;secret_data\u0026#34;): secret = self.generate_secret( name=f\u0026#34;{service_name}-secret\u0026#34;, data=config[\u0026#34;secret_data\u0026#34;] ) manifests.append(secret) # ç”ŸæˆIngress ingress_rules = [] for config in service_configs: if config.get(\u0026#34;external_access\u0026#34;): ingress_rules.append({ \u0026#34;host\u0026#34;: config.get(\u0026#34;host\u0026#34;, f\u0026#34;{config[\u0026#39;name\u0026#39;]}.example.com\u0026#34;), \u0026#34;http\u0026#34;: { \u0026#34;paths\u0026#34;: [{ \u0026#34;path\u0026#34;: config.get(\u0026#34;path\u0026#34;, \u0026#34;/\u0026#34;), \u0026#34;pathType\u0026#34;: \u0026#34;Prefix\u0026#34;, \u0026#34;backend\u0026#34;: { \u0026#34;service\u0026#34;: { \u0026#34;name\u0026#34;: config[\u0026#34;name\u0026#34;], \u0026#34;port\u0026#34;: { \u0026#34;number\u0026#34;: config.get(\u0026#34;port\u0026#34;, 80) } } } }] } }) if ingress_rules: ingress = self.generate_ingress( name=\u0026#34;microservices-ingress\u0026#34;, rules=ingress_rules ) manifests.append(ingress) # è½¬æ¢ä¸ºYAML yaml_content = \u0026#34;\u0026#34; for manifest in manifests: yaml_content += \u0026#34;---\\n\u0026#34; yaml_content += yaml.dump(manifest, default_flow_style=False, allow_unicode=True) yaml_content += \u0026#34;\\n\u0026#34; return yaml_content # ä½¿ç”¨ç¤ºä¾‹ def kubernetes_deployment_example(): generator = KubernetesManifestGenerator() # å®šä¹‰æœåŠ¡é…ç½® service_configs = [ { \u0026#34;name\u0026#34;: \u0026#34;user-service\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;myregistry/user-service:v1.0.0\u0026#34;, \u0026#34;replicas\u0026#34;: 3, \u0026#34;port\u0026#34;: 80, \u0026#34;target_port\u0026#34;: 8080, \u0026#34;external_access\u0026#34;: True, \u0026#34;host\u0026#34;: \u0026#34;api.example.com\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/users\u0026#34;, \u0026#34;resources\u0026#34;: ResourceRequirements( cpu_request=\u0026#34;200m\u0026#34;, cpu_limit=\u0026#34;1000m\u0026#34;, memory_request=\u0026#34;256Mi\u0026#34;, memory_limit=\u0026#34;1Gi\u0026#34; ), \u0026#34;health_check\u0026#34;: HealthCheck( path=\u0026#34;/health\u0026#34;, port=8080 ), \u0026#34;env_vars\u0026#34;: { \u0026#34;SPRING_PROFILES_ACTIVE\u0026#34;: \u0026#34;production\u0026#34;, \u0026#34;DATABASE_URL\u0026#34;: \u0026#34;postgresql://db:5432/userdb\u0026#34; }, \u0026#34;config_data\u0026#34;: { \u0026#34;application.yml\u0026#34;: \u0026#34;\u0026#34;\u0026#34; server: port: 8080 spring: datasource: url: ${DATABASE_URL} username: ${DB_USERNAME} password: ${DB_PASSWORD} logging: level: com.example: INFO \u0026#34;\u0026#34;\u0026#34;.strip() }, \u0026#34;secret_data\u0026#34;: { \u0026#34;DB_USERNAME\u0026#34;: \u0026#34;user_service\u0026#34;, \u0026#34;DB_PASSWORD\u0026#34;: \u0026#34;secure_password_123\u0026#34; } }, { \u0026#34;name\u0026#34;: \u0026#34;order-service\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;myregistry/order-service:v1.0.0\u0026#34;, \u0026#34;replicas\u0026#34;: 5, \u0026#34;port\u0026#34;: 80, \u0026#34;target_port\u0026#34;: 8080, \u0026#34;external_access\u0026#34;: True, \u0026#34;host\u0026#34;: \u0026#34;api.example.com\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/orders\u0026#34;, \u0026#34;min_replicas\u0026#34;: 3, \u0026#34;max_replicas\u0026#34;: 15, \u0026#34;env_vars\u0026#34;: { \u0026#34;SPRING_PROFILES_ACTIVE\u0026#34;: \u0026#34;production\u0026#34;, \u0026#34;KAFKA_BROKERS\u0026#34;: \u0026#34;kafka:9092\u0026#34; } }, { \u0026#34;name\u0026#34;: \u0026#34;payment-service\u0026#34;, \u0026#34;image\u0026#34;: \u0026#34;myregistry/payment-service:v1.0.0\u0026#34;, \u0026#34;replicas\u0026#34;: 2, \u0026#34;port\u0026#34;: 80, \u0026#34;target_port\u0026#34;: 8080, \u0026#34;external_access\u0026#34;: False, \u0026#34;env_vars\u0026#34;: { \u0026#34;PAYMENT_GATEWAY_URL\u0026#34;: \u0026#34;https://payment-gateway.example.com\u0026#34;, \u0026#34;ENCRYPTION_KEY\u0026#34;: \u0026#34;payment_encryption_key\u0026#34; } } ] # ç”Ÿæˆå®Œæ•´æ¸…å• manifest_yaml = generator.generate_complete_manifest(service_configs) print(\u0026#34;=== Kuberneteséƒ¨ç½²æ¸…å• ===\u0026#34;) print(manifest_yaml) # ä¿å­˜åˆ°æ–‡ä»¶ with open(\u0026#34;microservices-deployment.yaml\u0026#34;, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: f.write(manifest_yaml) print(\u0026#34;æ¸…å•å·²ä¿å­˜åˆ° microservices-deployment.yaml\u0026#34;) return manifest_yaml if __name__ == \u0026#34;__main__\u0026#34;: kubernetes_deployment_example() æœåŠ¡ç½‘æ ¼æ¶æ„ IstioæœåŠ¡ç½‘æ ¼é…ç½® import yaml from typing import Dict, List, Any, Optional from dataclasses import dataclass from enum import Enum class TrafficPolicy(Enum): ROUND_ROBIN = \u0026#34;ROUND_ROBIN\u0026#34; LEAST_CONN = \u0026#34;LEAST_CONN\u0026#34; RANDOM = \u0026#34;RANDOM\u0026#34; PASSTHROUGH = \u0026#34;PASSTHROUGH\u0026#34; class TLSMode(Enum): DISABLE = \u0026#34;DISABLE\u0026#34; SIMPLE = \u0026#34;SIMPLE\u0026#34; MUTUAL = \u0026#34;MUTUAL\u0026#34; ISTIO_MUTUAL = \u0026#34;ISTIO_MUTUAL\u0026#34; @dataclass class CircuitBreakerConfig: consecutive_errors: int = 5 interval: str = \u0026#34;30s\u0026#34; base_ejection_time: str = \u0026#34;30s\u0026#34; max_ejection_percent: int = 50 @dataclass class RetryConfig: attempts: int = 3 per_try_timeout: str = \u0026#34;2s\u0026#34; retry_on: str = \u0026#34;5xx,reset,connect-failure,refused-stream\u0026#34; class IstioServiceMeshManager: def __init__(self, namespace: str = \u0026#34;microservices\u0026#34;): self.namespace = namespace def generate_virtual_service(self, service_name: str, hosts: List[str], routes: List[Dict[str, Any]], fault_injection: Optional[Dict[str, Any]] = None, timeout: Optional[str] = None, retry: Optional[RetryConfig] = None) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆVirtualServiceé…ç½®\u0026#34;\u0026#34;\u0026#34; virtual_service = { \u0026#34;apiVersion\u0026#34;: \u0026#34;networking.istio.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;VirtualService\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: f\u0026#34;{service_name}-vs\u0026#34;, \u0026#34;namespace\u0026#34;: self.namespace }, \u0026#34;spec\u0026#34;: { \u0026#34;hosts\u0026#34;: hosts, \u0026#34;http\u0026#34;: [] } } for route in routes: http_route = { \u0026#34;match\u0026#34;: route.get(\u0026#34;match\u0026#34;, [{\u0026#34;uri\u0026#34;: {\u0026#34;prefix\u0026#34;: \u0026#34;/\u0026#34;}}]), \u0026#34;route\u0026#34;: route[\u0026#34;destinations\u0026#34;] } # æ·»åŠ è¶…æ—¶é…ç½® if timeout: http_route[\u0026#34;timeout\u0026#34;] = timeout # æ·»åŠ é‡è¯•é…ç½® if retry: http_route[\u0026#34;retries\u0026#34;] = { \u0026#34;attempts\u0026#34;: retry.attempts, \u0026#34;perTryTimeout\u0026#34;: retry.per_try_timeout, \u0026#34;retryOn\u0026#34;: retry.retry_on } # æ·»åŠ æ•…éšœæ³¨å…¥ if fault_injection: http_route[\u0026#34;fault\u0026#34;] = fault_injection virtual_service[\u0026#34;spec\u0026#34;][\u0026#34;http\u0026#34;].append(http_route) return virtual_service def generate_destination_rule(self, service_name: str, host: str, traffic_policy: TrafficPolicy = TrafficPolicy.ROUND_ROBIN, circuit_breaker: Optional[CircuitBreakerConfig] = None, tls_mode: TLSMode = TLSMode.ISTIO_MUTUAL, subsets: Optional[List[Dict[str, Any]]] = None) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆDestinationRuleé…ç½®\u0026#34;\u0026#34;\u0026#34; destination_rule = { \u0026#34;apiVersion\u0026#34;: \u0026#34;networking.istio.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;DestinationRule\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: f\u0026#34;{service_name}-dr\u0026#34;, \u0026#34;namespace\u0026#34;: self.namespace }, \u0026#34;spec\u0026#34;: { \u0026#34;host\u0026#34;: host, \u0026#34;trafficPolicy\u0026#34;: { \u0026#34;loadBalancer\u0026#34;: { \u0026#34;simple\u0026#34;: traffic_policy.value }, \u0026#34;tls\u0026#34;: { \u0026#34;mode\u0026#34;: tls_mode.value } } } } # æ·»åŠ ç†”æ–­å™¨é…ç½® if circuit_breaker: destination_rule[\u0026#34;spec\u0026#34;][\u0026#34;trafficPolicy\u0026#34;][\u0026#34;outlierDetection\u0026#34;] = { \u0026#34;consecutiveErrors\u0026#34;: circuit_breaker.consecutive_errors, \u0026#34;interval\u0026#34;: circuit_breaker.interval, \u0026#34;baseEjectionTime\u0026#34;: circuit_breaker.base_ejection_time, \u0026#34;maxEjectionPercent\u0026#34;: circuit_breaker.max_ejection_percent } # æ·»åŠ å­é›†é…ç½®ï¼ˆç”¨äºé‡‘ä¸é›€éƒ¨ç½²ï¼‰ if subsets: destination_rule[\u0026#34;spec\u0026#34;][\u0026#34;subsets\u0026#34;] = subsets return destination_rule def generate_gateway(self, name: str, hosts: List[str], port: int = 80, https_port: int = 443, tls_enabled: bool = True) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆGatewayé…ç½®\u0026#34;\u0026#34;\u0026#34; servers = [{ \u0026#34;port\u0026#34;: { \u0026#34;number\u0026#34;: port, \u0026#34;name\u0026#34;: \u0026#34;http\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;HTTP\u0026#34; }, \u0026#34;hosts\u0026#34;: hosts }] if tls_enabled: servers.append({ \u0026#34;port\u0026#34;: { \u0026#34;number\u0026#34;: https_port, \u0026#34;name\u0026#34;: \u0026#34;https\u0026#34;, \u0026#34;protocol\u0026#34;: \u0026#34;HTTPS\u0026#34; }, \u0026#34;tls\u0026#34;: { \u0026#34;mode\u0026#34;: \u0026#34;SIMPLE\u0026#34;, \u0026#34;credentialName\u0026#34;: f\u0026#34;{name}-tls\u0026#34; }, \u0026#34;hosts\u0026#34;: hosts }) return { \u0026#34;apiVersion\u0026#34;: \u0026#34;networking.istio.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Gateway\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: name, \u0026#34;namespace\u0026#34;: self.namespace }, \u0026#34;spec\u0026#34;: { \u0026#34;selector\u0026#34;: { \u0026#34;istio\u0026#34;: \u0026#34;ingressgateway\u0026#34; }, \u0026#34;servers\u0026#34;: servers } } def generate_service_entry(self, name: str, hosts: List[str], ports: List[Dict[str, Any]], location: str = \u0026#34;MESH_EXTERNAL\u0026#34;) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆServiceEntryé…ç½®ï¼ˆç”¨äºå¤–éƒ¨æœåŠ¡ï¼‰\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;apiVersion\u0026#34;: \u0026#34;networking.istio.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;ServiceEntry\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: name, \u0026#34;namespace\u0026#34;: self.namespace }, \u0026#34;spec\u0026#34;: { \u0026#34;hosts\u0026#34;: hosts, \u0026#34;ports\u0026#34;: ports, \u0026#34;location\u0026#34;: location, \u0026#34;resolution\u0026#34;: \u0026#34;DNS\u0026#34; } } def generate_authorization_policy(self, name: str, selector: Dict[str, str], rules: List[Dict[str, Any]]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆAuthorizationPolicyé…ç½®\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;apiVersion\u0026#34;: \u0026#34;security.istio.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;AuthorizationPolicy\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: name, \u0026#34;namespace\u0026#34;: self.namespace }, \u0026#34;spec\u0026#34;: { \u0026#34;selector\u0026#34;: { \u0026#34;matchLabels\u0026#34;: selector }, \u0026#34;rules\u0026#34;: rules } } def generate_peer_authentication(self, name: str, selector: Optional[Dict[str, str]] = None, mtls_mode: str = \u0026#34;STRICT\u0026#34;) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆPeerAuthenticationé…ç½®\u0026#34;\u0026#34;\u0026#34; policy = { \u0026#34;apiVersion\u0026#34;: \u0026#34;security.istio.io/v1beta1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;PeerAuthentication\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: name, \u0026#34;namespace\u0026#34;: self.namespace }, \u0026#34;spec\u0026#34;: { \u0026#34;mtls\u0026#34;: { \u0026#34;mode\u0026#34;: mtls_mode } } } if selector: policy[\u0026#34;spec\u0026#34;][\u0026#34;selector\u0026#34;] = { \u0026#34;matchLabels\u0026#34;: selector } return policy def generate_telemetry_config(self, name: str, metrics_config: Optional[Dict[str, Any]] = None, tracing_config: Optional[Dict[str, Any]] = None) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆTelemetryé…ç½®\u0026#34;\u0026#34;\u0026#34; telemetry = { \u0026#34;apiVersion\u0026#34;: \u0026#34;telemetry.istio.io/v1alpha1\u0026#34;, \u0026#34;kind\u0026#34;: \u0026#34;Telemetry\u0026#34;, \u0026#34;metadata\u0026#34;: { \u0026#34;name\u0026#34;: name, \u0026#34;namespace\u0026#34;: self.namespace }, \u0026#34;spec\u0026#34;: {} } if metrics_config: telemetry[\u0026#34;spec\u0026#34;][\u0026#34;metrics\u0026#34;] = [metrics_config] if tracing_config: telemetry[\u0026#34;spec\u0026#34;][\u0026#34;tracing\u0026#34;] = [tracing_config] return telemetry def generate_canary_deployment_config(self, service_name: str, stable_version: str, canary_version: str, canary_weight: int = 10) -\u0026gt; List[Dict[str, Any]]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆé‡‘ä¸é›€éƒ¨ç½²é…ç½®\u0026#34;\u0026#34;\u0026#34; configs = [] # DestinationRule with subsets destination_rule = self.generate_destination_rule( service_name=service_name, host=service_name, subsets=[ { \u0026#34;name\u0026#34;: \u0026#34;stable\u0026#34;, \u0026#34;labels\u0026#34;: {\u0026#34;version\u0026#34;: stable_version} }, { \u0026#34;name\u0026#34;: \u0026#34;canary\u0026#34;, \u0026#34;labels\u0026#34;: {\u0026#34;version\u0026#34;: canary_version} } ] ) configs.append(destination_rule) # VirtualService for traffic splitting virtual_service = self.generate_virtual_service( service_name=service_name, hosts=[service_name], routes=[{ \u0026#34;match\u0026#34;: [{\u0026#34;uri\u0026#34;: {\u0026#34;prefix\u0026#34;: \u0026#34;/\u0026#34;}}], \u0026#34;destinations\u0026#34;: [ { \u0026#34;destination\u0026#34;: { \u0026#34;host\u0026#34;: service_name, \u0026#34;subset\u0026#34;: \u0026#34;stable\u0026#34; }, \u0026#34;weight\u0026#34;: 100 - canary_weight }, { \u0026#34;destination\u0026#34;: { \u0026#34;host\u0026#34;: service_name, \u0026#34;subset\u0026#34;: \u0026#34;canary\u0026#34; }, \u0026#34;weight\u0026#34;: canary_weight } ] }] ) configs.append(virtual_service) return configs def generate_complete_service_mesh_config(self, services: List[Dict[str, Any]]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆå®Œæ•´çš„æœåŠ¡ç½‘æ ¼é…ç½®\u0026#34;\u0026#34;\u0026#34; manifests = [] # ç”ŸæˆGateway gateway = self.generate_gateway( name=\u0026#34;microservices-gateway\u0026#34;, hosts=[\u0026#34;api.example.com\u0026#34;, \u0026#34;*.example.com\u0026#34;] ) manifests.append(gateway) # ä¸ºæ¯ä¸ªæœåŠ¡ç”Ÿæˆé…ç½® for service in services: service_name = service[\u0026#34;name\u0026#34;] # VirtualService virtual_service = self.generate_virtual_service( service_name=service_name, hosts=[f\u0026#34;api.example.com\u0026#34;], routes=[{ \u0026#34;match\u0026#34;: [{\u0026#34;uri\u0026#34;: {\u0026#34;prefix\u0026#34;: service.get(\u0026#34;path\u0026#34;, f\u0026#34;/{service_name}\u0026#34;)}}], \u0026#34;destinations\u0026#34;: [{ \u0026#34;destination\u0026#34;: { \u0026#34;host\u0026#34;: service_name, \u0026#34;port\u0026#34;: {\u0026#34;number\u0026#34;: service.get(\u0026#34;port\u0026#34;, 80)} } }] }], timeout=service.get(\u0026#34;timeout\u0026#34;, \u0026#34;30s\u0026#34;), retry=RetryConfig( attempts=service.get(\u0026#34;retry_attempts\u0026#34;, 3), per_try_timeout=service.get(\u0026#34;per_try_timeout\u0026#34;, \u0026#34;2s\u0026#34;) ) ) manifests.append(virtual_service) # DestinationRule destination_rule = self.generate_destination_rule( service_name=service_name, host=service_name, traffic_policy=TrafficPolicy.ROUND_ROBIN, circuit_breaker=CircuitBreakerConfig( consecutive_errors=service.get(\u0026#34;circuit_breaker_errors\u0026#34;, 5), interval=\u0026#34;30s\u0026#34;, base_ejection_time=\u0026#34;30s\u0026#34;, max_ejection_percent=50 ) ) manifests.append(destination_rule) # Authorization Policy if service.get(\u0026#34;auth_required\u0026#34;, True): auth_policy = self.generate_authorization_policy( name=f\u0026#34;{service_name}-auth\u0026#34;, selector={\u0026#34;app\u0026#34;: service_name}, rules=[{ \u0026#34;from\u0026#34;: [{ \u0026#34;source\u0026#34;: { \u0026#34;principals\u0026#34;: [\u0026#34;cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account\u0026#34;] } }], \u0026#34;to\u0026#34;: [{ \u0026#34;operation\u0026#34;: { \u0026#34;methods\u0026#34;: [\u0026#34;GET\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;DELETE\u0026#34;] } }] }] ) manifests.append(auth_policy) # å…¨å±€mTLSç­–ç•¥ peer_auth = self.generate_peer_authentication( name=\u0026#34;default\u0026#34;, mtls_mode=\u0026#34;STRICT\u0026#34; ) manifests.append(peer_auth) # é¥æµ‹é…ç½® telemetry = self.generate_telemetry_config( name=\u0026#34;default-metrics\u0026#34;, metrics_config={ \u0026#34;providers\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;prometheus\u0026#34; }] }, tracing_config={ \u0026#34;providers\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;jaeger\u0026#34; }] } ) manifests.append(telemetry) # è½¬æ¢ä¸ºYAML yaml_content = \u0026#34;\u0026#34; for manifest in manifests: yaml_content += \u0026#34;---\\n\u0026#34; yaml_content += yaml.dump(manifest, default_flow_style=False, allow_unicode=True) yaml_content += \u0026#34;\\n\u0026#34; return yaml_content # ä½¿ç”¨ç¤ºä¾‹ def istio_service_mesh_example(): mesh_manager = IstioServiceMeshManager() # å®šä¹‰æœåŠ¡é…ç½® services = [ { \u0026#34;name\u0026#34;: \u0026#34;user-service\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/users\u0026#34;, \u0026#34;port\u0026#34;: 80, \u0026#34;timeout\u0026#34;: \u0026#34;10s\u0026#34;, \u0026#34;retry_attempts\u0026#34;: 3, \u0026#34;per_try_timeout\u0026#34;: \u0026#34;3s\u0026#34;, \u0026#34;circuit_breaker_errors\u0026#34;: 5, \u0026#34;auth_required\u0026#34;: True }, { \u0026#34;name\u0026#34;: \u0026#34;order-service\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/orders\u0026#34;, \u0026#34;port\u0026#34;: 80, \u0026#34;timeout\u0026#34;: \u0026#34;15s\u0026#34;, \u0026#34;retry_attempts\u0026#34;: 2, \u0026#34;per_try_timeout\u0026#34;: \u0026#34;5s\u0026#34;, \u0026#34;circuit_breaker_errors\u0026#34;: 3, \u0026#34;auth_required\u0026#34;: True }, { \u0026#34;name\u0026#34;: \u0026#34;payment-service\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/payments\u0026#34;, \u0026#34;port\u0026#34;: 80, \u0026#34;timeout\u0026#34;: \u0026#34;20s\u0026#34;, \u0026#34;retry_attempts\u0026#34;: 5, \u0026#34;per_try_timeout\u0026#34;: \u0026#34;4s\u0026#34;, \u0026#34;circuit_breaker_errors\u0026#34;: 2, \u0026#34;auth_required\u0026#34;: True } ] # ç”Ÿæˆå®Œæ•´é…ç½® mesh_config = mesh_manager.generate_complete_service_mesh_config(services) print(\u0026#34;=== IstioæœåŠ¡ç½‘æ ¼é…ç½® ===\u0026#34;) print(mesh_config) # ç”Ÿæˆé‡‘ä¸é›€éƒ¨ç½²ç¤ºä¾‹ print(\u0026#34;\\n=== é‡‘ä¸é›€éƒ¨ç½²é…ç½®ç¤ºä¾‹ ===\u0026#34;) canary_configs = mesh_manager.generate_canary_deployment_config( service_name=\u0026#34;user-service\u0026#34;, stable_version=\u0026#34;v1.0.0\u0026#34;, canary_version=\u0026#34;v1.1.0\u0026#34;, canary_weight=20 ) for config in canary_configs: print(\u0026#34;---\u0026#34;) print(yaml.dump(config, default_flow_style=False, allow_unicode=True)) # ä¿å­˜é…ç½® with open(\u0026#34;istio-service-mesh.yaml\u0026#34;, \u0026#34;w\u0026#34;, encoding=\u0026#34;utf-8\u0026#34;) as f: f.write(mesh_config) print(\u0026#34;æœåŠ¡ç½‘æ ¼é…ç½®å·²ä¿å­˜åˆ° istio-service-mesh.yaml\u0026#34;) return mesh_config if __name__ == \u0026#34;__main__\u0026#34;: istio_service_mesh_example() APIç½‘å…³è®¾è®¡ ç»Ÿä¸€APIç½‘å…³å®ç° from typing import Dict, List, Any, Optional, Callable from dataclasses import dataclass, field from enum import Enum import time import json import hashlib import jwt from abc import ABC, abstractmethod class RateLimitStrategy(Enum): FIXED_WINDOW = \u0026#34;fixed_window\u0026#34; SLIDING_WINDOW = \u0026#34;sliding_window\u0026#34; TOKEN_BUCKET = \u0026#34;token_bucket\u0026#34; class AuthenticationMethod(Enum): JWT = \u0026#34;jwt\u0026#34; API_KEY = \u0026#34;api_key\u0026#34; OAUTH2 = \u0026#34;oauth2\u0026#34; BASIC = \u0026#34;basic\u0026#34; @dataclass class RouteConfig: path: str method: str upstream_service: str upstream_path: str timeout: int = 30 retries: int = 3 auth_required: bool = True rate_limit: Optional[Dict[str, Any]] = None cache_ttl: Optional[int] = None transform_request: Optional[Callable] = None transform_response: Optional[Callable] = None @dataclass class RateLimitConfig: strategy: RateLimitStrategy requests_per_window: int window_size_seconds: int burst_capacity: Optional[int] = None @dataclass class CircuitBreakerConfig: failure_threshold: int = 5 recovery_timeout: int = 60 success_threshold: int = 3 class APIGateway: def __init__(self): self.routes: Dict[str, RouteConfig] = {} self.rate_limiters: Dict[str, \u0026#39;RateLimiter\u0026#39;] = {} self.circuit_breakers: Dict[str, \u0026#39;CircuitBreaker\u0026#39;] = {} self.cache: Dict[str, Any] = {} self.middleware_stack: List[Callable] = [] self.auth_providers: Dict[AuthenticationMethod, \u0026#39;AuthProvider\u0026#39;] = {} def add_route(self, route: RouteConfig): \u0026#34;\u0026#34;\u0026#34;æ·»åŠ è·¯ç”±é…ç½®\u0026#34;\u0026#34;\u0026#34; route_key = f\u0026#34;{route.method}:{route.path}\u0026#34; self.routes[route_key] = route # åˆå§‹åŒ–é€Ÿç‡é™åˆ¶å™¨ if route.rate_limit: rate_limit_config = RateLimitConfig(**route.rate_limit) self.rate_limiters[route_key] = RateLimiter(rate_limit_config) # åˆå§‹åŒ–ç†”æ–­å™¨ self.circuit_breakers[route.upstream_service] = CircuitBreaker( CircuitBreakerConfig() ) def add_middleware(self, middleware: Callable): \u0026#34;\u0026#34;\u0026#34;æ·»åŠ ä¸­é—´ä»¶\u0026#34;\u0026#34;\u0026#34; self.middleware_stack.append(middleware) def add_auth_provider(self, method: AuthenticationMethod, provider: \u0026#39;AuthProvider\u0026#39;): \u0026#34;\u0026#34;\u0026#34;æ·»åŠ è®¤è¯æä¾›è€…\u0026#34;\u0026#34;\u0026#34; self.auth_providers[method] = provider def process_request(self, request: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;å¤„ç†è¯·æ±‚\u0026#34;\u0026#34;\u0026#34; try: # è·¯ç”±åŒ¹é… route = self._match_route(request) if not route: return self._error_response(404, \u0026#34;Route not found\u0026#34;) # æ‰§è¡Œä¸­é—´ä»¶ for middleware in self.middleware_stack: result = middleware(request) if result.get(\u0026#34;error\u0026#34;): return result # è®¤è¯æ£€æŸ¥ if route.auth_required: auth_result = self._authenticate(request) if not auth_result.get(\u0026#34;success\u0026#34;): return self._error_response(401, \u0026#34;Authentication failed\u0026#34;) request[\u0026#34;user\u0026#34;] = auth_result.get(\u0026#34;user\u0026#34;) # é€Ÿç‡é™åˆ¶æ£€æŸ¥ if not self._check_rate_limit(route, request): return self._error_response(429, \u0026#34;Rate limit exceeded\u0026#34;) # ç¼“å­˜æ£€æŸ¥ cache_key = self._generate_cache_key(route, request) if route.cache_ttl and cache_key in self.cache: cached_response = self.cache[cache_key] if time.time() - cached_response[\u0026#34;timestamp\u0026#34;] \u0026lt; route.cache_ttl: return cached_response[\u0026#34;data\u0026#34;] # ç†”æ–­å™¨æ£€æŸ¥ circuit_breaker = self.circuit_breakers.get(route.upstream_service) if circuit_breaker and not circuit_breaker.can_execute(): return self._error_response(503, \u0026#34;Service temporarily unavailable\u0026#34;) # è¯·æ±‚è½¬æ¢ if route.transform_request: request = route.transform_request(request) # è½¬å‘è¯·æ±‚åˆ°ä¸Šæ¸¸æœåŠ¡ response = self._forward_request(route, request) # è®°å½•ç†”æ–­å™¨æˆåŠŸ if circuit_breaker: circuit_breaker.record_success() # å“åº”è½¬æ¢ if route.transform_response: response = route.transform_response(response) # ç¼“å­˜å“åº” if route.cache_ttl and response.get(\u0026#34;status\u0026#34;) == 200: self.cache[cache_key] = { \u0026#34;data\u0026#34;: response, \u0026#34;timestamp\u0026#34;: time.time() } return response except Exception as e: # è®°å½•ç†”æ–­å™¨å¤±è´¥ if \u0026#39;route\u0026#39; in locals() and route: circuit_breaker = self.circuit_breakers.get(route.upstream_service) if circuit_breaker: circuit_breaker.record_failure() return self._error_response(500, f\u0026#34;Internal server error: {str(e)}\u0026#34;) def _match_route(self, request: Dict[str, Any]) -\u0026gt; Optional[RouteConfig]: \u0026#34;\u0026#34;\u0026#34;åŒ¹é…è·¯ç”±\u0026#34;\u0026#34;\u0026#34; method = request.get(\u0026#34;method\u0026#34;, \u0026#34;GET\u0026#34;) path = request.get(\u0026#34;path\u0026#34;, \u0026#34;/\u0026#34;) # ç²¾ç¡®åŒ¹é… route_key = f\u0026#34;{method}:{path}\u0026#34; if route_key in self.routes: return self.routes[route_key] # æ¨¡å¼åŒ¹é…ï¼ˆç®€åŒ–å®ç°ï¼‰ for key, route in self.routes.items(): route_method, route_path = key.split(\u0026#34;:\u0026#34;, 1) if route_method == method and self._path_matches(path, route_path): return route return None def _path_matches(self, request_path: str, route_path: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;è·¯å¾„åŒ¹é…ï¼ˆæ”¯æŒé€šé…ç¬¦ï¼‰\u0026#34;\u0026#34;\u0026#34; if \u0026#34;*\u0026#34; in route_path: prefix = route_path.replace(\u0026#34;*\u0026#34;, \u0026#34;\u0026#34;) return request_path.startswith(prefix) return request_path == route_path def _authenticate(self, request: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è®¤è¯å¤„ç†\u0026#34;\u0026#34;\u0026#34; auth_header = request.get(\u0026#34;headers\u0026#34;, {}).get(\u0026#34;Authorization\u0026#34;, \u0026#34;\u0026#34;) if auth_header.startswith(\u0026#34;Bearer \u0026#34;): token = auth_header[7:] jwt_provider = self.auth_providers.get(AuthenticationMethod.JWT) if jwt_provider: return jwt_provider.validate(token) elif auth_header.startswith(\u0026#34;Basic \u0026#34;): basic_provider = self.auth_providers.get(AuthenticationMethod.BASIC) if basic_provider: return basic_provider.validate(auth_header[6:]) api_key = request.get(\u0026#34;headers\u0026#34;, {}).get(\u0026#34;X-API-Key\u0026#34;) if api_key: api_key_provider = self.auth_providers.get(AuthenticationMethod.API_KEY) if api_key_provider: return api_key_provider.validate(api_key) return {\u0026#34;success\u0026#34;: False, \u0026#34;error\u0026#34;: \u0026#34;No valid authentication method found\u0026#34;} def _check_rate_limit(self, route: RouteConfig, request: Dict[str, Any]) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ£€æŸ¥é€Ÿç‡é™åˆ¶\u0026#34;\u0026#34;\u0026#34; route_key = f\u0026#34;{route.method}:{route.path}\u0026#34; rate_limiter = self.rate_limiters.get(route_key) if not rate_limiter: return True client_id = self._get_client_id(request) return rate_limiter.allow_request(client_id) def _get_client_id(self, request: Dict[str, Any]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;è·å–å®¢æˆ·ç«¯æ ‡è¯†\u0026#34;\u0026#34;\u0026#34; # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·ID if \u0026#34;user\u0026#34; in request and \u0026#34;id\u0026#34; in request[\u0026#34;user\u0026#34;]: return f\u0026#34;user:{request[\u0026#39;user\u0026#39;][\u0026#39;id\u0026#39;]}\u0026#34; # ä½¿ç”¨IPåœ°å€ return f\u0026#34;ip:{request.get(\u0026#39;client_ip\u0026#39;, \u0026#39;unknown\u0026#39;)}\u0026#34; def _generate_cache_key(self, route: RouteConfig, request: Dict[str, Any]) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆç¼“å­˜é”®\u0026#34;\u0026#34;\u0026#34; key_parts = [ route.method, route.path, json.dumps(request.get(\u0026#34;query_params\u0026#34;, {}), sort_keys=True), json.dumps(request.get(\u0026#34;body\u0026#34;, {}), sort_keys=True) ] key_string = \u0026#34;|\u0026#34;.join(key_parts) return hashlib.md5(key_string.encode()).hexdigest() def _forward_request(self, route: RouteConfig, request: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è½¬å‘è¯·æ±‚åˆ°ä¸Šæ¸¸æœåŠ¡ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰\u0026#34;\u0026#34;\u0026#34; # è¿™é‡Œåº”è¯¥å®ç°å®é™…çš„HTTPè¯·æ±‚è½¬å‘ # ä¸ºäº†æ¼”ç¤ºï¼Œè¿”å›æ¨¡æ‹Ÿå“åº” return { \u0026#34;status\u0026#34;: 200, \u0026#34;headers\u0026#34;: {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;}, \u0026#34;body\u0026#34;: { \u0026#34;message\u0026#34;: f\u0026#34;Response from {route.upstream_service}\u0026#34;, \u0026#34;path\u0026#34;: route.upstream_path, \u0026#34;timestamp\u0026#34;: time.time() } } def _error_response(self, status: int, message: str) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;ç”Ÿæˆé”™è¯¯å“åº”\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;status\u0026#34;: status, \u0026#34;headers\u0026#34;: {\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;}, \u0026#34;body\u0026#34;: { \u0026#34;error\u0026#34;: message, \u0026#34;timestamp\u0026#34;: time.time() } } class RateLimiter: def __init__(self, config: RateLimitConfig): self.config = config self.windows: Dict[str, Dict[str, Any]] = {} def allow_request(self, client_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ£€æŸ¥æ˜¯å¦å…è®¸è¯·æ±‚\u0026#34;\u0026#34;\u0026#34; current_time = time.time() if self.config.strategy == RateLimitStrategy.FIXED_WINDOW: return self._fixed_window_check(client_id, current_time) elif self.config.strategy == RateLimitStrategy.SLIDING_WINDOW: return self._sliding_window_check(client_id, current_time) elif self.config.strategy == RateLimitStrategy.TOKEN_BUCKET: return self._token_bucket_check(client_id, current_time) return True def _fixed_window_check(self, client_id: str, current_time: float) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;å›ºå®šçª—å£ç®—æ³•\u0026#34;\u0026#34;\u0026#34; window_start = int(current_time // self.config.window_size_seconds) * self.config.window_size_seconds if client_id not in self.windows: self.windows[client_id] = {\u0026#34;window_start\u0026#34;: window_start, \u0026#34;count\u0026#34;: 0} client_window = self.windows[client_id] # é‡ç½®çª—å£ if client_window[\u0026#34;window_start\u0026#34;] \u0026lt; window_start: client_window[\u0026#34;window_start\u0026#34;] = window_start client_window[\u0026#34;count\u0026#34;] = 0 # æ£€æŸ¥é™åˆ¶ if client_window[\u0026#34;count\u0026#34;] \u0026gt;= self.config.requests_per_window: return False client_window[\u0026#34;count\u0026#34;] += 1 return True def _sliding_window_check(self, client_id: str, current_time: float) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ»‘åŠ¨çª—å£ç®—æ³•\u0026#34;\u0026#34;\u0026#34; if client_id not in self.windows: self.windows[client_id] = {\u0026#34;requests\u0026#34;: []} client_window = self.windows[client_id] # æ¸…ç†è¿‡æœŸè¯·æ±‚ window_start = current_time - self.config.window_size_seconds client_window[\u0026#34;requests\u0026#34;] = [ req_time for req_time in client_window[\u0026#34;requests\u0026#34;] if req_time \u0026gt; window_start ] # æ£€æŸ¥é™åˆ¶ if len(client_window[\u0026#34;requests\u0026#34;]) \u0026gt;= self.config.requests_per_window: return False client_window[\u0026#34;requests\u0026#34;].append(current_time) return True def _token_bucket_check(self, client_id: str, current_time: float) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;ä»¤ç‰Œæ¡¶ç®—æ³•\u0026#34;\u0026#34;\u0026#34; if client_id not in self.windows: self.windows[client_id] = { \u0026#34;tokens\u0026#34;: self.config.burst_capacity or self.config.requests_per_window, \u0026#34;last_refill\u0026#34;: current_time } bucket = self.windows[client_id] # è®¡ç®—éœ€è¦æ·»åŠ çš„ä»¤ç‰Œ time_passed = current_time - bucket[\u0026#34;last_refill\u0026#34;] tokens_to_add = int(time_passed * (self.config.requests_per_window / self.config.window_size_seconds)) if tokens_to_add \u0026gt; 0: max_tokens = self.config.burst_capacity or self.config.requests_per_window bucket[\u0026#34;tokens\u0026#34;] = min(max_tokens, bucket[\u0026#34;tokens\u0026#34;] + tokens_to_add) bucket[\u0026#34;last_refill\u0026#34;] = current_time # æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨ä»¤ç‰Œ if bucket[\u0026#34;tokens\u0026#34;] \u0026lt; 1: return False bucket[\u0026#34;tokens\u0026#34;] -= 1 return True class CircuitBreaker: def __init__(self, config: CircuitBreakerConfig): self.config = config self.failure_count = 0 self.success_count = 0 self.last_failure_time = 0 self.state = \u0026#34;CLOSED\u0026#34; # CLOSED, OPEN, HALF_OPEN def can_execute(self) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ£€æŸ¥æ˜¯å¦å¯ä»¥æ‰§è¡Œè¯·æ±‚\u0026#34;\u0026#34;\u0026#34; current_time = time.time() if self.state == \u0026#34;CLOSED\u0026#34;: return True elif self.state == \u0026#34;OPEN\u0026#34;: if current_time - self.last_failure_time \u0026gt; self.config.recovery_timeout: self.state = \u0026#34;HALF_OPEN\u0026#34; self.success_count = 0 return True return False elif self.state == \u0026#34;HALF_OPEN\u0026#34;: return True return False def record_success(self): \u0026#34;\u0026#34;\u0026#34;è®°å½•æˆåŠŸ\u0026#34;\u0026#34;\u0026#34; if self.state == \u0026#34;HALF_OPEN\u0026#34;: self.success_count += 1 if self.success_count \u0026gt;= self.config.success_threshold: self.state = \u0026#34;CLOSED\u0026#34; self.failure_count = 0 elif self.state == \u0026#34;CLOSED\u0026#34;: self.failure_count = max(0, self.failure_count - 1) def record_failure(self): \u0026#34;\u0026#34;\u0026#34;è®°å½•å¤±è´¥\u0026#34;\u0026#34;\u0026#34; self.failure_count += 1 self.last_failure_time = time.time() if self.failure_count \u0026gt;= self.config.failure_threshold: self.state = \u0026#34;OPEN\u0026#34; class AuthProvider(ABC): @abstractmethod def validate(self, credentials: str) -\u0026gt; Dict[str, Any]: pass class JWTAuthProvider(AuthProvider): def __init__(self, secret_key: str, algorithm: str = \u0026#34;HS256\u0026#34;): self.secret_key = secret_key self.algorithm = algorithm def validate(self, token: str) -\u0026gt; Dict[str, Any]: try: payload = jwt.decode(token, self.secret_key, algorithms=[self.algorithm]) return { \u0026#34;success\u0026#34;: True, \u0026#34;user\u0026#34;: { \u0026#34;id\u0026#34;: payload.get(\u0026#34;user_id\u0026#34;), \u0026#34;username\u0026#34;: payload.get(\u0026#34;username\u0026#34;), \u0026#34;roles\u0026#34;: payload.get(\u0026#34;roles\u0026#34;, []) } } except jwt.ExpiredSignatureError: return {\u0026#34;success\u0026#34;: False, \u0026#34;error\u0026#34;: \u0026#34;Token expired\u0026#34;} except jwt.InvalidTokenError: return {\u0026#34;success\u0026#34;: False, \u0026#34;error\u0026#34;: \u0026#34;Invalid token\u0026#34;} class APIKeyAuthProvider(AuthProvider): def __init__(self, valid_keys: Dict[str, Dict[str, Any]]): self.valid_keys = valid_keys def validate(self, api_key: str) -\u0026gt; Dict[str, Any]: if api_key in self.valid_keys: return { \u0026#34;success\u0026#34;: True, \u0026#34;user\u0026#34;: self.valid_keys[api_key] } return {\u0026#34;success\u0026#34;: False, \u0026#34;error\u0026#34;: \u0026#34;Invalid API key\u0026#34;} # ä¸­é—´ä»¶ç¤ºä¾‹ def logging_middleware(request: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;æ—¥å¿—ä¸­é—´ä»¶\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Request: {request.get(\u0026#39;method\u0026#39;)} {request.get(\u0026#39;path\u0026#39;)} from {request.get(\u0026#39;client_ip\u0026#39;)}\u0026#34;) return {\u0026#34;success\u0026#34;: True} def cors_middleware(request: Dict[str, Any]) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;CORSä¸­é—´ä»¶\u0026#34;\u0026#34;\u0026#34; # æ·»åŠ CORSå¤´ if \u0026#34;headers\u0026#34; not in request: request[\u0026#34;headers\u0026#34;] = {} request[\u0026#34;headers\u0026#34;][\u0026#34;Access-Control-Allow-Origin\u0026#34;] = \u0026#34;*\u0026#34; request[\u0026#34;headers\u0026#34;][\u0026#34;Access-Control-Allow-Methods\u0026#34;] = \u0026#34;GET, POST, PUT, DELETE, OPTIONS\u0026#34; request[\u0026#34;headers\u0026#34;][\u0026#34;Access-Control-Allow-Headers\u0026#34;] = \u0026#34;Content-Type, Authorization\u0026#34; return {\u0026#34;success\u0026#34;: True} # ä½¿ç”¨ç¤ºä¾‹ def api_gateway_example(): # åˆ›å»ºAPIç½‘å…³ gateway = APIGateway() # æ·»åŠ è®¤è¯æä¾›è€… jwt_provider = JWTAuthProvider(\u0026#34;your-secret-key\u0026#34;) api_key_provider = APIKeyAuthProvider({ \u0026#34;key123\u0026#34;: {\u0026#34;id\u0026#34;: \u0026#34;user1\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;admin\u0026#34;, \u0026#34;roles\u0026#34;: [\u0026#34;admin\u0026#34;]}, \u0026#34;key456\u0026#34;: {\u0026#34;id\u0026#34;: \u0026#34;user2\u0026#34;, \u0026#34;username\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;roles\u0026#34;: [\u0026#34;user\u0026#34;]} }) gateway.add_auth_provider(AuthenticationMethod.JWT, jwt_provider) gateway.add_auth_provider(AuthenticationMethod.API_KEY, api_key_provider) # æ·»åŠ ä¸­é—´ä»¶ gateway.add_middleware(logging_middleware) gateway.add_middleware(cors_middleware) # æ·»åŠ è·¯ç”± routes = [ RouteConfig( path=\u0026#34;/api/users/*\u0026#34;, method=\u0026#34;GET\u0026#34;, upstream_service=\u0026#34;user-service\u0026#34;, upstream_path=\u0026#34;/users\u0026#34;, timeout=10, retries=3, auth_required=True, rate_limit={ \u0026#34;strategy\u0026#34;: RateLimitStrategy.SLIDING_WINDOW, \u0026#34;requests_per_window\u0026#34;: 100, \u0026#34;window_size_seconds\u0026#34;: 60 }, cache_ttl=300 ), RouteConfig( path=\u0026#34;/api/orders/*\u0026#34;, method=\u0026#34;POST\u0026#34;, upstream_service=\u0026#34;order-service\u0026#34;, upstream_path=\u0026#34;/orders\u0026#34;, timeout=30, retries=2, auth_required=True, rate_limit={ \u0026#34;strategy\u0026#34;: RateLimitStrategy.TOKEN_BUCKET, \u0026#34;requests_per_window\u0026#34;: 10, \u0026#34;window_size_seconds\u0026#34;: 60, \u0026#34;burst_capacity\u0026#34;: 20 } ), RouteConfig( path=\u0026#34;/api/health\u0026#34;, method=\u0026#34;GET\u0026#34;, upstream_service=\u0026#34;health-service\u0026#34;, upstream_path=\u0026#34;/health\u0026#34;, timeout=5, retries=1, auth_required=False, cache_ttl=60 ) ] for route in routes: gateway.add_route(route) # æ¨¡æ‹Ÿè¯·æ±‚å¤„ç† test_requests = [ { \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/api/users/123\u0026#34;, \u0026#34;headers\u0026#34;: {\u0026#34;Authorization\u0026#34;: \u0026#34;Bearer valid-jwt-token\u0026#34;, \u0026#34;X-API-Key\u0026#34;: \u0026#34;key123\u0026#34;}, \u0026#34;client_ip\u0026#34;: \u0026#34;192.168.1.100\u0026#34; }, { \u0026#34;method\u0026#34;: \u0026#34;POST\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/api/orders/create\u0026#34;, \u0026#34;headers\u0026#34;: {\u0026#34;Authorization\u0026#34;: \u0026#34;Bearer valid-jwt-token\u0026#34;}, \u0026#34;body\u0026#34;: {\u0026#34;product_id\u0026#34;: \u0026#34;123\u0026#34;, \u0026#34;quantity\u0026#34;: 2}, \u0026#34;client_ip\u0026#34;: \u0026#34;192.168.1.101\u0026#34; }, { \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;path\u0026#34;: \u0026#34;/api/health\u0026#34;, \u0026#34;client_ip\u0026#34;: \u0026#34;192.168.1.102\u0026#34; } ] print(\u0026#34;=== APIç½‘å…³è¯·æ±‚å¤„ç†ç¤ºä¾‹ ===\u0026#34;) for i, request in enumerate(test_requests, 1): print(f\u0026#34;\\nè¯·æ±‚ {i}:\u0026#34;) response = gateway.process_request(request) print(f\u0026#34;å“åº”çŠ¶æ€: {response.get(\u0026#39;status\u0026#39;)}\u0026#34;) print(f\u0026#34;å“åº”ä½“: {json.dumps(response.get(\u0026#39;body\u0026#39;, {}), indent=2, ensure_ascii=False)}\u0026#34;) return gateway if __name__ == \u0026#34;__main__\u0026#34;: api_gateway_example() 5. é…ç½®ç®¡ç†ä¸æœåŠ¡å‘ç° 5.1 é…ç½®ä¸­å¿ƒè®¾è®¡ import consul import etcd3 from typing import Dict, Any, Optional, List, Callable from dataclasses import dataclass from enum import Enum import json import yaml import threading import time class ConfigFormat(Enum): JSON = \u0026#34;json\u0026#34; YAML = \u0026#34;yaml\u0026#34; PROPERTIES = \u0026#34;properties\u0026#34; ENV = \u0026#34;env\u0026#34; @dataclass class ConfigItem: key: str value: Any version: int format: ConfigFormat encrypted: bool = False tags: List[str] = None class ConfigChangeEvent: def __init__(self, key: str, old_value: Any, new_value: Any, action: str): self.key = key self.old_value = old_value self.new_value = new_value self.action = action # CREATE, UPDATE, DELETE self.timestamp = time.time() class ConfigCenter: def __init__(self, backend: str = \u0026#34;consul\u0026#34;, **kwargs): self.backend = backend self.watchers: Dict[str, List[Callable]] = {} self.cache: Dict[str, ConfigItem] = {} self.encryption_key = kwargs.get(\u0026#34;encryption_key\u0026#34;) if backend == \u0026#34;consul\u0026#34;: self.client = consul.Consul( host=kwargs.get(\u0026#34;host\u0026#34;, \u0026#34;localhost\u0026#34;), port=kwargs.get(\u0026#34;port\u0026#34;, 8500) ) elif backend == \u0026#34;etcd\u0026#34;: self.client = etcd3.client( host=kwargs.get(\u0026#34;host\u0026#34;, \u0026#34;localhost\u0026#34;), port=kwargs.get(\u0026#34;port\u0026#34;, 2379) ) else: raise ValueError(f\u0026#34;Unsupported backend: {backend}\u0026#34;) def put(self, key: str, value: Any, format: ConfigFormat = ConfigFormat.JSON, encrypted: bool = False, tags: List[str] = None) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;å­˜å‚¨é…ç½®é¡¹\u0026#34;\u0026#34;\u0026#34; try: # åºåˆ—åŒ–å€¼ serialized_value = self._serialize_value(value, format) # åŠ å¯†ï¼ˆå¦‚æœéœ€è¦ï¼‰ if encrypted and self.encryption_key: serialized_value = self._encrypt(serialized_value) # å­˜å‚¨åˆ°åç«¯ if self.backend == \u0026#34;consul\u0026#34;: success, _ = self.client.kv.put(key, serialized_value) elif self.backend == \u0026#34;etcd\u0026#34;: self.client.put(key, serialized_value) success = True if success: # æ›´æ–°ç¼“å­˜ old_item = self.cache.get(key) new_version = (old_item.version + 1) if old_item else 1 config_item = ConfigItem( key=key, value=value, version=new_version, format=format, encrypted=encrypted, tags=tags or [] ) self.cache[key] = config_item # è§¦å‘å˜æ›´äº‹ä»¶ action = \u0026#34;UPDATE\u0026#34; if old_item else \u0026#34;CREATE\u0026#34; old_value = old_item.value if old_item else None self._notify_watchers(key, old_value, value, action) return success except Exception as e: print(f\u0026#34;Failed to put config {key}: {e}\u0026#34;) return False def get(self, key: str, default: Any = None) -\u0026gt; Any: \u0026#34;\u0026#34;\u0026#34;è·å–é…ç½®é¡¹\u0026#34;\u0026#34;\u0026#34; try: # å…ˆæ£€æŸ¥ç¼“å­˜ if key in self.cache: return self.cache[key].value # ä»åç«¯è·å– if self.backend == \u0026#34;consul\u0026#34;: _, data = self.client.kv.get(key) if data is None: return default raw_value = data[\u0026#39;Value\u0026#39;].decode(\u0026#39;utf-8\u0026#39;) elif self.backend == \u0026#34;etcd\u0026#34;: result = self.client.get(key) if result[0] is None: return default raw_value = result[0].decode(\u0026#39;utf-8\u0026#39;) # è§£å¯†ï¼ˆå¦‚æœéœ€è¦ï¼‰ if self._is_encrypted(raw_value): raw_value = self._decrypt(raw_value) # ååºåˆ—åŒ– value = self._deserialize_value(raw_value, ConfigFormat.JSON) # æ›´æ–°ç¼“å­˜ config_item = ConfigItem( key=key, value=value, version=1, format=ConfigFormat.JSON ) self.cache[key] = config_item return value except Exception as e: print(f\u0026#34;Failed to get config {key}: {e}\u0026#34;) return default def delete(self, key: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;åˆ é™¤é…ç½®é¡¹\u0026#34;\u0026#34;\u0026#34; try: old_item = self.cache.get(key) if self.backend == \u0026#34;consul\u0026#34;: success = self.client.kv.delete(key) elif self.backend == \u0026#34;etcd\u0026#34;: self.client.delete(key) success = True if success: # ä»ç¼“å­˜ä¸­åˆ é™¤ if key in self.cache: del self.cache[key] # è§¦å‘å˜æ›´äº‹ä»¶ if old_item: self._notify_watchers(key, old_item.value, None, \u0026#34;DELETE\u0026#34;) return success except Exception as e: print(f\u0026#34;Failed to delete config {key}: {e}\u0026#34;) return False def watch(self, key_prefix: str, callback: Callable[[ConfigChangeEvent], None]): \u0026#34;\u0026#34;\u0026#34;ç›‘å¬é…ç½®å˜æ›´\u0026#34;\u0026#34;\u0026#34; if key_prefix not in self.watchers: self.watchers[key_prefix] = [] self.watchers[key_prefix].append(callback) # å¯åŠ¨ç›‘å¬çº¿ç¨‹ if self.backend == \u0026#34;consul\u0026#34;: threading.Thread( target=self._consul_watch, args=(key_prefix,), daemon=True ).start() elif self.backend == \u0026#34;etcd\u0026#34;: threading.Thread( target=self._etcd_watch, args=(key_prefix,), daemon=True ).start() def _consul_watch(self, key_prefix: str): \u0026#34;\u0026#34;\u0026#34;Consulç›‘å¬å®ç°\u0026#34;\u0026#34;\u0026#34; index = None while True: try: index, data = self.client.kv.get(key_prefix, index=index, wait=\u0026#39;30s\u0026#39;, recurse=True) # å¤„ç†å˜æ›´äº‹ä»¶ # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…éœ€è¦æ¯”è¾ƒå‰åçŠ¶æ€ time.sleep(1) except Exception as e: print(f\u0026#34;Consul watch error: {e}\u0026#34;) time.sleep(5) def _etcd_watch(self, key_prefix: str): \u0026#34;\u0026#34;\u0026#34;etcdç›‘å¬å®ç°\u0026#34;\u0026#34;\u0026#34; try: events_iterator, cancel = self.client.watch_prefix(key_prefix) for event in events_iterator: # å¤„ç†etcdäº‹ä»¶ pass except Exception as e: print(f\u0026#34;etcd watch error: {e}\u0026#34;) def _serialize_value(self, value: Any, format: ConfigFormat) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;åºåˆ—åŒ–å€¼\u0026#34;\u0026#34;\u0026#34; if format == ConfigFormat.JSON: return json.dumps(value, ensure_ascii=False) elif format == ConfigFormat.YAML: return yaml.dump(value, allow_unicode=True) elif format == ConfigFormat.PROPERTIES: # ç®€åŒ–å®ç° if isinstance(value, dict): return \u0026#39;\\n\u0026#39;.join([f\u0026#34;{k}={v}\u0026#34; for k, v in value.items()]) return str(value) else: return str(value) def _deserialize_value(self, raw_value: str, format: ConfigFormat) -\u0026gt; Any: \u0026#34;\u0026#34;\u0026#34;ååºåˆ—åŒ–å€¼\u0026#34;\u0026#34;\u0026#34; if format == ConfigFormat.JSON: return json.loads(raw_value) elif format == ConfigFormat.YAML: return yaml.safe_load(raw_value) elif format == ConfigFormat.PROPERTIES: # ç®€åŒ–å®ç° result = {} for line in raw_value.split(\u0026#39;\\n\u0026#39;): if \u0026#39;=\u0026#39; in line: k, v = line.split(\u0026#39;=\u0026#39;, 1) result[k.strip()] = v.strip() return result else: return raw_value def _encrypt(self, value: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;åŠ å¯†å€¼ï¼ˆç®€åŒ–å®ç°ï¼‰\u0026#34;\u0026#34;\u0026#34; # å®é™…åº”è¯¥ä½¿ç”¨AESç­‰åŠ å¯†ç®—æ³• return f\u0026#34;encrypted:{value}\u0026#34; def _decrypt(self, encrypted_value: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;è§£å¯†å€¼ï¼ˆç®€åŒ–å®ç°ï¼‰\u0026#34;\u0026#34;\u0026#34; if encrypted_value.startswith(\u0026#34;encrypted:\u0026#34;): return encrypted_value[10:] return encrypted_value def _is_encrypted(self, value: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ£€æŸ¥æ˜¯å¦åŠ å¯†\u0026#34;\u0026#34;\u0026#34; return value.startswith(\u0026#34;encrypted:\u0026#34;) def _notify_watchers(self, key: str, old_value: Any, new_value: Any, action: str): \u0026#34;\u0026#34;\u0026#34;é€šçŸ¥ç›‘å¬å™¨\u0026#34;\u0026#34;\u0026#34; event = ConfigChangeEvent(key, old_value, new_value, action) for prefix, callbacks in self.watchers.items(): if key.startswith(prefix): for callback in callbacks: try: callback(event) except Exception as e: print(f\u0026#34;Watcher callback error: {e}\u0026#34;) ### 5.2 æœåŠ¡å‘ç°æœºåˆ¶ class ServiceInstance: def __init__(self, service_id: str, service_name: str, host: str, port: int, metadata: Dict[str, str] = None, health_check_url: str = None): self.service_id = service_id self.service_name = service_name self.host = host self.port = port self.metadata = metadata or {} self.health_check_url = health_check_url self.last_heartbeat = time.time() self.status = \u0026#34;UP\u0026#34; def to_dict(self) -\u0026gt; Dict[str, Any]: return { \u0026#34;service_id\u0026#34;: self.service_id, \u0026#34;service_name\u0026#34;: self.service_name, \u0026#34;host\u0026#34;: self.host, \u0026#34;port\u0026#34;: self.port, \u0026#34;metadata\u0026#34;: self.metadata, \u0026#34;health_check_url\u0026#34;: self.health_check_url, \u0026#34;last_heartbeat\u0026#34;: self.last_heartbeat, \u0026#34;status\u0026#34;: self.status } class ServiceRegistry: def __init__(self, backend: str = \u0026#34;consul\u0026#34;, **kwargs): self.backend = backend self.services: Dict[str, ServiceInstance] = {} self.health_check_interval = kwargs.get(\u0026#34;health_check_interval\u0026#34;, 30) if backend == \u0026#34;consul\u0026#34;: self.client = consul.Consul( host=kwargs.get(\u0026#34;host\u0026#34;, \u0026#34;localhost\u0026#34;), port=kwargs.get(\u0026#34;port\u0026#34;, 8500) ) elif backend == \u0026#34;etcd\u0026#34;: self.client = etcd3.client( host=kwargs.get(\u0026#34;host\u0026#34;, \u0026#34;localhost\u0026#34;), port=kwargs.get(\u0026#34;port\u0026#34;, 2379) ) # å¯åŠ¨å¥åº·æ£€æŸ¥ threading.Thread(target=self._health_check_loop, daemon=True).start() def register(self, instance: ServiceInstance) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ³¨å†ŒæœåŠ¡å®ä¾‹\u0026#34;\u0026#34;\u0026#34; try: service_key = f\u0026#34;services/{instance.service_name}/{instance.service_id}\u0026#34; service_data = json.dumps(instance.to_dict()) if self.backend == \u0026#34;consul\u0026#34;: # æ³¨å†Œåˆ°Consul success = self.client.agent.service.register( name=instance.service_name, service_id=instance.service_id, address=instance.host, port=instance.port, tags=list(instance.metadata.keys()), check=consul.Check.http( instance.health_check_url, interval=\u0026#34;30s\u0026#34; ) if instance.health_check_url else None ) elif self.backend == \u0026#34;etcd\u0026#34;: # æ³¨å†Œåˆ°etcd self.client.put(service_key, service_data, lease=self._create_lease()) success = True if success: self.services[instance.service_id] = instance print(f\u0026#34;Service {instance.service_name}:{instance.service_id} registered\u0026#34;) return success except Exception as e: print(f\u0026#34;Failed to register service: {e}\u0026#34;) return False def deregister(self, service_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ³¨é”€æœåŠ¡å®ä¾‹\u0026#34;\u0026#34;\u0026#34; try: if self.backend == \u0026#34;consul\u0026#34;: self.client.agent.service.deregister(service_id) elif self.backend == \u0026#34;etcd\u0026#34;: # ä»etcdåˆ é™¤ for service_name in self._get_all_service_names(): service_key = f\u0026#34;services/{service_name}/{service_id}\u0026#34; self.client.delete(service_key) if service_id in self.services: service_name = self.services[service_id].service_name del self.services[service_id] print(f\u0026#34;Service {service_name}:{service_id} deregistered\u0026#34;) return True except Exception as e: print(f\u0026#34;Failed to deregister service: {e}\u0026#34;) return False def discover(self, service_name: str) -\u0026gt; List[ServiceInstance]: \u0026#34;\u0026#34;\u0026#34;å‘ç°æœåŠ¡å®ä¾‹\u0026#34;\u0026#34;\u0026#34; try: instances = [] if self.backend == \u0026#34;consul\u0026#34;: _, services = self.client.health.service(service_name, passing=True) for service in services: service_info = service[\u0026#39;Service\u0026#39;] instance = ServiceInstance( service_id=service_info[\u0026#39;ID\u0026#39;], service_name=service_info[\u0026#39;Service\u0026#39;], host=service_info[\u0026#39;Address\u0026#39;], port=service_info[\u0026#39;Port\u0026#39;], metadata={tag: \u0026#34;\u0026#34; for tag in service_info.get(\u0026#39;Tags\u0026#39;, [])} ) instances.append(instance) elif self.backend == \u0026#34;etcd\u0026#34;: service_prefix = f\u0026#34;services/{service_name}/\u0026#34; for value, _ in self.client.get_prefix(service_prefix): service_data = json.loads(value.decode(\u0026#39;utf-8\u0026#39;)) instance = ServiceInstance(**service_data) if instance.status == \u0026#34;UP\u0026#34;: instances.append(instance) return instances except Exception as e: print(f\u0026#34;Failed to discover service {service_name}: {e}\u0026#34;) return [] def heartbeat(self, service_id: str) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;å‘é€å¿ƒè·³\u0026#34;\u0026#34;\u0026#34; if service_id in self.services: self.services[service_id].last_heartbeat = time.time() self.services[service_id].status = \u0026#34;UP\u0026#34; return True return False def _create_lease(self) -\u0026gt; int: \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºetcdç§Ÿçº¦\u0026#34;\u0026#34;\u0026#34; if self.backend == \u0026#34;etcd\u0026#34;: lease = self.client.lease(ttl=self.health_check_interval * 2) return lease.id return 0 def _get_all_service_names(self) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;è·å–æ‰€æœ‰æœåŠ¡åç§°\u0026#34;\u0026#34;\u0026#34; service_names = set() for instance in self.services.values(): service_names.add(instance.service_name) return list(service_names) def _health_check_loop(self): \u0026#34;\u0026#34;\u0026#34;å¥åº·æ£€æŸ¥å¾ªç¯\u0026#34;\u0026#34;\u0026#34; while True: try: current_time = time.time() timeout_threshold = current_time - (self.health_check_interval * 2) # æ£€æŸ¥è¶…æ—¶çš„æœåŠ¡ timeout_services = [] for service_id, instance in self.services.items(): if instance.last_heartbeat \u0026lt; timeout_threshold: timeout_services.append(service_id) # æ ‡è®°è¶…æ—¶æœåŠ¡ä¸ºDOWN for service_id in timeout_services: self.services[service_id].status = \u0026#34;DOWN\u0026#34; print(f\u0026#34;Service {service_id} marked as DOWN due to timeout\u0026#34;) time.sleep(self.health_check_interval) except Exception as e: print(f\u0026#34;Health check error: {e}\u0026#34;) time.sleep(5) # ä½¿ç”¨ç¤ºä¾‹ def config_and_discovery_example(): # é…ç½®ä¸­å¿ƒç¤ºä¾‹ print(\u0026#34;=== é…ç½®ä¸­å¿ƒç¤ºä¾‹ ===\u0026#34;) config_center = ConfigCenter(backend=\u0026#34;consul\u0026#34;) # å­˜å‚¨é…ç½® app_config = { \u0026#34;database\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 5432, \u0026#34;name\u0026#34;: \u0026#34;myapp\u0026#34; }, \u0026#34;redis\u0026#34;: { \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, \u0026#34;port\u0026#34;: 6379 }, \u0026#34;feature_flags\u0026#34;: { \u0026#34;new_ui\u0026#34;: True, \u0026#34;beta_features\u0026#34;: False } } config_center.put(\u0026#34;app/config\u0026#34;, app_config, ConfigFormat.JSON) config_center.put(\u0026#34;app/secret\u0026#34;, \u0026#34;sensitive-data\u0026#34;, encrypted=True) # è¯»å–é…ç½® retrieved_config = config_center.get(\u0026#34;app/config\u0026#34;) print(f\u0026#34;Retrieved config: {json.dumps(retrieved_config, indent=2, ensure_ascii=False)}\u0026#34;) # ç›‘å¬é…ç½®å˜æ›´ def config_change_handler(event: ConfigChangeEvent): print(f\u0026#34;Config changed: {event.key} -\u0026gt; {event.action}\u0026#34;) print(f\u0026#34;Old value: {event.old_value}\u0026#34;) print(f\u0026#34;New value: {event.new_value}\u0026#34;) config_center.watch(\u0026#34;app/\u0026#34;, config_change_handler) # æœåŠ¡å‘ç°ç¤ºä¾‹ print(\u0026#34;\\n=== æœåŠ¡å‘ç°ç¤ºä¾‹ ===\u0026#34;) service_registry = ServiceRegistry(backend=\u0026#34;consul\u0026#34;) # æ³¨å†ŒæœåŠ¡å®ä¾‹ user_service_1 = ServiceInstance( service_id=\u0026#34;user-service-1\u0026#34;, service_name=\u0026#34;user-service\u0026#34;, host=\u0026#34;192.168.1.10\u0026#34;, port=8080, metadata={\u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;us-east-1\u0026#34;}, health_check_url=\u0026#34;http://192.168.1.10:8080/health\u0026#34; ) user_service_2 = ServiceInstance( service_id=\u0026#34;user-service-2\u0026#34;, service_name=\u0026#34;user-service\u0026#34;, host=\u0026#34;192.168.1.11\u0026#34;, port=8080, metadata={\u0026#34;version\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;zone\u0026#34;: \u0026#34;us-east-1\u0026#34;}, health_check_url=\u0026#34;http://192.168.1.11:8080/health\u0026#34; ) service_registry.register(user_service_1) service_registry.register(user_service_2) # å‘ç°æœåŠ¡ instances = service_registry.discover(\u0026#34;user-service\u0026#34;) print(f\u0026#34;Discovered {len(instances)} instances of user-service:\u0026#34;) for instance in instances: print(f\u0026#34; - {instance.host}:{instance.port} (ID: {instance.service_id})\u0026#34;) # æ¨¡æ‹Ÿå¿ƒè·³ service_registry.heartbeat(\u0026#34;user-service-1\u0026#34;) service_registry.heartbeat(\u0026#34;user-service-2\u0026#34;) return config_center, service_registry ## 6. ç›‘æ§ä¸æ—¥å¿— ### 6.1 åˆ†å¸ƒå¼é“¾è·¯è¿½è¸ª ```python import uuid import time import json from typing import Dict, Any, Optional, List from dataclasses import dataclass, asdict from contextlib import contextmanager import threading from collections import defaultdict @dataclass class Span: trace_id: str span_id: str parent_span_id: Optional[str] operation_name: str start_time: float end_time: Optional[float] = None duration: Optional[float] = None tags: Dict[str, Any] = None logs: List[Dict[str, Any]] = None status: str = \u0026#34;OK\u0026#34; # OK, ERROR, TIMEOUT def __post_init__(self): if self.tags is None: self.tags = {} if self.logs is None: self.logs = [] def finish(self): \u0026#34;\u0026#34;\u0026#34;ç»“æŸSpan\u0026#34;\u0026#34;\u0026#34; self.end_time = time.time() self.duration = self.end_time - self.start_time def set_tag(self, key: str, value: Any): \u0026#34;\u0026#34;\u0026#34;è®¾ç½®æ ‡ç­¾\u0026#34;\u0026#34;\u0026#34; self.tags[key] = value def log(self, message: str, level: str = \u0026#34;INFO\u0026#34;, **kwargs): \u0026#34;\u0026#34;\u0026#34;æ·»åŠ æ—¥å¿—\u0026#34;\u0026#34;\u0026#34; log_entry = { \u0026#34;timestamp\u0026#34;: time.time(), \u0026#34;level\u0026#34;: level, \u0026#34;message\u0026#34;: message, **kwargs } self.logs.append(log_entry) def set_error(self, error: Exception): \u0026#34;\u0026#34;\u0026#34;è®¾ç½®é”™è¯¯çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; self.status = \u0026#34;ERROR\u0026#34; self.set_tag(\u0026#34;error\u0026#34;, True) self.set_tag(\u0026#34;error.kind\u0026#34;, type(error).__name__) self.set_tag(\u0026#34;error.message\u0026#34;, str(error)) self.log(f\u0026#34;Error occurred: {error}\u0026#34;, level=\u0026#34;ERROR\u0026#34;) class TraceContext: def __init__(self): self.trace_id: Optional[str] = None self.span_stack: List[Span] = [] def get_current_span(self) -\u0026gt; Optional[Span]: \u0026#34;\u0026#34;\u0026#34;è·å–å½“å‰Span\u0026#34;\u0026#34;\u0026#34; return self.span_stack[-1] if self.span_stack else None def push_span(self, span: Span): \u0026#34;\u0026#34;\u0026#34;å‹å…¥Span\u0026#34;\u0026#34;\u0026#34; self.span_stack.append(span) if not self.trace_id: self.trace_id = span.trace_id def pop_span(self) -\u0026gt; Optional[Span]: \u0026#34;\u0026#34;\u0026#34;å¼¹å‡ºSpan\u0026#34;\u0026#34;\u0026#34; return self.span_stack.pop() if self.span_stack else None class Tracer: def __init__(self, service_name: str): self.service_name = service_name self.local = threading.local() self.spans: Dict[str, List[Span]] = defaultdict(list) self.samplers = [] self.reporters = [] def _get_context(self) -\u0026gt; TraceContext: \u0026#34;\u0026#34;\u0026#34;è·å–å½“å‰çº¿ç¨‹çš„è¿½è¸ªä¸Šä¸‹æ–‡\u0026#34;\u0026#34;\u0026#34; if not hasattr(self.local, \u0026#39;context\u0026#39;): self.local.context = TraceContext() return self.local.context def start_span(self, operation_name: str, parent_span: Optional[Span] = None, tags: Dict[str, Any] = None) -\u0026gt; Span: \u0026#34;\u0026#34;\u0026#34;å¼€å§‹ä¸€ä¸ªæ–°çš„Span\u0026#34;\u0026#34;\u0026#34; context = self._get_context() # ç¡®å®šçˆ¶Span if parent_span is None: parent_span = context.get_current_span() # ç”ŸæˆID if parent_span: trace_id = parent_span.trace_id parent_span_id = parent_span.span_id else: trace_id = str(uuid.uuid4()) parent_span_id = None span_id = str(uuid.uuid4()) # åˆ›å»ºSpan span = Span( trace_id=trace_id, span_id=span_id, parent_span_id=parent_span_id, operation_name=operation_name, start_time=time.time(), tags=tags or {} ) # è®¾ç½®æœåŠ¡æ ‡ç­¾ span.set_tag(\u0026#34;service.name\u0026#34;, self.service_name) span.set_tag(\u0026#34;span.kind\u0026#34;, \u0026#34;internal\u0026#34;) # å‹å…¥ä¸Šä¸‹æ–‡ context.push_span(span) return span def finish_span(self, span: Span): \u0026#34;\u0026#34;\u0026#34;ç»“æŸSpan\u0026#34;\u0026#34;\u0026#34; span.finish() context = self._get_context() # ä»ä¸Šä¸‹æ–‡ä¸­ç§»é™¤ if context.get_current_span() == span: context.pop_span() # å­˜å‚¨Span self.spans[span.trace_id].append(span) # æŠ¥å‘ŠSpan for reporter in self.reporters: try: reporter.report(span) except Exception as e: print(f\u0026#34;Reporter error: {e}\u0026#34;) @contextmanager def span(self, operation_name: str, **kwargs): \u0026#34;\u0026#34;\u0026#34;Spanä¸Šä¸‹æ–‡ç®¡ç†å™¨\u0026#34;\u0026#34;\u0026#34; span = self.start_span(operation_name, **kwargs) try: yield span except Exception as e: span.set_error(e) raise finally: self.finish_span(span) def inject(self, span: Span, carrier: Dict[str, str]): \u0026#34;\u0026#34;\u0026#34;æ³¨å…¥è¿½è¸ªä¿¡æ¯åˆ°è½½ä½“ï¼ˆå¦‚HTTPå¤´ï¼‰\u0026#34;\u0026#34;\u0026#34; carrier[\u0026#34;X-Trace-ID\u0026#34;] = span.trace_id carrier[\u0026#34;X-Span-ID\u0026#34;] = span.span_id def extract(self, carrier: Dict[str, str]) -\u0026gt; Optional[Span]: \u0026#34;\u0026#34;\u0026#34;ä»è½½ä½“ä¸­æå–è¿½è¸ªä¿¡æ¯\u0026#34;\u0026#34;\u0026#34; trace_id = carrier.get(\u0026#34;X-Trace-ID\u0026#34;) span_id = carrier.get(\u0026#34;X-Span-ID\u0026#34;) if trace_id and span_id: # åˆ›å»ºä¸€ä¸ªè™šæ‹Ÿçš„çˆ¶Spanç”¨äºä¸Šä¸‹æ–‡ä¼ é€’ return Span( trace_id=trace_id, span_id=span_id, parent_span_id=None, operation_name=\u0026#34;extracted\u0026#34;, start_time=time.time() ) return None def get_trace(self, trace_id: str) -\u0026gt; List[Span]: \u0026#34;\u0026#34;\u0026#34;è·å–å®Œæ•´çš„è¿½è¸ªé“¾è·¯\u0026#34;\u0026#34;\u0026#34; return self.spans.get(trace_id, []) def add_reporter(self, reporter: \u0026#39;SpanReporter\u0026#39;): \u0026#34;\u0026#34;\u0026#34;æ·»åŠ SpanæŠ¥å‘Šå™¨\u0026#34;\u0026#34;\u0026#34; self.reporters.append(reporter) class SpanReporter: def report(self, span: Span): \u0026#34;\u0026#34;\u0026#34;æŠ¥å‘ŠSpan\u0026#34;\u0026#34;\u0026#34; raise NotImplementedError class ConsoleReporter(SpanReporter): def report(self, span: Span): \u0026#34;\u0026#34;\u0026#34;æ§åˆ¶å°æŠ¥å‘Šå™¨\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Span: {span.operation_name} [{span.trace_id}:{span.span_id}] \u0026#34; f\u0026#34;Duration: {span.duration:.3f}s Status: {span.status}\u0026#34;) class JaegerReporter(SpanReporter): def __init__(self, jaeger_endpoint: str): self.jaeger_endpoint = jaeger_endpoint def report(self, span: Span): \u0026#34;\u0026#34;\u0026#34;JaegeræŠ¥å‘Šå™¨ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰\u0026#34;\u0026#34;\u0026#34; # å®é™…å®ç°éœ€è¦å‘é€åˆ°Jaeger Collector jaeger_span = { \u0026#34;traceID\u0026#34;: span.trace_id, \u0026#34;spanID\u0026#34;: span.span_id, \u0026#34;parentSpanID\u0026#34;: span.parent_span_id, \u0026#34;operationName\u0026#34;: span.operation_name, \u0026#34;startTime\u0026#34;: int(span.start_time * 1000000), # å¾®ç§’ \u0026#34;duration\u0026#34;: int((span.duration or 0) * 1000000), \u0026#34;tags\u0026#34;: [{\u0026#34;key\u0026#34;: k, \u0026#34;value\u0026#34;: v} for k, v in span.tags.items()], \u0026#34;logs\u0026#34;: span.logs } print(f\u0026#34;Sending to Jaeger: {json.dumps(jaeger_span, indent=2)}\u0026#34;) ### 6.2 æŒ‡æ ‡æ”¶é›†ä¸ç›‘æ§ class MetricType(Enum): COUNTER = \u0026#34;counter\u0026#34; GAUGE = \u0026#34;gauge\u0026#34; HISTOGRAM = \u0026#34;histogram\u0026#34; SUMMARY = \u0026#34;summary\u0026#34; @dataclass class Metric: name: str type: MetricType value: float labels: Dict[str, str] timestamp: float help_text: str = \u0026#34;\u0026#34; class MetricsCollector: def __init__(self): self.metrics: Dict[str, Metric] = {} self.counters: Dict[str, float] = defaultdict(float) self.gauges: Dict[str, float] = {} self.histograms: Dict[str, List[float]] = defaultdict(list) def counter(self, name: str, labels: Dict[str, str] = None, help_text: str = \u0026#34;\u0026#34;): \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºè®¡æ•°å™¨\u0026#34;\u0026#34;\u0026#34; return Counter(name, labels or {}, help_text, self) def gauge(self, name: str, labels: Dict[str, str] = None, help_text: str = \u0026#34;\u0026#34;): \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºä»ªè¡¨ç›˜\u0026#34;\u0026#34;\u0026#34; return Gauge(name, labels or {}, help_text, self) def histogram(self, name: str, buckets: List[float] = None, labels: Dict[str, str] = None, help_text: str = \u0026#34;\u0026#34;): \u0026#34;\u0026#34;\u0026#34;åˆ›å»ºç›´æ–¹å›¾\u0026#34;\u0026#34;\u0026#34; return Histogram(name, buckets or [0.1, 0.5, 1.0, 2.5, 5.0, 10.0], labels or {}, help_text, self) def record_metric(self, metric: Metric): \u0026#34;\u0026#34;\u0026#34;è®°å½•æŒ‡æ ‡\u0026#34;\u0026#34;\u0026#34; metric_key = f\u0026#34;{metric.name}:{json.dumps(metric.labels, sort_keys=True)}\u0026#34; self.metrics[metric_key] = metric def get_metrics(self) -\u0026gt; List[Metric]: \u0026#34;\u0026#34;\u0026#34;è·å–æ‰€æœ‰æŒ‡æ ‡\u0026#34;\u0026#34;\u0026#34; return list(self.metrics.values()) def export_prometheus(self) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;å¯¼å‡ºPrometheusæ ¼å¼\u0026#34;\u0026#34;\u0026#34; lines = [] for metric in self.metrics.values(): if metric.help_text: lines.append(f\u0026#34;# HELP {metric.name} {metric.help_text}\u0026#34;) lines.append(f\u0026#34;# TYPE {metric.name} {metric.type.value}\u0026#34;) labels_str = \u0026#34;\u0026#34; if metric.labels: label_pairs = [f\u0026#39;{k}=\u0026#34;{v}\u0026#34;\u0026#39; for k, v in metric.labels.items()] labels_str = \u0026#34;{\u0026#34; + \u0026#34;,\u0026#34;.join(label_pairs) + \u0026#34;}\u0026#34; lines.append(f\u0026#34;{metric.name}{labels_str} {metric.value}\u0026#34;) return \u0026#34;\\n\u0026#34;.join(lines) class Counter: def __init__(self, name: str, labels: Dict[str, str], help_text: str, collector: MetricsCollector): self.name = name self.labels = labels self.help_text = help_text self.collector = collector self.value = 0 def inc(self, amount: float = 1): \u0026#34;\u0026#34;\u0026#34;å¢åŠ è®¡æ•°\u0026#34;\u0026#34;\u0026#34; self.value += amount metric = Metric( name=self.name, type=MetricType.COUNTER, value=self.value, labels=self.labels, timestamp=time.time(), help_text=self.help_text ) self.collector.record_metric(metric) class Gauge: def __init__(self, name: str, labels: Dict[str, str], help_text: str, collector: MetricsCollector): self.name = name self.labels = labels self.help_text = help_text self.collector = collector self.value = 0 def set(self, value: float): \u0026#34;\u0026#34;\u0026#34;è®¾ç½®å€¼\u0026#34;\u0026#34;\u0026#34; self.value = value metric = Metric( name=self.name, type=MetricType.GAUGE, value=self.value, labels=self.labels, timestamp=time.time(), help_text=self.help_text ) self.collector.record_metric(metric) def inc(self, amount: float = 1): \u0026#34;\u0026#34;\u0026#34;å¢åŠ å€¼\u0026#34;\u0026#34;\u0026#34; self.set(self.value + amount) def dec(self, amount: float = 1): \u0026#34;\u0026#34;\u0026#34;å‡å°‘å€¼\u0026#34;\u0026#34;\u0026#34; self.set(self.value - amount) class Histogram: def __init__(self, name: str, buckets: List[float], labels: Dict[str, str], help_text: str, collector: MetricsCollector): self.name = name self.buckets = sorted(buckets) self.labels = labels self.help_text = help_text self.collector = collector self.observations = [] self.count = 0 self.sum = 0 def observe(self, value: float): \u0026#34;\u0026#34;\u0026#34;è§‚å¯Ÿå€¼\u0026#34;\u0026#34;\u0026#34; self.observations.append(value) self.count += 1 self.sum += value # è®°å½•ç›´æ–¹å›¾æŒ‡æ ‡ for bucket in self.buckets: bucket_count = sum(1 for obs in self.observations if obs \u0026lt;= bucket) bucket_labels = {**self.labels, \u0026#34;le\u0026#34;: str(bucket)} metric = Metric( name=f\u0026#34;{self.name}_bucket\u0026#34;, type=MetricType.HISTOGRAM, value=bucket_count, labels=bucket_labels, timestamp=time.time(), help_text=self.help_text ) self.collector.record_metric(metric) # è®°å½•æ€»æ•°å’Œæ€»å’Œ count_metric = Metric( name=f\u0026#34;{self.name}_count\u0026#34;, type=MetricType.HISTOGRAM, value=self.count, labels=self.labels, timestamp=time.time() ) self.collector.record_metric(count_metric) sum_metric = Metric( name=f\u0026#34;{self.name}_sum\u0026#34;, type=MetricType.HISTOGRAM, value=self.sum, labels=self.labels, timestamp=time.time() ) self.collector.record_metric(sum_metric) # ä½¿ç”¨ç¤ºä¾‹ def monitoring_example(): print(\u0026#34;=== åˆ†å¸ƒå¼é“¾è·¯è¿½è¸ªç¤ºä¾‹ ===\u0026#34;) # åˆ›å»ºè¿½è¸ªå™¨ tracer = Tracer(\u0026#34;user-service\u0026#34;) tracer.add_reporter(ConsoleReporter()) tracer.add_reporter(JaegerReporter(\u0026#34;http://jaeger:14268/api/traces\u0026#34;)) # æ¨¡æ‹ŸæœåŠ¡è°ƒç”¨é“¾è·¯ with tracer.span(\u0026#34;handle_user_request\u0026#34;) as root_span: root_span.set_tag(\u0026#34;user.id\u0026#34;, \u0026#34;12345\u0026#34;) root_span.set_tag(\u0026#34;http.method\u0026#34;, \u0026#34;GET\u0026#34;) root_span.set_tag(\u0026#34;http.url\u0026#34;, \u0026#34;/api/users/12345\u0026#34;) # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢ with tracer.span(\u0026#34;database_query\u0026#34;, parent_span=root_span) as db_span: db_span.set_tag(\u0026#34;db.type\u0026#34;, \u0026#34;postgresql\u0026#34;) db_span.set_tag(\u0026#34;db.statement\u0026#34;, \u0026#34;SELECT * FROM users WHERE id = $1\u0026#34;) time.sleep(0.1) # æ¨¡æ‹ŸæŸ¥è¯¢æ—¶é—´ db_span.log(\u0026#34;Query executed successfully\u0026#34;) # æ¨¡æ‹Ÿç¼“å­˜æŸ¥è¯¢ with tracer.span(\u0026#34;cache_lookup\u0026#34;, parent_span=root_span) as cache_span: cache_span.set_tag(\u0026#34;cache.type\u0026#34;, \u0026#34;redis\u0026#34;) cache_span.set_tag(\u0026#34;cache.key\u0026#34;, \u0026#34;user:12345\u0026#34;) time.sleep(0.02) # æ¨¡æ‹Ÿç¼“å­˜æŸ¥è¯¢æ—¶é—´ cache_span.log(\u0026#34;Cache hit\u0026#34;) # æ¨¡æ‹Ÿå¤–éƒ¨APIè°ƒç”¨ with tracer.span(\u0026#34;external_api_call\u0026#34;, parent_span=root_span) as api_span: api_span.set_tag(\u0026#34;http.method\u0026#34;, \u0026#34;POST\u0026#34;) api_span.set_tag(\u0026#34;http.url\u0026#34;, \u0026#34;https://api.external.com/validate\u0026#34;) api_span.set_tag(\u0026#34;span.kind\u0026#34;, \u0026#34;client\u0026#34;) time.sleep(0.3) # æ¨¡æ‹ŸAPIè°ƒç”¨æ—¶é—´ api_span.log(\u0026#34;External API call completed\u0026#34;) root_span.log(\u0026#34;Request processed successfully\u0026#34;) print(\u0026#34;\\n=== æŒ‡æ ‡æ”¶é›†ç¤ºä¾‹ ===\u0026#34;) # åˆ›å»ºæŒ‡æ ‡æ”¶é›†å™¨ metrics = MetricsCollector() # åˆ›å»ºå„ç§æŒ‡æ ‡ request_counter = metrics.counter( \u0026#34;http_requests_total\u0026#34;, labels={\u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;/api/users\u0026#34;}, help_text=\u0026#34;Total number of HTTP requests\u0026#34; ) response_time_histogram = metrics.histogram( \u0026#34;http_request_duration_seconds\u0026#34;, buckets=[0.1, 0.5, 1.0, 2.5, 5.0], labels={\u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;endpoint\u0026#34;: \u0026#34;/api/users\u0026#34;}, help_text=\u0026#34;HTTP request duration in seconds\u0026#34; ) active_connections_gauge = metrics.gauge( \u0026#34;active_connections\u0026#34;, help_text=\u0026#34;Number of active connections\u0026#34; ) # æ¨¡æ‹ŸæŒ‡æ ‡æ”¶é›† for i in range(10): request_counter.inc() response_time = 0.1 + (i * 0.05) # æ¨¡æ‹Ÿå“åº”æ—¶é—´ response_time_histogram.observe(response_time) active_connections_gauge.set(50 + i) time.sleep(0.1) # å¯¼å‡ºPrometheusæ ¼å¼ print(\u0026#34;Prometheusæ ¼å¼æŒ‡æ ‡:\u0026#34;) print(metrics.export_prometheus()) return tracer, metrics ## 7. éƒ¨ç½²ç­–ç•¥ ### 7.1 è“ç»¿éƒ¨ç½² ```python import time from typing import Dict, List, Optional from dataclasses import dataclass from enum import Enum class DeploymentStatus(Enum): PENDING = \u0026#34;pending\u0026#34; DEPLOYING = \u0026#34;deploying\u0026#34; DEPLOYED = \u0026#34;deployed\u0026#34; TESTING = \u0026#34;testing\u0026#34; ACTIVE = \u0026#34;active\u0026#34; FAILED = \u0026#34;failed\u0026#34; ROLLED_BACK = \u0026#34;rolled_back\u0026#34; @dataclass class Environment: name: str version: str replicas: int status: DeploymentStatus health_check_url: str traffic_percentage: float = 0 deployment_time: Optional[float] = None class BlueGreenDeployment: def __init__(self, service_name: str, load_balancer: \u0026#39;LoadBalancer\u0026#39;): self.service_name = service_name self.load_balancer = load_balancer self.blue_env: Optional[Environment] = None self.green_env: Optional[Environment] = None self.active_env: Optional[str] = None def deploy(self, new_version: str, replicas: int = 3, health_check_url: str = \u0026#34;/health\u0026#34;) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ‰§è¡Œè“ç»¿éƒ¨ç½²\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Starting blue-green deployment for {self.service_name} version {new_version}\u0026#34;) # ç¡®å®šéƒ¨ç½²ç¯å¢ƒ if self.active_env == \u0026#34;blue\u0026#34; or self.active_env is None: deploy_env = \u0026#34;green\u0026#34; standby_env = self.green_env else: deploy_env = \u0026#34;blue\u0026#34; standby_env = self.blue_env # åˆ›å»ºæ–°ç¯å¢ƒ new_env = Environment( name=deploy_env, version=new_version, replicas=replicas, status=DeploymentStatus.PENDING, health_check_url=health_check_url, deployment_time=time.time() ) try: # 1. éƒ¨ç½²æ–°ç‰ˆæœ¬ print(f\u0026#34;Deploying {new_version} to {deploy_env} environment\u0026#34;) new_env.status = DeploymentStatus.DEPLOYING self._deploy_to_environment(new_env) new_env.status = DeploymentStatus.DEPLOYED # 2. å¥åº·æ£€æŸ¥ print(f\u0026#34;Running health checks for {deploy_env} environment\u0026#34;) new_env.status = DeploymentStatus.TESTING if not self._health_check(new_env): new_env.status = DeploymentStatus.FAILED print(f\u0026#34;Health check failed for {deploy_env} environment\u0026#34;) return False # 3. åˆ‡æ¢æµé‡ print(f\u0026#34;Switching traffic to {deploy_env} environment\u0026#34;) self._switch_traffic(deploy_env) new_env.status = DeploymentStatus.ACTIVE new_env.traffic_percentage = 100 # 4. æ›´æ–°ç¯å¢ƒçŠ¶æ€ if deploy_env == \u0026#34;blue\u0026#34;: self.blue_env = new_env if self.green_env: self.green_env.traffic_percentage = 0 self.green_env.status = DeploymentStatus.DEPLOYED else: self.green_env = new_env if self.blue_env: self.blue_env.traffic_percentage = 0 self.blue_env.status = DeploymentStatus.DEPLOYED self.active_env = deploy_env print(f\u0026#34;Blue-green deployment completed successfully\u0026#34;) return True except Exception as e: print(f\u0026#34;Deployment failed: {e}\u0026#34;) new_env.status = DeploymentStatus.FAILED return False def rollback(self) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;å›æ»šåˆ°ä¸Šä¸€ä¸ªç‰ˆæœ¬\u0026#34;\u0026#34;\u0026#34; if not self.active_env: print(\u0026#34;No active environment to rollback from\u0026#34;) return False # ç¡®å®šå›æ»šç›®æ ‡ if self.active_env == \u0026#34;blue\u0026#34;: rollback_env = \u0026#34;green\u0026#34; target_env = self.green_env else: rollback_env = \u0026#34;blue\u0026#34; target_env = self.blue_env if not target_env or target_env.status != DeploymentStatus.DEPLOYED: print(f\u0026#34;No valid environment to rollback to\u0026#34;) return False try: print(f\u0026#34;Rolling back to {rollback_env} environment (version {target_env.version})\u0026#34;) # åˆ‡æ¢æµé‡ self._switch_traffic(rollback_env) # æ›´æ–°çŠ¶æ€ target_env.status = DeploymentStatus.ACTIVE target_env.traffic_percentage = 100 current_env = self.blue_env if self.active_env == \u0026#34;blue\u0026#34; else self.green_env if current_env: current_env.status = DeploymentStatus.ROLLED_BACK current_env.traffic_percentage = 0 self.active_env = rollback_env print(f\u0026#34;Rollback completed successfully\u0026#34;) return True except Exception as e: print(f\u0026#34;Rollback failed: {e}\u0026#34;) return False def _deploy_to_environment(self, env: Environment): \u0026#34;\u0026#34;\u0026#34;éƒ¨ç½²åˆ°æŒ‡å®šç¯å¢ƒï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34; Creating {env.replicas} replicas for version {env.version}\u0026#34;) time.sleep(2) # æ¨¡æ‹Ÿéƒ¨ç½²æ—¶é—´ print(f\u0026#34; Deployment to {env.name} environment completed\u0026#34;) def _health_check(self, env: Environment) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;å¥åº·æ£€æŸ¥ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34; Checking health endpoint: {env.health_check_url}\u0026#34;) time.sleep(1) # æ¨¡æ‹Ÿå¥åº·æ£€æŸ¥æ—¶é—´ # æ¨¡æ‹Ÿå¥åº·æ£€æŸ¥ç»“æœï¼ˆ90%æˆåŠŸç‡ï¼‰ import random success = random.random() \u0026gt; 0.1 if success: print(f\u0026#34; Health check passed for {env.name} environment\u0026#34;) else: print(f\u0026#34; Health check failed for {env.name} environment\u0026#34;) return success def _switch_traffic(self, target_env: str): \u0026#34;\u0026#34;\u0026#34;åˆ‡æ¢æµé‡ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34; Updating load balancer to route traffic to {target_env}\u0026#34;) self.load_balancer.switch_target(self.service_name, target_env) time.sleep(0.5) # æ¨¡æ‹Ÿåˆ‡æ¢æ—¶é—´ def get_status(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è·å–éƒ¨ç½²çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;service_name\u0026#34;: self.service_name, \u0026#34;active_environment\u0026#34;: self.active_env, \u0026#34;blue_environment\u0026#34;: asdict(self.blue_env) if self.blue_env else None, \u0026#34;green_environment\u0026#34;: asdict(self.green_env) if self.green_env else None } ### 7.2 é‡‘ä¸é›€éƒ¨ç½² class CanaryDeployment: def __init__(self, service_name: str, load_balancer: \u0026#39;LoadBalancer\u0026#39;): self.service_name = service_name self.load_balancer = load_balancer self.stable_env: Optional[Environment] = None self.canary_env: Optional[Environment] = None self.traffic_split_steps = [5, 10, 25, 50, 75, 100] # æµé‡åˆ†é…æ­¥éª¤ self.current_step = 0 def deploy(self, new_version: str, replicas: int = 1, health_check_url: str = \u0026#34;/health\u0026#34;) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;æ‰§è¡Œé‡‘ä¸é›€éƒ¨ç½²\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Starting canary deployment for {self.service_name} version {new_version}\u0026#34;) # åˆ›å»ºé‡‘ä¸é›€ç¯å¢ƒ self.canary_env = Environment( name=\u0026#34;canary\u0026#34;, version=new_version, replicas=replicas, status=DeploymentStatus.PENDING, health_check_url=health_check_url, deployment_time=time.time() ) try: # 1. éƒ¨ç½²é‡‘ä¸é›€ç‰ˆæœ¬ print(f\u0026#34;Deploying canary version {new_version}\u0026#34;) self.canary_env.status = DeploymentStatus.DEPLOYING self._deploy_to_environment(self.canary_env) self.canary_env.status = DeploymentStatus.DEPLOYED # 2. å¥åº·æ£€æŸ¥ if not self._health_check(self.canary_env): self.canary_env.status = DeploymentStatus.FAILED return False # 3. é€æ­¥å¢åŠ æµé‡ self.current_step = 0 for step_percentage in self.traffic_split_steps: print(f\u0026#34;Increasing canary traffic to {step_percentage}%\u0026#34;) # æ›´æ–°æµé‡åˆ†é… self._update_traffic_split(step_percentage) # ç›‘æ§æŒ‡æ ‡ if not self._monitor_canary_metrics(): print(f\u0026#34;Canary metrics failed at {step_percentage}% traffic\u0026#34;) self._rollback_canary() return False # ç­‰å¾…è§‚å¯ŸæœŸ self._wait_observation_period() self.current_step += 1 # 4. å®Œæˆéƒ¨ç½² self._promote_canary() print(\u0026#34;Canary deployment completed successfully\u0026#34;) return True except Exception as e: print(f\u0026#34;Canary deployment failed: {e}\u0026#34;) self._rollback_canary() return False def _deploy_to_environment(self, env: Environment): \u0026#34;\u0026#34;\u0026#34;éƒ¨ç½²åˆ°ç¯å¢ƒ\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34; Creating {env.replicas} canary replicas\u0026#34;) time.sleep(1) # æ¨¡æ‹Ÿéƒ¨ç½²æ—¶é—´ def _health_check(self, env: Environment) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;å¥åº·æ£€æŸ¥\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34; Running health check for canary environment\u0026#34;) time.sleep(0.5) return True # ç®€åŒ–å®ç°ï¼Œæ€»æ˜¯è¿”å›æˆåŠŸ def _update_traffic_split(self, canary_percentage: float): \u0026#34;\u0026#34;\u0026#34;æ›´æ–°æµé‡åˆ†é…\u0026#34;\u0026#34;\u0026#34; stable_percentage = 100 - canary_percentage if self.stable_env: self.stable_env.traffic_percentage = stable_percentage self.canary_env.traffic_percentage = canary_percentage # æ›´æ–°è´Ÿè½½å‡è¡¡å™¨ self.load_balancer.update_traffic_split( self.service_name, {\u0026#34;stable\u0026#34;: stable_percentage, \u0026#34;canary\u0026#34;: canary_percentage} ) def _monitor_canary_metrics(self) -\u0026gt; bool: \u0026#34;\u0026#34;\u0026#34;ç›‘æ§é‡‘ä¸é›€æŒ‡æ ‡\u0026#34;\u0026#34;\u0026#34; print(\u0026#34; Monitoring canary metrics...\u0026#34;) time.sleep(1) # æ¨¡æ‹Ÿç›‘æ§æ—¶é—´ # æ¨¡æ‹ŸæŒ‡æ ‡æ£€æŸ¥ï¼ˆé”™è¯¯ç‡ã€å“åº”æ—¶é—´ç­‰ï¼‰ import random # æ¨¡æ‹ŸæŒ‡æ ‡ error_rate = random.uniform(0, 0.05) # 0-5%é”™è¯¯ç‡ response_time = random.uniform(100, 500) # 100-500mså“åº”æ—¶é—´ print(f\u0026#34; Error rate: {error_rate:.2%}\u0026#34;) print(f\u0026#34; Response time: {response_time:.0f}ms\u0026#34;) # æ£€æŸ¥é˜ˆå€¼ if error_rate \u0026gt; 0.03: # é”™è¯¯ç‡è¶…è¿‡3% print(\u0026#34; Error rate threshold exceeded\u0026#34;) return False if response_time \u0026gt; 400: # å“åº”æ—¶é—´è¶…è¿‡400ms print(\u0026#34; Response time threshold exceeded\u0026#34;) return False print(\u0026#34; Metrics within acceptable range\u0026#34;) return True def _wait_observation_period(self): \u0026#34;\u0026#34;\u0026#34;ç­‰å¾…è§‚å¯ŸæœŸ\u0026#34;\u0026#34;\u0026#34; observation_time = 30 # 30ç§’è§‚å¯ŸæœŸ print(f\u0026#34; Waiting {observation_time}s observation period...\u0026#34;) time.sleep(2) # æ¨¡æ‹Ÿç­‰å¾…æ—¶é—´ï¼ˆå®é™…åº”è¯¥æ˜¯30ç§’ï¼‰ def _rollback_canary(self): \u0026#34;\u0026#34;\u0026#34;å›æ»šé‡‘ä¸é›€éƒ¨ç½²\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Rolling back canary deployment\u0026#34;) if self.canary_env: self.canary_env.status = DeploymentStatus.ROLLED_BACK self.canary_env.traffic_percentage = 0 if self.stable_env: self.stable_env.traffic_percentage = 100 # æ›´æ–°è´Ÿè½½å‡è¡¡å™¨ self.load_balancer.update_traffic_split( self.service_name, {\u0026#34;stable\u0026#34;: 100, \u0026#34;canary\u0026#34;: 0} ) def _promote_canary(self): \u0026#34;\u0026#34;\u0026#34;æå‡é‡‘ä¸é›€ä¸ºç¨³å®šç‰ˆæœ¬\u0026#34;\u0026#34;\u0026#34; print(\u0026#34;Promoting canary to stable\u0026#34;) # å°†é‡‘ä¸é›€ç¯å¢ƒè®¾ä¸ºç¨³å®šç¯å¢ƒ if self.canary_env: self.canary_env.status = DeploymentStatus.ACTIVE self.canary_env.traffic_percentage = 100 # æ›´æ–°ç¨³å®šç¯å¢ƒ self.stable_env = Environment( name=\u0026#34;stable\u0026#34;, version=self.canary_env.version, replicas=self.canary_env.replicas * 3, # æ‰©å±•å‰¯æœ¬æ•° status=DeploymentStatus.ACTIVE, health_check_url=self.canary_env.health_check_url, traffic_percentage=100 ) # æ¸…ç†é‡‘ä¸é›€ç¯å¢ƒ self.canary_env = None def get_status(self) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;è·å–éƒ¨ç½²çŠ¶æ€\u0026#34;\u0026#34;\u0026#34; return { \u0026#34;service_name\u0026#34;: self.service_name, \u0026#34;current_step\u0026#34;: self.current_step, \u0026#34;total_steps\u0026#34;: len(self.traffic_split_steps), \u0026#34;stable_environment\u0026#34;: asdict(self.stable_env) if self.stable_env else None, \u0026#34;canary_environment\u0026#34;: asdict(self.canary_env) if self.canary_env else None } class LoadBalancer: def __init__(self): self.routes: Dict[str, Dict[str, float]] = {} def switch_target(self, service_name: str, target_env: str): \u0026#34;\u0026#34;\u0026#34;åˆ‡æ¢ç›®æ ‡ç¯å¢ƒ\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Load balancer: Switching {service_name} traffic to {target_env}\u0026#34;) self.routes[service_name] = {target_env: 100} def update_traffic_split(self, service_name: str, traffic_split: Dict[str, float]): \u0026#34;\u0026#34;\u0026#34;æ›´æ–°æµé‡åˆ†é…\u0026#34;\u0026#34;\u0026#34; print(f\u0026#34;Load balancer: Updating {service_name} traffic split: {traffic_split}\u0026#34;) self.routes[service_name] = traffic_split # ä½¿ç”¨ç¤ºä¾‹ def deployment_example(): print(\u0026#34;=== è“ç»¿éƒ¨ç½²ç¤ºä¾‹ ===\u0026#34;) # åˆ›å»ºè´Ÿè½½å‡è¡¡å™¨ load_balancer = LoadBalancer() # åˆ›å»ºè“ç»¿éƒ¨ç½²ç®¡ç†å™¨ blue_green = BlueGreenDeployment(\u0026#34;user-service\u0026#34;, load_balancer) # æ‰§è¡Œç¬¬ä¸€æ¬¡éƒ¨ç½² blue_green.deploy(\u0026#34;v1.0.0\u0026#34;, replicas=3) print(f\u0026#34;Status: {json.dumps(blue_green.get_status(), indent=2, ensure_ascii=False)}\u0026#34;) # æ‰§è¡Œç¬¬äºŒæ¬¡éƒ¨ç½² time.sleep(1) blue_green.deploy(\u0026#34;v1.1.0\u0026#34;, replicas=3) print(f\u0026#34;Status: {json.dumps(blue_green.get_status(), indent=2, ensure_ascii=False)}\u0026#34;) # æ¨¡æ‹Ÿå›æ»š time.sleep(1) blue_green.rollback() print(f\u0026#34;Status after rollback: {json.dumps(blue_green.get_status(), indent=2, ensure_ascii=False)}\u0026#34;) print(\u0026#34;\\n=== é‡‘ä¸é›€éƒ¨ç½²ç¤ºä¾‹ ===\u0026#34;) # åˆ›å»ºé‡‘ä¸é›€éƒ¨ç½²ç®¡ç†å™¨ canary = CanaryDeployment(\u0026#34;order-service\u0026#34;, load_balancer) # è®¾ç½®ç¨³å®šç‰ˆæœ¬ canary.stable_env = Environment( name=\u0026#34;stable\u0026#34;, version=\u0026#34;v2.0.0\u0026#34;, replicas=5, status=DeploymentStatus.ACTIVE, health_check_url=\u0026#34;/health\u0026#34;, traffic_percentage=100 ) # æ‰§è¡Œé‡‘ä¸é›€éƒ¨ç½² canary.deploy(\u0026#34;v2.1.0\u0026#34;, replicas=1) print(f\u0026#34;Canary status: {json.dumps(canary.get_status(), indent=2, ensure_ascii=False)}\u0026#34;) return blue_green, canary ## 8. æœ€ä½³å®è·µä¸æ€»ç»“ ### 8.1 æ¶æ„è®¾è®¡åŸåˆ™ 1. **å•ä¸€èŒè´£åŸåˆ™**ï¼šæ¯ä¸ªå¾®æœåŠ¡åº”è¯¥åªè´Ÿè´£ä¸€ä¸ªä¸šåŠ¡åŠŸèƒ½ 2. **æœåŠ¡è‡ªæ²»**ï¼šæœåŠ¡åº”è¯¥èƒ½å¤Ÿç‹¬ç«‹å¼€å‘ã€éƒ¨ç½²å’Œæ‰©å±• 3. **å»ä¸­å¿ƒåŒ–æ²»ç†**ï¼šé¿å…å•ç‚¹æ•…éšœï¼Œé‡‡ç”¨åˆ†å¸ƒå¼æ²»ç† 4. **å®¹é”™è®¾è®¡**ï¼šå‡è®¾æ•…éšœä¼šå‘ç”Ÿï¼Œè®¾è®¡å®¹é”™æœºåˆ¶ 5. **å¯è§‚æµ‹æ€§**ï¼šç¡®ä¿ç³»ç»Ÿçš„å¯ç›‘æ§ã€å¯è¿½è¸ªã€å¯è°ƒè¯• ### 8.2 å®æ–½å»ºè®® 1. **æ¸è¿›å¼è¿ç§»**ï¼šä»å•ä½“åº”ç”¨é€æ­¥æ‹†åˆ†ä¸ºå¾®æœåŠ¡ 2. **å›¢é˜Ÿç»„ç»‡**ï¼šæŒ‰ç…§åº·å¨å®šå¾‹ç»„ç»‡å›¢é˜Ÿç»“æ„ 3. **æŠ€æœ¯æ ˆç»Ÿä¸€**ï¼šåœ¨ç»„ç»‡å†…ä¿æŒé€‚åº¦çš„æŠ€æœ¯æ ˆç»Ÿä¸€ 4. **è‡ªåŠ¨åŒ–ä¼˜å…ˆ**ï¼šæŠ•èµ„äºCI/CDå’Œè‡ªåŠ¨åŒ–æµ‹è¯• 5. **æ–‡æ¡£å’ŒåŸ¹è®­**ï¼šå»ºç«‹å®Œå–„çš„æ–‡æ¡£å’ŒåŸ¹è®­ä½“ç³» ### 8.3 å¸¸è§é™·é˜± 1. **è¿‡åº¦æ‹†åˆ†**ï¼šé¿å…åˆ›å»ºè¿‡å¤šçš„å¾®æœåŠ¡ 2. **åˆ†å¸ƒå¼äº‹åŠ¡**ï¼šå°½é‡é¿å…è·¨æœåŠ¡äº‹åŠ¡ 3. **ç½‘ç»œå»¶è¿Ÿ**ï¼šè€ƒè™‘æœåŠ¡é—´é€šä¿¡çš„ç½‘ç»œå¼€é”€ 4. **æ•°æ®ä¸€è‡´æ€§**ï¼šé‡‡ç”¨æœ€ç»ˆä¸€è‡´æ€§è€Œéå¼ºä¸€è‡´æ€§ 5. **è¿ç»´å¤æ‚æ€§**ï¼šå‡†å¤‡å¥½åº”å¯¹å¢åŠ çš„è¿ç»´å¤æ‚æ€§ ## æ€»ç»“ äº‘åŸç”Ÿå¾®æœåŠ¡æ¶æ„æ˜¯ç°ä»£åº”ç”¨å¼€å‘çš„é‡è¦è¶‹åŠ¿ï¼Œå®ƒé€šè¿‡æœåŠ¡æ‹†åˆ†ã€å®¹å™¨åŒ–ã€æœåŠ¡ç½‘æ ¼ç­‰æŠ€æœ¯ï¼Œå®ç°äº†åº”ç”¨çš„é«˜å¯ç”¨ã€é«˜æ‰©å±•å’Œé«˜æ•ˆè¿ç»´ã€‚æœ¬æ–‡ä»æ¶æ„è®¾è®¡ã€æŠ€æœ¯å®ç°åˆ°éƒ¨ç½²ç­–ç•¥ï¼Œå…¨é¢ä»‹ç»äº†äº‘åŸç”Ÿå¾®æœåŠ¡æ¶æ„çš„æ ¸å¿ƒæ¦‚å¿µå’Œæœ€ä½³å®è·µã€‚ åœ¨å®æ–½å¾®æœåŠ¡æ¶æ„æ—¶ï¼Œéœ€è¦ç»¼åˆè€ƒè™‘ä¸šåŠ¡éœ€æ±‚ã€å›¢é˜Ÿèƒ½åŠ›å’ŒæŠ€æœ¯æˆç†Ÿåº¦ï¼Œé‡‡ç”¨æ¸è¿›å¼çš„æ–¹æ³•ï¼Œé€æ­¥æ„å»ºå’Œå®Œå–„å¾®æœåŠ¡ä½“ç³»ã€‚åŒæ—¶ï¼Œè¦é‡è§†ç›‘æ§ã€æ—¥å¿—ã€å®‰å…¨ç­‰éåŠŸèƒ½æ€§éœ€æ±‚ï¼Œç¡®ä¿ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå¯ç»´æŠ¤æ€§ã€‚ é€šè¿‡åˆç†çš„æ¶æ„è®¾è®¡å’Œå·¥ç¨‹å®è·µï¼Œå¾®æœåŠ¡æ¶æ„èƒ½å¤Ÿå¸®åŠ©ç»„ç»‡æ„å»ºæ›´åŠ çµæ´»ã€å¯æ‰©å±•çš„äº‘åŸç”Ÿåº”ç”¨ï¼Œæ”¯æ’‘ä¸šåŠ¡çš„å¿«é€Ÿå‘å±•å’Œåˆ›æ–°ã€‚ if __name__ == \u0026#34;__main__\u0026#34;: # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹ print(\u0026#34;=== äº‘åŸç”Ÿå¾®æœåŠ¡æ¶æ„å®Œæ•´ç¤ºä¾‹ ===\\n\u0026#34;) # é…ç½®ç®¡ç†å’ŒæœåŠ¡å‘ç° config_center, service_registry = config_and_discovery_example() print(\u0026#34;\\n\u0026#34; + \u0026#34;=\u0026#34;*50 + \u0026#34;\\n\u0026#34;) # ç›‘æ§å’Œè¿½è¸ª tracer, metrics = monitoring_example() print(\u0026#34;\\n\u0026#34; + \u0026#34;=\u0026#34;*50 + \u0026#34;\\n\u0026#34;) # éƒ¨ç½²ç­–ç•¥ blue_green, canary = deployment_example() ","content":"äº‘åŸç”Ÿå¾®æœåŠ¡æ¶æ„è®¾è®¡ä¸å®è·µ å¼•è¨€ éšç€æ•°å­—åŒ–è½¬å‹çš„æ·±å…¥æ¨è¿›ï¼Œä¼ ç»Ÿçš„å•ä½“åº”ç”¨æ¶æ„å·²ç»æ— æ³•æ»¡è¶³ç°ä»£ä¼ä¸šå¯¹æ•æ·æ€§ã€å¯æ‰©å±•æ€§å’Œé«˜å¯ç”¨æ€§çš„éœ€æ±‚ã€‚äº‘åŸç”Ÿå¾®æœåŠ¡æ¶æ„ä½œä¸ºç°ä»£åº”ç”¨å¼€å‘çš„ä¸»æµæ¨¡å¼ï¼Œé€šè¿‡å°†å¤æ‚çš„å•ä½“åº”ç”¨æ‹†åˆ†ä¸ºå¤šä¸ªç‹¬ç«‹çš„å¾®æœåŠ¡ï¼Œå®ç°äº†æ›´å¥½çš„å¯ç»´æŠ¤æ€§ã€å¯æ‰©å±•æ€§å’ŒæŠ€æœ¯å¤šæ ·æ€§ã€‚\næœ¬æ–‡å°†æ·±å…¥æ¢è®¨äº‘åŸç”Ÿå¾®æœåŠ¡æ¶æ„çš„è®¾è®¡åŸåˆ™ã€å®ç°æ–¹æ¡ˆå’Œæœ€ä½³å®è·µï¼Œå¸®åŠ©ä¼ä¸šæ„å»ºé«˜æ•ˆã€å¯é çš„å¾®æœåŠ¡ç³»ç»Ÿã€‚\nç›®å½• å¾®æœåŠ¡æ¶æ„æ¦‚è¿° æœåŠ¡æ‹†åˆ†ç­–ç•¥ å®¹å™¨åŒ–ä¸ç¼–æ’ æœåŠ¡ç½‘æ ¼æ¶æ„ APIç½‘å…³è®¾è®¡ æ•°æ®ç®¡ç†ç­–ç•¥ ç›‘æ§ä¸å¯è§‚æµ‹æ€§ æœ€ä½³å®è·µä¸å»ºè®® æ€»ç»“ å¾®æœåŠ¡æ¶æ„æ¦‚è¿° æ¶æ„æ¼”è¿›è·¯å¾„ graph TD A[å•ä½“åº”ç”¨] --\u0026amp;gt; B[åˆ†å±‚æ¶æ„] B --\u0026amp;gt; C[SOAæ¶æ„] C --\u0026amp;gt; D[å¾®æœåŠ¡æ¶æ„] D --\u0026amp;gt; E[äº‘åŸç”Ÿå¾®æœåŠ¡] A1[éƒ¨ç½²ç®€å•\u0026amp;lt;br/\u0026amp;gt;å¼€å‘æ•ˆç‡é«˜\u0026amp;lt;br/\u0026amp;gt;æŠ€æœ¯æ ˆç»Ÿä¸€] --\u0026amp;gt; A B1[æ¨¡å—åŒ–\u0026amp;lt;br/\u0026amp;gt;èŒè´£åˆ†ç¦»\u0026amp;lt;br/\u0026amp;gt;å¯ç»´æŠ¤æ€§æå‡] --\u0026amp;gt; B C1[æœåŠ¡é‡ç”¨\u0026amp;lt;br/\u0026amp;gt;æ¾è€¦åˆ\u0026amp;lt;br/\u0026amp;gt;æ ‡å‡†åŒ–æ¥å£] --\u0026amp;gt; C â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["äº‘æ¶æ„","å¾®æœåŠ¡","äº‘åŸç”Ÿ","Kubernetes","æœåŠ¡ç½‘æ ¼"],"categories":["äº‘è®¡ç®—"],"author":"äº‘æ¶æ„ä¸“å®¶","readingTime":38,"wordCount":8016,"section":"posts","type":"posts","draft":false,"featured":false,"series":null},{"title":"äº‘åŸç”Ÿåº”ç”¨çš„å®¹å™¨åŒ–æ¶æ„ç­–ç•¥ï¼šæ„å»ºç°ä»£åŒ–ã€å¯æ‰©å±•çš„åº”ç”¨å¹³å°","url":"https://www.dishuihengxin.com/posts/cloud-native-containerization-architecture/","summary":"ç›®å½• äº‘åŸç”Ÿå®¹å™¨åŒ–æ¦‚è¿° å®¹å™¨è®¾è®¡æ¨¡å¼ä¸æœ€ä½³å®è·µ Kubernetesç¼–æ’å¹³å°æ¶æ„ å®¹å™¨ç½‘ç»œä¸å­˜å‚¨æ¶æ„ æœåŠ¡ç½‘æ ¼ä¸æµé‡ç®¡ç† CI/CDæµæ°´çº¿è®¾è®¡ ç›‘æ§ä¸å¯è§‚æµ‹æ€§ å®‰å…¨ä¸åˆè§„ æ€»ç»“ äº‘åŸç”Ÿå®¹å™¨åŒ–æ¦‚è¿° äº‘åŸç”Ÿå®¹å™¨åŒ–æ˜¯ç°ä»£åº”ç”¨æ¶æ„çš„æ ¸å¿ƒï¼Œå®ƒé€šè¿‡å®¹å™¨æŠ€æœ¯å®ç°åº”ç”¨çš„æ ‡å‡†åŒ–ã€å¯ç§»æ¤æ€§å’Œå¯æ‰©å±•æ€§ã€‚\nå®¹å™¨åŒ–æ¶æ„å…¨æ™¯å›¾ graph TB subgraph \u0026#34;å¼€å‘å±‚\u0026#34; A[åº”ç”¨ä»£ç ] --\u0026gt; B[Dockerfile] B --\u0026gt; C[å®¹å™¨é•œåƒ] end subgraph \u0026#34;ç¼–æ’å±‚\u0026#34; D[Kubernetesé›†ç¾¤] E[Podç®¡ç†] F[Serviceå‘ç°] G[é…ç½®ç®¡ç†] end subgraph \u0026#34;å¹³å°å±‚\u0026#34; H[å®¹å™¨è¿è¡Œæ—¶] I[ç½‘ç»œæ’ä»¶] J[å­˜å‚¨æ’ä»¶] K[ç›‘æ§ç³»ç»Ÿ] end subgraph \u0026#34;åŸºç¡€è®¾æ–½å±‚\u0026#34; L[è®¡ç®—èŠ‚ç‚¹] M[ç½‘ç»œè®¾å¤‡] N[å­˜å‚¨ç³»ç»Ÿ] O[å®‰å…¨ç»„ä»¶] end C --\u0026gt; D D --\u0026gt; E D --\u0026gt; F D --\u0026gt; G E --\u0026gt; H F --\u0026gt; I G --\u0026gt; J K --\u0026gt; H H --\u0026gt; L I --\u0026gt; M J --\u0026gt; N K --\u0026gt; O å®¹å™¨åŒ–æ¶æ„åˆ†æå™¨ #!/usr/bin/env python3 \u0026#34;\u0026#34;\u0026#34; äº‘åŸç”Ÿå®¹å™¨åŒ–æ¶æ„åˆ†æå™¨ åˆ†æåº”ç”¨çš„å®¹å™¨åŒ–é€‚é…æ€§å’Œæ¶æ„å»ºè®® \u0026#34;\u0026#34;\u0026#34; import os import json import yaml import docker import kubernetes from typing import Dict, List, Any, Optional from dataclasses import dataclass, asdict from datetime import datetime import subprocess import tempfile from pathlib import Path @dataclass class ContainerizationAssessment: \u0026#34;\u0026#34;\u0026#34;å®¹å™¨åŒ–è¯„ä¼°ç»“æœ\u0026#34;\u0026#34;\u0026#34; application_name: str containerization_score: float recommendations: List[str] architecture_patterns: List[str] resource_requirements: Dict[str, Any] security_considerations: List[str] migration_complexity: str estimated_effort: str @dataclass class ApplicationProfile: \u0026#34;\u0026#34;\u0026#34;åº”ç”¨ç”»åƒ\u0026#34;\u0026#34;\u0026#34; name: str language: str framework: str dependencies: List[str] database_type: str external_services: List[str] file_system_usage: Dict[str, Any] network_requirements: Dict[str, Any] performance_requirements: Dict[str, Any] class ContainerArchitectureAnalyzer: \u0026#34;\u0026#34;\u0026#34;å®¹å™¨æ¶æ„åˆ†æå™¨\u0026#34;\u0026#34;\u0026#34; def __init__(self): self.docker_client = docker.from_env() self.supported_languages = { \u0026#39;python\u0026#39;: {\u0026#39;base_images\u0026#39;: [\u0026#39;python:3.9-slim\u0026#39;, \u0026#39;python:3.11-alpine\u0026#39;], \u0026#39;patterns\u0026#39;: [\u0026#39;multi-stage\u0026#39;, \u0026#39;distroless\u0026#39;]}, \u0026#39;java\u0026#39;: {\u0026#39;base_images\u0026#39;: [\u0026#39;openjdk:11-jre-slim\u0026#39;, \u0026#39;eclipse-temurin:17-jre\u0026#39;], \u0026#39;patterns\u0026#39;: [\u0026#39;multi-stage\u0026#39;, \u0026#39;jib\u0026#39;]}, \u0026#39;nodejs\u0026#39;: {\u0026#39;base_images\u0026#39;: [\u0026#39;node:18-alpine\u0026#39;, \u0026#39;node:20-slim\u0026#39;], \u0026#39;patterns\u0026#39;: [\u0026#39;multi-stage\u0026#39;, \u0026#39;npm-ci\u0026#39;]}, \u0026#39;go\u0026#39;: {\u0026#39;base_images\u0026#39;: [\u0026#39;golang:1.21-alpine\u0026#39;, \u0026#39;scratch\u0026#39;], \u0026#39;patterns\u0026#39;: [\u0026#39;multi-stage\u0026#39;, \u0026#39;static-binary\u0026#39;]}, \u0026#39;dotnet\u0026#39;: {\u0026#39;base_images\u0026#39;: [\u0026#39;mcr.microsoft.com/dotnet/aspnet:7.0\u0026#39;, \u0026#39;mcr.microsoft.com/dotnet/runtime:7.0\u0026#39;], \u0026#39;patterns\u0026#39;: [\u0026#39;multi-stage\u0026#39;, \u0026#39;self-contained\u0026#39;]} } def analyze_application(self, app_path: str) -\u0026gt; ApplicationProfile: \u0026#34;\u0026#34;\u0026#34;åˆ†æåº”ç”¨ç‰¹å¾\u0026#34;\u0026#34;\u0026#34; profile = ApplicationProfile( name=os.path.basename(app_path), language=self._detect_language(app_path), framework=self._detect_framework(app_path), dependencies=self._analyze_dependencies(app_path), database_type=self._detect_database_usage(app_path), external_services=self._detect_external_services(app_path), file_system_usage=self._analyze_file_system_usage(app_path), network_requirements=self._analyze_network_requirements(app_path), performance_requirements=self._analyze_performance_requirements(app_path) ) return profile def _detect_language(self, app_path: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;æ£€æµ‹ç¼–ç¨‹è¯­è¨€\u0026#34;\u0026#34;\u0026#34; language_indicators = { \u0026#39;python\u0026#39;: [\u0026#39;requirements.txt\u0026#39;, \u0026#39;setup.py\u0026#39;, \u0026#39;pyproject.toml\u0026#39;, \u0026#39;*.py\u0026#39;], \u0026#39;java\u0026#39;: [\u0026#39;pom.xml\u0026#39;, \u0026#39;build.gradle\u0026#39;, \u0026#39;*.java\u0026#39;], \u0026#39;nodejs\u0026#39;: [\u0026#39;package.json\u0026#39;, \u0026#39;*.js\u0026#39;, \u0026#39;*.ts\u0026#39;], \u0026#39;go\u0026#39;: [\u0026#39;go.mod\u0026#39;, \u0026#39;go.sum\u0026#39;, \u0026#39;*.go\u0026#39;], \u0026#39;dotnet\u0026#39;: [\u0026#39;*.csproj\u0026#39;, \u0026#39;*.sln\u0026#39;, \u0026#39;*.cs\u0026#39;] } for language, indicators in language_indicators.items(): for indicator in indicators: if indicator.startswith(\u0026#39;*\u0026#39;): # æ–‡ä»¶æ‰©å±•åæ£€æŸ¥ ext = indicator[1:] for root, dirs, files in os.walk(app_path): if any(f.endswith(ext) for f in files): return language else: # ç‰¹å®šæ–‡ä»¶æ£€æŸ¥ if os.path.exists(os.path.join(app_path, indicator)): return language return \u0026#39;unknown\u0026#39; def _detect_framework(self, app_path: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;æ£€æµ‹åº”ç”¨æ¡†æ¶\u0026#34;\u0026#34;\u0026#34; framework_patterns = { \u0026#39;django\u0026#39;: [\u0026#39;manage.py\u0026#39;, \u0026#39;settings.py\u0026#39;], \u0026#39;flask\u0026#39;: [\u0026#39;app.py\u0026#39;, \u0026#39;application.py\u0026#39;], \u0026#39;fastapi\u0026#39;: [\u0026#39;main.py\u0026#39;], \u0026#39;spring\u0026#39;: [\u0026#39;pom.xml\u0026#39;, \u0026#39;application.properties\u0026#39;], \u0026#39;express\u0026#39;: [\u0026#39;package.json\u0026#39;], \u0026#39;gin\u0026#39;: [\u0026#39;go.mod\u0026#39;], \u0026#39;aspnet\u0026#39;: [\u0026#39;*.csproj\u0026#39;] } for framework, patterns in framework_patterns.items(): for pattern in patterns: if pattern.startswith(\u0026#39;*\u0026#39;): ext = pattern[1:] for root, dirs, files in os.walk(app_path): if any(f.endswith(ext) for f in files): return framework else: if os.path.exists(os.path.join(app_path, pattern)): return framework return \u0026#39;unknown\u0026#39; def _analyze_dependencies(self, app_path: str) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;åˆ†æä¾èµ–é¡¹\u0026#34;\u0026#34;\u0026#34; dependencies = [] # Pythonä¾èµ– req_file = os.path.join(app_path, \u0026#39;requirements.txt\u0026#39;) if os.path.exists(req_file): with open(req_file, \u0026#39;r\u0026#39;) as f: dependencies.extend([line.strip() for line in f if line.strip() and not line.startswith(\u0026#39;#\u0026#39;)]) # Node.jsä¾èµ– package_file = os.path.join(app_path, \u0026#39;package.json\u0026#39;) if os.path.exists(package_file): with open(package_file, \u0026#39;r\u0026#39;) as f: package_data = json.load(f) if \u0026#39;dependencies\u0026#39; in package_data: dependencies.extend(package_data[\u0026#39;dependencies\u0026#39;].keys()) # Javaä¾èµ– (Maven) pom_file = os.path.join(app_path, \u0026#39;pom.xml\u0026#39;) if os.path.exists(pom_file): # ç®€åŒ–çš„XMLè§£æ with open(pom_file, \u0026#39;r\u0026#39;) as f: content = f.read() # è¿™é‡Œåº”è¯¥ä½¿ç”¨XMLè§£æå™¨ï¼Œç®€åŒ–å¤„ç† dependencies.append(\u0026#39;maven-dependencies\u0026#39;) return dependencies def _detect_database_usage(self, app_path: str) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;æ£€æµ‹æ•°æ®åº“ä½¿ç”¨\u0026#34;\u0026#34;\u0026#34; db_indicators = { \u0026#39;postgresql\u0026#39;: [\u0026#39;psycopg2\u0026#39;, \u0026#39;postgresql\u0026#39;, \u0026#39;postgres\u0026#39;], \u0026#39;mysql\u0026#39;: [\u0026#39;mysql\u0026#39;, \u0026#39;pymysql\u0026#39;, \u0026#39;mysql-connector\u0026#39;], \u0026#39;mongodb\u0026#39;: [\u0026#39;pymongo\u0026#39;, \u0026#39;mongodb\u0026#39;, \u0026#39;mongoose\u0026#39;], \u0026#39;redis\u0026#39;: [\u0026#39;redis\u0026#39;, \u0026#39;redis-py\u0026#39;], \u0026#39;sqlite\u0026#39;: [\u0026#39;sqlite3\u0026#39;, \u0026#39;sqlite\u0026#39;] } dependencies = self._analyze_dependencies(app_path) dep_str = \u0026#39; \u0026#39;.join(dependencies).lower() for db_type, indicators in db_indicators.items(): if any(indicator in dep_str for indicator in indicators): return db_type return \u0026#39;none\u0026#39; def _detect_external_services(self, app_path: str) -\u0026gt; List[str]: \u0026#34;\u0026#34;\u0026#34;æ£€æµ‹å¤–éƒ¨æœåŠ¡ä¾èµ–\u0026#34;\u0026#34;\u0026#34; services = [] # æ£€æŸ¥é…ç½®æ–‡ä»¶ä¸­çš„å¤–éƒ¨æœåŠ¡ config_files = [\u0026#39;config.yaml\u0026#39;, \u0026#39;config.json\u0026#39;, \u0026#39;.env\u0026#39;, \u0026#39;docker-compose.yml\u0026#39;] for config_file in config_files: config_path = os.path.join(app_path, config_file) if os.path.exists(config_path): with open(config_path, \u0026#39;r\u0026#39;) as f: content = f.read().lower() # æ£€æŸ¥å¸¸è§æœåŠ¡ service_patterns = [\u0026#39;redis\u0026#39;, \u0026#39;elasticsearch\u0026#39;, \u0026#39;kafka\u0026#39;, \u0026#39;rabbitmq\u0026#39;, \u0026#39;memcached\u0026#39;] for pattern in service_patterns: if pattern in content: services.append(pattern) return list(set(services)) def _analyze_file_system_usage(self, app_path: str) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;åˆ†ææ–‡ä»¶ç³»ç»Ÿä½¿ç”¨\u0026#34;\u0026#34;\u0026#34; usage = { \u0026#39;read_only\u0026#39;: True, \u0026#39;temp_files\u0026#39;: False, \u0026#39;log_files\u0026#39;: False, \u0026#39;upload_directory\u0026#39;: False, \u0026#39;config_files\u0026#39;: [] } # æ£€æŸ¥æ˜¯å¦æœ‰å†™å…¥æ“ä½œ for root, dirs, files in os.walk(app_path): for file in files: if file.endswith((\u0026#39;.log\u0026#39;, \u0026#39;.tmp\u0026#39;)): usage[\u0026#39;read_only\u0026#39;] = False usage[\u0026#39;temp_files\u0026#39;] = True if \u0026#39;upload\u0026#39; in file.lower() or \u0026#39;temp\u0026#39; in file.lower(): usage[\u0026#39;upload_directory\u0026#39;] = True usage[\u0026#39;read_only\u0026#39;] = False if file in [\u0026#39;config.yaml\u0026#39;, \u0026#39;config.json\u0026#39;, \u0026#39;.env\u0026#39;]: usage[\u0026#39;config_files\u0026#39;].append(file) return usage def _analyze_network_requirements(self, app_path: str) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;åˆ†æç½‘ç»œéœ€æ±‚\u0026#34;\u0026#34;\u0026#34; requirements = { \u0026#39;ports\u0026#39;: [], \u0026#39;protocols\u0026#39;: [\u0026#39;HTTP\u0026#39;], \u0026#39;load_balancing\u0026#39;: False, \u0026#39;ssl_termination\u0026#39;: False } # æ£€æŸ¥ç«¯å£é…ç½® config_files = [\u0026#39;config.yaml\u0026#39;, \u0026#39;config.json\u0026#39;, \u0026#39;.env\u0026#39;, \u0026#39;docker-compose.yml\u0026#39;] for config_file in config_files: config_path = os.path.join(app_path, config_file) if os.path.exists(config_path): with open(config_path, \u0026#39;r\u0026#39;) as f: content = f.read() # ç®€å•çš„ç«¯å£æ£€æµ‹ import re port_pattern = r\u0026#39;port[:\\s]*(\\d+)\u0026#39; ports = re.findall(port_pattern, content, re.IGNORECASE) requirements[\u0026#39;ports\u0026#39;].extend([int(p) for p in ports]) # æ£€æŸ¥HTTPS/SSL if \u0026#39;https\u0026#39; in content.lower() or \u0026#39;ssl\u0026#39; in content.lower(): requirements[\u0026#39;ssl_termination\u0026#39;] = True requirements[\u0026#39;protocols\u0026#39;].append(\u0026#39;HTTPS\u0026#39;) return requirements def _analyze_performance_requirements(self, app_path: str) -\u0026gt; Dict[str, Any]: \u0026#34;\u0026#34;\u0026#34;åˆ†ææ€§èƒ½éœ€æ±‚\u0026#34;\u0026#34;\u0026#34; requirements = { \u0026#39;cpu_intensive\u0026#39;: False, \u0026#39;memory_intensive\u0026#39;: False, \u0026#39;io_intensive\u0026#39;: False, \u0026#39;estimated_resources\u0026#39;: { \u0026#39;cpu\u0026#39;: \u0026#39;100m\u0026#39;, \u0026#39;memory\u0026#39;: \u0026#39;128Mi\u0026#39; } } # åŸºäºè¯­è¨€å’Œæ¡†æ¶ä¼°ç®—èµ„æºéœ€æ±‚ language = self._detect_language(app_path) framework = self._detect_framework(app_path) if language == \u0026#39;java\u0026#39;: requirements[\u0026#39;estimated_resources\u0026#39;] = {\u0026#39;cpu\u0026#39;: \u0026#39;500m\u0026#39;, \u0026#39;memory\u0026#39;: \u0026#39;512Mi\u0026#39;} requirements[\u0026#39;memory_intensive\u0026#39;] = True elif language == \u0026#39;python\u0026#39;: requirements[\u0026#39;estimated_resources\u0026#39;] = {\u0026#39;cpu\u0026#39;: \u0026#39;200m\u0026#39;, \u0026#39;memory\u0026#39;: \u0026#39;256Mi\u0026#39;} elif language == \u0026#39;nodejs\u0026#39;: requirements[\u0026#39;estimated_resources\u0026#39;] = {\u0026#39;cpu\u0026#39;: \u0026#39;100m\u0026#39;, \u0026#39;memory\u0026#39;: \u0026#39;128Mi\u0026#39;} elif language == \u0026#39;go\u0026#39;: requirements[\u0026#39;estimated_resources\u0026#39;] = {\u0026#39;cpu\u0026#39;: \u0026#39;100m\u0026#39;, \u0026#39;memory\u0026#39;: \u0026#39;64Mi\u0026#39;} return requirements def assess_containerization(self, profile: ApplicationProfile) -\u0026gt; ContainerizationAssessment: \u0026#34;\u0026#34;\u0026#34;è¯„ä¼°å®¹å™¨åŒ–é€‚é…æ€§\u0026#34;\u0026#34;\u0026#34; score = 0.0 recommendations = [] patterns = [] security_considerations = [] # è¯­è¨€æ”¯æŒè¯„åˆ† if profile.language in self.supported_languages: score += 20 patterns.extend(self.supported_languages[profile.language][\u0026#39;patterns\u0026#39;]) else: score += 5 recommendations.append(f\u0026#34;éœ€è¦ä¸º{profile.language}è¯­è¨€åˆ›å»ºè‡ªå®šä¹‰å®¹å™¨åŒ–æ–¹æ¡ˆ\u0026#34;) # ä¾èµ–ç®¡ç†è¯„åˆ† if profile.dependencies: score += 15 recommendations.append(\u0026#34;ä½¿ç”¨å¤šé˜¶æ®µæ„å»ºä¼˜åŒ–é•œåƒå¤§å°\u0026#34;) else: score += 10 # æ–‡ä»¶ç³»ç»Ÿä½¿ç”¨è¯„åˆ† if profile.file_system_usage[\u0026#39;read_only\u0026#39;]: score += 20 patterns.append(\u0026#39;immutable-container\u0026#39;) else: score += 10 recommendations.append(\u0026#34;è€ƒè™‘ä½¿ç”¨æŒä¹…å·å­˜å‚¨å¯å˜æ•°æ®\u0026#34;) security_considerations.append(\u0026#34;ç¡®ä¿å®¹å™¨æ–‡ä»¶ç³»ç»Ÿæƒé™é…ç½®æ­£ç¡®\u0026#34;) # å¤–éƒ¨æœåŠ¡ä¾èµ–è¯„åˆ† if profile.external_services: score += 10 recommendations.append(\u0026#34;ä½¿ç”¨æœåŠ¡å‘ç°æœºåˆ¶ç®¡ç†å¤–éƒ¨æœåŠ¡ä¾èµ–\u0026#34;) patterns.append(\u0026#39;sidecar-pattern\u0026#39;) else: score += 15 # ç½‘ç»œéœ€æ±‚è¯„åˆ† if profile.network_requirements[\u0026#39;ssl_termination\u0026#39;]: score += 10 security_considerations.append(\u0026#34;åœ¨Ingresså±‚å¤„ç†SSLç»ˆæ­¢\u0026#34;) # æ€§èƒ½éœ€æ±‚è¯„åˆ† if not profile.performance_requirements[\u0026#39;cpu_intensive\u0026#39;]: score += 10 if not profile.performance_requirements[\u0026#39;memory_intensive\u0026#39;]: score += 10 # ç¡®å®šè¿ç§»å¤æ‚åº¦ if score \u0026gt;= 80: complexity = \u0026#34;ä½\u0026#34; effort = \u0026#34;1-2å‘¨\u0026#34; elif score \u0026gt;= 60: complexity = \u0026#34;ä¸­ç­‰\u0026#34; effort = \u0026#34;2-4å‘¨\u0026#34; else: complexity = \u0026#34;é«˜\u0026#34; effort = \u0026#34;4-8å‘¨\u0026#34; return ContainerizationAssessment( application_name=profile.name, containerization_score=score, recommendations=recommendations, architecture_patterns=patterns, resource_requirements=profile.performance_requirements[\u0026#39;estimated_resources\u0026#39;], security_considerations=security_considerations, migration_complexity=complexity, estimated_effort=effort ) def generate_dockerfile(self, profile: ApplicationProfile) -\u0026gt; str: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆDockerfile\u0026#34;\u0026#34;\u0026#34; if profile.language not in self.supported_languages: return \u0026#34;# ä¸æ”¯æŒçš„è¯­è¨€ç±»å‹\u0026#34; lang_config = self.supported_languages[profile.language] base_image = lang_config[\u0026#39;base_images\u0026#39;][0] dockerfile_content = f\u0026#34;\u0026#34;\u0026#34;# å¤šé˜¶æ®µæ„å»ºDockerfile for {profile.name} # æ„å»ºé˜¶æ®µ FROM {base_image} AS builder WORKDIR /app # å¤åˆ¶ä¾èµ–æ–‡ä»¶ \u0026#34;\u0026#34;\u0026#34; if profile.language == \u0026#39;python\u0026#39;: dockerfile_content += \u0026#34;\u0026#34;\u0026#34;COPY requirements.txt . RUN pip install --no-cache-dir -r requirements.txt # å¤åˆ¶åº”ç”¨ä»£ç  COPY . . # è¿è¡Œé˜¶æ®µ FROM python:3.9-slim AS runtime WORKDIR /app # åˆ›å»ºérootç”¨æˆ· RUN groupadd -r appuser \u0026amp;\u0026amp; useradd -r -g appuser appuser # å¤åˆ¶ä¾èµ–å’Œåº”ç”¨ COPY --from=builder /usr/local/lib/python3.9/site-packages /usr/local/lib/python3.9/site-packages COPY --from=builder /app . # è®¾ç½®æƒé™ RUN chown -R appuser:appuser /app USER appuser # å¥åº·æ£€æŸ¥ HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\\ CMD curl -f http://localhost:8000/health || exit 1 # æš´éœ²ç«¯å£ EXPOSE 8000 # å¯åŠ¨å‘½ä»¤ CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] \u0026#34;\u0026#34;\u0026#34; elif profile.language == \u0026#39;java\u0026#39;: dockerfile_content += \u0026#34;\u0026#34;\u0026#34;COPY pom.xml . RUN mvn dependency:go-offline # å¤åˆ¶æºä»£ç å¹¶æ„å»º COPY src ./src RUN mvn clean package -DskipTests # è¿è¡Œé˜¶æ®µ FROM eclipse-temurin:17-jre AS runtime WORKDIR /app # åˆ›å»ºérootç”¨æˆ· RUN groupadd -r appuser \u0026amp;\u0026amp; useradd -r -g appuser appuser # å¤åˆ¶JARæ–‡ä»¶ COPY --from=builder /app/target/*.jar app.jar # è®¾ç½®æƒé™ RUN chown appuser:appuser app.jar USER appuser # å¥åº·æ£€æŸ¥ HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\\ CMD curl -f http://localhost:8080/actuator/health || exit 1 # æš´éœ²ç«¯å£ EXPOSE 8080 # å¯åŠ¨å‘½ä»¤ CMD [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;app.jar\u0026#34;] \u0026#34;\u0026#34;\u0026#34; elif profile.language == \u0026#39;nodejs\u0026#39;: dockerfile_content += \u0026#34;\u0026#34;\u0026#34;COPY package*.json ./ RUN npm ci --only=production # å¤åˆ¶åº”ç”¨ä»£ç  COPY . . # è¿è¡Œé˜¶æ®µ FROM node:18-alpine AS runtime WORKDIR /app # åˆ›å»ºérootç”¨æˆ· RUN addgroup -g 1001 -S nodejs \u0026amp;\u0026amp; adduser -S nodejs -u 1001 # å¤åˆ¶ä¾èµ–å’Œåº”ç”¨ COPY --from=builder /app/node_modules ./node_modules COPY --from=builder /app . # è®¾ç½®æƒé™ RUN chown -R nodejs:nodejs /app USER nodejs # å¥åº·æ£€æŸ¥ HEALTHCHECK --interval=30s --timeout=3s --start-period=5s --retries=3 \\\\ CMD curl -f http://localhost:3000/health || exit 1 # æš´éœ²ç«¯å£ EXPOSE 3000 # å¯åŠ¨å‘½ä»¤ CMD [\u0026#34;node\u0026#34;, \u0026#34;index.js\u0026#34;] \u0026#34;\u0026#34;\u0026#34; return dockerfile_content def generate_kubernetes_manifests(self, profile: ApplicationProfile, assessment: ContainerizationAssessment) -\u0026gt; Dict[str, str]: \u0026#34;\u0026#34;\u0026#34;ç”ŸæˆKubernetesæ¸…å•æ–‡ä»¶\u0026#34;\u0026#34;\u0026#34; manifests = {} # Deployment deployment = { \u0026#39;apiVersion\u0026#39;: \u0026#39;apps/v1\u0026#39;, \u0026#39;kind\u0026#39;: \u0026#39;Deployment\u0026#39;, \u0026#39;metadata\u0026#39;: { \u0026#39;name\u0026#39;: profile.name, \u0026#39;labels\u0026#39;: { \u0026#39;app\u0026#39;: profile.name, \u0026#39;version\u0026#39;: \u0026#39;v1\u0026#39; } }, \u0026#39;spec\u0026#39;: { \u0026#39;replicas\u0026#39;: 3, \u0026#39;selector\u0026#39;: { \u0026#39;matchLabels\u0026#39;: { \u0026#39;app\u0026#39;: profile.name } }, \u0026#39;template\u0026#39;: { \u0026#39;metadata\u0026#39;: { \u0026#39;labels\u0026#39;: { \u0026#39;app\u0026#39;: profile.name, \u0026#39;version\u0026#39;: \u0026#39;v1\u0026#39; } }, \u0026#39;spec\u0026#39;: { \u0026#39;containers\u0026#39;: [{ \u0026#39;name\u0026#39;: profile.name, \u0026#39;image\u0026#39;: f\u0026#39;{profile.name}:latest\u0026#39;, \u0026#39;ports\u0026#39;: [{ \u0026#39;containerPort\u0026#39;: profile.network_requirements.get(\u0026#39;ports\u0026#39;, [8080])[0] if profile.network_requirements.get(\u0026#39;ports\u0026#39;) else 8080 }], \u0026#39;resources\u0026#39;: { \u0026#39;requests\u0026#39;: assessment.resource_requirements, \u0026#39;limits\u0026#39;: { \u0026#39;cpu\u0026#39;: \u0026#39;1000m\u0026#39;, \u0026#39;memory\u0026#39;: \u0026#39;1Gi\u0026#39; } }, \u0026#39;livenessProbe\u0026#39;: { \u0026#39;httpGet\u0026#39;: { \u0026#39;path\u0026#39;: \u0026#39;/health\u0026#39;, \u0026#39;port\u0026#39;: profile.network_requirements.get(\u0026#39;ports\u0026#39;, [8080])[0] if profile.network_requirements.get(\u0026#39;ports\u0026#39;) else 8080 }, \u0026#39;initialDelaySeconds\u0026#39;: 30, \u0026#39;periodSeconds\u0026#39;: 10 }, \u0026#39;readinessProbe\u0026#39;: { \u0026#39;httpGet\u0026#39;: { \u0026#39;path\u0026#39;: \u0026#39;/ready\u0026#39;, \u0026#39;port\u0026#39;: profile.network_requirements.get(\u0026#39;ports\u0026#39;, [8080])[0] if profile.network_requirements.get(\u0026#39;ports\u0026#39;) else 8080 }, \u0026#39;initialDelaySeconds\u0026#39;: 5, \u0026#39;periodSeconds\u0026#39;: 5 } }], \u0026#39;securityContext\u0026#39;: { \u0026#39;runAsNonRoot\u0026#39;: True, \u0026#39;runAsUser\u0026#39;: 1001, \u0026#39;fsGroup\u0026#39;: 1001 } } } } } manifests[\u0026#39;deployment.yaml\u0026#39;] = yaml.dump(deployment, default_flow_style=False) # Service service = { \u0026#39;apiVersion\u0026#39;: \u0026#39;v1\u0026#39;, \u0026#39;kind\u0026#39;: \u0026#39;Service\u0026#39;, \u0026#39;metadata\u0026#39;: { \u0026#39;name\u0026#39;: profile.name, \u0026#39;labels\u0026#39;: { \u0026#39;app\u0026#39;: profile.name } }, \u0026#39;spec\u0026#39;: { \u0026#39;selector\u0026#39;: { \u0026#39;app\u0026#39;: profile.name }, \u0026#39;ports\u0026#39;: [{ \u0026#39;port\u0026#39;: 80, \u0026#39;targetPort\u0026#39;: profile.network_requirements.get(\u0026#39;ports\u0026#39;, [8080])[0] if profile.network_requirements.get(\u0026#39;ports\u0026#39;) else 8080, \u0026#39;protocol\u0026#39;: \u0026#39;TCP\u0026#39; }], \u0026#39;type\u0026#39;: \u0026#39;ClusterIP\u0026#39; } } manifests[\u0026#39;service.yaml\u0026#39;] = yaml.dump(service, default_flow_style=False) # ConfigMap (å¦‚æœæœ‰é…ç½®æ–‡ä»¶) if profile.file_system_usage[\u0026#39;config_files\u0026#39;]: configmap = { \u0026#39;apiVersion\u0026#39;: \u0026#39;v1\u0026#39;, \u0026#39;kind\u0026#39;: \u0026#39;ConfigMap\u0026#39;, \u0026#39;metadata\u0026#39;: { \u0026#39;name\u0026#39;: f\u0026#39;{profile.name}-config\u0026#39; }, \u0026#39;data\u0026#39;: { \u0026#39;app.properties\u0026#39;: \u0026#39;# åº”ç”¨é…ç½®\\nserver.port=8080\\n\u0026#39; } } manifests[\u0026#39;configmap.yaml\u0026#39;] = yaml.dump(configmap, default_flow_style=False) # Ingress (å¦‚æœéœ€è¦å¤–éƒ¨è®¿é—®) if profile.network_requirements.get(\u0026#39;ssl_termination\u0026#39;): ingress = { \u0026#39;apiVersion\u0026#39;: \u0026#39;networking.k8s.io/v1\u0026#39;, \u0026#39;kind\u0026#39;: \u0026#39;Ingress\u0026#39;, \u0026#39;metadata\u0026#39;: { \u0026#39;name\u0026#39;: profile.name, \u0026#39;annotations\u0026#39;: { \u0026#39;nginx.ingress.kubernetes.io/rewrite-target\u0026#39;: \u0026#39;/\u0026#39;, \u0026#39;cert-manager.io/cluster-issuer\u0026#39;: \u0026#39;letsencrypt-prod\u0026#39; } }, \u0026#39;spec\u0026#39;: { \u0026#39;tls\u0026#39;: [{ \u0026#39;hosts\u0026#39;: [f\u0026#39;{profile.name}.example.com\u0026#39;], \u0026#39;secretName\u0026#39;: f\u0026#39;{profile.name}-tls\u0026#39; }], \u0026#39;rules\u0026#39;: [{ \u0026#39;host\u0026#39;: f\u0026#39;{profile.name}.example.com\u0026#39;, \u0026#39;http\u0026#39;: { \u0026#39;paths\u0026#39;: [{ \u0026#39;path\u0026#39;: \u0026#39;/\u0026#39;, \u0026#39;pathType\u0026#39;: \u0026#39;Prefix\u0026#39;, \u0026#39;backend\u0026#39;: { \u0026#39;service\u0026#39;: { \u0026#39;name\u0026#39;: profile.name, \u0026#39;port\u0026#39;: { \u0026#39;number\u0026#39;: 80 } } } }] } }] } } manifests[\u0026#39;ingress.yaml\u0026#39;] = yaml.dump(ingress, default_flow_style=False) return manifests # ä½¿ç”¨ç¤ºä¾‹ def main(): \u0026#34;\u0026#34;\u0026#34;ä¸»å‡½æ•° - å®¹å™¨åŒ–æ¶æ„åˆ†æç¤ºä¾‹\u0026#34;\u0026#34;\u0026#34; analyzer = ContainerArchitectureAnalyzer() # åˆ†æç¤ºä¾‹åº”ç”¨ app_path = \u0026#34;/path/to/your/application\u0026#34; print(\u0026#34;=== äº‘åŸç”Ÿå®¹å™¨åŒ–æ¶æ„åˆ†æ ===\u0026#34;) # åˆ†æåº”ç”¨ profile = analyzer.analyze_application(app_path) print(f\u0026#34;\\nåº”ç”¨ç”»åƒ:\u0026#34;) print(f\u0026#34; åç§°: {profile.name}\u0026#34;) print(f\u0026#34; è¯­è¨€: {profile.language}\u0026#34;) print(f\u0026#34; æ¡†æ¶: {profile.framework}\u0026#34;) print(f\u0026#34; æ•°æ®åº“: {profile.database_type}\u0026#34;) print(f\u0026#34; å¤–éƒ¨æœåŠ¡: {\u0026#39;, \u0026#39;.join(profile.external_services)}\u0026#34;) # è¯„ä¼°å®¹å™¨åŒ–é€‚é…æ€§ assessment = analyzer.assess_containerization(profile) print(f\u0026#34;\\nå®¹å™¨åŒ–è¯„ä¼°:\u0026#34;) print(f\u0026#34; é€‚é…æ€§è¯„åˆ†: {assessment.containerization_score}/100\u0026#34;) print(f\u0026#34; è¿ç§»å¤æ‚åº¦: {assessment.migration_complexity}\u0026#34;) print(f\u0026#34; é¢„ä¼°å·¥ä½œé‡: {assessment.estimated_effort}\u0026#34;) print(f\u0026#34;\\næ¶æ„å»ºè®®:\u0026#34;) for rec in assessment.recommendations: print(f\u0026#34; - {rec}\u0026#34;) print(f\u0026#34;\\nè®¾è®¡æ¨¡å¼:\u0026#34;) for pattern in assessment.architecture_patterns: print(f\u0026#34; - {pattern}\u0026#34;) print(f\u0026#34;\\nå®‰å…¨è€ƒè™‘:\u0026#34;) for sec in assessment.security_considerations: print(f\u0026#34; - {sec}\u0026#34;) # ç”ŸæˆDockerfile dockerfile = analyzer.generate_dockerfile(profile) print(f\u0026#34;\\nç”Ÿæˆçš„Dockerfile:\u0026#34;) print(dockerfile) # ç”ŸæˆKubernetesæ¸…å• manifests = analyzer.generate_kubernetes_manifests(profile, assessment) print(f\u0026#34;\\nç”Ÿæˆçš„Kubernetesæ¸…å•æ–‡ä»¶:\u0026#34;) for filename, content in manifests.items(): print(f\u0026#34;\\n--- {filename} ---\u0026#34;) print(content) if __name__ == \u0026#34;__main__\u0026#34;: main() Kubernetesç¼–æ’å¹³å°æ¶æ„ Kubernetesä½œä¸ºå®¹å™¨ç¼–æ’å¹³å°çš„æ ¸å¿ƒï¼Œæä¾›äº†å®Œæ•´çš„å®¹å™¨ç”Ÿå‘½å‘¨æœŸç®¡ç†èƒ½åŠ›ã€‚\n","content":"ç›®å½• äº‘åŸç”Ÿå®¹å™¨åŒ–æ¦‚è¿° å®¹å™¨è®¾è®¡æ¨¡å¼ä¸æœ€ä½³å®è·µ Kubernetesç¼–æ’å¹³å°æ¶æ„ å®¹å™¨ç½‘ç»œä¸å­˜å‚¨æ¶æ„ æœåŠ¡ç½‘æ ¼ä¸æµé‡ç®¡ç† CI/CDæµæ°´çº¿è®¾è®¡ ç›‘æ§ä¸å¯è§‚æµ‹æ€§ å®‰å…¨ä¸åˆè§„ æ€»ç»“ äº‘åŸç”Ÿå®¹å™¨åŒ–æ¦‚è¿° äº‘åŸç”Ÿå®¹å™¨åŒ–æ˜¯ç°ä»£åº”ç”¨æ¶æ„çš„æ ¸å¿ƒï¼Œå®ƒé€šè¿‡å®¹å™¨æŠ€æœ¯å®ç°åº”ç”¨çš„æ ‡å‡†åŒ–ã€å¯ç§»æ¤æ€§å’Œå¯æ‰©å±•æ€§ã€‚\nå®¹å™¨åŒ–æ¶æ„å…¨æ™¯å›¾ graph TB subgraph \u0026amp;#34;å¼€å‘å±‚\u0026amp;#34; A[åº”ç”¨ä»£ç ] --\u0026amp;gt; B[Dockerfile] B --\u0026amp;gt; C[å®¹å™¨é•œåƒ] end subgraph \u0026amp;#34;ç¼–æ’å±‚\u0026amp;#34; D[Kubernetesé›†ç¾¤] E[Podç®¡ç†] F[Serviceå‘ç°] G[é…ç½®ç®¡ç†] end subgraph \u0026amp;#34;å¹³å°å±‚\u0026amp;#34; H[å®¹å™¨è¿è¡Œæ—¶] I[ç½‘ç»œæ’ä»¶] J[å­˜å‚¨æ’ä»¶] K[ç›‘æ§ç³»ç»Ÿ] end subgraph \u0026amp;#34;åŸºç¡€è®¾æ–½å±‚\u0026amp;#34; L[è®¡ç®—èŠ‚ç‚¹] M[ç½‘ç»œè®¾å¤‡] N[å­˜å‚¨ç³»ç»Ÿ] O[å®‰å…¨ç»„ä»¶] end C --\u0026amp;gt; D D --\u0026amp;gt; E D --\u0026amp;gt; F D --\u0026amp;gt; G E --\u0026amp;gt; H F --\u0026amp;gt; I G â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["äº‘æ¶æ„","å®¹å™¨åŒ–","äº‘åŸç”Ÿ","Kubernetes","Docker","å¾®æœåŠ¡"],"categories":["äº‘æ¶æ„"],"author":"åšä¸»","readingTime":31,"wordCount":6557,"section":"posts","type":"posts","draft":false,"featured":false,"series":["äº‘æ¶æ„è®¾è®¡ä¸å®è·µ"]},{"title":"ä¸»é¢˜å±•ç¤º - æš—é»‘æ¨¡å¼ä¸æ¶²æ€ç»ç’ƒæ•ˆæœ","url":"https://www.dishuihengxin.com/posts/theme-showcase/","summary":"ç°ä»£åŒ–ä¸»é¢˜è®¾è®¡å±•ç¤º æ¬¢è¿ä½“éªŒæˆ‘ä»¬å…¨æ–°çš„åšå®¢ä¸»é¢˜ï¼è¿™ä¸ªä¸»é¢˜èåˆäº†æœ€æ–°çš„è®¾è®¡è¶‹åŠ¿ï¼ŒåŒ…æ‹¬æš—é»‘æ¨¡å¼ã€æ¶²æ€ç»ç’ƒæ•ˆæœï¼ˆGlassmorphismï¼‰å’Œæµç•…çš„åŠ¨ç”»æ•ˆæœã€‚\nğŸŒ™ æš—é»‘æ¨¡å¼ ç‚¹å‡»å¯¼èˆªæ ä¸­çš„ä¸»é¢˜åˆ‡æ¢æŒ‰é’®ï¼Œå³å¯åœ¨æ˜äº®æ¨¡å¼å’Œæš—é»‘æ¨¡å¼ä¹‹é—´åˆ‡æ¢ã€‚ä¸»é¢˜ä¼šè‡ªåŠ¨ä¿å­˜æ‚¨çš„åå¥½è®¾ç½®ã€‚\nç‰¹æ€§ è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿä¸»é¢˜åå¥½ å¹³æ»‘çš„ä¸»é¢˜åˆ‡æ¢åŠ¨ç”» å®Œæ•´çš„æš—é»‘æ¨¡å¼é€‚é… æœ¬åœ°å­˜å‚¨åå¥½è®¾ç½® ğŸ”® æ¶²æ€ç»ç’ƒæ•ˆæœ æˆ‘ä»¬çš„è®¾è®¡é‡‡ç”¨äº†æµè¡Œçš„æ¶²æ€ç»ç’ƒæ•ˆæœï¼Œä¸ºç•Œé¢å¸¦æ¥ç°ä»£æ„Ÿå’Œå±‚æ¬¡æ„Ÿã€‚\nğŸ’¡ è®¾è®¡äº®ç‚¹ æ¶²æ€ç»ç’ƒæ•ˆæœé€šè¿‡åŠé€æ˜èƒŒæ™¯ã€æ¨¡ç³Šæ»¤é•œå’Œç²¾è‡´çš„è¾¹æ¡†ï¼Œåˆ›é€ å‡ºæ¼‚æµ®åœ¨èƒŒæ™¯ä¹‹ä¸Šçš„è§†è§‰æ•ˆæœã€‚ åº”ç”¨åœºæ™¯ å¯¼èˆªæ  å¡ç‰‡ç»„ä»¶ ä¾§è¾¹æ  æŒ‰é’®å…ƒç´  âœ¨ åŠ¨ç”»æ•ˆæœ ä¸»é¢˜åŒ…å«å¤šç§ç²¾å¿ƒè®¾è®¡çš„åŠ¨ç”»æ•ˆæœï¼š\næ‚¬åœæ•ˆæœ å¡ç‰‡æ‚¬åœæ—¶çš„ 3D å˜æ¢ æŒ‰é’®çš„æ¶Ÿæ¼ªæ•ˆæœ å¹³æ»‘çš„é¢œè‰²è¿‡æ¸¡ åŠ è½½åŠ¨ç”» é¡µé¢åˆ‡æ¢åŠ¨ç”» å†…å®¹åŠ è½½æ•ˆæœ æ»šåŠ¨è§†å·®æ•ˆæœ ğŸ¨ è§†è§‰å…ƒç´  æ¸å˜æ–‡å­— ä½¿ç”¨ CSS æ¸å˜åˆ›å»ºåŠ¨æ€çš„æ–‡å­—æ•ˆæœï¼Œè®©æ ‡é¢˜æ›´åŠ å¼•äººæ³¨ç›®ã€‚\néœ“è™¹å‘å…‰ ç‰¹æ®Šçš„éœ“è™¹å‘å…‰æ•ˆæœï¼Œä¸ºé‡è¦å…ƒç´ å¢åŠ è§†è§‰ç„¦ç‚¹ã€‚\næµä½“èƒŒæ™¯ åŠ¨æ€çš„æµä½“èƒŒæ™¯åŠ¨ç”»ï¼Œå¢åŠ é¡µé¢çš„æ´»åŠ›ã€‚\nğŸ“± å“åº”å¼è®¾è®¡ ä¸»é¢˜å®Œå…¨å“åº”å¼ï¼Œåœ¨å„ç§è®¾å¤‡ä¸Šéƒ½èƒ½æä¾›æœ€ä½³ä½“éªŒï¼š\næ¡Œé¢ç«¯: å®Œæ•´åŠŸèƒ½å’Œè§†è§‰æ•ˆæœ å¹³æ¿ç«¯: ä¼˜åŒ–çš„å¸ƒå±€å’Œäº¤äº’ ç§»åŠ¨ç«¯: ç®€åŒ–çš„ç•Œé¢å’Œè§¦æ‘¸ä¼˜åŒ– #!/bin/bash # Note VAR=\u0026#39;hello\u0026#39; for i in 2..8; do echo \u0026#34;hello kalid\u0026#34; docker ps done ğŸ› ï¸ æŠ€æœ¯å®ç° CSS å˜é‡ç³»ç»Ÿ ä½¿ç”¨ CSS è‡ªå®šä¹‰å±æ€§å®ç°ä¸»é¢˜åˆ‡æ¢ï¼š\n","content":"ç°ä»£åŒ–ä¸»é¢˜è®¾è®¡å±•ç¤º æ¬¢è¿ä½“éªŒæˆ‘ä»¬å…¨æ–°çš„åšå®¢ä¸»é¢˜ï¼è¿™ä¸ªä¸»é¢˜èåˆäº†æœ€æ–°çš„è®¾è®¡è¶‹åŠ¿ï¼ŒåŒ…æ‹¬æš—é»‘æ¨¡å¼ã€æ¶²æ€ç»ç’ƒæ•ˆæœï¼ˆGlassmorphismï¼‰å’Œæµç•…çš„åŠ¨ç”»æ•ˆæœã€‚\nğŸŒ™ æš—é»‘æ¨¡å¼ ç‚¹å‡»å¯¼èˆªæ ä¸­çš„ä¸»é¢˜åˆ‡æ¢æŒ‰é’®ï¼Œå³å¯åœ¨æ˜äº®æ¨¡å¼å’Œæš—é»‘æ¨¡å¼ä¹‹é—´åˆ‡æ¢ã€‚ä¸»é¢˜ä¼šè‡ªåŠ¨ä¿å­˜æ‚¨çš„åå¥½è®¾ç½®ã€‚\nç‰¹æ€§ è‡ªåŠ¨æ£€æµ‹ç³»ç»Ÿä¸»é¢˜åå¥½ å¹³æ»‘çš„ä¸»é¢˜åˆ‡æ¢åŠ¨ç”» å®Œæ•´çš„æš—é»‘æ¨¡å¼é€‚é… æœ¬åœ°å­˜å‚¨åå¥½è®¾ç½® ğŸ”® æ¶²æ€ç»ç’ƒæ•ˆæœ æˆ‘ä»¬çš„è®¾è®¡é‡‡ç”¨äº†æµè¡Œçš„æ¶²æ€ç»ç’ƒæ•ˆæœï¼Œä¸ºç•Œé¢å¸¦æ¥ç°ä»£æ„Ÿå’Œå±‚æ¬¡æ„Ÿã€‚\nğŸ’¡ è®¾è®¡äº®ç‚¹ æ¶²æ€ç»ç’ƒæ•ˆæœé€šè¿‡åŠé€æ˜èƒŒæ™¯ã€æ¨¡ç³Šæ»¤é•œå’Œç²¾è‡´çš„è¾¹æ¡†ï¼Œåˆ›é€ å‡ºæ¼‚æµ®åœ¨èƒŒæ™¯ä¹‹ä¸Šçš„è§†è§‰æ•ˆæœã€‚ åº”ç”¨åœºæ™¯ å¯¼èˆªæ  å¡ç‰‡ç»„ä»¶ ä¾§è¾¹æ  æŒ‰é’®å…ƒç´  âœ¨ åŠ¨ç”»æ•ˆæœ ä¸»é¢˜åŒ…å«å¤šç§ç²¾å¿ƒè®¾è®¡çš„åŠ¨ç”»æ•ˆæœï¼š\næ‚¬åœæ•ˆæœ å¡ç‰‡æ‚¬åœæ—¶çš„ 3D å˜æ¢ æŒ‰é’®çš„æ¶Ÿæ¼ªæ•ˆæœ å¹³æ»‘çš„é¢œè‰²è¿‡æ¸¡ åŠ è½½åŠ¨ç”» é¡µé¢åˆ‡æ¢åŠ¨ç”» å†…å®¹åŠ è½½æ•ˆæœ æ»šåŠ¨è§†å·®æ•ˆæœ ğŸ¨ è§†è§‰å…ƒç´  æ¸å˜æ–‡å­— ä½¿ç”¨ CSS æ¸å˜åˆ›å»ºåŠ¨æ€çš„æ–‡å­—æ•ˆæœï¼Œè®©æ ‡é¢˜æ›´åŠ å¼•äººæ³¨ç›®ã€‚\néœ“è™¹å‘å…‰ ç‰¹æ®Šçš„éœ“è™¹å‘å…‰æ•ˆæœï¼Œä¸ºé‡è¦å…ƒç´ å¢åŠ è§†è§‰ç„¦ç‚¹ã€‚\næµä½“èƒŒæ™¯ åŠ¨æ€çš„æµä½“èƒŒæ™¯åŠ¨ç”»ï¼Œå¢åŠ é¡µé¢çš„æ´»åŠ›ã€‚\nğŸ“± å“åº”å¼è®¾è®¡ ä¸»é¢˜å®Œå…¨å“åº”å¼ï¼Œåœ¨å„ç§è®¾å¤‡ä¸Šéƒ½èƒ½æä¾›æœ€ä½³ä½“éªŒï¼š\næ¡Œé¢ç«¯: â€¦","date":"2025-12-31","lastmod":"2025-12-31","tags":["ä¸»é¢˜","è®¾è®¡","æš—é»‘æ¨¡å¼","æ¶²æ€ç»ç’ƒ"],"categories":["è®¾è®¡"],"author":"æ»´æ°´æ’å¿ƒ","readingTime":1,"wordCount":149,"section":"posts","type":"posts","draft":false,"featured":true,"series":null}]